// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py UTC_ARGS: --version 5
// RUN: %clang_cc1 -internal-isystem %S/Inputs/include \
// RUN:   -internal-isystem %S/../../lib/Headers/ \
// RUN:   -triple amdgcn-amd-amdhsa -emit-llvm %s -o - \
// RUN: | FileCheck %s --check-prefix=AMDGPU
//
// RUN: %clang_cc1 -internal-isystem %S/Inputs/include \
// RUN:   -internal-isystem %S/../../lib/Headers/ \
// RUN:   -target-feature +ptx62 \
// RUN:   -triple nvptx64-nvidia-cuda -emit-llvm %s -o - \
// RUN: | FileCheck %s --check-prefix=NVPTX

#define _DEFAULT_ATTRS __attribute__((always_inline))
#include <gpuintrin.h>

// AMDGPU-LABEL: define dso_local void @foo(
// AMDGPU-SAME: ) #[[ATTR0:[0-9]+]] {
// AMDGPU-NEXT:  [[ENTRY:.*:]]
// AMDGPU-NEXT:    [[RETVAL_I116:%.*]] = alloca i64, align 8, addrspace(5)
// AMDGPU-NEXT:    [[RETVAL_I114:%.*]] = alloca i32, align 4, addrspace(5)
// AMDGPU-NEXT:    [[RETVAL_I112:%.*]] = alloca i32, align 4, addrspace(5)
// AMDGPU-NEXT:    [[RETVAL_I110:%.*]] = alloca i32, align 4, addrspace(5)
// AMDGPU-NEXT:    [[RETVAL_I19_I:%.*]] = alloca i32, align 4, addrspace(5)
// AMDGPU-NEXT:    [[RETVAL_I17_I:%.*]] = alloca i32, align 4, addrspace(5)
// AMDGPU-NEXT:    [[RETVAL_I15_I:%.*]] = alloca i32, align 4, addrspace(5)
// AMDGPU-NEXT:    [[RETVAL_I12_I:%.*]] = alloca i32, align 4, addrspace(5)
// AMDGPU-NEXT:    [[RETVAL_I9_I:%.*]] = alloca i32, align 4, addrspace(5)
// AMDGPU-NEXT:    [[RETVAL_I_I103:%.*]] = alloca i32, align 4, addrspace(5)
// AMDGPU-NEXT:    [[RETVAL_I104:%.*]] = alloca i32, align 4, addrspace(5)
// AMDGPU-NEXT:    [[RETVAL_I101:%.*]] = alloca i32, align 4, addrspace(5)
// AMDGPU-NEXT:    [[RETVAL_I99:%.*]] = alloca i32, align 4, addrspace(5)
// AMDGPU-NEXT:    [[RETVAL_I97:%.*]] = alloca i32, align 4, addrspace(5)
// AMDGPU-NEXT:    [[RETVAL_I7_I:%.*]] = alloca i32, align 4, addrspace(5)
// AMDGPU-NEXT:    [[RETVAL_I4_I87:%.*]] = alloca i32, align 4, addrspace(5)
// AMDGPU-NEXT:    [[RETVAL_I_I88:%.*]] = alloca i32, align 4, addrspace(5)
// AMDGPU-NEXT:    [[RETVAL_I89:%.*]] = alloca i32, align 4, addrspace(5)
// AMDGPU-NEXT:    [[RETVAL_I84:%.*]] = alloca i32, align 4, addrspace(5)
// AMDGPU-NEXT:    [[RETVAL_I81:%.*]] = alloca i32, align 4, addrspace(5)
// AMDGPU-NEXT:    [[RETVAL_I78:%.*]] = alloca i32, align 4, addrspace(5)
// AMDGPU-NEXT:    [[RETVAL_I26_I:%.*]] = alloca i32, align 4, addrspace(5)
// AMDGPU-NEXT:    [[RETVAL_I24_I:%.*]] = alloca i32, align 4, addrspace(5)
// AMDGPU-NEXT:    [[RETVAL_I22_I:%.*]] = alloca i32, align 4, addrspace(5)
// AMDGPU-NEXT:    [[RETVAL_I18_I:%.*]] = alloca i32, align 4, addrspace(5)
// AMDGPU-NEXT:    [[RETVAL_I14_I:%.*]] = alloca i32, align 4, addrspace(5)
// AMDGPU-NEXT:    [[RETVAL_I_I70:%.*]] = alloca i32, align 4, addrspace(5)
// AMDGPU-NEXT:    [[RETVAL_I71:%.*]] = alloca i64, align 8, addrspace(5)
// AMDGPU-NEXT:    [[RETVAL_I68:%.*]] = alloca i32, align 4, addrspace(5)
// AMDGPU-NEXT:    [[RETVAL_I66:%.*]] = alloca i32, align 4, addrspace(5)
// AMDGPU-NEXT:    [[RETVAL_I64:%.*]] = alloca i32, align 4, addrspace(5)
// AMDGPU-NEXT:    [[RETVAL_I8_I:%.*]] = alloca i32, align 4, addrspace(5)
// AMDGPU-NEXT:    [[RETVAL_I4_I:%.*]] = alloca i32, align 4, addrspace(5)
// AMDGPU-NEXT:    [[RETVAL_I_I58:%.*]] = alloca i32, align 4, addrspace(5)
// AMDGPU-NEXT:    [[RETVAL_I59:%.*]] = alloca i64, align 8, addrspace(5)
// AMDGPU-NEXT:    [[RETVAL_I54:%.*]] = alloca i32, align 4, addrspace(5)
// AMDGPU-NEXT:    [[RETVAL_I50:%.*]] = alloca i32, align 4, addrspace(5)
// AMDGPU-NEXT:    [[RETVAL_I47:%.*]] = alloca i32, align 4, addrspace(5)
// AMDGPU-NEXT:    [[RETVAL_I_I:%.*]] = alloca i64, align 8, addrspace(5)
// AMDGPU-NEXT:    [[__LANE_MASK_ADDR_I_I:%.*]] = alloca i64, align 8, addrspace(5)
// AMDGPU-NEXT:    [[RETVAL_I42:%.*]] = alloca i1, align 1, addrspace(5)
// AMDGPU-NEXT:    [[__LANE_MASK_ADDR_I43:%.*]] = alloca i64, align 8, addrspace(5)
// AMDGPU-NEXT:    [[RETVAL_I38:%.*]] = alloca i64, align 8, addrspace(5)
// AMDGPU-NEXT:    [[__LANE_MASK_ADDR_I39:%.*]] = alloca i64, align 8, addrspace(5)
// AMDGPU-NEXT:    [[RETVAL_I32:%.*]] = alloca i32, align 4, addrspace(5)
// AMDGPU-NEXT:    [[__LANE_MASK_ADDR_I33:%.*]] = alloca i64, align 8, addrspace(5)
// AMDGPU-NEXT:    [[__IDX_ADDR_I:%.*]] = alloca i32, align 4, addrspace(5)
// AMDGPU-NEXT:    [[__X_ADDR_I34:%.*]] = alloca i32, align 4, addrspace(5)
// AMDGPU-NEXT:    [[__LANE_MASK_ADDR_I30:%.*]] = alloca i64, align 8, addrspace(5)
// AMDGPU-NEXT:    [[RETVAL_I24:%.*]] = alloca i64, align 8, addrspace(5)
// AMDGPU-NEXT:    [[__LANE_MASK_ADDR_I25:%.*]] = alloca i64, align 8, addrspace(5)
// AMDGPU-NEXT:    [[__X_ADDR_I26:%.*]] = alloca i8, align 1, addrspace(5)
// AMDGPU-NEXT:    [[RETVAL_I:%.*]] = alloca i32, align 4, addrspace(5)
// AMDGPU-NEXT:    [[__LANE_MASK_ADDR_I:%.*]] = alloca i64, align 8, addrspace(5)
// AMDGPU-NEXT:    [[__X_ADDR_I:%.*]] = alloca i32, align 4, addrspace(5)
// AMDGPU-NEXT:    [[RETVAL_ASCAST_I48:%.*]] = addrspacecast ptr addrspace(5) [[RETVAL_I47]] to ptr
// AMDGPU-NEXT:    [[TMP0:%.*]] = call align 4 dereferenceable(64) ptr addrspace(4) @llvm.amdgcn.dispatch.ptr()
// AMDGPU-NEXT:    [[TMP1:%.*]] = getelementptr i8, ptr addrspace(4) [[TMP0]], i32 12
// AMDGPU-NEXT:    [[TMP2:%.*]] = load i32, ptr addrspace(4) [[TMP1]], align 4, !invariant.load [[META3:![0-9]+]]
// AMDGPU-NEXT:    [[TMP3:%.*]] = call align 8 dereferenceable(256) ptr addrspace(4) @llvm.amdgcn.implicitarg.ptr()
// AMDGPU-NEXT:    [[TMP4:%.*]] = getelementptr i8, ptr addrspace(4) [[TMP3]], i32 12
// AMDGPU-NEXT:    [[TMP5:%.*]] = load i16, ptr addrspace(4) [[TMP4]], align 2, !range [[RNG4:![0-9]+]], !invariant.load [[META3]], !noundef [[META3]]
// AMDGPU-NEXT:    [[CONV_I49:%.*]] = zext i16 [[TMP5]] to i32
// AMDGPU-NEXT:    [[DIV_I:%.*]] = udiv i32 [[TMP2]], [[CONV_I49]]
// AMDGPU-NEXT:    [[RETVAL_ASCAST_I51:%.*]] = addrspacecast ptr addrspace(5) [[RETVAL_I50]] to ptr
// AMDGPU-NEXT:    [[TMP6:%.*]] = call align 4 dereferenceable(64) ptr addrspace(4) @llvm.amdgcn.dispatch.ptr()
// AMDGPU-NEXT:    [[TMP7:%.*]] = getelementptr i8, ptr addrspace(4) [[TMP6]], i32 16
// AMDGPU-NEXT:    [[TMP8:%.*]] = load i32, ptr addrspace(4) [[TMP7]], align 4, !invariant.load [[META3]]
// AMDGPU-NEXT:    [[TMP9:%.*]] = call align 8 dereferenceable(256) ptr addrspace(4) @llvm.amdgcn.implicitarg.ptr()
// AMDGPU-NEXT:    [[TMP10:%.*]] = getelementptr i8, ptr addrspace(4) [[TMP9]], i32 14
// AMDGPU-NEXT:    [[TMP11:%.*]] = load i16, ptr addrspace(4) [[TMP10]], align 2, !range [[RNG4]], !invariant.load [[META3]], !noundef [[META3]]
// AMDGPU-NEXT:    [[CONV_I52:%.*]] = zext i16 [[TMP11]] to i32
// AMDGPU-NEXT:    [[DIV_I53:%.*]] = udiv i32 [[TMP8]], [[CONV_I52]]
// AMDGPU-NEXT:    [[RETVAL_ASCAST_I55:%.*]] = addrspacecast ptr addrspace(5) [[RETVAL_I54]] to ptr
// AMDGPU-NEXT:    [[TMP12:%.*]] = call align 4 dereferenceable(64) ptr addrspace(4) @llvm.amdgcn.dispatch.ptr()
// AMDGPU-NEXT:    [[TMP13:%.*]] = getelementptr i8, ptr addrspace(4) [[TMP12]], i32 20
// AMDGPU-NEXT:    [[TMP14:%.*]] = load i32, ptr addrspace(4) [[TMP13]], align 4, !invariant.load [[META3]]
// AMDGPU-NEXT:    [[TMP15:%.*]] = call align 8 dereferenceable(256) ptr addrspace(4) @llvm.amdgcn.implicitarg.ptr()
// AMDGPU-NEXT:    [[TMP16:%.*]] = getelementptr i8, ptr addrspace(4) [[TMP15]], i32 16
// AMDGPU-NEXT:    [[TMP17:%.*]] = load i16, ptr addrspace(4) [[TMP16]], align 2, !range [[RNG4]], !invariant.load [[META3]], !noundef [[META3]]
// AMDGPU-NEXT:    [[CONV_I56:%.*]] = zext i16 [[TMP17]] to i32
// AMDGPU-NEXT:    [[DIV_I57:%.*]] = udiv i32 [[TMP14]], [[CONV_I56]]
// AMDGPU-NEXT:    [[RETVAL_ASCAST_I60:%.*]] = addrspacecast ptr addrspace(5) [[RETVAL_I59]] to ptr
// AMDGPU-NEXT:    [[RETVAL_ASCAST_I_I61:%.*]] = addrspacecast ptr addrspace(5) [[RETVAL_I_I58]] to ptr
// AMDGPU-NEXT:    [[TMP18:%.*]] = call align 4 dereferenceable(64) ptr addrspace(4) @llvm.amdgcn.dispatch.ptr()
// AMDGPU-NEXT:    [[TMP19:%.*]] = getelementptr i8, ptr addrspace(4) [[TMP18]], i32 12
// AMDGPU-NEXT:    [[TMP20:%.*]] = load i32, ptr addrspace(4) [[TMP19]], align 4, !invariant.load [[META3]]
// AMDGPU-NEXT:    [[TMP21:%.*]] = call align 8 dereferenceable(256) ptr addrspace(4) @llvm.amdgcn.implicitarg.ptr()
// AMDGPU-NEXT:    [[TMP22:%.*]] = getelementptr i8, ptr addrspace(4) [[TMP21]], i32 12
// AMDGPU-NEXT:    [[TMP23:%.*]] = load i16, ptr addrspace(4) [[TMP22]], align 2, !range [[RNG4]], !invariant.load [[META3]], !noundef [[META3]]
// AMDGPU-NEXT:    [[CONV_I_I62:%.*]] = zext i16 [[TMP23]] to i32
// AMDGPU-NEXT:    [[DIV_I_I:%.*]] = udiv i32 [[TMP20]], [[CONV_I_I62]]
// AMDGPU-NEXT:    [[RETVAL_ASCAST_I5_I:%.*]] = addrspacecast ptr addrspace(5) [[RETVAL_I4_I]] to ptr
// AMDGPU-NEXT:    [[TMP24:%.*]] = call align 4 dereferenceable(64) ptr addrspace(4) @llvm.amdgcn.dispatch.ptr()
// AMDGPU-NEXT:    [[TMP25:%.*]] = getelementptr i8, ptr addrspace(4) [[TMP24]], i32 16
// AMDGPU-NEXT:    [[TMP26:%.*]] = load i32, ptr addrspace(4) [[TMP25]], align 4, !invariant.load [[META3]]
// AMDGPU-NEXT:    [[TMP27:%.*]] = call align 8 dereferenceable(256) ptr addrspace(4) @llvm.amdgcn.implicitarg.ptr()
// AMDGPU-NEXT:    [[TMP28:%.*]] = getelementptr i8, ptr addrspace(4) [[TMP27]], i32 14
// AMDGPU-NEXT:    [[TMP29:%.*]] = load i16, ptr addrspace(4) [[TMP28]], align 2, !range [[RNG4]], !invariant.load [[META3]], !noundef [[META3]]
// AMDGPU-NEXT:    [[CONV_I6_I:%.*]] = zext i16 [[TMP29]] to i32
// AMDGPU-NEXT:    [[DIV_I7_I:%.*]] = udiv i32 [[TMP26]], [[CONV_I6_I]]
// AMDGPU-NEXT:    [[MUL_I:%.*]] = mul i32 [[DIV_I_I]], [[DIV_I7_I]]
// AMDGPU-NEXT:    [[RETVAL_ASCAST_I9_I:%.*]] = addrspacecast ptr addrspace(5) [[RETVAL_I8_I]] to ptr
// AMDGPU-NEXT:    [[TMP30:%.*]] = call align 4 dereferenceable(64) ptr addrspace(4) @llvm.amdgcn.dispatch.ptr()
// AMDGPU-NEXT:    [[TMP31:%.*]] = getelementptr i8, ptr addrspace(4) [[TMP30]], i32 20
// AMDGPU-NEXT:    [[TMP32:%.*]] = load i32, ptr addrspace(4) [[TMP31]], align 4, !invariant.load [[META3]]
// AMDGPU-NEXT:    [[TMP33:%.*]] = call align 8 dereferenceable(256) ptr addrspace(4) @llvm.amdgcn.implicitarg.ptr()
// AMDGPU-NEXT:    [[TMP34:%.*]] = getelementptr i8, ptr addrspace(4) [[TMP33]], i32 16
// AMDGPU-NEXT:    [[TMP35:%.*]] = load i16, ptr addrspace(4) [[TMP34]], align 2, !range [[RNG4]], !invariant.load [[META3]], !noundef [[META3]]
// AMDGPU-NEXT:    [[CONV_I10_I:%.*]] = zext i16 [[TMP35]] to i32
// AMDGPU-NEXT:    [[DIV_I11_I:%.*]] = udiv i32 [[TMP32]], [[CONV_I10_I]]
// AMDGPU-NEXT:    [[MUL3_I:%.*]] = mul i32 [[MUL_I]], [[DIV_I11_I]]
// AMDGPU-NEXT:    [[CONV_I63:%.*]] = zext i32 [[MUL3_I]] to i64
// AMDGPU-NEXT:    [[RETVAL_ASCAST_I65:%.*]] = addrspacecast ptr addrspace(5) [[RETVAL_I64]] to ptr
// AMDGPU-NEXT:    [[TMP36:%.*]] = call i32 @llvm.amdgcn.workgroup.id.x()
// AMDGPU-NEXT:    [[RETVAL_ASCAST_I67:%.*]] = addrspacecast ptr addrspace(5) [[RETVAL_I66]] to ptr
// AMDGPU-NEXT:    [[TMP37:%.*]] = call i32 @llvm.amdgcn.workgroup.id.y()
// AMDGPU-NEXT:    [[RETVAL_ASCAST_I69:%.*]] = addrspacecast ptr addrspace(5) [[RETVAL_I68]] to ptr
// AMDGPU-NEXT:    [[TMP38:%.*]] = call i32 @llvm.amdgcn.workgroup.id.z()
// AMDGPU-NEXT:    [[RETVAL_ASCAST_I72:%.*]] = addrspacecast ptr addrspace(5) [[RETVAL_I71]] to ptr
// AMDGPU-NEXT:    [[RETVAL_ASCAST_I23_I:%.*]] = addrspacecast ptr addrspace(5) [[RETVAL_I22_I]] to ptr
// AMDGPU-NEXT:    [[TMP39:%.*]] = call i32 @llvm.amdgcn.workgroup.id.x()
// AMDGPU-NEXT:    [[CONV_I73:%.*]] = zext i32 [[TMP39]] to i64
// AMDGPU-NEXT:    [[RETVAL_ASCAST_I15_I:%.*]] = addrspacecast ptr addrspace(5) [[RETVAL_I14_I]] to ptr
// AMDGPU-NEXT:    [[TMP40:%.*]] = call align 4 dereferenceable(64) ptr addrspace(4) @llvm.amdgcn.dispatch.ptr()
// AMDGPU-NEXT:    [[TMP41:%.*]] = getelementptr i8, ptr addrspace(4) [[TMP40]], i32 12
// AMDGPU-NEXT:    [[TMP42:%.*]] = load i32, ptr addrspace(4) [[TMP41]], align 4, !invariant.load [[META3]]
// AMDGPU-NEXT:    [[TMP43:%.*]] = call align 8 dereferenceable(256) ptr addrspace(4) @llvm.amdgcn.implicitarg.ptr()
// AMDGPU-NEXT:    [[TMP44:%.*]] = getelementptr i8, ptr addrspace(4) [[TMP43]], i32 12
// AMDGPU-NEXT:    [[TMP45:%.*]] = load i16, ptr addrspace(4) [[TMP44]], align 2, !range [[RNG4]], !invariant.load [[META3]], !noundef [[META3]]
// AMDGPU-NEXT:    [[CONV_I16_I:%.*]] = zext i16 [[TMP45]] to i32
// AMDGPU-NEXT:    [[DIV_I17_I:%.*]] = udiv i32 [[TMP42]], [[CONV_I16_I]]
// AMDGPU-NEXT:    [[CONV2_I:%.*]] = zext i32 [[DIV_I17_I]] to i64
// AMDGPU-NEXT:    [[RETVAL_ASCAST_I25_I:%.*]] = addrspacecast ptr addrspace(5) [[RETVAL_I24_I]] to ptr
// AMDGPU-NEXT:    [[TMP46:%.*]] = call i32 @llvm.amdgcn.workgroup.id.y()
// AMDGPU-NEXT:    [[CONV4_I:%.*]] = zext i32 [[TMP46]] to i64
// AMDGPU-NEXT:    [[MUL_I74:%.*]] = mul i64 [[CONV2_I]], [[CONV4_I]]
// AMDGPU-NEXT:    [[ADD_I:%.*]] = add i64 [[CONV_I73]], [[MUL_I74]]
// AMDGPU-NEXT:    [[RETVAL_ASCAST_I_I75:%.*]] = addrspacecast ptr addrspace(5) [[RETVAL_I_I70]] to ptr
// AMDGPU-NEXT:    [[TMP47:%.*]] = call align 4 dereferenceable(64) ptr addrspace(4) @llvm.amdgcn.dispatch.ptr()
// AMDGPU-NEXT:    [[TMP48:%.*]] = getelementptr i8, ptr addrspace(4) [[TMP47]], i32 12
// AMDGPU-NEXT:    [[TMP49:%.*]] = load i32, ptr addrspace(4) [[TMP48]], align 4, !invariant.load [[META3]]
// AMDGPU-NEXT:    [[TMP50:%.*]] = call align 8 dereferenceable(256) ptr addrspace(4) @llvm.amdgcn.implicitarg.ptr()
// AMDGPU-NEXT:    [[TMP51:%.*]] = getelementptr i8, ptr addrspace(4) [[TMP50]], i32 12
// AMDGPU-NEXT:    [[TMP52:%.*]] = load i16, ptr addrspace(4) [[TMP51]], align 2, !range [[RNG4]], !invariant.load [[META3]], !noundef [[META3]]
// AMDGPU-NEXT:    [[CONV_I_I76:%.*]] = zext i16 [[TMP52]] to i32
// AMDGPU-NEXT:    [[DIV_I_I77:%.*]] = udiv i32 [[TMP49]], [[CONV_I_I76]]
// AMDGPU-NEXT:    [[CONV6_I:%.*]] = zext i32 [[DIV_I_I77]] to i64
// AMDGPU-NEXT:    [[RETVAL_ASCAST_I19_I:%.*]] = addrspacecast ptr addrspace(5) [[RETVAL_I18_I]] to ptr
// AMDGPU-NEXT:    [[TMP53:%.*]] = call align 4 dereferenceable(64) ptr addrspace(4) @llvm.amdgcn.dispatch.ptr()
// AMDGPU-NEXT:    [[TMP54:%.*]] = getelementptr i8, ptr addrspace(4) [[TMP53]], i32 16
// AMDGPU-NEXT:    [[TMP55:%.*]] = load i32, ptr addrspace(4) [[TMP54]], align 4, !invariant.load [[META3]]
// AMDGPU-NEXT:    [[TMP56:%.*]] = call align 8 dereferenceable(256) ptr addrspace(4) @llvm.amdgcn.implicitarg.ptr()
// AMDGPU-NEXT:    [[TMP57:%.*]] = getelementptr i8, ptr addrspace(4) [[TMP56]], i32 14
// AMDGPU-NEXT:    [[TMP58:%.*]] = load i16, ptr addrspace(4) [[TMP57]], align 2, !range [[RNG4]], !invariant.load [[META3]], !noundef [[META3]]
// AMDGPU-NEXT:    [[CONV_I20_I:%.*]] = zext i16 [[TMP58]] to i32
// AMDGPU-NEXT:    [[DIV_I21_I:%.*]] = udiv i32 [[TMP55]], [[CONV_I20_I]]
// AMDGPU-NEXT:    [[CONV8_I:%.*]] = zext i32 [[DIV_I21_I]] to i64
// AMDGPU-NEXT:    [[MUL9_I:%.*]] = mul i64 [[CONV6_I]], [[CONV8_I]]
// AMDGPU-NEXT:    [[RETVAL_ASCAST_I27_I:%.*]] = addrspacecast ptr addrspace(5) [[RETVAL_I26_I]] to ptr
// AMDGPU-NEXT:    [[TMP59:%.*]] = call i32 @llvm.amdgcn.workgroup.id.z()
// AMDGPU-NEXT:    [[CONV11_I:%.*]] = zext i32 [[TMP59]] to i64
// AMDGPU-NEXT:    [[MUL12_I:%.*]] = mul i64 [[MUL9_I]], [[CONV11_I]]
// AMDGPU-NEXT:    [[ADD13_I:%.*]] = add i64 [[ADD_I]], [[MUL12_I]]
// AMDGPU-NEXT:    [[RETVAL_ASCAST_I79:%.*]] = addrspacecast ptr addrspace(5) [[RETVAL_I78]] to ptr
// AMDGPU-NEXT:    [[TMP60:%.*]] = call align 8 dereferenceable(256) ptr addrspace(4) @llvm.amdgcn.implicitarg.ptr()
// AMDGPU-NEXT:    [[TMP61:%.*]] = getelementptr i8, ptr addrspace(4) [[TMP60]], i32 12
// AMDGPU-NEXT:    [[TMP62:%.*]] = load i16, ptr addrspace(4) [[TMP61]], align 2, !range [[RNG4]], !invariant.load [[META3]], !noundef [[META3]]
// AMDGPU-NEXT:    [[CONV_I80:%.*]] = zext i16 [[TMP62]] to i32
// AMDGPU-NEXT:    [[RETVAL_ASCAST_I82:%.*]] = addrspacecast ptr addrspace(5) [[RETVAL_I81]] to ptr
// AMDGPU-NEXT:    [[TMP63:%.*]] = call align 8 dereferenceable(256) ptr addrspace(4) @llvm.amdgcn.implicitarg.ptr()
// AMDGPU-NEXT:    [[TMP64:%.*]] = getelementptr i8, ptr addrspace(4) [[TMP63]], i32 14
// AMDGPU-NEXT:    [[TMP65:%.*]] = load i16, ptr addrspace(4) [[TMP64]], align 2, !range [[RNG4]], !invariant.load [[META3]], !noundef [[META3]]
// AMDGPU-NEXT:    [[CONV_I83:%.*]] = zext i16 [[TMP65]] to i32
// AMDGPU-NEXT:    [[RETVAL_ASCAST_I85:%.*]] = addrspacecast ptr addrspace(5) [[RETVAL_I84]] to ptr
// AMDGPU-NEXT:    [[TMP66:%.*]] = call align 8 dereferenceable(256) ptr addrspace(4) @llvm.amdgcn.implicitarg.ptr()
// AMDGPU-NEXT:    [[TMP67:%.*]] = getelementptr i8, ptr addrspace(4) [[TMP66]], i32 16
// AMDGPU-NEXT:    [[TMP68:%.*]] = load i16, ptr addrspace(4) [[TMP67]], align 2, !range [[RNG4]], !invariant.load [[META3]], !noundef [[META3]]
// AMDGPU-NEXT:    [[CONV_I86:%.*]] = zext i16 [[TMP68]] to i32
// AMDGPU-NEXT:    [[RETVAL_ASCAST_I90:%.*]] = addrspacecast ptr addrspace(5) [[RETVAL_I89]] to ptr
// AMDGPU-NEXT:    [[RETVAL_ASCAST_I_I91:%.*]] = addrspacecast ptr addrspace(5) [[RETVAL_I_I88]] to ptr
// AMDGPU-NEXT:    [[TMP69:%.*]] = call align 8 dereferenceable(256) ptr addrspace(4) @llvm.amdgcn.implicitarg.ptr()
// AMDGPU-NEXT:    [[TMP70:%.*]] = getelementptr i8, ptr addrspace(4) [[TMP69]], i32 12
// AMDGPU-NEXT:    [[TMP71:%.*]] = load i16, ptr addrspace(4) [[TMP70]], align 2, !range [[RNG4]], !invariant.load [[META3]], !noundef [[META3]]
// AMDGPU-NEXT:    [[CONV_I_I92:%.*]] = zext i16 [[TMP71]] to i32
// AMDGPU-NEXT:    [[RETVAL_ASCAST_I5_I93:%.*]] = addrspacecast ptr addrspace(5) [[RETVAL_I4_I87]] to ptr
// AMDGPU-NEXT:    [[TMP72:%.*]] = call align 8 dereferenceable(256) ptr addrspace(4) @llvm.amdgcn.implicitarg.ptr()
// AMDGPU-NEXT:    [[TMP73:%.*]] = getelementptr i8, ptr addrspace(4) [[TMP72]], i32 14
// AMDGPU-NEXT:    [[TMP74:%.*]] = load i16, ptr addrspace(4) [[TMP73]], align 2, !range [[RNG4]], !invariant.load [[META3]], !noundef [[META3]]
// AMDGPU-NEXT:    [[CONV_I6_I94:%.*]] = zext i16 [[TMP74]] to i32
// AMDGPU-NEXT:    [[MUL_I95:%.*]] = mul i32 [[CONV_I_I92]], [[CONV_I6_I94]]
// AMDGPU-NEXT:    [[RETVAL_ASCAST_I8_I:%.*]] = addrspacecast ptr addrspace(5) [[RETVAL_I7_I]] to ptr
// AMDGPU-NEXT:    [[TMP75:%.*]] = call align 8 dereferenceable(256) ptr addrspace(4) @llvm.amdgcn.implicitarg.ptr()
// AMDGPU-NEXT:    [[TMP76:%.*]] = getelementptr i8, ptr addrspace(4) [[TMP75]], i32 16
// AMDGPU-NEXT:    [[TMP77:%.*]] = load i16, ptr addrspace(4) [[TMP76]], align 2, !range [[RNG4]], !invariant.load [[META3]], !noundef [[META3]]
// AMDGPU-NEXT:    [[CONV_I9_I:%.*]] = zext i16 [[TMP77]] to i32
// AMDGPU-NEXT:    [[MUL3_I96:%.*]] = mul i32 [[MUL_I95]], [[CONV_I9_I]]
// AMDGPU-NEXT:    [[RETVAL_ASCAST_I98:%.*]] = addrspacecast ptr addrspace(5) [[RETVAL_I97]] to ptr
// AMDGPU-NEXT:    [[TMP78:%.*]] = call noundef range(i32 0, 1024) i32 @llvm.amdgcn.workitem.id.x()
// AMDGPU-NEXT:    [[RETVAL_ASCAST_I100:%.*]] = addrspacecast ptr addrspace(5) [[RETVAL_I99]] to ptr
// AMDGPU-NEXT:    [[TMP79:%.*]] = call noundef range(i32 0, 1024) i32 @llvm.amdgcn.workitem.id.y()
// AMDGPU-NEXT:    [[RETVAL_ASCAST_I102:%.*]] = addrspacecast ptr addrspace(5) [[RETVAL_I101]] to ptr
// AMDGPU-NEXT:    [[TMP80:%.*]] = call noundef range(i32 0, 1024) i32 @llvm.amdgcn.workitem.id.z()
// AMDGPU-NEXT:    [[RETVAL_ASCAST_I105:%.*]] = addrspacecast ptr addrspace(5) [[RETVAL_I104]] to ptr
// AMDGPU-NEXT:    [[RETVAL_ASCAST_I16_I:%.*]] = addrspacecast ptr addrspace(5) [[RETVAL_I15_I]] to ptr
// AMDGPU-NEXT:    [[TMP81:%.*]] = call noundef range(i32 0, 1024) i32 @llvm.amdgcn.workitem.id.x()
// AMDGPU-NEXT:    [[RETVAL_ASCAST_I10_I:%.*]] = addrspacecast ptr addrspace(5) [[RETVAL_I9_I]] to ptr
// AMDGPU-NEXT:    [[TMP82:%.*]] = call align 8 dereferenceable(256) ptr addrspace(4) @llvm.amdgcn.implicitarg.ptr()
// AMDGPU-NEXT:    [[TMP83:%.*]] = getelementptr i8, ptr addrspace(4) [[TMP82]], i32 12
// AMDGPU-NEXT:    [[TMP84:%.*]] = load i16, ptr addrspace(4) [[TMP83]], align 2, !range [[RNG4]], !invariant.load [[META3]], !noundef [[META3]]
// AMDGPU-NEXT:    [[CONV_I11_I:%.*]] = zext i16 [[TMP84]] to i32
// AMDGPU-NEXT:    [[RETVAL_ASCAST_I18_I:%.*]] = addrspacecast ptr addrspace(5) [[RETVAL_I17_I]] to ptr
// AMDGPU-NEXT:    [[TMP85:%.*]] = call noundef range(i32 0, 1024) i32 @llvm.amdgcn.workitem.id.y()
// AMDGPU-NEXT:    [[MUL_I106:%.*]] = mul i32 [[CONV_I11_I]], [[TMP85]]
// AMDGPU-NEXT:    [[ADD_I107:%.*]] = add i32 [[TMP81]], [[MUL_I106]]
// AMDGPU-NEXT:    [[RETVAL_ASCAST_I_I108:%.*]] = addrspacecast ptr addrspace(5) [[RETVAL_I_I103]] to ptr
// AMDGPU-NEXT:    [[TMP86:%.*]] = call align 8 dereferenceable(256) ptr addrspace(4) @llvm.amdgcn.implicitarg.ptr()
// AMDGPU-NEXT:    [[TMP87:%.*]] = getelementptr i8, ptr addrspace(4) [[TMP86]], i32 12
// AMDGPU-NEXT:    [[TMP88:%.*]] = load i16, ptr addrspace(4) [[TMP87]], align 2, !range [[RNG4]], !invariant.load [[META3]], !noundef [[META3]]
// AMDGPU-NEXT:    [[CONV_I_I109:%.*]] = zext i16 [[TMP88]] to i32
// AMDGPU-NEXT:    [[RETVAL_ASCAST_I13_I:%.*]] = addrspacecast ptr addrspace(5) [[RETVAL_I12_I]] to ptr
// AMDGPU-NEXT:    [[TMP89:%.*]] = call align 8 dereferenceable(256) ptr addrspace(4) @llvm.amdgcn.implicitarg.ptr()
// AMDGPU-NEXT:    [[TMP90:%.*]] = getelementptr i8, ptr addrspace(4) [[TMP89]], i32 14
// AMDGPU-NEXT:    [[TMP91:%.*]] = load i16, ptr addrspace(4) [[TMP90]], align 2, !range [[RNG4]], !invariant.load [[META3]], !noundef [[META3]]
// AMDGPU-NEXT:    [[CONV_I14_I:%.*]] = zext i16 [[TMP91]] to i32
// AMDGPU-NEXT:    [[MUL5_I:%.*]] = mul i32 [[CONV_I_I109]], [[CONV_I14_I]]
// AMDGPU-NEXT:    [[RETVAL_ASCAST_I20_I:%.*]] = addrspacecast ptr addrspace(5) [[RETVAL_I19_I]] to ptr
// AMDGPU-NEXT:    [[TMP92:%.*]] = call noundef range(i32 0, 1024) i32 @llvm.amdgcn.workitem.id.z()
// AMDGPU-NEXT:    [[MUL7_I:%.*]] = mul i32 [[MUL5_I]], [[TMP92]]
// AMDGPU-NEXT:    [[ADD8_I:%.*]] = add i32 [[ADD_I107]], [[MUL7_I]]
// AMDGPU-NEXT:    [[RETVAL_ASCAST_I111:%.*]] = addrspacecast ptr addrspace(5) [[RETVAL_I110]] to ptr
// AMDGPU-NEXT:    [[TMP93:%.*]] = call i32 @llvm.amdgcn.wavefrontsize()
// AMDGPU-NEXT:    [[RETVAL_ASCAST_I115:%.*]] = addrspacecast ptr addrspace(5) [[RETVAL_I114]] to ptr
// AMDGPU-NEXT:    [[TMP94:%.*]] = call i32 @llvm.amdgcn.mbcnt.lo(i32 -1, i32 0)
// AMDGPU-NEXT:    [[TMP95:%.*]] = call i32 @llvm.amdgcn.mbcnt.hi(i32 -1, i32 [[TMP94]])
// AMDGPU-NEXT:    [[RETVAL_ASCAST_I117:%.*]] = addrspacecast ptr addrspace(5) [[RETVAL_I116]] to ptr
// AMDGPU-NEXT:    [[TMP96:%.*]] = call i64 @llvm.amdgcn.ballot.i64(i1 true)
// AMDGPU-NEXT:    [[RETVAL_ASCAST_I:%.*]] = addrspacecast ptr addrspace(5) [[RETVAL_I]] to ptr
// AMDGPU-NEXT:    [[__LANE_MASK_ADDR_ASCAST_I:%.*]] = addrspacecast ptr addrspace(5) [[__LANE_MASK_ADDR_I]] to ptr
// AMDGPU-NEXT:    [[__X_ADDR_ASCAST_I:%.*]] = addrspacecast ptr addrspace(5) [[__X_ADDR_I]] to ptr
// AMDGPU-NEXT:    store i64 -1, ptr [[__LANE_MASK_ADDR_ASCAST_I]], align 8
// AMDGPU-NEXT:    store i32 -1, ptr [[__X_ADDR_ASCAST_I]], align 4
// AMDGPU-NEXT:    [[TMP97:%.*]] = load i32, ptr [[__X_ADDR_ASCAST_I]], align 4
// AMDGPU-NEXT:    [[TMP98:%.*]] = call i32 @llvm.amdgcn.readfirstlane.i32(i32 [[TMP97]])
// AMDGPU-NEXT:    [[RETVAL_ASCAST_I27:%.*]] = addrspacecast ptr addrspace(5) [[RETVAL_I24]] to ptr
// AMDGPU-NEXT:    [[__LANE_MASK_ADDR_ASCAST_I28:%.*]] = addrspacecast ptr addrspace(5) [[__LANE_MASK_ADDR_I25]] to ptr
// AMDGPU-NEXT:    [[__X_ADDR_ASCAST_I29:%.*]] = addrspacecast ptr addrspace(5) [[__X_ADDR_I26]] to ptr
// AMDGPU-NEXT:    store i64 -1, ptr [[__LANE_MASK_ADDR_ASCAST_I28]], align 8
// AMDGPU-NEXT:    store i8 1, ptr [[__X_ADDR_ASCAST_I29]], align 1
// AMDGPU-NEXT:    [[TMP99:%.*]] = load i64, ptr [[__LANE_MASK_ADDR_ASCAST_I28]], align 8
// AMDGPU-NEXT:    [[TMP100:%.*]] = load i8, ptr [[__X_ADDR_ASCAST_I29]], align 1
// AMDGPU-NEXT:    [[LOADEDV_I:%.*]] = trunc i8 [[TMP100]] to i1
// AMDGPU-NEXT:    [[TMP101:%.*]] = call i64 @llvm.amdgcn.ballot.i64(i1 [[LOADEDV_I]])
// AMDGPU-NEXT:    [[AND_I:%.*]] = and i64 [[TMP99]], [[TMP101]]
// AMDGPU-NEXT:    call void @llvm.amdgcn.s.barrier()
// AMDGPU-NEXT:    fence syncscope("workgroup") acquire
// AMDGPU-NEXT:    [[__LANE_MASK_ADDR_ASCAST_I31:%.*]] = addrspacecast ptr addrspace(5) [[__LANE_MASK_ADDR_I30]] to ptr
// AMDGPU-NEXT:    store i64 -1, ptr [[__LANE_MASK_ADDR_ASCAST_I31]], align 8
// AMDGPU-NEXT:    call void @llvm.amdgcn.wave.barrier()
// AMDGPU-NEXT:    [[RETVAL_ASCAST_I35:%.*]] = addrspacecast ptr addrspace(5) [[RETVAL_I32]] to ptr
// AMDGPU-NEXT:    [[__LANE_MASK_ADDR_ASCAST_I36:%.*]] = addrspacecast ptr addrspace(5) [[__LANE_MASK_ADDR_I33]] to ptr
// AMDGPU-NEXT:    [[__IDX_ADDR_ASCAST_I:%.*]] = addrspacecast ptr addrspace(5) [[__IDX_ADDR_I]] to ptr
// AMDGPU-NEXT:    [[__X_ADDR_ASCAST_I37:%.*]] = addrspacecast ptr addrspace(5) [[__X_ADDR_I34]] to ptr
// AMDGPU-NEXT:    store i64 -1, ptr [[__LANE_MASK_ADDR_ASCAST_I36]], align 8
// AMDGPU-NEXT:    store i32 -1, ptr [[__IDX_ADDR_ASCAST_I]], align 4
// AMDGPU-NEXT:    store i32 -1, ptr [[__X_ADDR_ASCAST_I37]], align 4
// AMDGPU-NEXT:    [[TMP102:%.*]] = load i32, ptr [[__IDX_ADDR_ASCAST_I]], align 4
// AMDGPU-NEXT:    [[SHL_I:%.*]] = shl i32 [[TMP102]], 2
// AMDGPU-NEXT:    [[TMP103:%.*]] = load i32, ptr [[__X_ADDR_ASCAST_I37]], align 4
// AMDGPU-NEXT:    [[TMP104:%.*]] = call i32 @llvm.amdgcn.ds.bpermute(i32 [[SHL_I]], i32 [[TMP103]])
// AMDGPU-NEXT:    [[RETVAL_ASCAST_I40:%.*]] = addrspacecast ptr addrspace(5) [[RETVAL_I38]] to ptr
// AMDGPU-NEXT:    [[__LANE_MASK_ADDR_ASCAST_I41:%.*]] = addrspacecast ptr addrspace(5) [[__LANE_MASK_ADDR_I39]] to ptr
// AMDGPU-NEXT:    store i64 -1, ptr [[__LANE_MASK_ADDR_ASCAST_I41]], align 8
// AMDGPU-NEXT:    [[TMP105:%.*]] = load i64, ptr [[__LANE_MASK_ADDR_ASCAST_I41]], align 8
// AMDGPU-NEXT:    [[TMP106:%.*]] = call i64 @llvm.cttz.i64(i64 [[TMP105]], i1 true)
// AMDGPU-NEXT:    [[TMP107:%.*]] = add i64 [[TMP106]], 1
// AMDGPU-NEXT:    [[ISZERO_I:%.*]] = icmp eq i64 [[TMP105]], 0
// AMDGPU-NEXT:    [[FFS_I:%.*]] = select i1 [[ISZERO_I]], i64 0, i64 [[TMP107]]
// AMDGPU-NEXT:    [[CAST_I:%.*]] = trunc i64 [[FFS_I]] to i32
// AMDGPU-NEXT:    [[SUB_I:%.*]] = sub nsw i32 [[CAST_I]], 1
// AMDGPU-NEXT:    [[CONV_I:%.*]] = sext i32 [[SUB_I]] to i64
// AMDGPU-NEXT:    [[RETVAL_ASCAST_I44:%.*]] = addrspacecast ptr addrspace(5) [[RETVAL_I42]] to ptr
// AMDGPU-NEXT:    [[__LANE_MASK_ADDR_ASCAST_I45:%.*]] = addrspacecast ptr addrspace(5) [[__LANE_MASK_ADDR_I43]] to ptr
// AMDGPU-NEXT:    store i64 -1, ptr [[__LANE_MASK_ADDR_ASCAST_I45]], align 8
// AMDGPU-NEXT:    [[RETVAL_ASCAST_I113:%.*]] = addrspacecast ptr addrspace(5) [[RETVAL_I112]] to ptr
// AMDGPU-NEXT:    [[TMP108:%.*]] = call i32 @llvm.amdgcn.mbcnt.lo(i32 -1, i32 0)
// AMDGPU-NEXT:    [[TMP109:%.*]] = call i32 @llvm.amdgcn.mbcnt.hi(i32 -1, i32 [[TMP108]])
// AMDGPU-NEXT:    [[CONV_I46:%.*]] = zext i32 [[TMP109]] to i64
// AMDGPU-NEXT:    [[TMP110:%.*]] = load i64, ptr [[__LANE_MASK_ADDR_ASCAST_I45]], align 8
// AMDGPU-NEXT:    [[RETVAL_ASCAST_I_I:%.*]] = addrspacecast ptr addrspace(5) [[RETVAL_I_I]] to ptr
// AMDGPU-NEXT:    [[__LANE_MASK_ADDR_ASCAST_I_I:%.*]] = addrspacecast ptr addrspace(5) [[__LANE_MASK_ADDR_I_I]] to ptr
// AMDGPU-NEXT:    store i64 [[TMP110]], ptr [[__LANE_MASK_ADDR_ASCAST_I_I]], align 8
// AMDGPU-NEXT:    [[TMP111:%.*]] = load i64, ptr [[__LANE_MASK_ADDR_ASCAST_I_I]], align 8
// AMDGPU-NEXT:    [[TMP112:%.*]] = call i64 @llvm.cttz.i64(i64 [[TMP111]], i1 true)
// AMDGPU-NEXT:    [[TMP113:%.*]] = add i64 [[TMP112]], 1
// AMDGPU-NEXT:    [[ISZERO_I_I:%.*]] = icmp eq i64 [[TMP111]], 0
// AMDGPU-NEXT:    [[FFS_I_I:%.*]] = select i1 [[ISZERO_I_I]], i64 0, i64 [[TMP113]]
// AMDGPU-NEXT:    [[CAST_I_I:%.*]] = trunc i64 [[FFS_I_I]] to i32
// AMDGPU-NEXT:    [[SUB_I_I:%.*]] = sub nsw i32 [[CAST_I_I]], 1
// AMDGPU-NEXT:    [[CONV_I_I:%.*]] = sext i32 [[SUB_I_I]] to i64
// AMDGPU-NEXT:    [[CMP_I:%.*]] = icmp eq i64 [[CONV_I46]], [[CONV_I_I]]
// AMDGPU-NEXT:    call void @llvm.amdgcn.endpgm()
// AMDGPU-NEXT:    unreachable
//
// NVPTX-LABEL: define dso_local void @foo(
// NVPTX-SAME: ) #[[ATTR0:[0-9]+]] {
// NVPTX-NEXT:  [[ENTRY:.*:]]
// NVPTX-NEXT:    [[__LANE_MASK_ADDR_I_I:%.*]] = alloca i64, align 8
// NVPTX-NEXT:    [[__LANE_MASK_ADDR_I42:%.*]] = alloca i64, align 8
// NVPTX-NEXT:    [[__LANE_MASK_ADDR_I37:%.*]] = alloca i64, align 8
// NVPTX-NEXT:    [[__LANE_MASK_ADDR_I30:%.*]] = alloca i64, align 8
// NVPTX-NEXT:    [[__IDX_ADDR_I:%.*]] = alloca i32, align 4
// NVPTX-NEXT:    [[__X_ADDR_I31:%.*]] = alloca i32, align 4
// NVPTX-NEXT:    [[__MASK_I32:%.*]] = alloca i32, align 4
// NVPTX-NEXT:    [[__BITMASK_I:%.*]] = alloca i32, align 4
// NVPTX-NEXT:    [[__LANE_MASK_ADDR_I28:%.*]] = alloca i64, align 8
// NVPTX-NEXT:    [[__LANE_MASK_ADDR_I24:%.*]] = alloca i64, align 8
// NVPTX-NEXT:    [[__X_ADDR_I25:%.*]] = alloca i8, align 1
// NVPTX-NEXT:    [[__MASK_I26:%.*]] = alloca i32, align 4
// NVPTX-NEXT:    [[__LANE_MASK_ADDR_I:%.*]] = alloca i64, align 8
// NVPTX-NEXT:    [[__X_ADDR_I:%.*]] = alloca i32, align 4
// NVPTX-NEXT:    [[__MASK_I:%.*]] = alloca i32, align 4
// NVPTX-NEXT:    [[__ID_I:%.*]] = alloca i32, align 4
// NVPTX-NEXT:    [[TMP0:%.*]] = call range(i32 1, -2147483648) i32 @llvm.nvvm.read.ptx.sreg.nctaid.x()
// NVPTX-NEXT:    [[TMP1:%.*]] = call range(i32 1, 65536) i32 @llvm.nvvm.read.ptx.sreg.nctaid.y()
// NVPTX-NEXT:    [[TMP2:%.*]] = call range(i32 1, 65536) i32 @llvm.nvvm.read.ptx.sreg.nctaid.z()
// NVPTX-NEXT:    [[TMP3:%.*]] = call range(i32 1, -2147483648) i32 @llvm.nvvm.read.ptx.sreg.nctaid.x()
// NVPTX-NEXT:    [[TMP4:%.*]] = call range(i32 1, 65536) i32 @llvm.nvvm.read.ptx.sreg.nctaid.y()
// NVPTX-NEXT:    [[MUL_I:%.*]] = mul i32 [[TMP3]], [[TMP4]]
// NVPTX-NEXT:    [[TMP5:%.*]] = call range(i32 1, 65536) i32 @llvm.nvvm.read.ptx.sreg.nctaid.z()
// NVPTX-NEXT:    [[MUL3_I:%.*]] = mul i32 [[MUL_I]], [[TMP5]]
// NVPTX-NEXT:    [[CONV_I45:%.*]] = zext i32 [[MUL3_I]] to i64
// NVPTX-NEXT:    [[TMP6:%.*]] = call range(i32 0, 2147483647) i32 @llvm.nvvm.read.ptx.sreg.ctaid.x()
// NVPTX-NEXT:    [[TMP7:%.*]] = call range(i32 0, 65535) i32 @llvm.nvvm.read.ptx.sreg.ctaid.y()
// NVPTX-NEXT:    [[TMP8:%.*]] = call range(i32 0, 65535) i32 @llvm.nvvm.read.ptx.sreg.ctaid.z()
// NVPTX-NEXT:    [[TMP9:%.*]] = call range(i32 0, 2147483647) i32 @llvm.nvvm.read.ptx.sreg.ctaid.x()
// NVPTX-NEXT:    [[CONV_I46:%.*]] = zext i32 [[TMP9]] to i64
// NVPTX-NEXT:    [[TMP10:%.*]] = call range(i32 1, -2147483648) i32 @llvm.nvvm.read.ptx.sreg.nctaid.x()
// NVPTX-NEXT:    [[CONV2_I:%.*]] = zext i32 [[TMP10]] to i64
// NVPTX-NEXT:    [[TMP11:%.*]] = call range(i32 0, 65535) i32 @llvm.nvvm.read.ptx.sreg.ctaid.y()
// NVPTX-NEXT:    [[CONV4_I:%.*]] = zext i32 [[TMP11]] to i64
// NVPTX-NEXT:    [[MUL_I47:%.*]] = mul i64 [[CONV2_I]], [[CONV4_I]]
// NVPTX-NEXT:    [[ADD_I:%.*]] = add i64 [[CONV_I46]], [[MUL_I47]]
// NVPTX-NEXT:    [[TMP12:%.*]] = call range(i32 1, -2147483648) i32 @llvm.nvvm.read.ptx.sreg.nctaid.x()
// NVPTX-NEXT:    [[CONV6_I:%.*]] = zext i32 [[TMP12]] to i64
// NVPTX-NEXT:    [[TMP13:%.*]] = call range(i32 1, 65536) i32 @llvm.nvvm.read.ptx.sreg.nctaid.y()
// NVPTX-NEXT:    [[CONV8_I:%.*]] = zext i32 [[TMP13]] to i64
// NVPTX-NEXT:    [[MUL9_I:%.*]] = mul i64 [[CONV6_I]], [[CONV8_I]]
// NVPTX-NEXT:    [[TMP14:%.*]] = call range(i32 0, 65535) i32 @llvm.nvvm.read.ptx.sreg.ctaid.z()
// NVPTX-NEXT:    [[CONV11_I:%.*]] = zext i32 [[TMP14]] to i64
// NVPTX-NEXT:    [[MUL12_I:%.*]] = mul i64 [[MUL9_I]], [[CONV11_I]]
// NVPTX-NEXT:    [[ADD13_I:%.*]] = add i64 [[ADD_I]], [[MUL12_I]]
// NVPTX-NEXT:    [[TMP15:%.*]] = call range(i32 1, 1025) i32 @llvm.nvvm.read.ptx.sreg.ntid.x()
// NVPTX-NEXT:    [[TMP16:%.*]] = call range(i32 1, 1025) i32 @llvm.nvvm.read.ptx.sreg.ntid.y()
// NVPTX-NEXT:    [[TMP17:%.*]] = call range(i32 1, 65) i32 @llvm.nvvm.read.ptx.sreg.ntid.z()
// NVPTX-NEXT:    [[TMP18:%.*]] = call range(i32 1, 1025) i32 @llvm.nvvm.read.ptx.sreg.ntid.x()
// NVPTX-NEXT:    [[TMP19:%.*]] = call range(i32 1, 1025) i32 @llvm.nvvm.read.ptx.sreg.ntid.y()
// NVPTX-NEXT:    [[MUL_I48:%.*]] = mul i32 [[TMP18]], [[TMP19]]
// NVPTX-NEXT:    [[TMP20:%.*]] = call range(i32 1, 65) i32 @llvm.nvvm.read.ptx.sreg.ntid.z()
// NVPTX-NEXT:    [[MUL3_I49:%.*]] = mul i32 [[MUL_I48]], [[TMP20]]
// NVPTX-NEXT:    [[TMP21:%.*]] = call range(i32 0, 1024) i32 @llvm.nvvm.read.ptx.sreg.tid.x()
// NVPTX-NEXT:    [[TMP22:%.*]] = call range(i32 0, 1024) i32 @llvm.nvvm.read.ptx.sreg.tid.y()
// NVPTX-NEXT:    [[TMP23:%.*]] = call range(i32 0, 64) i32 @llvm.nvvm.read.ptx.sreg.tid.z()
// NVPTX-NEXT:    [[TMP24:%.*]] = call range(i32 0, 1024) i32 @llvm.nvvm.read.ptx.sreg.tid.x()
// NVPTX-NEXT:    [[TMP25:%.*]] = call range(i32 1, 1025) i32 @llvm.nvvm.read.ptx.sreg.ntid.x()
// NVPTX-NEXT:    [[TMP26:%.*]] = call range(i32 0, 1024) i32 @llvm.nvvm.read.ptx.sreg.tid.y()
// NVPTX-NEXT:    [[MUL_I50:%.*]] = mul i32 [[TMP25]], [[TMP26]]
// NVPTX-NEXT:    [[ADD_I51:%.*]] = add i32 [[TMP24]], [[MUL_I50]]
// NVPTX-NEXT:    [[TMP27:%.*]] = call range(i32 1, 1025) i32 @llvm.nvvm.read.ptx.sreg.ntid.x()
// NVPTX-NEXT:    [[TMP28:%.*]] = call range(i32 1, 1025) i32 @llvm.nvvm.read.ptx.sreg.ntid.y()
// NVPTX-NEXT:    [[MUL5_I:%.*]] = mul i32 [[TMP27]], [[TMP28]]
// NVPTX-NEXT:    [[TMP29:%.*]] = call range(i32 0, 64) i32 @llvm.nvvm.read.ptx.sreg.tid.z()
// NVPTX-NEXT:    [[MUL7_I:%.*]] = mul i32 [[MUL5_I]], [[TMP29]]
// NVPTX-NEXT:    [[ADD8_I:%.*]] = add i32 [[ADD_I51]], [[MUL7_I]]
// NVPTX-NEXT:    [[TMP30:%.*]] = call range(i32 32, 33) i32 @llvm.nvvm.read.ptx.sreg.warpsize()
// NVPTX-NEXT:    [[TMP31:%.*]] = call range(i32 0, 32) i32 @llvm.nvvm.read.ptx.sreg.laneid()
// NVPTX-NEXT:    [[TMP32:%.*]] = call i32 @llvm.nvvm.activemask()
// NVPTX-NEXT:    [[CONV_I52:%.*]] = zext i32 [[TMP32]] to i64
// NVPTX-NEXT:    store i64 -1, ptr [[__LANE_MASK_ADDR_I]], align 8
// NVPTX-NEXT:    store i32 -1, ptr [[__X_ADDR_I]], align 4
// NVPTX-NEXT:    [[TMP33:%.*]] = load i64, ptr [[__LANE_MASK_ADDR_I]], align 8
// NVPTX-NEXT:    [[CONV_I:%.*]] = trunc i64 [[TMP33]] to i32
// NVPTX-NEXT:    store i32 [[CONV_I]], ptr [[__MASK_I]], align 4
// NVPTX-NEXT:    [[TMP34:%.*]] = load i32, ptr [[__MASK_I]], align 4
// NVPTX-NEXT:    [[TMP35:%.*]] = call i32 @llvm.cttz.i32(i32 [[TMP34]], i1 true)
// NVPTX-NEXT:    [[TMP36:%.*]] = add i32 [[TMP35]], 1
// NVPTX-NEXT:    [[ISZERO_I:%.*]] = icmp eq i32 [[TMP34]], 0
// NVPTX-NEXT:    [[FFS_I:%.*]] = select i1 [[ISZERO_I]], i32 0, i32 [[TMP36]]
// NVPTX-NEXT:    [[SUB_I:%.*]] = sub nsw i32 [[FFS_I]], 1
// NVPTX-NEXT:    store i32 [[SUB_I]], ptr [[__ID_I]], align 4
// NVPTX-NEXT:    [[TMP37:%.*]] = load i32, ptr [[__MASK_I]], align 4
// NVPTX-NEXT:    [[TMP38:%.*]] = load i32, ptr [[__X_ADDR_I]], align 4
// NVPTX-NEXT:    [[TMP39:%.*]] = load i32, ptr [[__ID_I]], align 4
// NVPTX-NEXT:    [[TMP40:%.*]] = call range(i32 32, 33) i32 @llvm.nvvm.read.ptx.sreg.warpsize()
// NVPTX-NEXT:    [[SUB1_I:%.*]] = sub i32 [[TMP40]], 1
// NVPTX-NEXT:    [[TMP41:%.*]] = call i32 @llvm.nvvm.shfl.sync.idx.i32(i32 [[TMP37]], i32 [[TMP38]], i32 [[TMP39]], i32 [[SUB1_I]])
// NVPTX-NEXT:    store i64 -1, ptr [[__LANE_MASK_ADDR_I24]], align 8
// NVPTX-NEXT:    store i8 1, ptr [[__X_ADDR_I25]], align 1
// NVPTX-NEXT:    [[TMP42:%.*]] = load i64, ptr [[__LANE_MASK_ADDR_I24]], align 8
// NVPTX-NEXT:    [[CONV_I27:%.*]] = trunc i64 [[TMP42]] to i32
// NVPTX-NEXT:    store i32 [[CONV_I27]], ptr [[__MASK_I26]], align 4
// NVPTX-NEXT:    [[TMP43:%.*]] = load i32, ptr [[__MASK_I26]], align 4
// NVPTX-NEXT:    [[TMP44:%.*]] = load i8, ptr [[__X_ADDR_I25]], align 1
// NVPTX-NEXT:    [[LOADEDV_I:%.*]] = trunc i8 [[TMP44]] to i1
// NVPTX-NEXT:    [[TMP45:%.*]] = call i32 @llvm.nvvm.vote.ballot.sync(i32 [[TMP43]], i1 [[LOADEDV_I]])
// NVPTX-NEXT:    [[CONV1_I:%.*]] = zext i32 [[TMP45]] to i64
// NVPTX-NEXT:    call void @llvm.nvvm.barrier0()
// NVPTX-NEXT:    store i64 -1, ptr [[__LANE_MASK_ADDR_I28]], align 8
// NVPTX-NEXT:    [[TMP46:%.*]] = load i64, ptr [[__LANE_MASK_ADDR_I28]], align 8
// NVPTX-NEXT:    [[CONV_I29:%.*]] = trunc i64 [[TMP46]] to i32
// NVPTX-NEXT:    call void @llvm.nvvm.bar.warp.sync(i32 [[CONV_I29]])
// NVPTX-NEXT:    store i64 -1, ptr [[__LANE_MASK_ADDR_I30]], align 8
// NVPTX-NEXT:    store i32 -1, ptr [[__IDX_ADDR_I]], align 4
// NVPTX-NEXT:    store i32 -1, ptr [[__X_ADDR_I31]], align 4
// NVPTX-NEXT:    [[TMP47:%.*]] = load i64, ptr [[__LANE_MASK_ADDR_I30]], align 8
// NVPTX-NEXT:    [[CONV_I33:%.*]] = trunc i64 [[TMP47]] to i32
// NVPTX-NEXT:    store i32 [[CONV_I33]], ptr [[__MASK_I32]], align 4
// NVPTX-NEXT:    [[TMP48:%.*]] = load i32, ptr [[__MASK_I32]], align 4
// NVPTX-NEXT:    [[TMP49:%.*]] = load i32, ptr [[__IDX_ADDR_I]], align 4
// NVPTX-NEXT:    [[SHR_I:%.*]] = lshr i32 [[TMP48]], [[TMP49]]
// NVPTX-NEXT:    [[AND_I:%.*]] = and i32 [[SHR_I]], 1
// NVPTX-NEXT:    store i32 [[AND_I]], ptr [[__BITMASK_I]], align 4
// NVPTX-NEXT:    [[TMP50:%.*]] = load i32, ptr [[__BITMASK_I]], align 4
// NVPTX-NEXT:    [[SUB_I34:%.*]] = sub i32 0, [[TMP50]]
// NVPTX-NEXT:    [[TMP51:%.*]] = load i32, ptr [[__MASK_I32]], align 4
// NVPTX-NEXT:    [[TMP52:%.*]] = load i32, ptr [[__X_ADDR_I31]], align 4
// NVPTX-NEXT:    [[TMP53:%.*]] = load i32, ptr [[__IDX_ADDR_I]], align 4
// NVPTX-NEXT:    [[TMP54:%.*]] = call range(i32 32, 33) i32 @llvm.nvvm.read.ptx.sreg.warpsize()
// NVPTX-NEXT:    [[SUB1_I36:%.*]] = sub i32 [[TMP54]], 1
// NVPTX-NEXT:    [[TMP55:%.*]] = call i32 @llvm.nvvm.shfl.sync.idx.i32(i32 [[TMP51]], i32 [[TMP52]], i32 [[TMP53]], i32 [[SUB1_I36]])
// NVPTX-NEXT:    [[AND2_I:%.*]] = and i32 [[SUB_I34]], [[TMP55]]
// NVPTX-NEXT:    store i64 -1, ptr [[__LANE_MASK_ADDR_I37]], align 8
// NVPTX-NEXT:    [[TMP56:%.*]] = load i64, ptr [[__LANE_MASK_ADDR_I37]], align 8
// NVPTX-NEXT:    [[TMP57:%.*]] = call i64 @llvm.cttz.i64(i64 [[TMP56]], i1 true)
// NVPTX-NEXT:    [[TMP58:%.*]] = add i64 [[TMP57]], 1
// NVPTX-NEXT:    [[ISZERO_I38:%.*]] = icmp eq i64 [[TMP56]], 0
// NVPTX-NEXT:    [[FFS_I39:%.*]] = select i1 [[ISZERO_I38]], i64 0, i64 [[TMP58]]
// NVPTX-NEXT:    [[CAST_I:%.*]] = trunc i64 [[FFS_I39]] to i32
// NVPTX-NEXT:    [[SUB_I40:%.*]] = sub nsw i32 [[CAST_I]], 1
// NVPTX-NEXT:    [[CONV_I41:%.*]] = sext i32 [[SUB_I40]] to i64
// NVPTX-NEXT:    store i64 -1, ptr [[__LANE_MASK_ADDR_I42]], align 8
// NVPTX-NEXT:    [[TMP59:%.*]] = call range(i32 0, 32) i32 @llvm.nvvm.read.ptx.sreg.laneid()
// NVPTX-NEXT:    [[CONV_I44:%.*]] = zext i32 [[TMP59]] to i64
// NVPTX-NEXT:    [[TMP60:%.*]] = load i64, ptr [[__LANE_MASK_ADDR_I42]], align 8
// NVPTX-NEXT:    store i64 [[TMP60]], ptr [[__LANE_MASK_ADDR_I_I]], align 8
// NVPTX-NEXT:    [[TMP61:%.*]] = load i64, ptr [[__LANE_MASK_ADDR_I_I]], align 8
// NVPTX-NEXT:    [[TMP62:%.*]] = call i64 @llvm.cttz.i64(i64 [[TMP61]], i1 true)
// NVPTX-NEXT:    [[TMP63:%.*]] = add i64 [[TMP62]], 1
// NVPTX-NEXT:    [[ISZERO_I_I:%.*]] = icmp eq i64 [[TMP61]], 0
// NVPTX-NEXT:    [[FFS_I_I:%.*]] = select i1 [[ISZERO_I_I]], i64 0, i64 [[TMP63]]
// NVPTX-NEXT:    [[CAST_I_I:%.*]] = trunc i64 [[FFS_I_I]] to i32
// NVPTX-NEXT:    [[SUB_I_I:%.*]] = sub nsw i32 [[CAST_I_I]], 1
// NVPTX-NEXT:    [[CONV_I_I:%.*]] = sext i32 [[SUB_I_I]] to i64
// NVPTX-NEXT:    [[CMP_I:%.*]] = icmp eq i64 [[CONV_I44]], [[CONV_I_I]]
// NVPTX-NEXT:    call void @llvm.nvvm.exit()
// NVPTX-NEXT:    unreachable
//
void foo() {
  __gpu_num_blocks_x();
  __gpu_num_blocks_y();
  __gpu_num_blocks_z();
  __gpu_num_blocks();
  __gpu_block_id_x();
  __gpu_block_id_y();
  __gpu_block_id_z();
  __gpu_block_id();
  __gpu_num_threads_x();
  __gpu_num_threads_y();
  __gpu_num_threads_z();
  __gpu_num_threads();
  __gpu_thread_id_x();
  __gpu_thread_id_y();
  __gpu_thread_id_z();
  __gpu_thread_id();
  __gpu_num_lanes();
  __gpu_lane_id();
  __gpu_lane_mask();
  __gpu_broadcast(-1, -1);
  __gpu_ballot(-1, 1);
  __gpu_sync_threads();
  __gpu_sync_lane(-1);
  __gpu_shuffle_idx(-1, -1, -1);
  __gpu_first_lane_id(-1);
  __gpu_is_first_lane(-1);
  __gpu_exit();
}
//.
// AMDGPU: [[META3]] = !{}
// AMDGPU: [[RNG4]] = !{i16 1, i16 1025}
//.
