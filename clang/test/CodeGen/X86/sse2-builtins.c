// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py UTC_ARGS: --version 4
// RUN: %clang_cc1 -flax-vector-conversions=none -ffreestanding %s -triple=x86_64-apple-darwin -target-feature +sse2 -emit-llvm -o - -Wall -Werror | FileCheck %s --check-prefixes=CHECK,X64
// RUN: %clang_cc1 -flax-vector-conversions=none -ffreestanding %s -triple=x86_64-apple-darwin -target-feature +sse2 -fno-signed-char -emit-llvm -o - -Wall -Werror | FileCheck %s --check-prefixes=CHECK,X64
// RUN: %clang_cc1 -flax-vector-conversions=none -ffreestanding %s -triple=i386-apple-darwin -target-feature +sse2 -emit-llvm -o - -Wall -Werror | FileCheck %s --check-prefixes=CHECK,X86
// RUN: %clang_cc1 -flax-vector-conversions=none -ffreestanding %s -triple=i386-apple-darwin -target-feature +sse2 -fno-signed-char -emit-llvm -o - -Wall -Werror | FileCheck %s --check-prefixes=CHECK,X86
// RUN: %clang_cc1 -flax-vector-conversions=none -fms-extensions -fms-compatibility -ffreestanding %s -triple=x86_64-windows-msvc -target-feature +sse2 -emit-llvm -o - -Wall -Werror | FileCheck %s --check-prefixes=CHECK,X64


#include <immintrin.h>

// NOTE: This should match the tests in llvm/test/CodeGen/X86/sse2-intrinsics-fast-isel.ll

//
// X86-LABEL: define <2 x i64> @test_mm_add_epi8(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0:[0-9]+]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x i64> [[TMP2]] to <16 x i8>
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = bitcast <2 x i64> [[TMP4]] to <16 x i8>
// X86-NEXT:    [[ADD_I:%.*]] = add <16 x i8> [[TMP3]], [[TMP5]]
// X86-NEXT:    [[TMP6:%.*]] = bitcast <16 x i8> [[ADD_I]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP6]]
//
__m128i test_mm_add_epi8(__m128i A, __m128i B) {
  return _mm_add_epi8(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_add_epi16(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x i64> [[TMP2]] to <8 x i16>
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = bitcast <2 x i64> [[TMP4]] to <8 x i16>
// X86-NEXT:    [[ADD_I:%.*]] = add <8 x i16> [[TMP3]], [[TMP5]]
// X86-NEXT:    [[TMP6:%.*]] = bitcast <8 x i16> [[ADD_I]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP6]]
//
__m128i test_mm_add_epi16(__m128i A, __m128i B) {
  return _mm_add_epi16(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_add_epi32(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x i64> [[TMP2]] to <4 x i32>
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = bitcast <2 x i64> [[TMP4]] to <4 x i32>
// X86-NEXT:    [[ADD_I:%.*]] = add <4 x i32> [[TMP3]], [[TMP5]]
// X86-NEXT:    [[TMP6:%.*]] = bitcast <4 x i32> [[ADD_I]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP6]]
//
__m128i test_mm_add_epi32(__m128i A, __m128i B) {
  return _mm_add_epi32(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_add_epi64(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[ADD_I:%.*]] = add <2 x i64> [[TMP2]], [[TMP3]]
// X86-NEXT:    ret <2 x i64> [[ADD_I]]
//
__m128i test_mm_add_epi64(__m128i A, __m128i B) {
  return _mm_add_epi64(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_add_pd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[ADD_I:%.*]] = fadd <2 x double> [[TMP2]], [[TMP3]]
// X86-NEXT:    store <2 x double> [[ADD_I]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP4]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP5]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP6:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP6]]
//
__m128d test_mm_add_pd(__m128d A, __m128d B) {
  return _mm_add_pd(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_add_sd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[VECEXT_I:%.*]] = extractelement <2 x double> [[TMP2]], i32 0
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[VECEXT1_I:%.*]] = extractelement <2 x double> [[TMP3]], i32 0
// X86-NEXT:    [[ADD_I:%.*]] = fadd double [[VECEXT1_I]], [[VECEXT_I]]
// X86-NEXT:    [[TMP4:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[VECINS_I:%.*]] = insertelement <2 x double> [[TMP4]], double [[ADD_I]], i32 0
// X86-NEXT:    store <2 x double> [[VECINS_I]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP5]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP6:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP6]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP7:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP7]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP8:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP8]]
//
__m128d test_mm_add_sd(__m128d A, __m128d B) {
  return _mm_add_sd(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_adds_epi8(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x i64> [[TMP2]] to <16 x i8>
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = bitcast <2 x i64> [[TMP4]] to <16 x i8>
// X86-NEXT:    [[ELT_SAT_I:%.*]] = call <16 x i8> @llvm.sadd.sat.v16i8(<16 x i8> [[TMP3]], <16 x i8> [[TMP5]])
// X86-NEXT:    [[TMP6:%.*]] = bitcast <16 x i8> [[ELT_SAT_I]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP6]]
//
__m128i test_mm_adds_epi8(__m128i A, __m128i B) {
  return _mm_adds_epi8(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_adds_epi16(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x i64> [[TMP2]] to <8 x i16>
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = bitcast <2 x i64> [[TMP4]] to <8 x i16>
// X86-NEXT:    [[ELT_SAT_I:%.*]] = call <8 x i16> @llvm.sadd.sat.v8i16(<8 x i16> [[TMP3]], <8 x i16> [[TMP5]])
// X86-NEXT:    [[TMP6:%.*]] = bitcast <8 x i16> [[ELT_SAT_I]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP6]]
//
__m128i test_mm_adds_epi16(__m128i A, __m128i B) {
  return _mm_adds_epi16(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_adds_epu8(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x i64> [[TMP2]] to <16 x i8>
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = bitcast <2 x i64> [[TMP4]] to <16 x i8>
// X86-NEXT:    [[ELT_SAT_I:%.*]] = call <16 x i8> @llvm.uadd.sat.v16i8(<16 x i8> [[TMP3]], <16 x i8> [[TMP5]])
// X86-NEXT:    [[TMP6:%.*]] = bitcast <16 x i8> [[ELT_SAT_I]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP6]]
//
__m128i test_mm_adds_epu8(__m128i A, __m128i B) {
  return _mm_adds_epu8(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_adds_epu16(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x i64> [[TMP2]] to <8 x i16>
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = bitcast <2 x i64> [[TMP4]] to <8 x i16>
// X86-NEXT:    [[ELT_SAT_I:%.*]] = call <8 x i16> @llvm.uadd.sat.v8i16(<8 x i16> [[TMP3]], <8 x i16> [[TMP5]])
// X86-NEXT:    [[TMP6:%.*]] = bitcast <8 x i16> [[ELT_SAT_I]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP6]]
//
__m128i test_mm_adds_epu16(__m128i A, __m128i B) {
  return _mm_adds_epu16(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_and_pd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x double> [[TMP2]] to <2 x i64>
// X86-NEXT:    [[TMP4:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = bitcast <2 x double> [[TMP4]] to <2 x i64>
// X86-NEXT:    [[AND_I:%.*]] = and <2 x i64> [[TMP3]], [[TMP5]]
// X86-NEXT:    [[TMP6:%.*]] = bitcast <2 x i64> [[AND_I]] to <2 x double>
// X86-NEXT:    store <2 x double> [[TMP6]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP7:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP7]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP8:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP8]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP9:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP9]]
//
__m128d test_mm_and_pd(__m128d A, __m128d B) {
  return _mm_and_pd(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_and_si128(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[AND_I:%.*]] = and <2 x i64> [[TMP2]], [[TMP3]]
// X86-NEXT:    ret <2 x i64> [[AND_I]]
//
__m128i test_mm_and_si128(__m128i A, __m128i B) {
  return _mm_and_si128(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_andnot_pd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x double> [[TMP2]] to <2 x i64>
// X86-NEXT:    [[NOT_I:%.*]] = xor <2 x i64> [[TMP3]], <i64 -1, i64 -1>
// X86-NEXT:    [[TMP4:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = bitcast <2 x double> [[TMP4]] to <2 x i64>
// X86-NEXT:    [[AND_I:%.*]] = and <2 x i64> [[NOT_I]], [[TMP5]]
// X86-NEXT:    [[TMP6:%.*]] = bitcast <2 x i64> [[AND_I]] to <2 x double>
// X86-NEXT:    store <2 x double> [[TMP6]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP7:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP7]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP8:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP8]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP9:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP9]]
//
__m128d test_mm_andnot_pd(__m128d A, __m128d B) {
  return _mm_andnot_pd(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_andnot_si128(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[NOT_I:%.*]] = xor <2 x i64> [[TMP2]], <i64 -1, i64 -1>
// X86-NEXT:    [[TMP3:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[AND_I:%.*]] = and <2 x i64> [[NOT_I]], [[TMP3]]
// X86-NEXT:    ret <2 x i64> [[AND_I]]
//
__m128i test_mm_andnot_si128(__m128i A, __m128i B) {
  return _mm_andnot_si128(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_avg_epu8(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x i64> [[TMP2]] to <16 x i8>
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = bitcast <2 x i64> [[TMP4]] to <16 x i8>
// X86-NEXT:    [[TMP6:%.*]] = call <16 x i8> @llvm.x86.sse2.pavg.b(<16 x i8> [[TMP3]], <16 x i8> [[TMP5]])
// X86-NEXT:    [[TMP7:%.*]] = bitcast <16 x i8> [[TMP6]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP7]]
//
__m128i test_mm_avg_epu8(__m128i A, __m128i B) {
  return _mm_avg_epu8(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_avg_epu16(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x i64> [[TMP2]] to <8 x i16>
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = bitcast <2 x i64> [[TMP4]] to <8 x i16>
// X86-NEXT:    [[TMP6:%.*]] = call <8 x i16> @llvm.x86.sse2.pavg.w(<8 x i16> [[TMP3]], <8 x i16> [[TMP5]])
// X86-NEXT:    [[TMP7:%.*]] = bitcast <8 x i16> [[TMP6]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP7]]
//
__m128i test_mm_avg_epu16(__m128i A, __m128i B) {
  return _mm_avg_epu16(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_bslli_si128(
// X86-SAME: <2 x i64> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[CAST:%.*]] = bitcast <2 x i64> [[TMP0]] to <16 x i8>
// X86-NEXT:    [[PSLLDQ:%.*]] = shufflevector <16 x i8> zeroinitializer, <16 x i8> [[CAST]], <16 x i32> <i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26>
// X86-NEXT:    [[CAST1:%.*]] = bitcast <16 x i8> [[PSLLDQ]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[CAST1]]
//
__m128i test_mm_bslli_si128(__m128i A) {
  return _mm_bslli_si128(A, 5);
}

//
// X86-LABEL: define <2 x i64> @test_mm_bsrli_si128(
// X86-SAME: <2 x i64> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[CAST:%.*]] = bitcast <2 x i64> [[TMP0]] to <16 x i8>
// X86-NEXT:    [[PSRLDQ:%.*]] = shufflevector <16 x i8> [[CAST]], <16 x i8> zeroinitializer, <16 x i32> <i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20>
// X86-NEXT:    [[CAST1:%.*]] = bitcast <16 x i8> [[PSRLDQ]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[CAST1]]
//
__m128i test_mm_bsrli_si128(__m128i A) {
  return _mm_bsrli_si128(A, 5);
}

//
// X86-LABEL: define <2 x i64> @test_mm_castpd_ps(
// X86-SAME: <2 x double> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = bitcast <2 x double> [[TMP1]] to <4 x float>
// X86-NEXT:    store <4 x float> [[TMP2]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP3]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP4:%.*]] = load <4 x float>, ptr [[COERCE]], align 16
// X86-NEXT:    store <4 x float> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128 test_mm_castpd_ps(__m128d A) {
  return _mm_castpd_ps(A);
}

//
// X86-LABEL: define <2 x i64> @test_mm_castpd_si128(
// X86-SAME: <2 x double> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = bitcast <2 x double> [[TMP1]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP2]]
//
__m128i test_mm_castpd_si128(__m128d A) {
  return _mm_castpd_si128(A);
}

//
// X86-LABEL: define <2 x i64> @test_mm_castps_pd(
// X86-SAME: <4 x float> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// X86-NEXT:    store <4 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = bitcast <4 x float> [[TMP1]] to <2 x double>
// X86-NEXT:    store <2 x double> [[TMP2]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP3]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP4:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128d test_mm_castps_pd(__m128 A) {
  return _mm_castps_pd(A);
}

//
// X86-LABEL: define <2 x i64> @test_mm_castps_si128(
// X86-SAME: <4 x float> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// X86-NEXT:    store <4 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = bitcast <4 x float> [[TMP1]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP2]]
//
__m128i test_mm_castps_si128(__m128 A) {
  return _mm_castps_si128(A);
}

//
// X86-LABEL: define <2 x i64> @test_mm_castsi128_pd(
// X86-SAME: <2 x i64> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = bitcast <2 x i64> [[TMP1]] to <2 x double>
// X86-NEXT:    store <2 x double> [[TMP2]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP3]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP4:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128d test_mm_castsi128_pd(__m128i A) {
  return _mm_castsi128_pd(A);
}

//
// X86-LABEL: define <2 x i64> @test_mm_castsi128_ps(
// X86-SAME: <2 x i64> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = bitcast <2 x i64> [[TMP1]] to <4 x float>
// X86-NEXT:    store <4 x float> [[TMP2]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP3]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP4:%.*]] = load <4 x float>, ptr [[COERCE]], align 16
// X86-NEXT:    store <4 x float> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128 test_mm_castsi128_ps(__m128i A) {
  return _mm_castsi128_ps(A);
}

//
// X86-LABEL: define void @test_mm_clflush(
// X86-SAME: ptr noundef [[A:%.*]]) #[[ATTR1:[0-9]+]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    call void @llvm.x86.sse2.clflush(ptr [[TMP0]])
// X86-NEXT:    ret void
//
void test_mm_clflush(void* A) {
  _mm_clflush(A);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmp_pd_eq_oq(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = fcmp oeq <2 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <2 x i1> [[TMP2]] to <2 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <2 x i64> [[TMP3]] to <2 x double>
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128d test_mm_cmp_pd_eq_oq(__m128d a, __m128d b) {
  return _mm_cmp_pd(a, b, _CMP_EQ_OQ);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmp_pd_lt_os(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = fcmp olt <2 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <2 x i1> [[TMP2]] to <2 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <2 x i64> [[TMP3]] to <2 x double>
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128d test_mm_cmp_pd_lt_os(__m128d a, __m128d b) {
  return _mm_cmp_pd(a, b, _CMP_LT_OS);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmp_pd_le_os(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = fcmp ole <2 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <2 x i1> [[TMP2]] to <2 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <2 x i64> [[TMP3]] to <2 x double>
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128d test_mm_cmp_pd_le_os(__m128d a, __m128d b) {
  return _mm_cmp_pd(a, b, _CMP_LE_OS);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmp_pd_unord_q(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = fcmp uno <2 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <2 x i1> [[TMP2]] to <2 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <2 x i64> [[TMP3]] to <2 x double>
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128d test_mm_cmp_pd_unord_q(__m128d a, __m128d b) {
  return _mm_cmp_pd(a, b, _CMP_UNORD_Q);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmp_pd_neq_uq(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = fcmp une <2 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <2 x i1> [[TMP2]] to <2 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <2 x i64> [[TMP3]] to <2 x double>
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128d test_mm_cmp_pd_neq_uq(__m128d a, __m128d b) {
  return _mm_cmp_pd(a, b, _CMP_NEQ_UQ);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmp_pd_nlt_us(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = fcmp uge <2 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <2 x i1> [[TMP2]] to <2 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <2 x i64> [[TMP3]] to <2 x double>
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128d test_mm_cmp_pd_nlt_us(__m128d a, __m128d b) {
  return _mm_cmp_pd(a, b, _CMP_NLT_US);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmp_pd_nle_us(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = fcmp ugt <2 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <2 x i1> [[TMP2]] to <2 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <2 x i64> [[TMP3]] to <2 x double>
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128d test_mm_cmp_pd_nle_us(__m128d a, __m128d b) {
  return _mm_cmp_pd(a, b, _CMP_NLE_US);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmp_pd_ord_q(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = fcmp ord <2 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <2 x i1> [[TMP2]] to <2 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <2 x i64> [[TMP3]] to <2 x double>
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128d test_mm_cmp_pd_ord_q(__m128d a, __m128d b) {
  return _mm_cmp_pd(a, b, _CMP_ORD_Q);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmp_sd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = call <2 x double> @llvm.x86.sse2.cmp.sd(<2 x double> [[TMP0]], <2 x double> [[TMP1]], i8 7)
// X86-NEXT:    store <2 x double> [[TMP2]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP3]]
//
__m128d test_mm_cmp_sd(__m128d A, __m128d B) {
  return _mm_cmp_sd(A, B, _CMP_ORD_Q);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmpeq_epi8(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x i64> [[TMP2]] to <16 x i8>
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = bitcast <2 x i64> [[TMP4]] to <16 x i8>
// X86-NEXT:    [[CMP_I:%.*]] = icmp eq <16 x i8> [[TMP3]], [[TMP5]]
// X86-NEXT:    [[SEXT_I:%.*]] = sext <16 x i1> [[CMP_I]] to <16 x i8>
// X86-NEXT:    [[TMP6:%.*]] = bitcast <16 x i8> [[SEXT_I]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP6]]
//
__m128i test_mm_cmpeq_epi8(__m128i A, __m128i B) {
  return _mm_cmpeq_epi8(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmpeq_epi16(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x i64> [[TMP2]] to <8 x i16>
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = bitcast <2 x i64> [[TMP4]] to <8 x i16>
// X86-NEXT:    [[CMP_I:%.*]] = icmp eq <8 x i16> [[TMP3]], [[TMP5]]
// X86-NEXT:    [[SEXT_I:%.*]] = sext <8 x i1> [[CMP_I]] to <8 x i16>
// X86-NEXT:    [[TMP6:%.*]] = bitcast <8 x i16> [[SEXT_I]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP6]]
//
__m128i test_mm_cmpeq_epi16(__m128i A, __m128i B) {
  return _mm_cmpeq_epi16(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmpeq_epi32(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x i64> [[TMP2]] to <4 x i32>
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = bitcast <2 x i64> [[TMP4]] to <4 x i32>
// X86-NEXT:    [[CMP_I:%.*]] = icmp eq <4 x i32> [[TMP3]], [[TMP5]]
// X86-NEXT:    [[SEXT_I:%.*]] = sext <4 x i1> [[CMP_I]] to <4 x i32>
// X86-NEXT:    [[TMP6:%.*]] = bitcast <4 x i32> [[SEXT_I]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP6]]
//
__m128i test_mm_cmpeq_epi32(__m128i A, __m128i B) {
  return _mm_cmpeq_epi32(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmpeq_pd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = fcmp oeq <2 x double> [[TMP2]], [[TMP3]]
// X86-NEXT:    [[TMP5:%.*]] = sext <2 x i1> [[TMP4]] to <2 x i64>
// X86-NEXT:    [[TMP6:%.*]] = bitcast <2 x i64> [[TMP5]] to <2 x double>
// X86-NEXT:    store <2 x double> [[TMP6]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP7:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP7]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP8:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP8]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP9:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP9]]
//
__m128d test_mm_cmpeq_pd(__m128d A, __m128d B) {
  return _mm_cmpeq_pd(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmpeq_sd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = call <2 x double> @llvm.x86.sse2.cmp.sd(<2 x double> [[TMP2]], <2 x double> [[TMP3]], i8 0)
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP5]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP6:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP6]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP7:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP7]]
//
__m128d test_mm_cmpeq_sd(__m128d A, __m128d B) {
  return _mm_cmpeq_sd(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmpge_pd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = fcmp ole <2 x double> [[TMP2]], [[TMP3]]
// X86-NEXT:    [[TMP5:%.*]] = sext <2 x i1> [[TMP4]] to <2 x i64>
// X86-NEXT:    [[TMP6:%.*]] = bitcast <2 x i64> [[TMP5]] to <2 x double>
// X86-NEXT:    store <2 x double> [[TMP6]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP7:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP7]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP8:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP8]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP9:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP9]]
//
__m128d test_mm_cmpge_pd(__m128d A, __m128d B) {
  return _mm_cmpge_pd(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmpge_sd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__C_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = call <2 x double> @llvm.x86.sse2.cmp.sd(<2 x double> [[TMP2]], <2 x double> [[TMP3]], i8 2)
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[__C_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x double>, ptr [[__C_I]], align 16
// X86-NEXT:    [[VECEXT_I:%.*]] = extractelement <2 x double> [[TMP5]], i32 0
// X86-NEXT:    [[VECINIT_I:%.*]] = insertelement <2 x double> poison, double [[VECEXT_I]], i32 0
// X86-NEXT:    [[TMP6:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[VECEXT1_I:%.*]] = extractelement <2 x double> [[TMP6]], i32 1
// X86-NEXT:    [[VECINIT2_I:%.*]] = insertelement <2 x double> [[VECINIT_I]], double [[VECEXT1_I]], i32 1
// X86-NEXT:    store <2 x double> [[VECINIT2_I]], ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    [[TMP7:%.*]] = load <2 x double>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP7]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP8:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP8]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP9:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP9]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP10:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP10]]
//
__m128d test_mm_cmpge_sd(__m128d A, __m128d B) {
  return _mm_cmpge_sd(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmpgt_epi8(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x i64> [[TMP2]] to <16 x i8>
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = bitcast <2 x i64> [[TMP4]] to <16 x i8>
// X86-NEXT:    [[CMP_I:%.*]] = icmp sgt <16 x i8> [[TMP3]], [[TMP5]]
// X86-NEXT:    [[SEXT_I:%.*]] = sext <16 x i1> [[CMP_I]] to <16 x i8>
// X86-NEXT:    [[TMP6:%.*]] = bitcast <16 x i8> [[SEXT_I]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP6]]
//
__m128i test_mm_cmpgt_epi8(__m128i A, __m128i B) {
  return _mm_cmpgt_epi8(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmpgt_epi16(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x i64> [[TMP2]] to <8 x i16>
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = bitcast <2 x i64> [[TMP4]] to <8 x i16>
// X86-NEXT:    [[CMP_I:%.*]] = icmp sgt <8 x i16> [[TMP3]], [[TMP5]]
// X86-NEXT:    [[SEXT_I:%.*]] = sext <8 x i1> [[CMP_I]] to <8 x i16>
// X86-NEXT:    [[TMP6:%.*]] = bitcast <8 x i16> [[SEXT_I]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP6]]
//
__m128i test_mm_cmpgt_epi16(__m128i A, __m128i B) {
  return _mm_cmpgt_epi16(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmpgt_epi32(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x i64> [[TMP2]] to <4 x i32>
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = bitcast <2 x i64> [[TMP4]] to <4 x i32>
// X86-NEXT:    [[CMP_I:%.*]] = icmp sgt <4 x i32> [[TMP3]], [[TMP5]]
// X86-NEXT:    [[SEXT_I:%.*]] = sext <4 x i1> [[CMP_I]] to <4 x i32>
// X86-NEXT:    [[TMP6:%.*]] = bitcast <4 x i32> [[SEXT_I]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP6]]
//
__m128i test_mm_cmpgt_epi32(__m128i A, __m128i B) {
  return _mm_cmpgt_epi32(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmpgt_pd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = fcmp olt <2 x double> [[TMP2]], [[TMP3]]
// X86-NEXT:    [[TMP5:%.*]] = sext <2 x i1> [[TMP4]] to <2 x i64>
// X86-NEXT:    [[TMP6:%.*]] = bitcast <2 x i64> [[TMP5]] to <2 x double>
// X86-NEXT:    store <2 x double> [[TMP6]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP7:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP7]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP8:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP8]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP9:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP9]]
//
__m128d test_mm_cmpgt_pd(__m128d A, __m128d B) {
  return _mm_cmpgt_pd(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmpgt_sd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__C_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = call <2 x double> @llvm.x86.sse2.cmp.sd(<2 x double> [[TMP2]], <2 x double> [[TMP3]], i8 1)
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[__C_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x double>, ptr [[__C_I]], align 16
// X86-NEXT:    [[VECEXT_I:%.*]] = extractelement <2 x double> [[TMP5]], i32 0
// X86-NEXT:    [[VECINIT_I:%.*]] = insertelement <2 x double> poison, double [[VECEXT_I]], i32 0
// X86-NEXT:    [[TMP6:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[VECEXT1_I:%.*]] = extractelement <2 x double> [[TMP6]], i32 1
// X86-NEXT:    [[VECINIT2_I:%.*]] = insertelement <2 x double> [[VECINIT_I]], double [[VECEXT1_I]], i32 1
// X86-NEXT:    store <2 x double> [[VECINIT2_I]], ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    [[TMP7:%.*]] = load <2 x double>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP7]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP8:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP8]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP9:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP9]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP10:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP10]]
//
__m128d test_mm_cmpgt_sd(__m128d A, __m128d B) {
  return _mm_cmpgt_sd(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmple_pd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = fcmp ole <2 x double> [[TMP2]], [[TMP3]]
// X86-NEXT:    [[TMP5:%.*]] = sext <2 x i1> [[TMP4]] to <2 x i64>
// X86-NEXT:    [[TMP6:%.*]] = bitcast <2 x i64> [[TMP5]] to <2 x double>
// X86-NEXT:    store <2 x double> [[TMP6]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP7:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP7]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP8:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP8]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP9:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP9]]
//
__m128d test_mm_cmple_pd(__m128d A, __m128d B) {
  return _mm_cmple_pd(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmple_sd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = call <2 x double> @llvm.x86.sse2.cmp.sd(<2 x double> [[TMP2]], <2 x double> [[TMP3]], i8 2)
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP5]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP6:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP6]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP7:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP7]]
//
__m128d test_mm_cmple_sd(__m128d A, __m128d B) {
  return _mm_cmple_sd(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmplt_epi8(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__B_ADDR_I_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP2]], ptr [[__A_ADDR_I_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP3]], ptr [[__B_ADDR_I_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = bitcast <2 x i64> [[TMP4]] to <16 x i8>
// X86-NEXT:    [[TMP6:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I_I]], align 16
// X86-NEXT:    [[TMP7:%.*]] = bitcast <2 x i64> [[TMP6]] to <16 x i8>
// X86-NEXT:    [[CMP_I_I:%.*]] = icmp sgt <16 x i8> [[TMP5]], [[TMP7]]
// X86-NEXT:    [[SEXT_I_I:%.*]] = sext <16 x i1> [[CMP_I_I]] to <16 x i8>
// X86-NEXT:    [[TMP8:%.*]] = bitcast <16 x i8> [[SEXT_I_I]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP8]]
//
__m128i test_mm_cmplt_epi8(__m128i A, __m128i B) {
  return _mm_cmplt_epi8(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmplt_epi16(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__B_ADDR_I_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP2]], ptr [[__A_ADDR_I_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP3]], ptr [[__B_ADDR_I_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = bitcast <2 x i64> [[TMP4]] to <8 x i16>
// X86-NEXT:    [[TMP6:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I_I]], align 16
// X86-NEXT:    [[TMP7:%.*]] = bitcast <2 x i64> [[TMP6]] to <8 x i16>
// X86-NEXT:    [[CMP_I_I:%.*]] = icmp sgt <8 x i16> [[TMP5]], [[TMP7]]
// X86-NEXT:    [[SEXT_I_I:%.*]] = sext <8 x i1> [[CMP_I_I]] to <8 x i16>
// X86-NEXT:    [[TMP8:%.*]] = bitcast <8 x i16> [[SEXT_I_I]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP8]]
//
__m128i test_mm_cmplt_epi16(__m128i A, __m128i B) {
  return _mm_cmplt_epi16(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmplt_epi32(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__B_ADDR_I_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP2]], ptr [[__A_ADDR_I_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP3]], ptr [[__B_ADDR_I_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = bitcast <2 x i64> [[TMP4]] to <4 x i32>
// X86-NEXT:    [[TMP6:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I_I]], align 16
// X86-NEXT:    [[TMP7:%.*]] = bitcast <2 x i64> [[TMP6]] to <4 x i32>
// X86-NEXT:    [[CMP_I_I:%.*]] = icmp sgt <4 x i32> [[TMP5]], [[TMP7]]
// X86-NEXT:    [[SEXT_I_I:%.*]] = sext <4 x i1> [[CMP_I_I]] to <4 x i32>
// X86-NEXT:    [[TMP8:%.*]] = bitcast <4 x i32> [[SEXT_I_I]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP8]]
//
__m128i test_mm_cmplt_epi32(__m128i A, __m128i B) {
  return _mm_cmplt_epi32(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmplt_pd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = fcmp olt <2 x double> [[TMP2]], [[TMP3]]
// X86-NEXT:    [[TMP5:%.*]] = sext <2 x i1> [[TMP4]] to <2 x i64>
// X86-NEXT:    [[TMP6:%.*]] = bitcast <2 x i64> [[TMP5]] to <2 x double>
// X86-NEXT:    store <2 x double> [[TMP6]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP7:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP7]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP8:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP8]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP9:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP9]]
//
__m128d test_mm_cmplt_pd(__m128d A, __m128d B) {
  return _mm_cmplt_pd(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmplt_sd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = call <2 x double> @llvm.x86.sse2.cmp.sd(<2 x double> [[TMP2]], <2 x double> [[TMP3]], i8 1)
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP5]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP6:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP6]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP7:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP7]]
//
__m128d test_mm_cmplt_sd(__m128d A, __m128d B) {
  return _mm_cmplt_sd(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmpneq_pd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = fcmp une <2 x double> [[TMP2]], [[TMP3]]
// X86-NEXT:    [[TMP5:%.*]] = sext <2 x i1> [[TMP4]] to <2 x i64>
// X86-NEXT:    [[TMP6:%.*]] = bitcast <2 x i64> [[TMP5]] to <2 x double>
// X86-NEXT:    store <2 x double> [[TMP6]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP7:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP7]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP8:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP8]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP9:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP9]]
//
__m128d test_mm_cmpneq_pd(__m128d A, __m128d B) {
  return _mm_cmpneq_pd(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmpneq_sd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = call <2 x double> @llvm.x86.sse2.cmp.sd(<2 x double> [[TMP2]], <2 x double> [[TMP3]], i8 4)
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP5]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP6:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP6]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP7:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP7]]
//
__m128d test_mm_cmpneq_sd(__m128d A, __m128d B) {
  return _mm_cmpneq_sd(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmpnge_pd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = fcmp ugt <2 x double> [[TMP2]], [[TMP3]]
// X86-NEXT:    [[TMP5:%.*]] = sext <2 x i1> [[TMP4]] to <2 x i64>
// X86-NEXT:    [[TMP6:%.*]] = bitcast <2 x i64> [[TMP5]] to <2 x double>
// X86-NEXT:    store <2 x double> [[TMP6]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP7:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP7]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP8:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP8]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP9:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP9]]
//
__m128d test_mm_cmpnge_pd(__m128d A, __m128d B) {
  return _mm_cmpnge_pd(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmpnge_sd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__C_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = call <2 x double> @llvm.x86.sse2.cmp.sd(<2 x double> [[TMP2]], <2 x double> [[TMP3]], i8 6)
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[__C_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x double>, ptr [[__C_I]], align 16
// X86-NEXT:    [[VECEXT_I:%.*]] = extractelement <2 x double> [[TMP5]], i32 0
// X86-NEXT:    [[VECINIT_I:%.*]] = insertelement <2 x double> poison, double [[VECEXT_I]], i32 0
// X86-NEXT:    [[TMP6:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[VECEXT1_I:%.*]] = extractelement <2 x double> [[TMP6]], i32 1
// X86-NEXT:    [[VECINIT2_I:%.*]] = insertelement <2 x double> [[VECINIT_I]], double [[VECEXT1_I]], i32 1
// X86-NEXT:    store <2 x double> [[VECINIT2_I]], ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    [[TMP7:%.*]] = load <2 x double>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP7]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP8:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP8]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP9:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP9]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP10:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP10]]
//
__m128d test_mm_cmpnge_sd(__m128d A, __m128d B) {
  return _mm_cmpnge_sd(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmpngt_pd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = fcmp uge <2 x double> [[TMP2]], [[TMP3]]
// X86-NEXT:    [[TMP5:%.*]] = sext <2 x i1> [[TMP4]] to <2 x i64>
// X86-NEXT:    [[TMP6:%.*]] = bitcast <2 x i64> [[TMP5]] to <2 x double>
// X86-NEXT:    store <2 x double> [[TMP6]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP7:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP7]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP8:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP8]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP9:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP9]]
//
__m128d test_mm_cmpngt_pd(__m128d A, __m128d B) {
  return _mm_cmpngt_pd(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmpngt_sd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__C_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = call <2 x double> @llvm.x86.sse2.cmp.sd(<2 x double> [[TMP2]], <2 x double> [[TMP3]], i8 5)
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[__C_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x double>, ptr [[__C_I]], align 16
// X86-NEXT:    [[VECEXT_I:%.*]] = extractelement <2 x double> [[TMP5]], i32 0
// X86-NEXT:    [[VECINIT_I:%.*]] = insertelement <2 x double> poison, double [[VECEXT_I]], i32 0
// X86-NEXT:    [[TMP6:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[VECEXT1_I:%.*]] = extractelement <2 x double> [[TMP6]], i32 1
// X86-NEXT:    [[VECINIT2_I:%.*]] = insertelement <2 x double> [[VECINIT_I]], double [[VECEXT1_I]], i32 1
// X86-NEXT:    store <2 x double> [[VECINIT2_I]], ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    [[TMP7:%.*]] = load <2 x double>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP7]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP8:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP8]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP9:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP9]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP10:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP10]]
//
__m128d test_mm_cmpngt_sd(__m128d A, __m128d B) {
  return _mm_cmpngt_sd(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmpnle_pd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = fcmp ugt <2 x double> [[TMP2]], [[TMP3]]
// X86-NEXT:    [[TMP5:%.*]] = sext <2 x i1> [[TMP4]] to <2 x i64>
// X86-NEXT:    [[TMP6:%.*]] = bitcast <2 x i64> [[TMP5]] to <2 x double>
// X86-NEXT:    store <2 x double> [[TMP6]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP7:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP7]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP8:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP8]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP9:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP9]]
//
__m128d test_mm_cmpnle_pd(__m128d A, __m128d B) {
  return _mm_cmpnle_pd(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmpnle_sd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = call <2 x double> @llvm.x86.sse2.cmp.sd(<2 x double> [[TMP2]], <2 x double> [[TMP3]], i8 6)
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP5]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP6:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP6]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP7:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP7]]
//
__m128d test_mm_cmpnle_sd(__m128d A, __m128d B) {
  return _mm_cmpnle_sd(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmpnlt_pd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = fcmp uge <2 x double> [[TMP2]], [[TMP3]]
// X86-NEXT:    [[TMP5:%.*]] = sext <2 x i1> [[TMP4]] to <2 x i64>
// X86-NEXT:    [[TMP6:%.*]] = bitcast <2 x i64> [[TMP5]] to <2 x double>
// X86-NEXT:    store <2 x double> [[TMP6]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP7:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP7]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP8:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP8]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP9:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP9]]
//
__m128d test_mm_cmpnlt_pd(__m128d A, __m128d B) {
  return _mm_cmpnlt_pd(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmpnlt_sd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = call <2 x double> @llvm.x86.sse2.cmp.sd(<2 x double> [[TMP2]], <2 x double> [[TMP3]], i8 5)
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP5]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP6:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP6]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP7:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP7]]
//
__m128d test_mm_cmpnlt_sd(__m128d A, __m128d B) {
  return _mm_cmpnlt_sd(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmpord_pd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = fcmp ord <2 x double> [[TMP2]], [[TMP3]]
// X86-NEXT:    [[TMP5:%.*]] = sext <2 x i1> [[TMP4]] to <2 x i64>
// X86-NEXT:    [[TMP6:%.*]] = bitcast <2 x i64> [[TMP5]] to <2 x double>
// X86-NEXT:    store <2 x double> [[TMP6]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP7:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP7]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP8:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP8]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP9:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP9]]
//
__m128d test_mm_cmpord_pd(__m128d A, __m128d B) {
  return _mm_cmpord_pd(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmpord_sd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = call <2 x double> @llvm.x86.sse2.cmp.sd(<2 x double> [[TMP2]], <2 x double> [[TMP3]], i8 7)
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP5]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP6:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP6]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP7:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP7]]
//
__m128d test_mm_cmpord_sd(__m128d A, __m128d B) {
  return _mm_cmpord_sd(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmpunord_pd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = fcmp uno <2 x double> [[TMP2]], [[TMP3]]
// X86-NEXT:    [[TMP5:%.*]] = sext <2 x i1> [[TMP4]] to <2 x i64>
// X86-NEXT:    [[TMP6:%.*]] = bitcast <2 x i64> [[TMP5]] to <2 x double>
// X86-NEXT:    store <2 x double> [[TMP6]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP7:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP7]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP8:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP8]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP9:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP9]]
//
__m128d test_mm_cmpunord_pd(__m128d A, __m128d B) {
  return _mm_cmpunord_pd(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmpunord_sd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = call <2 x double> @llvm.x86.sse2.cmp.sd(<2 x double> [[TMP2]], <2 x double> [[TMP3]], i8 3)
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP5]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP6:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP6]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP7:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP7]]
//
__m128d test_mm_cmpunord_sd(__m128d A, __m128d B) {
  return _mm_cmpunord_sd(A, B);
}

//
// X86-LABEL: define i32 @test_mm_comieq_sd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = call i32 @llvm.x86.sse2.comieq.sd(<2 x double> [[TMP2]], <2 x double> [[TMP3]])
// X86-NEXT:    ret i32 [[TMP4]]
//
int test_mm_comieq_sd(__m128d A, __m128d B) {
  return _mm_comieq_sd(A, B);
}

//
// X86-LABEL: define i32 @test_mm_comige_sd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = call i32 @llvm.x86.sse2.comige.sd(<2 x double> [[TMP2]], <2 x double> [[TMP3]])
// X86-NEXT:    ret i32 [[TMP4]]
//
int test_mm_comige_sd(__m128d A, __m128d B) {
  return _mm_comige_sd(A, B);
}

//
// X86-LABEL: define i32 @test_mm_comigt_sd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = call i32 @llvm.x86.sse2.comigt.sd(<2 x double> [[TMP2]], <2 x double> [[TMP3]])
// X86-NEXT:    ret i32 [[TMP4]]
//
int test_mm_comigt_sd(__m128d A, __m128d B) {
  return _mm_comigt_sd(A, B);
}

//
// X86-LABEL: define i32 @test_mm_comile_sd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = call i32 @llvm.x86.sse2.comile.sd(<2 x double> [[TMP2]], <2 x double> [[TMP3]])
// X86-NEXT:    ret i32 [[TMP4]]
//
int test_mm_comile_sd(__m128d A, __m128d B) {
  return _mm_comile_sd(A, B);
}

//
// X86-LABEL: define i32 @test_mm_comilt_sd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = call i32 @llvm.x86.sse2.comilt.sd(<2 x double> [[TMP2]], <2 x double> [[TMP3]])
// X86-NEXT:    ret i32 [[TMP4]]
//
int test_mm_comilt_sd(__m128d A, __m128d B) {
  return _mm_comilt_sd(A, B);
}

//
// X86-LABEL: define i32 @test_mm_comineq_sd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = call i32 @llvm.x86.sse2.comineq.sd(<2 x double> [[TMP2]], <2 x double> [[TMP3]])
// X86-NEXT:    ret i32 [[TMP4]]
//
int test_mm_comineq_sd(__m128d A, __m128d B) {
  return _mm_comineq_sd(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cvtepi32_pd(
// X86-SAME: <2 x i64> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = bitcast <2 x i64> [[TMP1]] to <4 x i32>
// X86-NEXT:    [[TMP3:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = bitcast <2 x i64> [[TMP3]] to <4 x i32>
// X86-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <4 x i32> [[TMP2]], <4 x i32> [[TMP4]], <2 x i32> <i32 0, i32 1>
// X86-NEXT:    [[CONV_I:%.*]] = sitofp <2 x i32> [[SHUFFLE_I]] to <2 x double>
// X86-NEXT:    store <2 x double> [[CONV_I]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP5]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP6:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP6]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP7:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP7]]
//
__m128d test_mm_cvtepi32_pd(__m128i A) {
  return _mm_cvtepi32_pd(A);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cvtepi32_ps(
// X86-SAME: <2 x i64> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = bitcast <2 x i64> [[TMP1]] to <4 x i32>
// X86-NEXT:    [[CONV_I:%.*]] = sitofp <4 x i32> [[TMP2]] to <4 x float>
// X86-NEXT:    store <4 x float> [[CONV_I]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP3]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP4:%.*]] = load <4 x float>, ptr [[COERCE]], align 16
// X86-NEXT:    store <4 x float> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128 test_mm_cvtepi32_ps(__m128i A) {
  return _mm_cvtepi32_ps(A);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cvtpd_epi32(
// X86-SAME: <2 x double> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = call <4 x i32> @llvm.x86.sse2.cvtpd2dq(<2 x double> [[TMP1]])
// X86-NEXT:    [[TMP3:%.*]] = bitcast <4 x i32> [[TMP2]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP3]]
//
__m128i test_mm_cvtpd_epi32(__m128d A) {
  return _mm_cvtpd_epi32(A);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cvtpd_ps(
// X86-SAME: <2 x double> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = call <4 x float> @llvm.x86.sse2.cvtpd2ps(<2 x double> [[TMP1]])
// X86-NEXT:    store <4 x float> [[TMP2]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP3]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP4:%.*]] = load <4 x float>, ptr [[COERCE]], align 16
// X86-NEXT:    store <4 x float> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128 test_mm_cvtpd_ps(__m128d A) {
  return _mm_cvtpd_ps(A);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cvtps_epi32(
// X86-SAME: <4 x float> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// X86-NEXT:    store <4 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = call <4 x i32> @llvm.x86.sse2.cvtps2dq(<4 x float> [[TMP1]])
// X86-NEXT:    [[TMP3:%.*]] = bitcast <4 x i32> [[TMP2]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP3]]
//
__m128i test_mm_cvtps_epi32(__m128 A) {
  return _mm_cvtps_epi32(A);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cvtps_pd(
// X86-SAME: <4 x float> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// X86-NEXT:    store <4 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <4 x float>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <4 x float> [[TMP1]], <4 x float> [[TMP2]], <2 x i32> <i32 0, i32 1>
// X86-NEXT:    [[CONV_I:%.*]] = fpext <2 x float> [[SHUFFLE_I]] to <2 x double>
// X86-NEXT:    store <2 x double> [[CONV_I]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP3]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP4:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128d test_mm_cvtps_pd(__m128 A) {
  return _mm_cvtps_pd(A);
}

//
// X86-LABEL: define double @test_mm_cvtsd_f64(
// X86-SAME: <2 x double> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[VECEXT_I:%.*]] = extractelement <2 x double> [[TMP1]], i32 0
// X86-NEXT:    ret double [[VECEXT_I]]
//
double test_mm_cvtsd_f64(__m128d A) {
  return _mm_cvtsd_f64(A);
}

//
// X86-LABEL: define i32 @test_mm_cvtsd_si32(
// X86-SAME: <2 x double> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = call i32 @llvm.x86.sse2.cvtsd2si(<2 x double> [[TMP1]])
// X86-NEXT:    ret i32 [[TMP2]]
//
int test_mm_cvtsd_si32(__m128d A) {
  return _mm_cvtsd_si32(A);
}

#ifdef __x86_64__
//
long long test_mm_cvtsd_si64(__m128d A) {
  return _mm_cvtsd_si64(A);
}
#endif

//
// X86-LABEL: define <2 x i64> @test_mm_cvtsd_ss(
// X86-SAME: <4 x float> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <4 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <4 x float>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = call <4 x float> @llvm.x86.sse2.cvtsd2ss(<4 x float> [[TMP2]], <2 x double> [[TMP3]])
// X86-NEXT:    store <4 x float> [[TMP4]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP5]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP6:%.*]] = load <4 x float>, ptr [[COERCE]], align 16
// X86-NEXT:    store <4 x float> [[TMP6]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP7:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP7]]
//
__m128 test_mm_cvtsd_ss(__m128 A, __m128d B) {
  return _mm_cvtsd_ss(A, B);
}

//
// X86-LABEL: define i32 @test_mm_cvtsi128_si32(
// X86-SAME: <2 x i64> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__B_I:%.*]] = alloca <4 x i32>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = bitcast <2 x i64> [[TMP1]] to <4 x i32>
// X86-NEXT:    store <4 x i32> [[TMP2]], ptr [[__B_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <4 x i32>, ptr [[__B_I]], align 16
// X86-NEXT:    [[VECEXT_I:%.*]] = extractelement <4 x i32> [[TMP3]], i32 0
// X86-NEXT:    ret i32 [[VECEXT_I]]
//
int test_mm_cvtsi128_si32(__m128i A) {
  return _mm_cvtsi128_si32(A);
}

//
// X86-LABEL: define i64 @test_mm_cvtsi128_si64(
// X86-SAME: <2 x i64> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[VECEXT_I:%.*]] = extractelement <2 x i64> [[TMP1]], i32 0
// X86-NEXT:    ret i64 [[VECEXT_I]]
//
long long test_mm_cvtsi128_si64(__m128i A) {
  return _mm_cvtsi128_si64(A);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cvtsi32_sd(
// X86-SAME: <2 x double> noundef [[A:%.*]], i32 noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store i32 [[B]], ptr [[B_ADDR]], align 4
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store i32 [[TMP1]], ptr [[__B_ADDR_I]], align 4
// X86-NEXT:    [[TMP2:%.*]] = load i32, ptr [[__B_ADDR_I]], align 4
// X86-NEXT:    [[CONV_I:%.*]] = sitofp i32 [[TMP2]] to double
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[VECINS_I:%.*]] = insertelement <2 x double> [[TMP3]], double [[CONV_I]], i32 0
// X86-NEXT:    store <2 x double> [[VECINS_I]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP5]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP6:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP6]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP7:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP7]]
//
__m128d test_mm_cvtsi32_sd(__m128d A, int B) {
  return _mm_cvtsi32_sd(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cvtsi32_si128(
// X86-SAME: i32 noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <4 x i32>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// X86-NEXT:    store i32 [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// X86-NEXT:    store i32 [[TMP0]], ptr [[__A_ADDR_I]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load i32, ptr [[__A_ADDR_I]], align 4
// X86-NEXT:    [[VECINIT_I:%.*]] = insertelement <4 x i32> poison, i32 [[TMP1]], i32 0
// X86-NEXT:    [[VECINIT1_I:%.*]] = insertelement <4 x i32> [[VECINIT_I]], i32 0, i32 1
// X86-NEXT:    [[VECINIT2_I:%.*]] = insertelement <4 x i32> [[VECINIT1_I]], i32 0, i32 2
// X86-NEXT:    [[VECINIT3_I:%.*]] = insertelement <4 x i32> [[VECINIT2_I]], i32 0, i32 3
// X86-NEXT:    store <4 x i32> [[VECINIT3_I]], ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <4 x i32>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <4 x i32> [[TMP2]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP3]]
//
__m128i test_mm_cvtsi32_si128(int A) {
  return _mm_cvtsi32_si128(A);
}

#ifdef __x86_64__
//
__m128d test_mm_cvtsi64_sd(__m128d A, long long B) {
  return _mm_cvtsi64_sd(A, B);
}
#endif

//
// X86-LABEL: define <2 x i64> @test_mm_cvtsi64_si128(
// X86-SAME: i64 noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca i64, align 8
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca i64, align 8
// X86-NEXT:    store i64 [[A]], ptr [[A_ADDR]], align 8
// X86-NEXT:    [[TMP0:%.*]] = load i64, ptr [[A_ADDR]], align 8
// X86-NEXT:    store i64 [[TMP0]], ptr [[__A_ADDR_I]], align 8
// X86-NEXT:    [[TMP1:%.*]] = load i64, ptr [[__A_ADDR_I]], align 8
// X86-NEXT:    [[VECINIT_I:%.*]] = insertelement <2 x i64> poison, i64 [[TMP1]], i32 0
// X86-NEXT:    [[VECINIT1_I:%.*]] = insertelement <2 x i64> [[VECINIT_I]], i64 0, i32 1
// X86-NEXT:    store <2 x i64> [[VECINIT1_I]], ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP2]]
//
__m128i test_mm_cvtsi64_si128(long long A) {
  return _mm_cvtsi64_si128(A);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cvtss_sd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <4 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <4 x float> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <4 x float> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <4 x float>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[VECEXT_I:%.*]] = extractelement <4 x float> [[TMP2]], i32 0
// X86-NEXT:    [[CONV_I:%.*]] = fpext float [[VECEXT_I]] to double
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[VECINS_I:%.*]] = insertelement <2 x double> [[TMP3]], double [[CONV_I]], i32 0
// X86-NEXT:    store <2 x double> [[VECINS_I]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP5]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP6:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP6]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP7:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP7]]
//
__m128d test_mm_cvtss_sd(__m128d A, __m128 B) {
  return _mm_cvtss_sd(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cvttpd_epi32(
// X86-SAME: <2 x double> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = call <4 x i32> @llvm.x86.sse2.cvttpd2dq(<2 x double> [[TMP1]])
// X86-NEXT:    [[TMP3:%.*]] = bitcast <4 x i32> [[TMP2]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP3]]
//
__m128i test_mm_cvttpd_epi32(__m128d A) {
  return _mm_cvttpd_epi32(A);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cvttps_epi32(
// X86-SAME: <4 x float> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// X86-NEXT:    store <4 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = call <4 x i32> @llvm.x86.sse2.cvttps2dq(<4 x float> [[TMP1]])
// X86-NEXT:    [[TMP3:%.*]] = bitcast <4 x i32> [[TMP2]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP3]]
//
__m128i test_mm_cvttps_epi32(__m128 A) {
  return _mm_cvttps_epi32(A);
}

//
// X86-LABEL: define i32 @test_mm_cvttsd_si32(
// X86-SAME: <2 x double> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = call i32 @llvm.x86.sse2.cvttsd2si(<2 x double> [[TMP1]])
// X86-NEXT:    ret i32 [[TMP2]]
//
int test_mm_cvttsd_si32(__m128d A) {
  return _mm_cvttsd_si32(A);
}

#ifdef __x86_64__
//
long long test_mm_cvttsd_si64(__m128d A) {
  return _mm_cvttsd_si64(A);
}
#endif

//
// X86-LABEL: define <2 x i64> @test_mm_div_pd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[DIV_I:%.*]] = fdiv <2 x double> [[TMP2]], [[TMP3]]
// X86-NEXT:    store <2 x double> [[DIV_I]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP4]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP5]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP6:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP6]]
//
__m128d test_mm_div_pd(__m128d A, __m128d B) {
  return _mm_div_pd(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_div_sd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[VECEXT_I:%.*]] = extractelement <2 x double> [[TMP2]], i32 0
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[VECEXT1_I:%.*]] = extractelement <2 x double> [[TMP3]], i32 0
// X86-NEXT:    [[DIV_I:%.*]] = fdiv double [[VECEXT1_I]], [[VECEXT_I]]
// X86-NEXT:    [[TMP4:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[VECINS_I:%.*]] = insertelement <2 x double> [[TMP4]], double [[DIV_I]], i32 0
// X86-NEXT:    store <2 x double> [[VECINS_I]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP5]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP6:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP6]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP7:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP7]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP8:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP8]]
//
__m128d test_mm_div_sd(__m128d A, __m128d B) {
  return _mm_div_sd(A, B);
}

// Lowering to pextrw requires optimization.
//
// X86-LABEL: define i32 @test_mm_extract_epi16(
// X86-SAME: <2 x i64> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = bitcast <2 x i64> [[TMP0]] to <8 x i16>
// X86-NEXT:    [[TMP2:%.*]] = extractelement <8 x i16> [[TMP1]], i64 1
// X86-NEXT:    [[CONV:%.*]] = zext i16 [[TMP2]] to i32
// X86-NEXT:    ret i32 [[CONV]]
//
int test_mm_extract_epi16(__m128i A) {
  return _mm_extract_epi16(A, 1);
}

//
// X86-LABEL: define <2 x i64> @test_mm_insert_epi16(
// X86-SAME: <2 x i64> noundef [[A:%.*]], i32 noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store i32 [[B]], ptr [[B_ADDR]], align 4
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = bitcast <2 x i64> [[TMP0]] to <8 x i16>
// X86-NEXT:    [[TMP2:%.*]] = load i32, ptr [[B_ADDR]], align 4
// X86-NEXT:    [[CONV:%.*]] = trunc i32 [[TMP2]] to i16
// X86-NEXT:    [[TMP3:%.*]] = insertelement <8 x i16> [[TMP1]], i16 [[CONV]], i64 0
// X86-NEXT:    [[TMP4:%.*]] = bitcast <8 x i16> [[TMP3]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP4]]
//
__m128i test_mm_insert_epi16(__m128i A, int B) {
  return _mm_insert_epi16(A, B, 0);
}

//
// X86-LABEL: define void @test_mm_lfence(
// X86-SAME: ) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    call void @llvm.x86.sse2.lfence()
// X86-NEXT:    ret void
//
void test_mm_lfence(void) {
  _mm_lfence();
}

//
// X86-LABEL: define <2 x i64> @test_mm_load_pd(
// X86-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__DP_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    store ptr [[TMP0]], ptr [[__DP_ADDR_I]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[__DP_ADDR_I]], align 4
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[TMP1]], align 16
// X86-NEXT:    store <2 x double> [[TMP2]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP3]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP4:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128d test_mm_load_pd(double const* A) {
  return _mm_load_pd(A);
}

//
// X86-LABEL: define <2 x i64> @test_mm_load_pd1(
// X86-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__DP_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__U_I:%.*]] = alloca double, align 8
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    store ptr [[TMP0]], ptr [[__DP_ADDR_I]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[__DP_ADDR_I]], align 4
// X86-NEXT:    [[TMP2:%.*]] = load double, ptr [[TMP1]], align 1
// X86-NEXT:    store double [[TMP2]], ptr [[__U_I]], align 8
// X86-NEXT:    [[TMP3:%.*]] = load double, ptr [[__U_I]], align 8
// X86-NEXT:    [[VECINIT_I:%.*]] = insertelement <2 x double> poison, double [[TMP3]], i32 0
// X86-NEXT:    [[TMP4:%.*]] = load double, ptr [[__U_I]], align 8
// X86-NEXT:    [[VECINIT2_I:%.*]] = insertelement <2 x double> [[VECINIT_I]], double [[TMP4]], i32 1
// X86-NEXT:    store <2 x double> [[VECINIT2_I]], ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x double>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP5]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP6:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP6]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP7:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP7]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP8:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP8]]
//
__m128d test_mm_load_pd1(double const* A) {
  return _mm_load_pd1(A);
}

//
// X86-LABEL: define <2 x i64> @test_mm_load_sd(
// X86-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__DP_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__U_I:%.*]] = alloca double, align 8
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    store ptr [[TMP0]], ptr [[__DP_ADDR_I]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[__DP_ADDR_I]], align 4
// X86-NEXT:    [[TMP2:%.*]] = load double, ptr [[TMP1]], align 1
// X86-NEXT:    store double [[TMP2]], ptr [[__U_I]], align 8
// X86-NEXT:    [[TMP3:%.*]] = load double, ptr [[__U_I]], align 8
// X86-NEXT:    [[VECINIT_I:%.*]] = insertelement <2 x double> poison, double [[TMP3]], i32 0
// X86-NEXT:    [[VECINIT2_I:%.*]] = insertelement <2 x double> [[VECINIT_I]], double 0.000000e+00, i32 1
// X86-NEXT:    store <2 x double> [[VECINIT2_I]], ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = load <2 x double>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP5]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP6:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP6]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP7:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP7]]
//
__m128d test_mm_load_sd(double const* A) {
  return _mm_load_sd(A);
}

//
// X86-LABEL: define <2 x i64> @test_mm_load_si128(
// X86-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__P_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    store ptr [[TMP0]], ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[TMP1]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP2]]
//
__m128i test_mm_load_si128(__m128i const* A) {
  return _mm_load_si128(A);
}

//
// X86-LABEL: define <2 x i64> @test_mm_load1_pd(
// X86-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__DP_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__U_I:%.*]] = alloca double, align 8
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    store ptr [[TMP0]], ptr [[__DP_ADDR_I]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[__DP_ADDR_I]], align 4
// X86-NEXT:    [[TMP2:%.*]] = load double, ptr [[TMP1]], align 1
// X86-NEXT:    store double [[TMP2]], ptr [[__U_I]], align 8
// X86-NEXT:    [[TMP3:%.*]] = load double, ptr [[__U_I]], align 8
// X86-NEXT:    [[VECINIT_I:%.*]] = insertelement <2 x double> poison, double [[TMP3]], i32 0
// X86-NEXT:    [[TMP4:%.*]] = load double, ptr [[__U_I]], align 8
// X86-NEXT:    [[VECINIT2_I:%.*]] = insertelement <2 x double> [[VECINIT_I]], double [[TMP4]], i32 1
// X86-NEXT:    store <2 x double> [[VECINIT2_I]], ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x double>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP5]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP6:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP6]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP7:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP7]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP8:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP8]]
//
__m128d test_mm_load1_pd(double const* A) {
  return _mm_load1_pd(A);
}

//
// X86-LABEL: define <2 x i64> @test_mm_loadh_pd(
// X86-SAME: <2 x double> noundef [[X:%.*]], ptr noundef [[Y:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__DP_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__U_I:%.*]] = alloca double, align 8
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[X_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[Y_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[X]], ptr [[X_ADDR]], align 16
// X86-NEXT:    store ptr [[Y]], ptr [[Y_ADDR]], align 4
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[X_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[Y_ADDR]], align 4
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store ptr [[TMP1]], ptr [[__DP_ADDR_I]], align 4
// X86-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[__DP_ADDR_I]], align 4
// X86-NEXT:    [[TMP3:%.*]] = load double, ptr [[TMP2]], align 1
// X86-NEXT:    store double [[TMP3]], ptr [[__U_I]], align 8
// X86-NEXT:    [[TMP4:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[VECEXT_I:%.*]] = extractelement <2 x double> [[TMP4]], i32 0
// X86-NEXT:    [[VECINIT_I:%.*]] = insertelement <2 x double> poison, double [[VECEXT_I]], i32 0
// X86-NEXT:    [[TMP5:%.*]] = load double, ptr [[__U_I]], align 8
// X86-NEXT:    [[VECINIT2_I:%.*]] = insertelement <2 x double> [[VECINIT_I]], double [[TMP5]], i32 1
// X86-NEXT:    store <2 x double> [[VECINIT2_I]], ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    [[TMP6:%.*]] = load <2 x double>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP6]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP7:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP7]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP8:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP8]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP9:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP9]]
//
__m128d test_mm_loadh_pd(__m128d x, void* y) {
  return _mm_loadh_pd(x, y);
}

//
// X86-LABEL: define <2 x i64> @test_mm_loadl_epi64(
// X86-SAME: ptr noundef [[Y:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__P_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[Y_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    store ptr [[Y]], ptr [[Y_ADDR]], align 4
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[Y_ADDR]], align 4
// X86-NEXT:    store ptr [[TMP0]], ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    [[TMP2:%.*]] = load i64, ptr [[TMP1]], align 1
// X86-NEXT:    [[VECINIT_I:%.*]] = insertelement <2 x i64> poison, i64 [[TMP2]], i32 0
// X86-NEXT:    [[VECINIT1_I:%.*]] = insertelement <2 x i64> [[VECINIT_I]], i64 0, i32 1
// X86-NEXT:    store <2 x i64> [[VECINIT1_I]], ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x i64>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP3]]
//
__m128i test_mm_loadl_epi64(__m128i* y) {
  return _mm_loadl_epi64(y);
}

//
// X86-LABEL: define <2 x i64> @test_mm_loadl_pd(
// X86-SAME: <2 x double> noundef [[X:%.*]], ptr noundef [[Y:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__DP_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__U_I:%.*]] = alloca double, align 8
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[X_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[Y_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[X]], ptr [[X_ADDR]], align 16
// X86-NEXT:    store ptr [[Y]], ptr [[Y_ADDR]], align 4
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[X_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[Y_ADDR]], align 4
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store ptr [[TMP1]], ptr [[__DP_ADDR_I]], align 4
// X86-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[__DP_ADDR_I]], align 4
// X86-NEXT:    [[TMP3:%.*]] = load double, ptr [[TMP2]], align 1
// X86-NEXT:    store double [[TMP3]], ptr [[__U_I]], align 8
// X86-NEXT:    [[TMP4:%.*]] = load double, ptr [[__U_I]], align 8
// X86-NEXT:    [[VECINIT_I:%.*]] = insertelement <2 x double> poison, double [[TMP4]], i32 0
// X86-NEXT:    [[TMP5:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[VECEXT_I:%.*]] = extractelement <2 x double> [[TMP5]], i32 1
// X86-NEXT:    [[VECINIT2_I:%.*]] = insertelement <2 x double> [[VECINIT_I]], double [[VECEXT_I]], i32 1
// X86-NEXT:    store <2 x double> [[VECINIT2_I]], ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    [[TMP6:%.*]] = load <2 x double>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP6]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP7:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP7]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP8:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP8]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP9:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP9]]
//
__m128d test_mm_loadl_pd(__m128d x, void* y) {
  return _mm_loadl_pd(x, y);
}

//
// X86-LABEL: define <2 x i64> @test_mm_loadr_pd(
// X86-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__DP_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__U_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    store ptr [[TMP0]], ptr [[__DP_ADDR_I]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[__DP_ADDR_I]], align 4
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[TMP1]], align 16
// X86-NEXT:    store <2 x double> [[TMP2]], ptr [[__U_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__U_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = load <2 x double>, ptr [[__U_I]], align 16
// X86-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <2 x double> [[TMP3]], <2 x double> [[TMP4]], <2 x i32> <i32 1, i32 0>
// X86-NEXT:    store <2 x double> [[SHUFFLE_I]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP5]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP6:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP6]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP7:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP7]]
//
__m128d test_mm_loadr_pd(double const* A) {
  return _mm_loadr_pd(A);
}

//
// X86-LABEL: define <2 x i64> @test_mm_loadu_pd(
// X86-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__DP_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    store ptr [[TMP0]], ptr [[__DP_ADDR_I]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[__DP_ADDR_I]], align 4
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[TMP1]], align 1
// X86-NEXT:    store <2 x double> [[TMP2]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP3]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP4:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128d test_mm_loadu_pd(double const* A) {
  return _mm_loadu_pd(A);
}

//
// X86-LABEL: define <2 x i64> @test_mm_loadu_si128(
// X86-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__P_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    store ptr [[TMP0]], ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[TMP1]], align 1
// X86-NEXT:    ret <2 x i64> [[TMP2]]
//
__m128i test_mm_loadu_si128(__m128i const* A) {
  return _mm_loadu_si128(A);
}

//
// X86-LABEL: define <2 x i64> @test_mm_loadu_si64(
// X86-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__U_I:%.*]] = alloca i64, align 8
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    store ptr [[TMP0]], ptr [[__A_ADDR_I]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[__A_ADDR_I]], align 4
// X86-NEXT:    [[TMP2:%.*]] = load i64, ptr [[TMP1]], align 1
// X86-NEXT:    store i64 [[TMP2]], ptr [[__U_I]], align 8
// X86-NEXT:    [[TMP3:%.*]] = load i64, ptr [[__U_I]], align 8
// X86-NEXT:    [[VECINIT_I:%.*]] = insertelement <2 x i64> poison, i64 [[TMP3]], i32 0
// X86-NEXT:    [[VECINIT1_I:%.*]] = insertelement <2 x i64> [[VECINIT_I]], i64 0, i32 1
// X86-NEXT:    store <2 x i64> [[VECINIT1_I]], ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP4]]
//
__m128i test_mm_loadu_si64(void const* A) {
  return _mm_loadu_si64(A);
}

//
// X86-LABEL: define <2 x i64> @test_mm_loadu_si32(
// X86-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__U_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <4 x i32>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    store ptr [[TMP0]], ptr [[__A_ADDR_I]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[__A_ADDR_I]], align 4
// X86-NEXT:    [[TMP2:%.*]] = load i32, ptr [[TMP1]], align 1
// X86-NEXT:    store i32 [[TMP2]], ptr [[__U_I]], align 4
// X86-NEXT:    [[TMP3:%.*]] = load i32, ptr [[__U_I]], align 4
// X86-NEXT:    [[VECINIT_I:%.*]] = insertelement <4 x i32> poison, i32 [[TMP3]], i32 0
// X86-NEXT:    [[VECINIT1_I:%.*]] = insertelement <4 x i32> [[VECINIT_I]], i32 0, i32 1
// X86-NEXT:    [[VECINIT2_I:%.*]] = insertelement <4 x i32> [[VECINIT1_I]], i32 0, i32 2
// X86-NEXT:    [[VECINIT3_I:%.*]] = insertelement <4 x i32> [[VECINIT2_I]], i32 0, i32 3
// X86-NEXT:    store <4 x i32> [[VECINIT3_I]], ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = load <4 x i32>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = bitcast <4 x i32> [[TMP4]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128i test_mm_loadu_si32(void const* A) {
  return _mm_loadu_si32(A);
}

//
// X86-LABEL: define <2 x i64> @test_mm_loadu_si16(
// X86-SAME: ptr noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__U_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x i16>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    store ptr [[TMP0]], ptr [[__A_ADDR_I]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[__A_ADDR_I]], align 4
// X86-NEXT:    [[TMP2:%.*]] = load i16, ptr [[TMP1]], align 1
// X86-NEXT:    store i16 [[TMP2]], ptr [[__U_I]], align 2
// X86-NEXT:    [[TMP3:%.*]] = load i16, ptr [[__U_I]], align 2
// X86-NEXT:    [[VECINIT_I:%.*]] = insertelement <8 x i16> poison, i16 [[TMP3]], i32 0
// X86-NEXT:    [[VECINIT1_I:%.*]] = insertelement <8 x i16> [[VECINIT_I]], i16 0, i32 1
// X86-NEXT:    [[VECINIT2_I:%.*]] = insertelement <8 x i16> [[VECINIT1_I]], i16 0, i32 2
// X86-NEXT:    [[VECINIT3_I:%.*]] = insertelement <8 x i16> [[VECINIT2_I]], i16 0, i32 3
// X86-NEXT:    [[VECINIT4_I:%.*]] = insertelement <8 x i16> [[VECINIT3_I]], i16 0, i32 4
// X86-NEXT:    [[VECINIT5_I:%.*]] = insertelement <8 x i16> [[VECINIT4_I]], i16 0, i32 5
// X86-NEXT:    [[VECINIT6_I:%.*]] = insertelement <8 x i16> [[VECINIT5_I]], i16 0, i32 6
// X86-NEXT:    [[VECINIT7_I:%.*]] = insertelement <8 x i16> [[VECINIT6_I]], i16 0, i32 7
// X86-NEXT:    store <8 x i16> [[VECINIT7_I]], ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = load <8 x i16>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = bitcast <8 x i16> [[TMP4]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128i test_mm_loadu_si16(void const* A) {
  return _mm_loadu_si16(A);
}

//
// X86-LABEL: define <2 x i64> @test_mm_madd_epi16(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x i64> [[TMP2]] to <8 x i16>
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = bitcast <2 x i64> [[TMP4]] to <8 x i16>
// X86-NEXT:    [[TMP6:%.*]] = call <4 x i32> @llvm.x86.sse2.pmadd.wd(<8 x i16> [[TMP3]], <8 x i16> [[TMP5]])
// X86-NEXT:    [[TMP7:%.*]] = bitcast <4 x i32> [[TMP6]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP7]]
//
__m128i test_mm_madd_epi16(__m128i A, __m128i B) {
  return _mm_madd_epi16(A, B);
}

//
// X86-LABEL: define void @test_mm_maskmoveu_si128(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]], ptr noundef [[C:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__D_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__N_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__P_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[C_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    store ptr [[C]], ptr [[C_ADDR]], align 4
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[C_ADDR]], align 4
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__D_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__N_ADDR_I]], align 16
// X86-NEXT:    store ptr [[TMP2]], ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    [[TMP3:%.*]] = load <2 x i64>, ptr [[__D_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = bitcast <2 x i64> [[TMP3]] to <16 x i8>
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[__N_ADDR_I]], align 16
// X86-NEXT:    [[TMP6:%.*]] = bitcast <2 x i64> [[TMP5]] to <16 x i8>
// X86-NEXT:    [[TMP7:%.*]] = load ptr, ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    call void @llvm.x86.sse2.maskmov.dqu(<16 x i8> [[TMP4]], <16 x i8> [[TMP6]], ptr [[TMP7]])
// X86-NEXT:    ret void
//
void test_mm_maskmoveu_si128(__m128i A, __m128i B, char* C) {
  _mm_maskmoveu_si128(A, B, C);
}

//
// X86-LABEL: define <2 x i64> @test_mm_max_epi16(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x i64> [[TMP2]] to <8 x i16>
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = bitcast <2 x i64> [[TMP4]] to <8 x i16>
// X86-NEXT:    [[ELT_MAX_I:%.*]] = call <8 x i16> @llvm.smax.v8i16(<8 x i16> [[TMP3]], <8 x i16> [[TMP5]])
// X86-NEXT:    [[TMP6:%.*]] = bitcast <8 x i16> [[ELT_MAX_I]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP6]]
//
__m128i test_mm_max_epi16(__m128i A, __m128i B) {
  return _mm_max_epi16(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_max_epu8(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x i64> [[TMP2]] to <16 x i8>
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = bitcast <2 x i64> [[TMP4]] to <16 x i8>
// X86-NEXT:    [[ELT_MAX_I:%.*]] = call <16 x i8> @llvm.umax.v16i8(<16 x i8> [[TMP3]], <16 x i8> [[TMP5]])
// X86-NEXT:    [[TMP6:%.*]] = bitcast <16 x i8> [[ELT_MAX_I]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP6]]
//
__m128i test_mm_max_epu8(__m128i A, __m128i B) {
  return _mm_max_epu8(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_max_pd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = call <2 x double> @llvm.x86.sse2.max.pd(<2 x double> [[TMP2]], <2 x double> [[TMP3]])
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP5]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP6:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP6]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP7:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP7]]
//
__m128d test_mm_max_pd(__m128d A, __m128d B) {
  return _mm_max_pd(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_max_sd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = call <2 x double> @llvm.x86.sse2.max.sd(<2 x double> [[TMP2]], <2 x double> [[TMP3]])
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP5]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP6:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP6]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP7:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP7]]
//
__m128d test_mm_max_sd(__m128d A, __m128d B) {
  return _mm_max_sd(A, B);
}

//
// X86-LABEL: define void @test_mm_mfence(
// X86-SAME: ) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    call void @llvm.x86.sse2.mfence()
// X86-NEXT:    ret void
//
void test_mm_mfence(void) {
  _mm_mfence();
}

//
// X86-LABEL: define <2 x i64> @test_mm_min_epi16(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x i64> [[TMP2]] to <8 x i16>
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = bitcast <2 x i64> [[TMP4]] to <8 x i16>
// X86-NEXT:    [[ELT_MIN_I:%.*]] = call <8 x i16> @llvm.smin.v8i16(<8 x i16> [[TMP3]], <8 x i16> [[TMP5]])
// X86-NEXT:    [[TMP6:%.*]] = bitcast <8 x i16> [[ELT_MIN_I]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP6]]
//
__m128i test_mm_min_epi16(__m128i A, __m128i B) {
  return _mm_min_epi16(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_min_epu8(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x i64> [[TMP2]] to <16 x i8>
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = bitcast <2 x i64> [[TMP4]] to <16 x i8>
// X86-NEXT:    [[ELT_MIN_I:%.*]] = call <16 x i8> @llvm.umin.v16i8(<16 x i8> [[TMP3]], <16 x i8> [[TMP5]])
// X86-NEXT:    [[TMP6:%.*]] = bitcast <16 x i8> [[ELT_MIN_I]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP6]]
//
__m128i test_mm_min_epu8(__m128i A, __m128i B) {
  return _mm_min_epu8(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_min_pd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = call <2 x double> @llvm.x86.sse2.min.pd(<2 x double> [[TMP2]], <2 x double> [[TMP3]])
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP5]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP6:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP6]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP7:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP7]]
//
__m128d test_mm_min_pd(__m128d A, __m128d B) {
  return _mm_min_pd(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_min_sd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = call <2 x double> @llvm.x86.sse2.min.sd(<2 x double> [[TMP2]], <2 x double> [[TMP3]])
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP5]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP6:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP6]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP7:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP7]]
//
__m128d test_mm_min_sd(__m128d A, __m128d B) {
  return _mm_min_sd(A, B);
}

//
// X86-LABEL: define i64 @test_mm_movepi64_pi64(
// X86-SAME: <2 x i64> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <1 x i64>, align 8
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <1 x i64>, align 8
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <1 x i64>, align 8
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[VECEXT_I:%.*]] = extractelement <2 x i64> [[TMP1]], i32 0
// X86-NEXT:    [[TMP2:%.*]] = bitcast i64 [[VECEXT_I]] to <1 x i64>
// X86-NEXT:    store <1 x i64> [[TMP2]], ptr [[RETVAL_I]], align 8
// X86-NEXT:    [[TMP3:%.*]] = load i64, ptr [[RETVAL_I]], align 8
// X86-NEXT:    store i64 [[TMP3]], ptr [[COERCE]], align 8
// X86-NEXT:    [[TMP4:%.*]] = load <1 x i64>, ptr [[COERCE]], align 8
// X86-NEXT:    store <1 x i64> [[TMP4]], ptr [[RETVAL]], align 8
// X86-NEXT:    [[TMP5:%.*]] = load i64, ptr [[RETVAL]], align 8
// X86-NEXT:    ret i64 [[TMP5]]
//
__m64 test_mm_movepi64_pi64(__m128i A)
{
  return _mm_movepi64_pi64(A);
}

//
// X86-LABEL: define <2 x i64> @test_mm_movpi64_epi64(
// X86-SAME: i64 noundef [[A_COERCE:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_I:%.*]] = alloca <1 x i64>, align 8
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <1 x i64>, align 8
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A:%.*]] = alloca <1 x i64>, align 8
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <1 x i64>, align 8
// X86-NEXT:    [[COERCE:%.*]] = alloca <1 x i64>, align 8
// X86-NEXT:    store i64 [[A_COERCE]], ptr [[A]], align 8
// X86-NEXT:    [[A1:%.*]] = load <1 x i64>, ptr [[A]], align 8
// X86-NEXT:    store <1 x i64> [[A1]], ptr [[A_ADDR]], align 8
// X86-NEXT:    [[TMP0:%.*]] = load <1 x i64>, ptr [[A_ADDR]], align 8
// X86-NEXT:    store <1 x i64> [[TMP0]], ptr [[COERCE]], align 8
// X86-NEXT:    [[TMP1:%.*]] = load i64, ptr [[COERCE]], align 8
// X86-NEXT:    store i64 [[TMP1]], ptr [[__A_I]], align 8
// X86-NEXT:    [[__A1_I:%.*]] = load <1 x i64>, ptr [[__A_I]], align 8
// X86-NEXT:    store <1 x i64> [[__A1_I]], ptr [[__A_ADDR_I]], align 8
// X86-NEXT:    [[TMP2:%.*]] = load <1 x i64>, ptr [[__A_ADDR_I]], align 8
// X86-NEXT:    [[TMP3:%.*]] = bitcast <1 x i64> [[TMP2]] to i64
// X86-NEXT:    [[VECINIT_I:%.*]] = insertelement <2 x i64> poison, i64 [[TMP3]], i32 0
// X86-NEXT:    [[VECINIT2_I:%.*]] = insertelement <2 x i64> [[VECINIT_I]], i64 0, i32 1
// X86-NEXT:    store <2 x i64> [[VECINIT2_I]], ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP4]]
//
__m128i test_mm_movpi64_epi64(__m64 A)
{
  return _mm_movpi64_epi64(A);
}

//
// X86-LABEL: define <2 x i64> @test_mm_move_epi64(
// X86-SAME: <2 x i64> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <2 x i64> [[TMP1]], <2 x i64> [[TMP2]], <2 x i32> <i32 0, i32 2>
// X86-NEXT:    ret <2 x i64> [[SHUFFLE_I]]
//
__m128i test_mm_move_epi64(__m128i A) {
  return _mm_move_epi64(A);
}

//
// X86-LABEL: define <2 x i64> @test_mm_move_sd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[VECEXT_I:%.*]] = extractelement <2 x double> [[TMP2]], i32 0
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[VECINS_I:%.*]] = insertelement <2 x double> [[TMP3]], double [[VECEXT_I]], i32 0
// X86-NEXT:    store <2 x double> [[VECINS_I]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP5]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP6:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP6]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP7:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP7]]
//
__m128d test_mm_move_sd(__m128d A, __m128d B) {
  return _mm_move_sd(A, B);
}

//
// X86-LABEL: define i32 @test_mm_movemask_epi8(
// X86-SAME: <2 x i64> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = bitcast <2 x i64> [[TMP1]] to <16 x i8>
// X86-NEXT:    [[TMP3:%.*]] = call i32 @llvm.x86.sse2.pmovmskb.128(<16 x i8> [[TMP2]])
// X86-NEXT:    ret i32 [[TMP3]]
//
int test_mm_movemask_epi8(__m128i A) {
  return _mm_movemask_epi8(A);
}

//
// X86-LABEL: define i32 @test_mm_movemask_pd(
// X86-SAME: <2 x double> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = call i32 @llvm.x86.sse2.movmsk.pd(<2 x double> [[TMP1]])
// X86-NEXT:    ret i32 [[TMP2]]
//
int test_mm_movemask_pd(__m128d A) {
  return _mm_movemask_pd(A);
}

//
// X86-LABEL: define <2 x i64> @test_mm_mul_epu32(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x i64> [[TMP2]] to <4 x i32>
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = bitcast <2 x i64> [[TMP4]] to <4 x i32>
// X86-NEXT:    [[TMP6:%.*]] = and <2 x i64> [[TMP2]], <i64 4294967295, i64 4294967295>
// X86-NEXT:    [[TMP7:%.*]] = and <2 x i64> [[TMP4]], <i64 4294967295, i64 4294967295>
// X86-NEXT:    [[TMP8:%.*]] = mul <2 x i64> [[TMP6]], [[TMP7]]
// X86-NEXT:    ret <2 x i64> [[TMP8]]
//
__m128i test_mm_mul_epu32(__m128i A, __m128i B) {
  return _mm_mul_epu32(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_mul_pd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[MUL_I:%.*]] = fmul <2 x double> [[TMP2]], [[TMP3]]
// X86-NEXT:    store <2 x double> [[MUL_I]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP4]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP5]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP6:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP6]]
//
__m128d test_mm_mul_pd(__m128d A, __m128d B) {
  return _mm_mul_pd(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_mul_sd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[VECEXT_I:%.*]] = extractelement <2 x double> [[TMP2]], i32 0
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[VECEXT1_I:%.*]] = extractelement <2 x double> [[TMP3]], i32 0
// X86-NEXT:    [[MUL_I:%.*]] = fmul double [[VECEXT1_I]], [[VECEXT_I]]
// X86-NEXT:    [[TMP4:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[VECINS_I:%.*]] = insertelement <2 x double> [[TMP4]], double [[MUL_I]], i32 0
// X86-NEXT:    store <2 x double> [[VECINS_I]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP5]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP6:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP6]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP7:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP7]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP8:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP8]]
//
__m128d test_mm_mul_sd(__m128d A, __m128d B) {
  return _mm_mul_sd(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_mulhi_epi16(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x i64> [[TMP2]] to <8 x i16>
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = bitcast <2 x i64> [[TMP4]] to <8 x i16>
// X86-NEXT:    [[TMP6:%.*]] = call <8 x i16> @llvm.x86.sse2.pmulh.w(<8 x i16> [[TMP3]], <8 x i16> [[TMP5]])
// X86-NEXT:    [[TMP7:%.*]] = bitcast <8 x i16> [[TMP6]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP7]]
//
__m128i test_mm_mulhi_epi16(__m128i A, __m128i B) {
  return _mm_mulhi_epi16(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_mulhi_epu16(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x i64> [[TMP2]] to <8 x i16>
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = bitcast <2 x i64> [[TMP4]] to <8 x i16>
// X86-NEXT:    [[TMP6:%.*]] = call <8 x i16> @llvm.x86.sse2.pmulhu.w(<8 x i16> [[TMP3]], <8 x i16> [[TMP5]])
// X86-NEXT:    [[TMP7:%.*]] = bitcast <8 x i16> [[TMP6]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP7]]
//
__m128i test_mm_mulhi_epu16(__m128i A, __m128i B) {
  return _mm_mulhi_epu16(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_mullo_epi16(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x i64> [[TMP2]] to <8 x i16>
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = bitcast <2 x i64> [[TMP4]] to <8 x i16>
// X86-NEXT:    [[MUL_I:%.*]] = mul <8 x i16> [[TMP3]], [[TMP5]]
// X86-NEXT:    [[TMP6:%.*]] = bitcast <8 x i16> [[MUL_I]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP6]]
//
__m128i test_mm_mullo_epi16(__m128i A, __m128i B) {
  return _mm_mullo_epi16(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_or_pd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x double> [[TMP2]] to <2 x i64>
// X86-NEXT:    [[TMP4:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = bitcast <2 x double> [[TMP4]] to <2 x i64>
// X86-NEXT:    [[OR_I:%.*]] = or <2 x i64> [[TMP3]], [[TMP5]]
// X86-NEXT:    [[TMP6:%.*]] = bitcast <2 x i64> [[OR_I]] to <2 x double>
// X86-NEXT:    store <2 x double> [[TMP6]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP7:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP7]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP8:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP8]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP9:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP9]]
//
__m128d test_mm_or_pd(__m128d A, __m128d B) {
  return _mm_or_pd(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_or_si128(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[OR_I:%.*]] = or <2 x i64> [[TMP2]], [[TMP3]]
// X86-NEXT:    ret <2 x i64> [[OR_I]]
//
__m128i test_mm_or_si128(__m128i A, __m128i B) {
  return _mm_or_si128(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_packs_epi16(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x i64> [[TMP2]] to <8 x i16>
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = bitcast <2 x i64> [[TMP4]] to <8 x i16>
// X86-NEXT:    [[TMP6:%.*]] = call <16 x i8> @llvm.x86.sse2.packsswb.128(<8 x i16> [[TMP3]], <8 x i16> [[TMP5]])
// X86-NEXT:    [[TMP7:%.*]] = bitcast <16 x i8> [[TMP6]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP7]]
//
__m128i test_mm_packs_epi16(__m128i A, __m128i B) {
  return _mm_packs_epi16(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_packs_epi32(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x i64> [[TMP2]] to <4 x i32>
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = bitcast <2 x i64> [[TMP4]] to <4 x i32>
// X86-NEXT:    [[TMP6:%.*]] = call <8 x i16> @llvm.x86.sse2.packssdw.128(<4 x i32> [[TMP3]], <4 x i32> [[TMP5]])
// X86-NEXT:    [[TMP7:%.*]] = bitcast <8 x i16> [[TMP6]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP7]]
//
__m128i test_mm_packs_epi32(__m128i A, __m128i B) {
  return _mm_packs_epi32(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_packus_epi16(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x i64> [[TMP2]] to <8 x i16>
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = bitcast <2 x i64> [[TMP4]] to <8 x i16>
// X86-NEXT:    [[TMP6:%.*]] = call <16 x i8> @llvm.x86.sse2.packuswb.128(<8 x i16> [[TMP3]], <8 x i16> [[TMP5]])
// X86-NEXT:    [[TMP7:%.*]] = bitcast <16 x i8> [[TMP6]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP7]]
//
__m128i test_mm_packus_epi16(__m128i A, __m128i B) {
  return _mm_packus_epi16(A, B);
}

//
// X86-LABEL: define void @test_mm_pause(
// X86-SAME: ) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    call void @llvm.x86.sse2.pause()
// X86-NEXT:    ret void
//
void test_mm_pause(void) {
  return _mm_pause();
}

//
// X86-LABEL: define <2 x i64> @test_mm_sad_epu8(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x i64> [[TMP2]] to <16 x i8>
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = bitcast <2 x i64> [[TMP4]] to <16 x i8>
// X86-NEXT:    [[TMP6:%.*]] = call <2 x i64> @llvm.x86.sse2.psad.bw(<16 x i8> [[TMP3]], <16 x i8> [[TMP5]])
// X86-NEXT:    ret <2 x i64> [[TMP6]]
//
__m128i test_mm_sad_epu8(__m128i A, __m128i B) {
  return _mm_sad_epu8(A, B);
}

//
__m128i test_mm_set_epi8(char A, char B, char C, char D,
                         char E, char F, char G, char H,
                         char I, char J, char K, char L,
                         char M, char N, char O, char P) {
  return _mm_set_epi8(A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P);
}

//
// X86-LABEL: define <2 x i64> @test_mm_set_epi16(
// X86-SAME: i16 noundef signext [[A:%.*]], i16 noundef signext [[B:%.*]], i16 noundef signext [[C:%.*]], i16 noundef signext [[D:%.*]], i16 noundef signext [[E:%.*]], i16 noundef signext [[F:%.*]], i16 noundef signext [[G:%.*]], i16 noundef signext [[H:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__W7_ADDR_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W6_ADDR_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W5_ADDR_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W4_ADDR_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W3_ADDR_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W2_ADDR_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W1_ADDR_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W0_ADDR_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x i16>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca i16, align 2
// X86-NEXT:    [[B_ADDR:%.*]] = alloca i16, align 2
// X86-NEXT:    [[C_ADDR:%.*]] = alloca i16, align 2
// X86-NEXT:    [[D_ADDR:%.*]] = alloca i16, align 2
// X86-NEXT:    [[E_ADDR:%.*]] = alloca i16, align 2
// X86-NEXT:    [[F_ADDR:%.*]] = alloca i16, align 2
// X86-NEXT:    [[G_ADDR:%.*]] = alloca i16, align 2
// X86-NEXT:    [[H_ADDR:%.*]] = alloca i16, align 2
// X86-NEXT:    store i16 [[A]], ptr [[A_ADDR]], align 2
// X86-NEXT:    store i16 [[B]], ptr [[B_ADDR]], align 2
// X86-NEXT:    store i16 [[C]], ptr [[C_ADDR]], align 2
// X86-NEXT:    store i16 [[D]], ptr [[D_ADDR]], align 2
// X86-NEXT:    store i16 [[E]], ptr [[E_ADDR]], align 2
// X86-NEXT:    store i16 [[F]], ptr [[F_ADDR]], align 2
// X86-NEXT:    store i16 [[G]], ptr [[G_ADDR]], align 2
// X86-NEXT:    store i16 [[H]], ptr [[H_ADDR]], align 2
// X86-NEXT:    [[TMP0:%.*]] = load i16, ptr [[A_ADDR]], align 2
// X86-NEXT:    [[TMP1:%.*]] = load i16, ptr [[B_ADDR]], align 2
// X86-NEXT:    [[TMP2:%.*]] = load i16, ptr [[C_ADDR]], align 2
// X86-NEXT:    [[TMP3:%.*]] = load i16, ptr [[D_ADDR]], align 2
// X86-NEXT:    [[TMP4:%.*]] = load i16, ptr [[E_ADDR]], align 2
// X86-NEXT:    [[TMP5:%.*]] = load i16, ptr [[F_ADDR]], align 2
// X86-NEXT:    [[TMP6:%.*]] = load i16, ptr [[G_ADDR]], align 2
// X86-NEXT:    [[TMP7:%.*]] = load i16, ptr [[H_ADDR]], align 2
// X86-NEXT:    store i16 [[TMP0]], ptr [[__W7_ADDR_I]], align 2
// X86-NEXT:    store i16 [[TMP1]], ptr [[__W6_ADDR_I]], align 2
// X86-NEXT:    store i16 [[TMP2]], ptr [[__W5_ADDR_I]], align 2
// X86-NEXT:    store i16 [[TMP3]], ptr [[__W4_ADDR_I]], align 2
// X86-NEXT:    store i16 [[TMP4]], ptr [[__W3_ADDR_I]], align 2
// X86-NEXT:    store i16 [[TMP5]], ptr [[__W2_ADDR_I]], align 2
// X86-NEXT:    store i16 [[TMP6]], ptr [[__W1_ADDR_I]], align 2
// X86-NEXT:    store i16 [[TMP7]], ptr [[__W0_ADDR_I]], align 2
// X86-NEXT:    [[TMP8:%.*]] = load i16, ptr [[__W0_ADDR_I]], align 2
// X86-NEXT:    [[VECINIT_I:%.*]] = insertelement <8 x i16> poison, i16 [[TMP8]], i32 0
// X86-NEXT:    [[TMP9:%.*]] = load i16, ptr [[__W1_ADDR_I]], align 2
// X86-NEXT:    [[VECINIT1_I:%.*]] = insertelement <8 x i16> [[VECINIT_I]], i16 [[TMP9]], i32 1
// X86-NEXT:    [[TMP10:%.*]] = load i16, ptr [[__W2_ADDR_I]], align 2
// X86-NEXT:    [[VECINIT2_I:%.*]] = insertelement <8 x i16> [[VECINIT1_I]], i16 [[TMP10]], i32 2
// X86-NEXT:    [[TMP11:%.*]] = load i16, ptr [[__W3_ADDR_I]], align 2
// X86-NEXT:    [[VECINIT3_I:%.*]] = insertelement <8 x i16> [[VECINIT2_I]], i16 [[TMP11]], i32 3
// X86-NEXT:    [[TMP12:%.*]] = load i16, ptr [[__W4_ADDR_I]], align 2
// X86-NEXT:    [[VECINIT4_I:%.*]] = insertelement <8 x i16> [[VECINIT3_I]], i16 [[TMP12]], i32 4
// X86-NEXT:    [[TMP13:%.*]] = load i16, ptr [[__W5_ADDR_I]], align 2
// X86-NEXT:    [[VECINIT5_I:%.*]] = insertelement <8 x i16> [[VECINIT4_I]], i16 [[TMP13]], i32 5
// X86-NEXT:    [[TMP14:%.*]] = load i16, ptr [[__W6_ADDR_I]], align 2
// X86-NEXT:    [[VECINIT6_I:%.*]] = insertelement <8 x i16> [[VECINIT5_I]], i16 [[TMP14]], i32 6
// X86-NEXT:    [[TMP15:%.*]] = load i16, ptr [[__W7_ADDR_I]], align 2
// X86-NEXT:    [[VECINIT7_I:%.*]] = insertelement <8 x i16> [[VECINIT6_I]], i16 [[TMP15]], i32 7
// X86-NEXT:    store <8 x i16> [[VECINIT7_I]], ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    [[TMP16:%.*]] = load <8 x i16>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    [[TMP17:%.*]] = bitcast <8 x i16> [[TMP16]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP17]]
//
__m128i test_mm_set_epi16(short A, short B, short C, short D,
                          short E, short F, short G, short H) {
  return _mm_set_epi16(A, B, C, D, E, F, G, H);
}

//
// X86-LABEL: define <2 x i64> @test_mm_set_epi32(
// X86-SAME: i32 noundef [[A:%.*]], i32 noundef [[B:%.*]], i32 noundef [[C:%.*]], i32 noundef [[D:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__I3_ADDR_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[__I2_ADDR_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[__I1_ADDR_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[__I0_ADDR_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <4 x i32>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// X86-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// X86-NEXT:    [[C_ADDR:%.*]] = alloca i32, align 4
// X86-NEXT:    [[D_ADDR:%.*]] = alloca i32, align 4
// X86-NEXT:    store i32 [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    store i32 [[B]], ptr [[B_ADDR]], align 4
// X86-NEXT:    store i32 [[C]], ptr [[C_ADDR]], align 4
// X86-NEXT:    store i32 [[D]], ptr [[D_ADDR]], align 4
// X86-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// X86-NEXT:    [[TMP2:%.*]] = load i32, ptr [[C_ADDR]], align 4
// X86-NEXT:    [[TMP3:%.*]] = load i32, ptr [[D_ADDR]], align 4
// X86-NEXT:    store i32 [[TMP0]], ptr [[__I3_ADDR_I]], align 4
// X86-NEXT:    store i32 [[TMP1]], ptr [[__I2_ADDR_I]], align 4
// X86-NEXT:    store i32 [[TMP2]], ptr [[__I1_ADDR_I]], align 4
// X86-NEXT:    store i32 [[TMP3]], ptr [[__I0_ADDR_I]], align 4
// X86-NEXT:    [[TMP4:%.*]] = load i32, ptr [[__I0_ADDR_I]], align 4
// X86-NEXT:    [[VECINIT_I:%.*]] = insertelement <4 x i32> poison, i32 [[TMP4]], i32 0
// X86-NEXT:    [[TMP5:%.*]] = load i32, ptr [[__I1_ADDR_I]], align 4
// X86-NEXT:    [[VECINIT1_I:%.*]] = insertelement <4 x i32> [[VECINIT_I]], i32 [[TMP5]], i32 1
// X86-NEXT:    [[TMP6:%.*]] = load i32, ptr [[__I2_ADDR_I]], align 4
// X86-NEXT:    [[VECINIT2_I:%.*]] = insertelement <4 x i32> [[VECINIT1_I]], i32 [[TMP6]], i32 2
// X86-NEXT:    [[TMP7:%.*]] = load i32, ptr [[__I3_ADDR_I]], align 4
// X86-NEXT:    [[VECINIT3_I:%.*]] = insertelement <4 x i32> [[VECINIT2_I]], i32 [[TMP7]], i32 3
// X86-NEXT:    store <4 x i32> [[VECINIT3_I]], ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    [[TMP8:%.*]] = load <4 x i32>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    [[TMP9:%.*]] = bitcast <4 x i32> [[TMP8]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP9]]
//
__m128i test_mm_set_epi32(int A, int B, int C, int D) {
  return _mm_set_epi32(A, B, C, D);
}

//
// X86-LABEL: define <2 x i64> @test_mm_set_epi64(
// X86-SAME: i64 noundef [[A_COERCE:%.*]], i64 noundef [[B_COERCE:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__Q1_ADDR_I4:%.*]] = alloca i64, align 8
// X86-NEXT:    [[__Q0_ADDR_I5:%.*]] = alloca i64, align 8
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__Q1_I:%.*]] = alloca <1 x i64>, align 8
// X86-NEXT:    [[__Q0_I:%.*]] = alloca <1 x i64>, align 8
// X86-NEXT:    [[__Q1_ADDR_I:%.*]] = alloca <1 x i64>, align 8
// X86-NEXT:    [[__Q0_ADDR_I:%.*]] = alloca <1 x i64>, align 8
// X86-NEXT:    [[A:%.*]] = alloca <1 x i64>, align 8
// X86-NEXT:    [[B:%.*]] = alloca <1 x i64>, align 8
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <1 x i64>, align 8
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <1 x i64>, align 8
// X86-NEXT:    [[COERCE:%.*]] = alloca <1 x i64>, align 8
// X86-NEXT:    [[COERCE3:%.*]] = alloca <1 x i64>, align 8
// X86-NEXT:    store i64 [[A_COERCE]], ptr [[A]], align 8
// X86-NEXT:    [[A1:%.*]] = load <1 x i64>, ptr [[A]], align 8
// X86-NEXT:    store i64 [[B_COERCE]], ptr [[B]], align 8
// X86-NEXT:    [[B2:%.*]] = load <1 x i64>, ptr [[B]], align 8
// X86-NEXT:    store <1 x i64> [[A1]], ptr [[A_ADDR]], align 8
// X86-NEXT:    store <1 x i64> [[B2]], ptr [[B_ADDR]], align 8
// X86-NEXT:    [[TMP0:%.*]] = load <1 x i64>, ptr [[A_ADDR]], align 8
// X86-NEXT:    [[TMP1:%.*]] = load <1 x i64>, ptr [[B_ADDR]], align 8
// X86-NEXT:    store <1 x i64> [[TMP0]], ptr [[COERCE]], align 8
// X86-NEXT:    [[TMP2:%.*]] = load i64, ptr [[COERCE]], align 8
// X86-NEXT:    store <1 x i64> [[TMP1]], ptr [[COERCE3]], align 8
// X86-NEXT:    [[TMP3:%.*]] = load i64, ptr [[COERCE3]], align 8
// X86-NEXT:    store i64 [[TMP2]], ptr [[__Q1_I]], align 8
// X86-NEXT:    [[__Q11_I:%.*]] = load <1 x i64>, ptr [[__Q1_I]], align 8
// X86-NEXT:    store i64 [[TMP3]], ptr [[__Q0_I]], align 8
// X86-NEXT:    [[__Q02_I:%.*]] = load <1 x i64>, ptr [[__Q0_I]], align 8
// X86-NEXT:    store <1 x i64> [[__Q11_I]], ptr [[__Q1_ADDR_I]], align 8
// X86-NEXT:    store <1 x i64> [[__Q02_I]], ptr [[__Q0_ADDR_I]], align 8
// X86-NEXT:    [[TMP4:%.*]] = load <1 x i64>, ptr [[__Q1_ADDR_I]], align 8
// X86-NEXT:    [[TMP5:%.*]] = bitcast <1 x i64> [[TMP4]] to i64
// X86-NEXT:    [[TMP6:%.*]] = load <1 x i64>, ptr [[__Q0_ADDR_I]], align 8
// X86-NEXT:    [[TMP7:%.*]] = bitcast <1 x i64> [[TMP6]] to i64
// X86-NEXT:    store i64 [[TMP5]], ptr [[__Q1_ADDR_I4]], align 8
// X86-NEXT:    store i64 [[TMP7]], ptr [[__Q0_ADDR_I5]], align 8
// X86-NEXT:    [[TMP8:%.*]] = load i64, ptr [[__Q0_ADDR_I5]], align 8
// X86-NEXT:    [[VECINIT_I:%.*]] = insertelement <2 x i64> poison, i64 [[TMP8]], i32 0
// X86-NEXT:    [[TMP9:%.*]] = load i64, ptr [[__Q1_ADDR_I4]], align 8
// X86-NEXT:    [[VECINIT1_I:%.*]] = insertelement <2 x i64> [[VECINIT_I]], i64 [[TMP9]], i32 1
// X86-NEXT:    store <2 x i64> [[VECINIT1_I]], ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    [[TMP10:%.*]] = load <2 x i64>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP10]]
//
__m128i test_mm_set_epi64(__m64 A, __m64 B) {
  return _mm_set_epi64(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_set_epi64x(
// X86-SAME: i64 noundef [[A:%.*]], i64 noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__Q1_ADDR_I:%.*]] = alloca i64, align 8
// X86-NEXT:    [[__Q0_ADDR_I:%.*]] = alloca i64, align 8
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca i64, align 8
// X86-NEXT:    [[B_ADDR:%.*]] = alloca i64, align 8
// X86-NEXT:    store i64 [[A]], ptr [[A_ADDR]], align 8
// X86-NEXT:    store i64 [[B]], ptr [[B_ADDR]], align 8
// X86-NEXT:    [[TMP0:%.*]] = load i64, ptr [[A_ADDR]], align 8
// X86-NEXT:    [[TMP1:%.*]] = load i64, ptr [[B_ADDR]], align 8
// X86-NEXT:    store i64 [[TMP0]], ptr [[__Q1_ADDR_I]], align 8
// X86-NEXT:    store i64 [[TMP1]], ptr [[__Q0_ADDR_I]], align 8
// X86-NEXT:    [[TMP2:%.*]] = load i64, ptr [[__Q0_ADDR_I]], align 8
// X86-NEXT:    [[VECINIT_I:%.*]] = insertelement <2 x i64> poison, i64 [[TMP2]], i32 0
// X86-NEXT:    [[TMP3:%.*]] = load i64, ptr [[__Q1_ADDR_I]], align 8
// X86-NEXT:    [[VECINIT1_I:%.*]] = insertelement <2 x i64> [[VECINIT_I]], i64 [[TMP3]], i32 1
// X86-NEXT:    store <2 x i64> [[VECINIT1_I]], ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP4]]
//
__m128i test_mm_set_epi64x(long long A, long long B) {
  return _mm_set_epi64x(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_set_pd(
// X86-SAME: double noundef [[A:%.*]], double noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__W_ADDR_I:%.*]] = alloca double, align 8
// X86-NEXT:    [[__X_ADDR_I:%.*]] = alloca double, align 8
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca double, align 8
// X86-NEXT:    [[B_ADDR:%.*]] = alloca double, align 8
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store double [[A]], ptr [[A_ADDR]], align 8
// X86-NEXT:    store double [[B]], ptr [[B_ADDR]], align 8
// X86-NEXT:    [[TMP0:%.*]] = load double, ptr [[A_ADDR]], align 8
// X86-NEXT:    [[TMP1:%.*]] = load double, ptr [[B_ADDR]], align 8
// X86-NEXT:    store double [[TMP0]], ptr [[__W_ADDR_I]], align 8
// X86-NEXT:    store double [[TMP1]], ptr [[__X_ADDR_I]], align 8
// X86-NEXT:    [[TMP2:%.*]] = load double, ptr [[__X_ADDR_I]], align 8
// X86-NEXT:    [[VECINIT_I:%.*]] = insertelement <2 x double> poison, double [[TMP2]], i32 0
// X86-NEXT:    [[TMP3:%.*]] = load double, ptr [[__W_ADDR_I]], align 8
// X86-NEXT:    [[VECINIT1_I:%.*]] = insertelement <2 x double> [[VECINIT_I]], double [[TMP3]], i32 1
// X86-NEXT:    store <2 x double> [[VECINIT1_I]], ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = load <2 x double>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP5]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP6:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP6]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP7:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP7]]
//
__m128d test_mm_set_pd(double A, double B) {
  return _mm_set_pd(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_set_pd1(
// X86-SAME: double noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I1:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__W_ADDR_I2:%.*]] = alloca double, align 8
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__W_ADDR_I:%.*]] = alloca double, align 8
// X86-NEXT:    [[COERCE_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca double, align 8
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store double [[A]], ptr [[A_ADDR]], align 8
// X86-NEXT:    [[TMP0:%.*]] = load double, ptr [[A_ADDR]], align 8
// X86-NEXT:    store double [[TMP0]], ptr [[__W_ADDR_I]], align 8
// X86-NEXT:    [[TMP1:%.*]] = load double, ptr [[__W_ADDR_I]], align 8
// X86-NEXT:    store double [[TMP1]], ptr [[__W_ADDR_I2]], align 8
// X86-NEXT:    [[TMP2:%.*]] = load double, ptr [[__W_ADDR_I2]], align 8
// X86-NEXT:    [[VECINIT_I:%.*]] = insertelement <2 x double> poison, double [[TMP2]], i32 0
// X86-NEXT:    [[TMP3:%.*]] = load double, ptr [[__W_ADDR_I2]], align 8
// X86-NEXT:    [[VECINIT1_I:%.*]] = insertelement <2 x double> [[VECINIT_I]], double [[TMP3]], i32 1
// X86-NEXT:    store <2 x double> [[VECINIT1_I]], ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = load <2 x double>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL_I1]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL_I1]], align 16
// X86-NEXT:    store <2 x i64> [[TMP5]], ptr [[COERCE_I]], align 16
// X86-NEXT:    [[TMP6:%.*]] = load <2 x double>, ptr [[COERCE_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP6]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP7:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP7]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP8:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP8]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP9:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP9]]
//
__m128d test_mm_set_pd1(double A) {
  return _mm_set_pd1(A);
}

//
// X86-LABEL: define <2 x i64> @test_mm_set_sd(
// X86-SAME: double noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__W_ADDR_I:%.*]] = alloca double, align 8
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca double, align 8
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store double [[A]], ptr [[A_ADDR]], align 8
// X86-NEXT:    [[TMP0:%.*]] = load double, ptr [[A_ADDR]], align 8
// X86-NEXT:    store double [[TMP0]], ptr [[__W_ADDR_I]], align 8
// X86-NEXT:    [[TMP1:%.*]] = load double, ptr [[__W_ADDR_I]], align 8
// X86-NEXT:    [[VECINIT_I:%.*]] = insertelement <2 x double> poison, double [[TMP1]], i32 0
// X86-NEXT:    [[VECINIT1_I:%.*]] = insertelement <2 x double> [[VECINIT_I]], double 0.000000e+00, i32 1
// X86-NEXT:    store <2 x double> [[VECINIT1_I]], ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP2]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP3]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP4:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128d test_mm_set_sd(double A) {
  return _mm_set_sd(A);
}

//
__m128i test_mm_set1_epi8(char A) {
  return _mm_set1_epi8(A);
}

//
// X86-LABEL: define <2 x i64> @test_mm_set1_epi16(
// X86-SAME: i16 noundef signext [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__W7_ADDR_I_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W6_ADDR_I_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W5_ADDR_I_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W4_ADDR_I_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W3_ADDR_I_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W2_ADDR_I_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W1_ADDR_I_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W0_ADDR_I_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x i16>, align 16
// X86-NEXT:    [[__W_ADDR_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[A_ADDR:%.*]] = alloca i16, align 2
// X86-NEXT:    store i16 [[A]], ptr [[A_ADDR]], align 2
// X86-NEXT:    [[TMP0:%.*]] = load i16, ptr [[A_ADDR]], align 2
// X86-NEXT:    store i16 [[TMP0]], ptr [[__W_ADDR_I]], align 2
// X86-NEXT:    [[TMP1:%.*]] = load i16, ptr [[__W_ADDR_I]], align 2
// X86-NEXT:    [[TMP2:%.*]] = load i16, ptr [[__W_ADDR_I]], align 2
// X86-NEXT:    [[TMP3:%.*]] = load i16, ptr [[__W_ADDR_I]], align 2
// X86-NEXT:    [[TMP4:%.*]] = load i16, ptr [[__W_ADDR_I]], align 2
// X86-NEXT:    [[TMP5:%.*]] = load i16, ptr [[__W_ADDR_I]], align 2
// X86-NEXT:    [[TMP6:%.*]] = load i16, ptr [[__W_ADDR_I]], align 2
// X86-NEXT:    [[TMP7:%.*]] = load i16, ptr [[__W_ADDR_I]], align 2
// X86-NEXT:    [[TMP8:%.*]] = load i16, ptr [[__W_ADDR_I]], align 2
// X86-NEXT:    store i16 [[TMP1]], ptr [[__W7_ADDR_I_I]], align 2
// X86-NEXT:    store i16 [[TMP2]], ptr [[__W6_ADDR_I_I]], align 2
// X86-NEXT:    store i16 [[TMP3]], ptr [[__W5_ADDR_I_I]], align 2
// X86-NEXT:    store i16 [[TMP4]], ptr [[__W4_ADDR_I_I]], align 2
// X86-NEXT:    store i16 [[TMP5]], ptr [[__W3_ADDR_I_I]], align 2
// X86-NEXT:    store i16 [[TMP6]], ptr [[__W2_ADDR_I_I]], align 2
// X86-NEXT:    store i16 [[TMP7]], ptr [[__W1_ADDR_I_I]], align 2
// X86-NEXT:    store i16 [[TMP8]], ptr [[__W0_ADDR_I_I]], align 2
// X86-NEXT:    [[TMP9:%.*]] = load i16, ptr [[__W0_ADDR_I_I]], align 2
// X86-NEXT:    [[VECINIT_I_I:%.*]] = insertelement <8 x i16> poison, i16 [[TMP9]], i32 0
// X86-NEXT:    [[TMP10:%.*]] = load i16, ptr [[__W1_ADDR_I_I]], align 2
// X86-NEXT:    [[VECINIT1_I_I:%.*]] = insertelement <8 x i16> [[VECINIT_I_I]], i16 [[TMP10]], i32 1
// X86-NEXT:    [[TMP11:%.*]] = load i16, ptr [[__W2_ADDR_I_I]], align 2
// X86-NEXT:    [[VECINIT2_I_I:%.*]] = insertelement <8 x i16> [[VECINIT1_I_I]], i16 [[TMP11]], i32 2
// X86-NEXT:    [[TMP12:%.*]] = load i16, ptr [[__W3_ADDR_I_I]], align 2
// X86-NEXT:    [[VECINIT3_I_I:%.*]] = insertelement <8 x i16> [[VECINIT2_I_I]], i16 [[TMP12]], i32 3
// X86-NEXT:    [[TMP13:%.*]] = load i16, ptr [[__W4_ADDR_I_I]], align 2
// X86-NEXT:    [[VECINIT4_I_I:%.*]] = insertelement <8 x i16> [[VECINIT3_I_I]], i16 [[TMP13]], i32 4
// X86-NEXT:    [[TMP14:%.*]] = load i16, ptr [[__W5_ADDR_I_I]], align 2
// X86-NEXT:    [[VECINIT5_I_I:%.*]] = insertelement <8 x i16> [[VECINIT4_I_I]], i16 [[TMP14]], i32 5
// X86-NEXT:    [[TMP15:%.*]] = load i16, ptr [[__W6_ADDR_I_I]], align 2
// X86-NEXT:    [[VECINIT6_I_I:%.*]] = insertelement <8 x i16> [[VECINIT5_I_I]], i16 [[TMP15]], i32 6
// X86-NEXT:    [[TMP16:%.*]] = load i16, ptr [[__W7_ADDR_I_I]], align 2
// X86-NEXT:    [[VECINIT7_I_I:%.*]] = insertelement <8 x i16> [[VECINIT6_I_I]], i16 [[TMP16]], i32 7
// X86-NEXT:    store <8 x i16> [[VECINIT7_I_I]], ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// X86-NEXT:    [[TMP17:%.*]] = load <8 x i16>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// X86-NEXT:    [[TMP18:%.*]] = bitcast <8 x i16> [[TMP17]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP18]]
//
__m128i test_mm_set1_epi16(short A) {
  return _mm_set1_epi16(A);
}

//
// X86-LABEL: define <2 x i64> @test_mm_set1_epi32(
// X86-SAME: i32 noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__I3_ADDR_I_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[__I2_ADDR_I_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[__I1_ADDR_I_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[__I0_ADDR_I_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <4 x i32>, align 16
// X86-NEXT:    [[__I_ADDR_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// X86-NEXT:    store i32 [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// X86-NEXT:    store i32 [[TMP0]], ptr [[__I_ADDR_I]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load i32, ptr [[__I_ADDR_I]], align 4
// X86-NEXT:    [[TMP2:%.*]] = load i32, ptr [[__I_ADDR_I]], align 4
// X86-NEXT:    [[TMP3:%.*]] = load i32, ptr [[__I_ADDR_I]], align 4
// X86-NEXT:    [[TMP4:%.*]] = load i32, ptr [[__I_ADDR_I]], align 4
// X86-NEXT:    store i32 [[TMP1]], ptr [[__I3_ADDR_I_I]], align 4
// X86-NEXT:    store i32 [[TMP2]], ptr [[__I2_ADDR_I_I]], align 4
// X86-NEXT:    store i32 [[TMP3]], ptr [[__I1_ADDR_I_I]], align 4
// X86-NEXT:    store i32 [[TMP4]], ptr [[__I0_ADDR_I_I]], align 4
// X86-NEXT:    [[TMP5:%.*]] = load i32, ptr [[__I0_ADDR_I_I]], align 4
// X86-NEXT:    [[VECINIT_I_I:%.*]] = insertelement <4 x i32> poison, i32 [[TMP5]], i32 0
// X86-NEXT:    [[TMP6:%.*]] = load i32, ptr [[__I1_ADDR_I_I]], align 4
// X86-NEXT:    [[VECINIT1_I_I:%.*]] = insertelement <4 x i32> [[VECINIT_I_I]], i32 [[TMP6]], i32 1
// X86-NEXT:    [[TMP7:%.*]] = load i32, ptr [[__I2_ADDR_I_I]], align 4
// X86-NEXT:    [[VECINIT2_I_I:%.*]] = insertelement <4 x i32> [[VECINIT1_I_I]], i32 [[TMP7]], i32 2
// X86-NEXT:    [[TMP8:%.*]] = load i32, ptr [[__I3_ADDR_I_I]], align 4
// X86-NEXT:    [[VECINIT3_I_I:%.*]] = insertelement <4 x i32> [[VECINIT2_I_I]], i32 [[TMP8]], i32 3
// X86-NEXT:    store <4 x i32> [[VECINIT3_I_I]], ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// X86-NEXT:    [[TMP9:%.*]] = load <4 x i32>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// X86-NEXT:    [[TMP10:%.*]] = bitcast <4 x i32> [[TMP9]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP10]]
//
__m128i test_mm_set1_epi32(int A) {
  return _mm_set1_epi32(A);
}

//
// X86-LABEL: define <2 x i64> @test_mm_set1_epi64(
// X86-SAME: i64 noundef [[A_COERCE:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__Q1_ADDR_I3_I:%.*]] = alloca i64, align 8
// X86-NEXT:    [[__Q0_ADDR_I4_I:%.*]] = alloca i64, align 8
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__Q1_I_I:%.*]] = alloca <1 x i64>, align 8
// X86-NEXT:    [[__Q0_I_I:%.*]] = alloca <1 x i64>, align 8
// X86-NEXT:    [[__Q1_ADDR_I_I:%.*]] = alloca <1 x i64>, align 8
// X86-NEXT:    [[__Q0_ADDR_I_I:%.*]] = alloca <1 x i64>, align 8
// X86-NEXT:    [[__Q_I:%.*]] = alloca <1 x i64>, align 8
// X86-NEXT:    [[__Q_ADDR_I:%.*]] = alloca <1 x i64>, align 8
// X86-NEXT:    [[COERCE_I:%.*]] = alloca <1 x i64>, align 8
// X86-NEXT:    [[COERCE2_I:%.*]] = alloca <1 x i64>, align 8
// X86-NEXT:    [[A:%.*]] = alloca <1 x i64>, align 8
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <1 x i64>, align 8
// X86-NEXT:    [[COERCE:%.*]] = alloca <1 x i64>, align 8
// X86-NEXT:    store i64 [[A_COERCE]], ptr [[A]], align 8
// X86-NEXT:    [[A1:%.*]] = load <1 x i64>, ptr [[A]], align 8
// X86-NEXT:    store <1 x i64> [[A1]], ptr [[A_ADDR]], align 8
// X86-NEXT:    [[TMP0:%.*]] = load <1 x i64>, ptr [[A_ADDR]], align 8
// X86-NEXT:    store <1 x i64> [[TMP0]], ptr [[COERCE]], align 8
// X86-NEXT:    [[TMP1:%.*]] = load i64, ptr [[COERCE]], align 8
// X86-NEXT:    store i64 [[TMP1]], ptr [[__Q_I]], align 8
// X86-NEXT:    [[__Q1_I:%.*]] = load <1 x i64>, ptr [[__Q_I]], align 8
// X86-NEXT:    store <1 x i64> [[__Q1_I]], ptr [[__Q_ADDR_I]], align 8
// X86-NEXT:    [[TMP2:%.*]] = load <1 x i64>, ptr [[__Q_ADDR_I]], align 8
// X86-NEXT:    [[TMP3:%.*]] = load <1 x i64>, ptr [[__Q_ADDR_I]], align 8
// X86-NEXT:    store <1 x i64> [[TMP2]], ptr [[COERCE_I]], align 8
// X86-NEXT:    [[TMP4:%.*]] = load i64, ptr [[COERCE_I]], align 8
// X86-NEXT:    store <1 x i64> [[TMP3]], ptr [[COERCE2_I]], align 8
// X86-NEXT:    [[TMP5:%.*]] = load i64, ptr [[COERCE2_I]], align 8
// X86-NEXT:    store i64 [[TMP4]], ptr [[__Q1_I_I]], align 8
// X86-NEXT:    [[__Q11_I_I:%.*]] = load <1 x i64>, ptr [[__Q1_I_I]], align 8
// X86-NEXT:    store i64 [[TMP5]], ptr [[__Q0_I_I]], align 8
// X86-NEXT:    [[__Q02_I_I:%.*]] = load <1 x i64>, ptr [[__Q0_I_I]], align 8
// X86-NEXT:    store <1 x i64> [[__Q11_I_I]], ptr [[__Q1_ADDR_I_I]], align 8
// X86-NEXT:    store <1 x i64> [[__Q02_I_I]], ptr [[__Q0_ADDR_I_I]], align 8
// X86-NEXT:    [[TMP6:%.*]] = load <1 x i64>, ptr [[__Q1_ADDR_I_I]], align 8
// X86-NEXT:    [[TMP7:%.*]] = bitcast <1 x i64> [[TMP6]] to i64
// X86-NEXT:    [[TMP8:%.*]] = load <1 x i64>, ptr [[__Q0_ADDR_I_I]], align 8
// X86-NEXT:    [[TMP9:%.*]] = bitcast <1 x i64> [[TMP8]] to i64
// X86-NEXT:    store i64 [[TMP7]], ptr [[__Q1_ADDR_I3_I]], align 8
// X86-NEXT:    store i64 [[TMP9]], ptr [[__Q0_ADDR_I4_I]], align 8
// X86-NEXT:    [[TMP10:%.*]] = load i64, ptr [[__Q0_ADDR_I4_I]], align 8
// X86-NEXT:    [[VECINIT_I_I:%.*]] = insertelement <2 x i64> poison, i64 [[TMP10]], i32 0
// X86-NEXT:    [[TMP11:%.*]] = load i64, ptr [[__Q1_ADDR_I3_I]], align 8
// X86-NEXT:    [[VECINIT1_I_I:%.*]] = insertelement <2 x i64> [[VECINIT_I_I]], i64 [[TMP11]], i32 1
// X86-NEXT:    store <2 x i64> [[VECINIT1_I_I]], ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// X86-NEXT:    [[TMP12:%.*]] = load <2 x i64>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP12]]
//
__m128i test_mm_set1_epi64(__m64 A) {
  return _mm_set1_epi64(A);
}

//
// X86-LABEL: define <2 x i64> @test_mm_set1_epi64x(
// X86-SAME: i64 noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__Q1_ADDR_I_I:%.*]] = alloca i64, align 8
// X86-NEXT:    [[__Q0_ADDR_I_I:%.*]] = alloca i64, align 8
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__Q_ADDR_I:%.*]] = alloca i64, align 8
// X86-NEXT:    [[A_ADDR:%.*]] = alloca i64, align 8
// X86-NEXT:    store i64 [[A]], ptr [[A_ADDR]], align 8
// X86-NEXT:    [[TMP0:%.*]] = load i64, ptr [[A_ADDR]], align 8
// X86-NEXT:    store i64 [[TMP0]], ptr [[__Q_ADDR_I]], align 8
// X86-NEXT:    [[TMP1:%.*]] = load i64, ptr [[__Q_ADDR_I]], align 8
// X86-NEXT:    [[TMP2:%.*]] = load i64, ptr [[__Q_ADDR_I]], align 8
// X86-NEXT:    store i64 [[TMP1]], ptr [[__Q1_ADDR_I_I]], align 8
// X86-NEXT:    store i64 [[TMP2]], ptr [[__Q0_ADDR_I_I]], align 8
// X86-NEXT:    [[TMP3:%.*]] = load i64, ptr [[__Q0_ADDR_I_I]], align 8
// X86-NEXT:    [[VECINIT_I_I:%.*]] = insertelement <2 x i64> poison, i64 [[TMP3]], i32 0
// X86-NEXT:    [[TMP4:%.*]] = load i64, ptr [[__Q1_ADDR_I_I]], align 8
// X86-NEXT:    [[VECINIT1_I_I:%.*]] = insertelement <2 x i64> [[VECINIT_I_I]], i64 [[TMP4]], i32 1
// X86-NEXT:    store <2 x i64> [[VECINIT1_I_I]], ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128i test_mm_set1_epi64x(long long A) {
  return _mm_set1_epi64x(A);
}

//
// X86-LABEL: define <2 x i64> @test_mm_set1_pd(
// X86-SAME: double noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__W_ADDR_I:%.*]] = alloca double, align 8
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca double, align 8
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store double [[A]], ptr [[A_ADDR]], align 8
// X86-NEXT:    [[TMP0:%.*]] = load double, ptr [[A_ADDR]], align 8
// X86-NEXT:    store double [[TMP0]], ptr [[__W_ADDR_I]], align 8
// X86-NEXT:    [[TMP1:%.*]] = load double, ptr [[__W_ADDR_I]], align 8
// X86-NEXT:    [[VECINIT_I:%.*]] = insertelement <2 x double> poison, double [[TMP1]], i32 0
// X86-NEXT:    [[TMP2:%.*]] = load double, ptr [[__W_ADDR_I]], align 8
// X86-NEXT:    [[VECINIT1_I:%.*]] = insertelement <2 x double> [[VECINIT_I]], double [[TMP2]], i32 1
// X86-NEXT:    store <2 x double> [[VECINIT1_I]], ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP3]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP4]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP5]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP6:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP6]]
//
__m128d test_mm_set1_pd(double A) {
  return _mm_set1_pd(A);
}

//
__m128i test_mm_setr_epi8(char A, char B, char C, char D,
                          char E, char F, char G, char H,
                          char I, char J, char K, char L,
                          char M, char N, char O, char P) {
  return _mm_setr_epi8(A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P);
}

//
// X86-LABEL: define <2 x i64> @test_mm_setr_epi16(
// X86-SAME: i16 noundef signext [[A:%.*]], i16 noundef signext [[B:%.*]], i16 noundef signext [[C:%.*]], i16 noundef signext [[D:%.*]], i16 noundef signext [[E:%.*]], i16 noundef signext [[F:%.*]], i16 noundef signext [[G:%.*]], i16 noundef signext [[H:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__W7_ADDR_I_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W6_ADDR_I_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W5_ADDR_I_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W4_ADDR_I_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W3_ADDR_I_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W2_ADDR_I_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W1_ADDR_I_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W0_ADDR_I_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x i16>, align 16
// X86-NEXT:    [[__W0_ADDR_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W1_ADDR_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W2_ADDR_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W3_ADDR_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W4_ADDR_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W5_ADDR_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W6_ADDR_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W7_ADDR_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[A_ADDR:%.*]] = alloca i16, align 2
// X86-NEXT:    [[B_ADDR:%.*]] = alloca i16, align 2
// X86-NEXT:    [[C_ADDR:%.*]] = alloca i16, align 2
// X86-NEXT:    [[D_ADDR:%.*]] = alloca i16, align 2
// X86-NEXT:    [[E_ADDR:%.*]] = alloca i16, align 2
// X86-NEXT:    [[F_ADDR:%.*]] = alloca i16, align 2
// X86-NEXT:    [[G_ADDR:%.*]] = alloca i16, align 2
// X86-NEXT:    [[H_ADDR:%.*]] = alloca i16, align 2
// X86-NEXT:    store i16 [[A]], ptr [[A_ADDR]], align 2
// X86-NEXT:    store i16 [[B]], ptr [[B_ADDR]], align 2
// X86-NEXT:    store i16 [[C]], ptr [[C_ADDR]], align 2
// X86-NEXT:    store i16 [[D]], ptr [[D_ADDR]], align 2
// X86-NEXT:    store i16 [[E]], ptr [[E_ADDR]], align 2
// X86-NEXT:    store i16 [[F]], ptr [[F_ADDR]], align 2
// X86-NEXT:    store i16 [[G]], ptr [[G_ADDR]], align 2
// X86-NEXT:    store i16 [[H]], ptr [[H_ADDR]], align 2
// X86-NEXT:    [[TMP0:%.*]] = load i16, ptr [[A_ADDR]], align 2
// X86-NEXT:    [[TMP1:%.*]] = load i16, ptr [[B_ADDR]], align 2
// X86-NEXT:    [[TMP2:%.*]] = load i16, ptr [[C_ADDR]], align 2
// X86-NEXT:    [[TMP3:%.*]] = load i16, ptr [[D_ADDR]], align 2
// X86-NEXT:    [[TMP4:%.*]] = load i16, ptr [[E_ADDR]], align 2
// X86-NEXT:    [[TMP5:%.*]] = load i16, ptr [[F_ADDR]], align 2
// X86-NEXT:    [[TMP6:%.*]] = load i16, ptr [[G_ADDR]], align 2
// X86-NEXT:    [[TMP7:%.*]] = load i16, ptr [[H_ADDR]], align 2
// X86-NEXT:    store i16 [[TMP0]], ptr [[__W0_ADDR_I]], align 2
// X86-NEXT:    store i16 [[TMP1]], ptr [[__W1_ADDR_I]], align 2
// X86-NEXT:    store i16 [[TMP2]], ptr [[__W2_ADDR_I]], align 2
// X86-NEXT:    store i16 [[TMP3]], ptr [[__W3_ADDR_I]], align 2
// X86-NEXT:    store i16 [[TMP4]], ptr [[__W4_ADDR_I]], align 2
// X86-NEXT:    store i16 [[TMP5]], ptr [[__W5_ADDR_I]], align 2
// X86-NEXT:    store i16 [[TMP6]], ptr [[__W6_ADDR_I]], align 2
// X86-NEXT:    store i16 [[TMP7]], ptr [[__W7_ADDR_I]], align 2
// X86-NEXT:    [[TMP8:%.*]] = load i16, ptr [[__W7_ADDR_I]], align 2
// X86-NEXT:    [[TMP9:%.*]] = load i16, ptr [[__W6_ADDR_I]], align 2
// X86-NEXT:    [[TMP10:%.*]] = load i16, ptr [[__W5_ADDR_I]], align 2
// X86-NEXT:    [[TMP11:%.*]] = load i16, ptr [[__W4_ADDR_I]], align 2
// X86-NEXT:    [[TMP12:%.*]] = load i16, ptr [[__W3_ADDR_I]], align 2
// X86-NEXT:    [[TMP13:%.*]] = load i16, ptr [[__W2_ADDR_I]], align 2
// X86-NEXT:    [[TMP14:%.*]] = load i16, ptr [[__W1_ADDR_I]], align 2
// X86-NEXT:    [[TMP15:%.*]] = load i16, ptr [[__W0_ADDR_I]], align 2
// X86-NEXT:    store i16 [[TMP8]], ptr [[__W7_ADDR_I_I]], align 2
// X86-NEXT:    store i16 [[TMP9]], ptr [[__W6_ADDR_I_I]], align 2
// X86-NEXT:    store i16 [[TMP10]], ptr [[__W5_ADDR_I_I]], align 2
// X86-NEXT:    store i16 [[TMP11]], ptr [[__W4_ADDR_I_I]], align 2
// X86-NEXT:    store i16 [[TMP12]], ptr [[__W3_ADDR_I_I]], align 2
// X86-NEXT:    store i16 [[TMP13]], ptr [[__W2_ADDR_I_I]], align 2
// X86-NEXT:    store i16 [[TMP14]], ptr [[__W1_ADDR_I_I]], align 2
// X86-NEXT:    store i16 [[TMP15]], ptr [[__W0_ADDR_I_I]], align 2
// X86-NEXT:    [[TMP16:%.*]] = load i16, ptr [[__W0_ADDR_I_I]], align 2
// X86-NEXT:    [[VECINIT_I_I:%.*]] = insertelement <8 x i16> poison, i16 [[TMP16]], i32 0
// X86-NEXT:    [[TMP17:%.*]] = load i16, ptr [[__W1_ADDR_I_I]], align 2
// X86-NEXT:    [[VECINIT1_I_I:%.*]] = insertelement <8 x i16> [[VECINIT_I_I]], i16 [[TMP17]], i32 1
// X86-NEXT:    [[TMP18:%.*]] = load i16, ptr [[__W2_ADDR_I_I]], align 2
// X86-NEXT:    [[VECINIT2_I_I:%.*]] = insertelement <8 x i16> [[VECINIT1_I_I]], i16 [[TMP18]], i32 2
// X86-NEXT:    [[TMP19:%.*]] = load i16, ptr [[__W3_ADDR_I_I]], align 2
// X86-NEXT:    [[VECINIT3_I_I:%.*]] = insertelement <8 x i16> [[VECINIT2_I_I]], i16 [[TMP19]], i32 3
// X86-NEXT:    [[TMP20:%.*]] = load i16, ptr [[__W4_ADDR_I_I]], align 2
// X86-NEXT:    [[VECINIT4_I_I:%.*]] = insertelement <8 x i16> [[VECINIT3_I_I]], i16 [[TMP20]], i32 4
// X86-NEXT:    [[TMP21:%.*]] = load i16, ptr [[__W5_ADDR_I_I]], align 2
// X86-NEXT:    [[VECINIT5_I_I:%.*]] = insertelement <8 x i16> [[VECINIT4_I_I]], i16 [[TMP21]], i32 5
// X86-NEXT:    [[TMP22:%.*]] = load i16, ptr [[__W6_ADDR_I_I]], align 2
// X86-NEXT:    [[VECINIT6_I_I:%.*]] = insertelement <8 x i16> [[VECINIT5_I_I]], i16 [[TMP22]], i32 6
// X86-NEXT:    [[TMP23:%.*]] = load i16, ptr [[__W7_ADDR_I_I]], align 2
// X86-NEXT:    [[VECINIT7_I_I:%.*]] = insertelement <8 x i16> [[VECINIT6_I_I]], i16 [[TMP23]], i32 7
// X86-NEXT:    store <8 x i16> [[VECINIT7_I_I]], ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// X86-NEXT:    [[TMP24:%.*]] = load <8 x i16>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// X86-NEXT:    [[TMP25:%.*]] = bitcast <8 x i16> [[TMP24]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP25]]
//
__m128i test_mm_setr_epi16(short A, short B, short C, short D,
                           short E, short F, short G, short H) {
  return _mm_setr_epi16(A, B, C, D, E, F, G, H);
}

//
// X86-LABEL: define <2 x i64> @test_mm_setr_epi32(
// X86-SAME: i32 noundef [[A:%.*]], i32 noundef [[B:%.*]], i32 noundef [[C:%.*]], i32 noundef [[D:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__I3_ADDR_I_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[__I2_ADDR_I_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[__I1_ADDR_I_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[__I0_ADDR_I_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <4 x i32>, align 16
// X86-NEXT:    [[__I0_ADDR_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[__I1_ADDR_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[__I2_ADDR_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[__I3_ADDR_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// X86-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// X86-NEXT:    [[C_ADDR:%.*]] = alloca i32, align 4
// X86-NEXT:    [[D_ADDR:%.*]] = alloca i32, align 4
// X86-NEXT:    store i32 [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    store i32 [[B]], ptr [[B_ADDR]], align 4
// X86-NEXT:    store i32 [[C]], ptr [[C_ADDR]], align 4
// X86-NEXT:    store i32 [[D]], ptr [[D_ADDR]], align 4
// X86-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// X86-NEXT:    [[TMP2:%.*]] = load i32, ptr [[C_ADDR]], align 4
// X86-NEXT:    [[TMP3:%.*]] = load i32, ptr [[D_ADDR]], align 4
// X86-NEXT:    store i32 [[TMP0]], ptr [[__I0_ADDR_I]], align 4
// X86-NEXT:    store i32 [[TMP1]], ptr [[__I1_ADDR_I]], align 4
// X86-NEXT:    store i32 [[TMP2]], ptr [[__I2_ADDR_I]], align 4
// X86-NEXT:    store i32 [[TMP3]], ptr [[__I3_ADDR_I]], align 4
// X86-NEXT:    [[TMP4:%.*]] = load i32, ptr [[__I3_ADDR_I]], align 4
// X86-NEXT:    [[TMP5:%.*]] = load i32, ptr [[__I2_ADDR_I]], align 4
// X86-NEXT:    [[TMP6:%.*]] = load i32, ptr [[__I1_ADDR_I]], align 4
// X86-NEXT:    [[TMP7:%.*]] = load i32, ptr [[__I0_ADDR_I]], align 4
// X86-NEXT:    store i32 [[TMP4]], ptr [[__I3_ADDR_I_I]], align 4
// X86-NEXT:    store i32 [[TMP5]], ptr [[__I2_ADDR_I_I]], align 4
// X86-NEXT:    store i32 [[TMP6]], ptr [[__I1_ADDR_I_I]], align 4
// X86-NEXT:    store i32 [[TMP7]], ptr [[__I0_ADDR_I_I]], align 4
// X86-NEXT:    [[TMP8:%.*]] = load i32, ptr [[__I0_ADDR_I_I]], align 4
// X86-NEXT:    [[VECINIT_I_I:%.*]] = insertelement <4 x i32> poison, i32 [[TMP8]], i32 0
// X86-NEXT:    [[TMP9:%.*]] = load i32, ptr [[__I1_ADDR_I_I]], align 4
// X86-NEXT:    [[VECINIT1_I_I:%.*]] = insertelement <4 x i32> [[VECINIT_I_I]], i32 [[TMP9]], i32 1
// X86-NEXT:    [[TMP10:%.*]] = load i32, ptr [[__I2_ADDR_I_I]], align 4
// X86-NEXT:    [[VECINIT2_I_I:%.*]] = insertelement <4 x i32> [[VECINIT1_I_I]], i32 [[TMP10]], i32 2
// X86-NEXT:    [[TMP11:%.*]] = load i32, ptr [[__I3_ADDR_I_I]], align 4
// X86-NEXT:    [[VECINIT3_I_I:%.*]] = insertelement <4 x i32> [[VECINIT2_I_I]], i32 [[TMP11]], i32 3
// X86-NEXT:    store <4 x i32> [[VECINIT3_I_I]], ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// X86-NEXT:    [[TMP12:%.*]] = load <4 x i32>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// X86-NEXT:    [[TMP13:%.*]] = bitcast <4 x i32> [[TMP12]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP13]]
//
__m128i test_mm_setr_epi32(int A, int B, int C, int D) {
  return _mm_setr_epi32(A, B, C, D);
}

//
// X86-LABEL: define <2 x i64> @test_mm_setr_epi64(
// X86-SAME: i64 noundef [[A_COERCE:%.*]], i64 noundef [[B_COERCE:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__Q1_ADDR_I4_I:%.*]] = alloca i64, align 8
// X86-NEXT:    [[__Q0_ADDR_I5_I:%.*]] = alloca i64, align 8
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__Q1_I_I:%.*]] = alloca <1 x i64>, align 8
// X86-NEXT:    [[__Q0_I_I:%.*]] = alloca <1 x i64>, align 8
// X86-NEXT:    [[__Q1_ADDR_I_I:%.*]] = alloca <1 x i64>, align 8
// X86-NEXT:    [[__Q0_ADDR_I_I:%.*]] = alloca <1 x i64>, align 8
// X86-NEXT:    [[__Q0_I:%.*]] = alloca <1 x i64>, align 8
// X86-NEXT:    [[__Q1_I:%.*]] = alloca <1 x i64>, align 8
// X86-NEXT:    [[__Q0_ADDR_I:%.*]] = alloca <1 x i64>, align 8
// X86-NEXT:    [[__Q1_ADDR_I:%.*]] = alloca <1 x i64>, align 8
// X86-NEXT:    [[COERCE_I:%.*]] = alloca <1 x i64>, align 8
// X86-NEXT:    [[COERCE3_I:%.*]] = alloca <1 x i64>, align 8
// X86-NEXT:    [[A:%.*]] = alloca <1 x i64>, align 8
// X86-NEXT:    [[B:%.*]] = alloca <1 x i64>, align 8
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <1 x i64>, align 8
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <1 x i64>, align 8
// X86-NEXT:    [[COERCE:%.*]] = alloca <1 x i64>, align 8
// X86-NEXT:    [[COERCE3:%.*]] = alloca <1 x i64>, align 8
// X86-NEXT:    store i64 [[A_COERCE]], ptr [[A]], align 8
// X86-NEXT:    [[A1:%.*]] = load <1 x i64>, ptr [[A]], align 8
// X86-NEXT:    store i64 [[B_COERCE]], ptr [[B]], align 8
// X86-NEXT:    [[B2:%.*]] = load <1 x i64>, ptr [[B]], align 8
// X86-NEXT:    store <1 x i64> [[A1]], ptr [[A_ADDR]], align 8
// X86-NEXT:    store <1 x i64> [[B2]], ptr [[B_ADDR]], align 8
// X86-NEXT:    [[TMP0:%.*]] = load <1 x i64>, ptr [[A_ADDR]], align 8
// X86-NEXT:    [[TMP1:%.*]] = load <1 x i64>, ptr [[B_ADDR]], align 8
// X86-NEXT:    store <1 x i64> [[TMP0]], ptr [[COERCE]], align 8
// X86-NEXT:    [[TMP2:%.*]] = load i64, ptr [[COERCE]], align 8
// X86-NEXT:    store <1 x i64> [[TMP1]], ptr [[COERCE3]], align 8
// X86-NEXT:    [[TMP3:%.*]] = load i64, ptr [[COERCE3]], align 8
// X86-NEXT:    store i64 [[TMP2]], ptr [[__Q0_I]], align 8
// X86-NEXT:    [[__Q01_I:%.*]] = load <1 x i64>, ptr [[__Q0_I]], align 8
// X86-NEXT:    store i64 [[TMP3]], ptr [[__Q1_I]], align 8
// X86-NEXT:    [[__Q12_I:%.*]] = load <1 x i64>, ptr [[__Q1_I]], align 8
// X86-NEXT:    store <1 x i64> [[__Q01_I]], ptr [[__Q0_ADDR_I]], align 8
// X86-NEXT:    store <1 x i64> [[__Q12_I]], ptr [[__Q1_ADDR_I]], align 8
// X86-NEXT:    [[TMP4:%.*]] = load <1 x i64>, ptr [[__Q1_ADDR_I]], align 8
// X86-NEXT:    [[TMP5:%.*]] = load <1 x i64>, ptr [[__Q0_ADDR_I]], align 8
// X86-NEXT:    store <1 x i64> [[TMP4]], ptr [[COERCE_I]], align 8
// X86-NEXT:    [[TMP6:%.*]] = load i64, ptr [[COERCE_I]], align 8
// X86-NEXT:    store <1 x i64> [[TMP5]], ptr [[COERCE3_I]], align 8
// X86-NEXT:    [[TMP7:%.*]] = load i64, ptr [[COERCE3_I]], align 8
// X86-NEXT:    store i64 [[TMP6]], ptr [[__Q1_I_I]], align 8
// X86-NEXT:    [[__Q11_I_I:%.*]] = load <1 x i64>, ptr [[__Q1_I_I]], align 8
// X86-NEXT:    store i64 [[TMP7]], ptr [[__Q0_I_I]], align 8
// X86-NEXT:    [[__Q02_I_I:%.*]] = load <1 x i64>, ptr [[__Q0_I_I]], align 8
// X86-NEXT:    store <1 x i64> [[__Q11_I_I]], ptr [[__Q1_ADDR_I_I]], align 8
// X86-NEXT:    store <1 x i64> [[__Q02_I_I]], ptr [[__Q0_ADDR_I_I]], align 8
// X86-NEXT:    [[TMP8:%.*]] = load <1 x i64>, ptr [[__Q1_ADDR_I_I]], align 8
// X86-NEXT:    [[TMP9:%.*]] = bitcast <1 x i64> [[TMP8]] to i64
// X86-NEXT:    [[TMP10:%.*]] = load <1 x i64>, ptr [[__Q0_ADDR_I_I]], align 8
// X86-NEXT:    [[TMP11:%.*]] = bitcast <1 x i64> [[TMP10]] to i64
// X86-NEXT:    store i64 [[TMP9]], ptr [[__Q1_ADDR_I4_I]], align 8
// X86-NEXT:    store i64 [[TMP11]], ptr [[__Q0_ADDR_I5_I]], align 8
// X86-NEXT:    [[TMP12:%.*]] = load i64, ptr [[__Q0_ADDR_I5_I]], align 8
// X86-NEXT:    [[VECINIT_I_I:%.*]] = insertelement <2 x i64> poison, i64 [[TMP12]], i32 0
// X86-NEXT:    [[TMP13:%.*]] = load i64, ptr [[__Q1_ADDR_I4_I]], align 8
// X86-NEXT:    [[VECINIT1_I_I:%.*]] = insertelement <2 x i64> [[VECINIT_I_I]], i64 [[TMP13]], i32 1
// X86-NEXT:    store <2 x i64> [[VECINIT1_I_I]], ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// X86-NEXT:    [[TMP14:%.*]] = load <2 x i64>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP14]]
//
__m128i test_mm_setr_epi64(__m64 A, __m64 B) {
  return _mm_setr_epi64(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_setr_pd(
// X86-SAME: double noundef [[A:%.*]], double noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__W_ADDR_I:%.*]] = alloca double, align 8
// X86-NEXT:    [[__X_ADDR_I:%.*]] = alloca double, align 8
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca double, align 8
// X86-NEXT:    [[B_ADDR:%.*]] = alloca double, align 8
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store double [[A]], ptr [[A_ADDR]], align 8
// X86-NEXT:    store double [[B]], ptr [[B_ADDR]], align 8
// X86-NEXT:    [[TMP0:%.*]] = load double, ptr [[A_ADDR]], align 8
// X86-NEXT:    [[TMP1:%.*]] = load double, ptr [[B_ADDR]], align 8
// X86-NEXT:    store double [[TMP0]], ptr [[__W_ADDR_I]], align 8
// X86-NEXT:    store double [[TMP1]], ptr [[__X_ADDR_I]], align 8
// X86-NEXT:    [[TMP2:%.*]] = load double, ptr [[__W_ADDR_I]], align 8
// X86-NEXT:    [[VECINIT_I:%.*]] = insertelement <2 x double> poison, double [[TMP2]], i32 0
// X86-NEXT:    [[TMP3:%.*]] = load double, ptr [[__X_ADDR_I]], align 8
// X86-NEXT:    [[VECINIT1_I:%.*]] = insertelement <2 x double> [[VECINIT_I]], double [[TMP3]], i32 1
// X86-NEXT:    store <2 x double> [[VECINIT1_I]], ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = load <2 x double>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP5]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP6:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP6]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP7:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP7]]
//
__m128d test_mm_setr_pd(double A, double B) {
  return _mm_setr_pd(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_setzero_pd(
// X86-SAME: ) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP2]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP3]]
//
__m128d test_mm_setzero_pd(void) {
  return _mm_setzero_pd();
}

//
// X86-LABEL: define <2 x i64> @test_mm_setzero_si128(
// X86-SAME: ) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP0]]
//
__m128i test_mm_setzero_si128(void) {
  return _mm_setzero_si128();
}

//
// X86-LABEL: define <2 x i64> @test_mm_shuffle_epi32(
// X86-SAME: <2 x i64> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = bitcast <2 x i64> [[TMP0]] to <4 x i32>
// X86-NEXT:    [[PERMIL:%.*]] = shufflevector <4 x i32> [[TMP1]], <4 x i32> poison, <4 x i32> zeroinitializer
// X86-NEXT:    [[TMP2:%.*]] = bitcast <4 x i32> [[PERMIL]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP2]]
//
__m128i test_mm_shuffle_epi32(__m128i A) {
  return _mm_shuffle_epi32(A, 0);
}

//
// X86-LABEL: define <2 x i64> @test_mm_shuffle_pd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[SHUFP:%.*]] = shufflevector <2 x double> [[TMP0]], <2 x double> [[TMP1]], <2 x i32> <i32 1, i32 2>
// X86-NEXT:    store <2 x double> [[SHUFP]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP2]]
//
__m128d test_mm_shuffle_pd(__m128d A, __m128d B) {
  return _mm_shuffle_pd(A, B, 1);
}

//
// X86-LABEL: define <2 x i64> @test_mm_shufflehi_epi16(
// X86-SAME: <2 x i64> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = bitcast <2 x i64> [[TMP0]] to <8 x i16>
// X86-NEXT:    [[PSHUFHW:%.*]] = shufflevector <8 x i16> [[TMP1]], <8 x i16> poison, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 4, i32 4, i32 4>
// X86-NEXT:    [[TMP2:%.*]] = bitcast <8 x i16> [[PSHUFHW]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP2]]
//
__m128i test_mm_shufflehi_epi16(__m128i A) {
  return _mm_shufflehi_epi16(A, 0);
}

//
// X86-LABEL: define <2 x i64> @test_mm_shufflelo_epi16(
// X86-SAME: <2 x i64> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = bitcast <2 x i64> [[TMP0]] to <8 x i16>
// X86-NEXT:    [[PSHUFLW:%.*]] = shufflevector <8 x i16> [[TMP1]], <8 x i16> poison, <8 x i32> <i32 0, i32 0, i32 0, i32 0, i32 4, i32 5, i32 6, i32 7>
// X86-NEXT:    [[TMP2:%.*]] = bitcast <8 x i16> [[PSHUFLW]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP2]]
//
__m128i test_mm_shufflelo_epi16(__m128i A) {
  return _mm_shufflelo_epi16(A, 0);
}

//
// X86-LABEL: define <2 x i64> @test_mm_sll_epi16(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__COUNT_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__COUNT_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x i64> [[TMP2]] to <8 x i16>
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[__COUNT_ADDR_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = bitcast <2 x i64> [[TMP4]] to <8 x i16>
// X86-NEXT:    [[TMP6:%.*]] = call <8 x i16> @llvm.x86.sse2.psll.w(<8 x i16> [[TMP3]], <8 x i16> [[TMP5]])
// X86-NEXT:    [[TMP7:%.*]] = bitcast <8 x i16> [[TMP6]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP7]]
//
__m128i test_mm_sll_epi16(__m128i A, __m128i B) {
  return _mm_sll_epi16(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_sll_epi32(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__COUNT_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__COUNT_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x i64> [[TMP2]] to <4 x i32>
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[__COUNT_ADDR_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = bitcast <2 x i64> [[TMP4]] to <4 x i32>
// X86-NEXT:    [[TMP6:%.*]] = call <4 x i32> @llvm.x86.sse2.psll.d(<4 x i32> [[TMP3]], <4 x i32> [[TMP5]])
// X86-NEXT:    [[TMP7:%.*]] = bitcast <4 x i32> [[TMP6]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP7]]
//
__m128i test_mm_sll_epi32(__m128i A, __m128i B) {
  return _mm_sll_epi32(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_sll_epi64(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__COUNT_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__COUNT_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x i64>, ptr [[__COUNT_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = call <2 x i64> @llvm.x86.sse2.psll.q(<2 x i64> [[TMP2]], <2 x i64> [[TMP3]])
// X86-NEXT:    ret <2 x i64> [[TMP4]]
//
__m128i test_mm_sll_epi64(__m128i A, __m128i B) {
  return _mm_sll_epi64(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_slli_epi16(
// X86-SAME: <2 x i64> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__COUNT_ADDR_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store i32 1, ptr [[__COUNT_ADDR_I]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = bitcast <2 x i64> [[TMP1]] to <8 x i16>
// X86-NEXT:    [[TMP3:%.*]] = load i32, ptr [[__COUNT_ADDR_I]], align 4
// X86-NEXT:    [[TMP4:%.*]] = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> [[TMP2]], i32 [[TMP3]])
// X86-NEXT:    [[TMP5:%.*]] = bitcast <8 x i16> [[TMP4]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128i test_mm_slli_epi16(__m128i A) {
  return _mm_slli_epi16(A, 1);
}

//
// X86-LABEL: define <2 x i64> @test_mm_slli_epi16_1(
// X86-SAME: <2 x i64> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__COUNT_ADDR_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store i32 -1, ptr [[__COUNT_ADDR_I]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = bitcast <2 x i64> [[TMP1]] to <8 x i16>
// X86-NEXT:    [[TMP3:%.*]] = load i32, ptr [[__COUNT_ADDR_I]], align 4
// X86-NEXT:    [[TMP4:%.*]] = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> [[TMP2]], i32 [[TMP3]])
// X86-NEXT:    [[TMP5:%.*]] = bitcast <8 x i16> [[TMP4]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128i test_mm_slli_epi16_1(__m128i A) {
  return _mm_slli_epi16(A, -1);
}

//
// X86-LABEL: define <2 x i64> @test_mm_slli_epi16_2(
// X86-SAME: <2 x i64> noundef [[A:%.*]], i32 noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__COUNT_ADDR_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store i32 [[B]], ptr [[B_ADDR]], align 4
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store i32 [[TMP1]], ptr [[__COUNT_ADDR_I]], align 4
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x i64> [[TMP2]] to <8 x i16>
// X86-NEXT:    [[TMP4:%.*]] = load i32, ptr [[__COUNT_ADDR_I]], align 4
// X86-NEXT:    [[TMP5:%.*]] = call <8 x i16> @llvm.x86.sse2.pslli.w(<8 x i16> [[TMP3]], i32 [[TMP4]])
// X86-NEXT:    [[TMP6:%.*]] = bitcast <8 x i16> [[TMP5]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP6]]
//
__m128i test_mm_slli_epi16_2(__m128i A, int B) {
  return _mm_slli_epi16(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_slli_epi32(
// X86-SAME: <2 x i64> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__COUNT_ADDR_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store i32 1, ptr [[__COUNT_ADDR_I]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = bitcast <2 x i64> [[TMP1]] to <4 x i32>
// X86-NEXT:    [[TMP3:%.*]] = load i32, ptr [[__COUNT_ADDR_I]], align 4
// X86-NEXT:    [[TMP4:%.*]] = call <4 x i32> @llvm.x86.sse2.pslli.d(<4 x i32> [[TMP2]], i32 [[TMP3]])
// X86-NEXT:    [[TMP5:%.*]] = bitcast <4 x i32> [[TMP4]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128i test_mm_slli_epi32(__m128i A) {
  return _mm_slli_epi32(A, 1);
}

//
// X86-LABEL: define <2 x i64> @test_mm_slli_epi32_1(
// X86-SAME: <2 x i64> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__COUNT_ADDR_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store i32 -1, ptr [[__COUNT_ADDR_I]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = bitcast <2 x i64> [[TMP1]] to <4 x i32>
// X86-NEXT:    [[TMP3:%.*]] = load i32, ptr [[__COUNT_ADDR_I]], align 4
// X86-NEXT:    [[TMP4:%.*]] = call <4 x i32> @llvm.x86.sse2.pslli.d(<4 x i32> [[TMP2]], i32 [[TMP3]])
// X86-NEXT:    [[TMP5:%.*]] = bitcast <4 x i32> [[TMP4]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128i test_mm_slli_epi32_1(__m128i A) {
  return _mm_slli_epi32(A, -1);
}

//
// X86-LABEL: define <2 x i64> @test_mm_slli_epi32_2(
// X86-SAME: <2 x i64> noundef [[A:%.*]], i32 noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__COUNT_ADDR_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store i32 [[B]], ptr [[B_ADDR]], align 4
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store i32 [[TMP1]], ptr [[__COUNT_ADDR_I]], align 4
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x i64> [[TMP2]] to <4 x i32>
// X86-NEXT:    [[TMP4:%.*]] = load i32, ptr [[__COUNT_ADDR_I]], align 4
// X86-NEXT:    [[TMP5:%.*]] = call <4 x i32> @llvm.x86.sse2.pslli.d(<4 x i32> [[TMP3]], i32 [[TMP4]])
// X86-NEXT:    [[TMP6:%.*]] = bitcast <4 x i32> [[TMP5]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP6]]
//
__m128i test_mm_slli_epi32_2(__m128i A, int B) {
  return _mm_slli_epi32(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_slli_epi64(
// X86-SAME: <2 x i64> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__COUNT_ADDR_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store i32 1, ptr [[__COUNT_ADDR_I]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load i32, ptr [[__COUNT_ADDR_I]], align 4
// X86-NEXT:    [[TMP3:%.*]] = call <2 x i64> @llvm.x86.sse2.pslli.q(<2 x i64> [[TMP1]], i32 [[TMP2]])
// X86-NEXT:    ret <2 x i64> [[TMP3]]
//
__m128i test_mm_slli_epi64(__m128i A) {
  return _mm_slli_epi64(A, 1);
}

//
// X86-LABEL: define <2 x i64> @test_mm_slli_epi64_1(
// X86-SAME: <2 x i64> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__COUNT_ADDR_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store i32 -1, ptr [[__COUNT_ADDR_I]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load i32, ptr [[__COUNT_ADDR_I]], align 4
// X86-NEXT:    [[TMP3:%.*]] = call <2 x i64> @llvm.x86.sse2.pslli.q(<2 x i64> [[TMP1]], i32 [[TMP2]])
// X86-NEXT:    ret <2 x i64> [[TMP3]]
//
__m128i test_mm_slli_epi64_1(__m128i A) {
  return _mm_slli_epi64(A, -1);
}

//
// X86-LABEL: define <2 x i64> @test_mm_slli_epi64_2(
// X86-SAME: <2 x i64> noundef [[A:%.*]], i32 noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__COUNT_ADDR_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store i32 [[B]], ptr [[B_ADDR]], align 4
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store i32 [[TMP1]], ptr [[__COUNT_ADDR_I]], align 4
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load i32, ptr [[__COUNT_ADDR_I]], align 4
// X86-NEXT:    [[TMP4:%.*]] = call <2 x i64> @llvm.x86.sse2.pslli.q(<2 x i64> [[TMP2]], i32 [[TMP3]])
// X86-NEXT:    ret <2 x i64> [[TMP4]]
//
__m128i test_mm_slli_epi64_2(__m128i A, int B) {
  return _mm_slli_epi64(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_slli_si128(
// X86-SAME: <2 x i64> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[CAST:%.*]] = bitcast <2 x i64> [[TMP0]] to <16 x i8>
// X86-NEXT:    [[PSLLDQ:%.*]] = shufflevector <16 x i8> zeroinitializer, <16 x i8> [[CAST]], <16 x i32> <i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26>
// X86-NEXT:    [[CAST1:%.*]] = bitcast <16 x i8> [[PSLLDQ]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[CAST1]]
//
__m128i test_mm_slli_si128(__m128i A) {
  return _mm_slli_si128(A, 5);
}

//
// X86-LABEL: define <2 x i64> @test_mm_slli_si128_2(
// X86-SAME: <2 x i64> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    ret <2 x i64> zeroinitializer
//
__m128i test_mm_slli_si128_2(__m128i A) {
  return _mm_slli_si128(A, 17);
}

//
// X86-LABEL: define <2 x i64> @test_mm_sqrt_pd(
// X86-SAME: <2 x double> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = call <2 x double> @llvm.sqrt.v2f64(<2 x double> [[TMP1]])
// X86-NEXT:    store <2 x double> [[TMP2]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP3]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP4:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128d test_mm_sqrt_pd(__m128d A) {
  return _mm_sqrt_pd(A);
}

//
// X86-LABEL: define <2 x i64> @test_mm_sqrt_sd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__C_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = extractelement <2 x double> [[TMP2]], i64 0
// X86-NEXT:    [[TMP4:%.*]] = call double @llvm.sqrt.f64(double [[TMP3]])
// X86-NEXT:    [[TMP5:%.*]] = insertelement <2 x double> [[TMP2]], double [[TMP4]], i64 0
// X86-NEXT:    store <2 x double> [[TMP5]], ptr [[__C_I]], align 16
// X86-NEXT:    [[TMP6:%.*]] = load <2 x double>, ptr [[__C_I]], align 16
// X86-NEXT:    [[VECEXT_I:%.*]] = extractelement <2 x double> [[TMP6]], i32 0
// X86-NEXT:    [[VECINIT_I:%.*]] = insertelement <2 x double> poison, double [[VECEXT_I]], i32 0
// X86-NEXT:    [[TMP7:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[VECEXT1_I:%.*]] = extractelement <2 x double> [[TMP7]], i32 1
// X86-NEXT:    [[VECINIT2_I:%.*]] = insertelement <2 x double> [[VECINIT_I]], double [[VECEXT1_I]], i32 1
// X86-NEXT:    store <2 x double> [[VECINIT2_I]], ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    [[TMP8:%.*]] = load <2 x double>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP8]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP9:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP9]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP10:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP10]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP11:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP11]]
//
__m128d test_mm_sqrt_sd(__m128d A, __m128d B) {
  return _mm_sqrt_sd(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_sra_epi16(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__COUNT_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__COUNT_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x i64> [[TMP2]] to <8 x i16>
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[__COUNT_ADDR_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = bitcast <2 x i64> [[TMP4]] to <8 x i16>
// X86-NEXT:    [[TMP6:%.*]] = call <8 x i16> @llvm.x86.sse2.psra.w(<8 x i16> [[TMP3]], <8 x i16> [[TMP5]])
// X86-NEXT:    [[TMP7:%.*]] = bitcast <8 x i16> [[TMP6]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP7]]
//
__m128i test_mm_sra_epi16(__m128i A, __m128i B) {
  return _mm_sra_epi16(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_sra_epi32(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__COUNT_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__COUNT_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x i64> [[TMP2]] to <4 x i32>
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[__COUNT_ADDR_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = bitcast <2 x i64> [[TMP4]] to <4 x i32>
// X86-NEXT:    [[TMP6:%.*]] = call <4 x i32> @llvm.x86.sse2.psra.d(<4 x i32> [[TMP3]], <4 x i32> [[TMP5]])
// X86-NEXT:    [[TMP7:%.*]] = bitcast <4 x i32> [[TMP6]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP7]]
//
__m128i test_mm_sra_epi32(__m128i A, __m128i B) {
  return _mm_sra_epi32(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_srai_epi16(
// X86-SAME: <2 x i64> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__COUNT_ADDR_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store i32 1, ptr [[__COUNT_ADDR_I]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = bitcast <2 x i64> [[TMP1]] to <8 x i16>
// X86-NEXT:    [[TMP3:%.*]] = load i32, ptr [[__COUNT_ADDR_I]], align 4
// X86-NEXT:    [[TMP4:%.*]] = call <8 x i16> @llvm.x86.sse2.psrai.w(<8 x i16> [[TMP2]], i32 [[TMP3]])
// X86-NEXT:    [[TMP5:%.*]] = bitcast <8 x i16> [[TMP4]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128i test_mm_srai_epi16(__m128i A) {
  return _mm_srai_epi16(A, 1);
}

//
// X86-LABEL: define <2 x i64> @test_mm_srai_epi16_1(
// X86-SAME: <2 x i64> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__COUNT_ADDR_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store i32 -1, ptr [[__COUNT_ADDR_I]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = bitcast <2 x i64> [[TMP1]] to <8 x i16>
// X86-NEXT:    [[TMP3:%.*]] = load i32, ptr [[__COUNT_ADDR_I]], align 4
// X86-NEXT:    [[TMP4:%.*]] = call <8 x i16> @llvm.x86.sse2.psrai.w(<8 x i16> [[TMP2]], i32 [[TMP3]])
// X86-NEXT:    [[TMP5:%.*]] = bitcast <8 x i16> [[TMP4]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128i test_mm_srai_epi16_1(__m128i A) {
  return _mm_srai_epi16(A, -1);
}

//
// X86-LABEL: define <2 x i64> @test_mm_srai_epi16_2(
// X86-SAME: <2 x i64> noundef [[A:%.*]], i32 noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__COUNT_ADDR_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store i32 [[B]], ptr [[B_ADDR]], align 4
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store i32 [[TMP1]], ptr [[__COUNT_ADDR_I]], align 4
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x i64> [[TMP2]] to <8 x i16>
// X86-NEXT:    [[TMP4:%.*]] = load i32, ptr [[__COUNT_ADDR_I]], align 4
// X86-NEXT:    [[TMP5:%.*]] = call <8 x i16> @llvm.x86.sse2.psrai.w(<8 x i16> [[TMP3]], i32 [[TMP4]])
// X86-NEXT:    [[TMP6:%.*]] = bitcast <8 x i16> [[TMP5]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP6]]
//
__m128i test_mm_srai_epi16_2(__m128i A, int B) {
  return _mm_srai_epi16(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_srai_epi32(
// X86-SAME: <2 x i64> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__COUNT_ADDR_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store i32 1, ptr [[__COUNT_ADDR_I]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = bitcast <2 x i64> [[TMP1]] to <4 x i32>
// X86-NEXT:    [[TMP3:%.*]] = load i32, ptr [[__COUNT_ADDR_I]], align 4
// X86-NEXT:    [[TMP4:%.*]] = call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> [[TMP2]], i32 [[TMP3]])
// X86-NEXT:    [[TMP5:%.*]] = bitcast <4 x i32> [[TMP4]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128i test_mm_srai_epi32(__m128i A) {
  return _mm_srai_epi32(A, 1);
}

//
// X86-LABEL: define <2 x i64> @test_mm_srai_epi32_1(
// X86-SAME: <2 x i64> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__COUNT_ADDR_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store i32 -1, ptr [[__COUNT_ADDR_I]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = bitcast <2 x i64> [[TMP1]] to <4 x i32>
// X86-NEXT:    [[TMP3:%.*]] = load i32, ptr [[__COUNT_ADDR_I]], align 4
// X86-NEXT:    [[TMP4:%.*]] = call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> [[TMP2]], i32 [[TMP3]])
// X86-NEXT:    [[TMP5:%.*]] = bitcast <4 x i32> [[TMP4]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128i test_mm_srai_epi32_1(__m128i A) {
  return _mm_srai_epi32(A, -1);
}

//
// X86-LABEL: define <2 x i64> @test_mm_srai_epi32_2(
// X86-SAME: <2 x i64> noundef [[A:%.*]], i32 noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__COUNT_ADDR_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store i32 [[B]], ptr [[B_ADDR]], align 4
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store i32 [[TMP1]], ptr [[__COUNT_ADDR_I]], align 4
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x i64> [[TMP2]] to <4 x i32>
// X86-NEXT:    [[TMP4:%.*]] = load i32, ptr [[__COUNT_ADDR_I]], align 4
// X86-NEXT:    [[TMP5:%.*]] = call <4 x i32> @llvm.x86.sse2.psrai.d(<4 x i32> [[TMP3]], i32 [[TMP4]])
// X86-NEXT:    [[TMP6:%.*]] = bitcast <4 x i32> [[TMP5]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP6]]
//
__m128i test_mm_srai_epi32_2(__m128i A, int B) {
  return _mm_srai_epi32(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_srl_epi16(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__COUNT_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__COUNT_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x i64> [[TMP2]] to <8 x i16>
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[__COUNT_ADDR_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = bitcast <2 x i64> [[TMP4]] to <8 x i16>
// X86-NEXT:    [[TMP6:%.*]] = call <8 x i16> @llvm.x86.sse2.psrl.w(<8 x i16> [[TMP3]], <8 x i16> [[TMP5]])
// X86-NEXT:    [[TMP7:%.*]] = bitcast <8 x i16> [[TMP6]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP7]]
//
__m128i test_mm_srl_epi16(__m128i A, __m128i B) {
  return _mm_srl_epi16(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_srl_epi32(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__COUNT_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__COUNT_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x i64> [[TMP2]] to <4 x i32>
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[__COUNT_ADDR_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = bitcast <2 x i64> [[TMP4]] to <4 x i32>
// X86-NEXT:    [[TMP6:%.*]] = call <4 x i32> @llvm.x86.sse2.psrl.d(<4 x i32> [[TMP3]], <4 x i32> [[TMP5]])
// X86-NEXT:    [[TMP7:%.*]] = bitcast <4 x i32> [[TMP6]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP7]]
//
__m128i test_mm_srl_epi32(__m128i A, __m128i B) {
  return _mm_srl_epi32(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_srl_epi64(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__COUNT_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__COUNT_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x i64>, ptr [[__COUNT_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = call <2 x i64> @llvm.x86.sse2.psrl.q(<2 x i64> [[TMP2]], <2 x i64> [[TMP3]])
// X86-NEXT:    ret <2 x i64> [[TMP4]]
//
__m128i test_mm_srl_epi64(__m128i A, __m128i B) {
  return _mm_srl_epi64(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_srli_epi16(
// X86-SAME: <2 x i64> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__COUNT_ADDR_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store i32 1, ptr [[__COUNT_ADDR_I]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = bitcast <2 x i64> [[TMP1]] to <8 x i16>
// X86-NEXT:    [[TMP3:%.*]] = load i32, ptr [[__COUNT_ADDR_I]], align 4
// X86-NEXT:    [[TMP4:%.*]] = call <8 x i16> @llvm.x86.sse2.psrli.w(<8 x i16> [[TMP2]], i32 [[TMP3]])
// X86-NEXT:    [[TMP5:%.*]] = bitcast <8 x i16> [[TMP4]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128i test_mm_srli_epi16(__m128i A) {
  return _mm_srli_epi16(A, 1);
}

//
// X86-LABEL: define <2 x i64> @test_mm_srli_epi16_1(
// X86-SAME: <2 x i64> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__COUNT_ADDR_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store i32 -1, ptr [[__COUNT_ADDR_I]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = bitcast <2 x i64> [[TMP1]] to <8 x i16>
// X86-NEXT:    [[TMP3:%.*]] = load i32, ptr [[__COUNT_ADDR_I]], align 4
// X86-NEXT:    [[TMP4:%.*]] = call <8 x i16> @llvm.x86.sse2.psrli.w(<8 x i16> [[TMP2]], i32 [[TMP3]])
// X86-NEXT:    [[TMP5:%.*]] = bitcast <8 x i16> [[TMP4]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128i test_mm_srli_epi16_1(__m128i A) {
  return _mm_srli_epi16(A, -1);
}

//
// X86-LABEL: define <2 x i64> @test_mm_srli_epi16_2(
// X86-SAME: <2 x i64> noundef [[A:%.*]], i32 noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__COUNT_ADDR_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store i32 [[B]], ptr [[B_ADDR]], align 4
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store i32 [[TMP1]], ptr [[__COUNT_ADDR_I]], align 4
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x i64> [[TMP2]] to <8 x i16>
// X86-NEXT:    [[TMP4:%.*]] = load i32, ptr [[__COUNT_ADDR_I]], align 4
// X86-NEXT:    [[TMP5:%.*]] = call <8 x i16> @llvm.x86.sse2.psrli.w(<8 x i16> [[TMP3]], i32 [[TMP4]])
// X86-NEXT:    [[TMP6:%.*]] = bitcast <8 x i16> [[TMP5]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP6]]
//
__m128i test_mm_srli_epi16_2(__m128i A, int B) {
  return _mm_srli_epi16(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_srli_epi32(
// X86-SAME: <2 x i64> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__COUNT_ADDR_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store i32 1, ptr [[__COUNT_ADDR_I]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = bitcast <2 x i64> [[TMP1]] to <4 x i32>
// X86-NEXT:    [[TMP3:%.*]] = load i32, ptr [[__COUNT_ADDR_I]], align 4
// X86-NEXT:    [[TMP4:%.*]] = call <4 x i32> @llvm.x86.sse2.psrli.d(<4 x i32> [[TMP2]], i32 [[TMP3]])
// X86-NEXT:    [[TMP5:%.*]] = bitcast <4 x i32> [[TMP4]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128i test_mm_srli_epi32(__m128i A) {
  return _mm_srli_epi32(A, 1);
}

//
// X86-LABEL: define <2 x i64> @test_mm_srli_epi32_1(
// X86-SAME: <2 x i64> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__COUNT_ADDR_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store i32 -1, ptr [[__COUNT_ADDR_I]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = bitcast <2 x i64> [[TMP1]] to <4 x i32>
// X86-NEXT:    [[TMP3:%.*]] = load i32, ptr [[__COUNT_ADDR_I]], align 4
// X86-NEXT:    [[TMP4:%.*]] = call <4 x i32> @llvm.x86.sse2.psrli.d(<4 x i32> [[TMP2]], i32 [[TMP3]])
// X86-NEXT:    [[TMP5:%.*]] = bitcast <4 x i32> [[TMP4]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128i test_mm_srli_epi32_1(__m128i A) {
  return _mm_srli_epi32(A, -1);
}

//
// X86-LABEL: define <2 x i64> @test_mm_srli_epi32_2(
// X86-SAME: <2 x i64> noundef [[A:%.*]], i32 noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__COUNT_ADDR_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store i32 [[B]], ptr [[B_ADDR]], align 4
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store i32 [[TMP1]], ptr [[__COUNT_ADDR_I]], align 4
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x i64> [[TMP2]] to <4 x i32>
// X86-NEXT:    [[TMP4:%.*]] = load i32, ptr [[__COUNT_ADDR_I]], align 4
// X86-NEXT:    [[TMP5:%.*]] = call <4 x i32> @llvm.x86.sse2.psrli.d(<4 x i32> [[TMP3]], i32 [[TMP4]])
// X86-NEXT:    [[TMP6:%.*]] = bitcast <4 x i32> [[TMP5]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP6]]
//
__m128i test_mm_srli_epi32_2(__m128i A, int B) {
  return _mm_srli_epi32(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_srli_epi64(
// X86-SAME: <2 x i64> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__COUNT_ADDR_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store i32 1, ptr [[__COUNT_ADDR_I]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load i32, ptr [[__COUNT_ADDR_I]], align 4
// X86-NEXT:    [[TMP3:%.*]] = call <2 x i64> @llvm.x86.sse2.psrli.q(<2 x i64> [[TMP1]], i32 [[TMP2]])
// X86-NEXT:    ret <2 x i64> [[TMP3]]
//
__m128i test_mm_srli_epi64(__m128i A) {
  return _mm_srli_epi64(A, 1);
}

//
// X86-LABEL: define <2 x i64> @test_mm_srli_epi64_1(
// X86-SAME: <2 x i64> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__COUNT_ADDR_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store i32 -1, ptr [[__COUNT_ADDR_I]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load i32, ptr [[__COUNT_ADDR_I]], align 4
// X86-NEXT:    [[TMP3:%.*]] = call <2 x i64> @llvm.x86.sse2.psrli.q(<2 x i64> [[TMP1]], i32 [[TMP2]])
// X86-NEXT:    ret <2 x i64> [[TMP3]]
//
__m128i test_mm_srli_epi64_1(__m128i A) {
  return _mm_srli_epi64(A, -1);
}

//
// X86-LABEL: define <2 x i64> @test_mm_srli_epi64_2(
// X86-SAME: <2 x i64> noundef [[A:%.*]], i32 noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__COUNT_ADDR_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store i32 [[B]], ptr [[B_ADDR]], align 4
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store i32 [[TMP1]], ptr [[__COUNT_ADDR_I]], align 4
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load i32, ptr [[__COUNT_ADDR_I]], align 4
// X86-NEXT:    [[TMP4:%.*]] = call <2 x i64> @llvm.x86.sse2.psrli.q(<2 x i64> [[TMP2]], i32 [[TMP3]])
// X86-NEXT:    ret <2 x i64> [[TMP4]]
//
__m128i test_mm_srli_epi64_2(__m128i A, int B) {
  return _mm_srli_epi64(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_srli_si128(
// X86-SAME: <2 x i64> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[CAST:%.*]] = bitcast <2 x i64> [[TMP0]] to <16 x i8>
// X86-NEXT:    [[PSRLDQ:%.*]] = shufflevector <16 x i8> [[CAST]], <16 x i8> zeroinitializer, <16 x i32> <i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20>
// X86-NEXT:    [[CAST1:%.*]] = bitcast <16 x i8> [[PSRLDQ]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[CAST1]]
//
__m128i test_mm_srli_si128(__m128i A) {
  return _mm_srli_si128(A, 5);
}

//
// X86-LABEL: define <2 x i64> @test_mm_srli_si128_2(
// X86-SAME: <2 x i64> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    ret <2 x i64> zeroinitializer
//
__m128i test_mm_srli_si128_2(__m128i A) {
  // ret <2 x i64> zeroinitializer
  return _mm_srli_si128(A, 17);
}

//
// X86-LABEL: define void @test_mm_store_pd(
// X86-SAME: ptr noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__DP_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store ptr [[TMP0]], ptr [[__DP_ADDR_I]], align 4
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[__DP_ADDR_I]], align 4
// X86-NEXT:    store <2 x double> [[TMP2]], ptr [[TMP3]], align 16
// X86-NEXT:    ret void
//
void test_mm_store_pd(double* A, __m128d B) {
  _mm_store_pd(A, B);
}

//
// X86-LABEL: define void @test_mm_store_pd1(
// X86-SAME: ptr noundef [[X:%.*]], <2 x double> noundef [[Y:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__DP_ADDR_I_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__DP_ADDR_I1:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I2:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__DP_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[X_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[Y_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store ptr [[X]], ptr [[X_ADDR]], align 4
// X86-NEXT:    store <2 x double> [[Y]], ptr [[Y_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[X_ADDR]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[Y_ADDR]], align 16
// X86-NEXT:    store ptr [[TMP0]], ptr [[__DP_ADDR_I]], align 4
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[__DP_ADDR_I]], align 4
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store ptr [[TMP2]], ptr [[__DP_ADDR_I1]], align 4
// X86-NEXT:    store <2 x double> [[TMP3]], ptr [[__A_ADDR_I2]], align 16
// X86-NEXT:    [[TMP4:%.*]] = load <2 x double>, ptr [[__A_ADDR_I2]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x double>, ptr [[__A_ADDR_I2]], align 16
// X86-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <2 x double> [[TMP4]], <2 x double> [[TMP5]], <2 x i32> zeroinitializer
// X86-NEXT:    store <2 x double> [[SHUFFLE_I]], ptr [[__A_ADDR_I2]], align 16
// X86-NEXT:    [[TMP6:%.*]] = load ptr, ptr [[__DP_ADDR_I1]], align 4
// X86-NEXT:    [[TMP7:%.*]] = load <2 x double>, ptr [[__A_ADDR_I2]], align 16
// X86-NEXT:    store ptr [[TMP6]], ptr [[__DP_ADDR_I_I]], align 4
// X86-NEXT:    store <2 x double> [[TMP7]], ptr [[__A_ADDR_I_I]], align 16
// X86-NEXT:    [[TMP8:%.*]] = load <2 x double>, ptr [[__A_ADDR_I_I]], align 16
// X86-NEXT:    [[TMP9:%.*]] = load ptr, ptr [[__DP_ADDR_I_I]], align 4
// X86-NEXT:    store <2 x double> [[TMP8]], ptr [[TMP9]], align 16
// X86-NEXT:    ret void
//
void test_mm_store_pd1(double* x, __m128d y) {
  _mm_store_pd1(x, y);
}

//
// X86-LABEL: define void @test_mm_store_sd(
// X86-SAME: ptr noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__DP_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store ptr [[TMP0]], ptr [[__DP_ADDR_I]], align 4
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[VECEXT_I:%.*]] = extractelement <2 x double> [[TMP2]], i32 0
// X86-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[__DP_ADDR_I]], align 4
// X86-NEXT:    store double [[VECEXT_I]], ptr [[TMP3]], align 1
// X86-NEXT:    ret void
//
void test_mm_store_sd(double* A, __m128d B) {
  _mm_store_sd(A, B);
}

//
// X86-LABEL: define void @test_mm_store_si128(
// X86-SAME: ptr noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__P_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store ptr [[TMP0]], ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    store <2 x i64> [[TMP2]], ptr [[TMP3]], align 16
// X86-NEXT:    ret void
//
void test_mm_store_si128(__m128i* A, __m128i B) {
  _mm_store_si128(A, B);
}

//
// X86-LABEL: define void @test_mm_store1_pd(
// X86-SAME: ptr noundef [[X:%.*]], <2 x double> noundef [[Y:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__DP_ADDR_I_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__DP_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[X_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[Y_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store ptr [[X]], ptr [[X_ADDR]], align 4
// X86-NEXT:    store <2 x double> [[Y]], ptr [[Y_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[X_ADDR]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[Y_ADDR]], align 16
// X86-NEXT:    store ptr [[TMP0]], ptr [[__DP_ADDR_I]], align 4
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <2 x double> [[TMP2]], <2 x double> [[TMP3]], <2 x i32> zeroinitializer
// X86-NEXT:    store <2 x double> [[SHUFFLE_I]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = load ptr, ptr [[__DP_ADDR_I]], align 4
// X86-NEXT:    [[TMP5:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store ptr [[TMP4]], ptr [[__DP_ADDR_I_I]], align 4
// X86-NEXT:    store <2 x double> [[TMP5]], ptr [[__A_ADDR_I_I]], align 16
// X86-NEXT:    [[TMP6:%.*]] = load <2 x double>, ptr [[__A_ADDR_I_I]], align 16
// X86-NEXT:    [[TMP7:%.*]] = load ptr, ptr [[__DP_ADDR_I_I]], align 4
// X86-NEXT:    store <2 x double> [[TMP6]], ptr [[TMP7]], align 16
// X86-NEXT:    ret void
//
void test_mm_store1_pd(double* x, __m128d y) {
  _mm_store1_pd(x, y);
}

//
// X86-LABEL: define void @test_mm_storeh_pd(
// X86-SAME: ptr noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__DP_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store ptr [[TMP0]], ptr [[__DP_ADDR_I]], align 4
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[VECEXT_I:%.*]] = extractelement <2 x double> [[TMP2]], i32 1
// X86-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[__DP_ADDR_I]], align 4
// X86-NEXT:    store double [[VECEXT_I]], ptr [[TMP3]], align 1
// X86-NEXT:    ret void
//
void test_mm_storeh_pd(double* A, __m128d B) {
  _mm_storeh_pd(A, B);
}

//
// X86-LABEL: define void @test_mm_storel_epi64(
// X86-SAME: <2 x i64> noundef [[X:%.*]], ptr noundef [[Y:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__P_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[X_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[Y_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    store <2 x i64> [[X]], ptr [[X_ADDR]], align 16
// X86-NEXT:    store ptr [[Y]], ptr [[Y_ADDR]], align 4
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[Y_ADDR]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[X_ADDR]], align 16
// X86-NEXT:    store ptr [[TMP0]], ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[VECEXT_I:%.*]] = extractelement <2 x i64> [[TMP2]], i32 0
// X86-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    store i64 [[VECEXT_I]], ptr [[TMP3]], align 1
// X86-NEXT:    ret void
//
void test_mm_storel_epi64(__m128i x, void* y) {
  _mm_storel_epi64(y, x);
}

//
// X86-LABEL: define void @test_mm_storel_pd(
// X86-SAME: ptr noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__DP_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store ptr [[TMP0]], ptr [[__DP_ADDR_I]], align 4
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[VECEXT_I:%.*]] = extractelement <2 x double> [[TMP2]], i32 0
// X86-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[__DP_ADDR_I]], align 4
// X86-NEXT:    store double [[VECEXT_I]], ptr [[TMP3]], align 1
// X86-NEXT:    ret void
//
void test_mm_storel_pd(double* A, __m128d B) {
  _mm_storel_pd(A, B);
}

//
// X86-LABEL: define void @test_mm_storer_pd(
// X86-SAME: <2 x double> noundef [[A:%.*]], ptr noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__DP_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store ptr [[B]], ptr [[B_ADDR]], align 4
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[B_ADDR]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    store ptr [[TMP0]], ptr [[__DP_ADDR_I]], align 4
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <2 x double> [[TMP2]], <2 x double> [[TMP3]], <2 x i32> <i32 1, i32 0>
// X86-NEXT:    store <2 x double> [[SHUFFLE_I]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load ptr, ptr [[__DP_ADDR_I]], align 4
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[TMP5]], align 16
// X86-NEXT:    ret void
//
void test_mm_storer_pd(__m128d A, double* B) {
  _mm_storer_pd(B, A);
}

//
// X86-LABEL: define void @test_mm_storeu_pd(
// X86-SAME: ptr noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__DP_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store ptr [[TMP0]], ptr [[__DP_ADDR_I]], align 4
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[__DP_ADDR_I]], align 4
// X86-NEXT:    store <2 x double> [[TMP2]], ptr [[TMP3]], align 1
// X86-NEXT:    ret void
//
void test_mm_storeu_pd(double* A, __m128d B) {
  _mm_storeu_pd(A, B);
}

//
// X86-LABEL: define void @test_mm_storeu_si128(
// X86-SAME: ptr noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__P_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store ptr [[TMP0]], ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    store <2 x i64> [[TMP2]], ptr [[TMP3]], align 1
// X86-NEXT:    ret void
//
void test_mm_storeu_si128(__m128i* A, __m128i B) {
  _mm_storeu_si128(A, B);
}

//
// X86-LABEL: define void @test_mm_storeu_si64(
// X86-SAME: ptr noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__P_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store ptr [[TMP0]], ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[VECEXT_I:%.*]] = extractelement <2 x i64> [[TMP2]], i32 0
// X86-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    store i64 [[VECEXT_I]], ptr [[TMP3]], align 1
// X86-NEXT:    ret void
//
void test_mm_storeu_si64(void* A, __m128i B) {
  _mm_storeu_si64(A, B);
}

//
// X86-LABEL: define void @test_mm_storeu_si32(
// X86-SAME: ptr noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__P_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store ptr [[TMP0]], ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x i64> [[TMP2]] to <4 x i32>
// X86-NEXT:    [[VECEXT_I:%.*]] = extractelement <4 x i32> [[TMP3]], i32 0
// X86-NEXT:    [[TMP4:%.*]] = load ptr, ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    store i32 [[VECEXT_I]], ptr [[TMP4]], align 1
// X86-NEXT:    ret void
//
void test_mm_storeu_si32(void* A, __m128i B) {
  _mm_storeu_si32(A, B);
}

//
// X86-LABEL: define void @test_mm_storeu_si16(
// X86-SAME: ptr noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__P_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store ptr [[TMP0]], ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x i64> [[TMP2]] to <8 x i16>
// X86-NEXT:    [[VECEXT_I:%.*]] = extractelement <8 x i16> [[TMP3]], i32 0
// X86-NEXT:    [[TMP4:%.*]] = load ptr, ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    store i16 [[VECEXT_I]], ptr [[TMP4]], align 1
// X86-NEXT:    ret void
//
void test_mm_storeu_si16(void* A, __m128i B) {
  _mm_storeu_si16(A, B);
}

//
// X86-LABEL: define void @test_mm_stream_pd(
// X86-SAME: ptr noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__P_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store ptr [[TMP0]], ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    store <2 x double> [[TMP2]], ptr [[TMP3]], align 16, !nontemporal [[META3:![0-9]+]]
// X86-NEXT:    ret void
//
void test_mm_stream_pd(double *A, __m128d B) {
  _mm_stream_pd(A, B);
}

//
// X86-LABEL: define void @test_mm_stream_pd_void(
// X86-SAME: ptr noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__P_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store ptr [[TMP0]], ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    store <2 x double> [[TMP2]], ptr [[TMP3]], align 16, !nontemporal [[META3]]
// X86-NEXT:    ret void
//
void test_mm_stream_pd_void(void *A, __m128d B) {
  _mm_stream_pd(A, B);
}

//
// X86-LABEL: define void @test_mm_stream_si32(
// X86-SAME: ptr noundef [[A:%.*]], i32 noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__P_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    store i32 [[B]], ptr [[B_ADDR]], align 4
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// X86-NEXT:    store ptr [[TMP0]], ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    store i32 [[TMP1]], ptr [[__A_ADDR_I]], align 4
// X86-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    [[TMP3:%.*]] = load i32, ptr [[__A_ADDR_I]], align 4
// X86-NEXT:    store i32 [[TMP3]], ptr [[TMP2]], align 1, !nontemporal [[META3]]
// X86-NEXT:    ret void
//
void test_mm_stream_si32(int *A, int B) {
  _mm_stream_si32(A, B);
}

//
// X86-LABEL: define void @test_mm_stream_si32_void(
// X86-SAME: ptr noundef [[A:%.*]], i32 noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__P_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    store i32 [[B]], ptr [[B_ADDR]], align 4
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// X86-NEXT:    store ptr [[TMP0]], ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    store i32 [[TMP1]], ptr [[__A_ADDR_I]], align 4
// X86-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    [[TMP3:%.*]] = load i32, ptr [[__A_ADDR_I]], align 4
// X86-NEXT:    store i32 [[TMP3]], ptr [[TMP2]], align 1, !nontemporal [[META3]]
// X86-NEXT:    ret void
//
void test_mm_stream_si32_void(void *A, int B) {
  _mm_stream_si32(A, B);
}

#ifdef __x86_64__
//
void test_mm_stream_si64(long long *A, long long B) {
  _mm_stream_si64(A, B);
}

//
void test_mm_stream_si64_void(void *A, long long B) {
  _mm_stream_si64(A, B);
}
#endif

//
// X86-LABEL: define void @test_mm_stream_si128(
// X86-SAME: ptr noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__P_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store ptr [[TMP0]], ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    store <2 x i64> [[TMP2]], ptr [[TMP3]], align 16, !nontemporal [[META3]]
// X86-NEXT:    ret void
//
void test_mm_stream_si128(__m128i *A, __m128i B) {
  _mm_stream_si128(A, B);
}

//
// X86-LABEL: define void @test_mm_stream_si128_void(
// X86-SAME: ptr noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__P_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store ptr [[TMP0]], ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    store <2 x i64> [[TMP2]], ptr [[TMP3]], align 16, !nontemporal [[META3]]
// X86-NEXT:    ret void
//
void test_mm_stream_si128_void(void *A, __m128i B) {
  _mm_stream_si128(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_sub_epi8(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x i64> [[TMP2]] to <16 x i8>
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = bitcast <2 x i64> [[TMP4]] to <16 x i8>
// X86-NEXT:    [[SUB_I:%.*]] = sub <16 x i8> [[TMP3]], [[TMP5]]
// X86-NEXT:    [[TMP6:%.*]] = bitcast <16 x i8> [[SUB_I]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP6]]
//
__m128i test_mm_sub_epi8(__m128i A, __m128i B) {
  return _mm_sub_epi8(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_sub_epi16(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x i64> [[TMP2]] to <8 x i16>
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = bitcast <2 x i64> [[TMP4]] to <8 x i16>
// X86-NEXT:    [[SUB_I:%.*]] = sub <8 x i16> [[TMP3]], [[TMP5]]
// X86-NEXT:    [[TMP6:%.*]] = bitcast <8 x i16> [[SUB_I]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP6]]
//
__m128i test_mm_sub_epi16(__m128i A, __m128i B) {
  return _mm_sub_epi16(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_sub_epi32(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x i64> [[TMP2]] to <4 x i32>
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = bitcast <2 x i64> [[TMP4]] to <4 x i32>
// X86-NEXT:    [[SUB_I:%.*]] = sub <4 x i32> [[TMP3]], [[TMP5]]
// X86-NEXT:    [[TMP6:%.*]] = bitcast <4 x i32> [[SUB_I]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP6]]
//
__m128i test_mm_sub_epi32(__m128i A, __m128i B) {
  return _mm_sub_epi32(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_sub_epi64(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[SUB_I:%.*]] = sub <2 x i64> [[TMP2]], [[TMP3]]
// X86-NEXT:    ret <2 x i64> [[SUB_I]]
//
__m128i test_mm_sub_epi64(__m128i A, __m128i B) {
  return _mm_sub_epi64(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_sub_pd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[SUB_I:%.*]] = fsub <2 x double> [[TMP2]], [[TMP3]]
// X86-NEXT:    store <2 x double> [[SUB_I]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP4]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP5]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP6:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP6]]
//
__m128d test_mm_sub_pd(__m128d A, __m128d B) {
  return _mm_sub_pd(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_sub_sd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[VECEXT_I:%.*]] = extractelement <2 x double> [[TMP2]], i32 0
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[VECEXT1_I:%.*]] = extractelement <2 x double> [[TMP3]], i32 0
// X86-NEXT:    [[SUB_I:%.*]] = fsub double [[VECEXT1_I]], [[VECEXT_I]]
// X86-NEXT:    [[TMP4:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[VECINS_I:%.*]] = insertelement <2 x double> [[TMP4]], double [[SUB_I]], i32 0
// X86-NEXT:    store <2 x double> [[VECINS_I]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP5]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP6:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP6]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP7:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP7]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP8:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP8]]
//
__m128d test_mm_sub_sd(__m128d A, __m128d B) {
  return _mm_sub_sd(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_subs_epi8(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x i64> [[TMP2]] to <16 x i8>
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = bitcast <2 x i64> [[TMP4]] to <16 x i8>
// X86-NEXT:    [[ELT_SAT_I:%.*]] = call <16 x i8> @llvm.ssub.sat.v16i8(<16 x i8> [[TMP3]], <16 x i8> [[TMP5]])
// X86-NEXT:    [[TMP6:%.*]] = bitcast <16 x i8> [[ELT_SAT_I]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP6]]
//
__m128i test_mm_subs_epi8(__m128i A, __m128i B) {
  return _mm_subs_epi8(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_subs_epi16(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x i64> [[TMP2]] to <8 x i16>
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = bitcast <2 x i64> [[TMP4]] to <8 x i16>
// X86-NEXT:    [[ELT_SAT_I:%.*]] = call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> [[TMP3]], <8 x i16> [[TMP5]])
// X86-NEXT:    [[TMP6:%.*]] = bitcast <8 x i16> [[ELT_SAT_I]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP6]]
//
__m128i test_mm_subs_epi16(__m128i A, __m128i B) {
  return _mm_subs_epi16(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_subs_epu8(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x i64> [[TMP2]] to <16 x i8>
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = bitcast <2 x i64> [[TMP4]] to <16 x i8>
// X86-NEXT:    [[ELT_SAT_I:%.*]] = call <16 x i8> @llvm.usub.sat.v16i8(<16 x i8> [[TMP3]], <16 x i8> [[TMP5]])
// X86-NEXT:    [[TMP6:%.*]] = bitcast <16 x i8> [[ELT_SAT_I]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP6]]
//
__m128i test_mm_subs_epu8(__m128i A, __m128i B) {
  return _mm_subs_epu8(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_subs_epu16(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x i64> [[TMP2]] to <8 x i16>
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = bitcast <2 x i64> [[TMP4]] to <8 x i16>
// X86-NEXT:    [[ELT_SAT_I:%.*]] = call <8 x i16> @llvm.usub.sat.v8i16(<8 x i16> [[TMP3]], <8 x i16> [[TMP5]])
// X86-NEXT:    [[TMP6:%.*]] = bitcast <8 x i16> [[ELT_SAT_I]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP6]]
//
__m128i test_mm_subs_epu16(__m128i A, __m128i B) {
  return _mm_subs_epu16(A, B);
}

//
// X86-LABEL: define i32 @test_mm_ucomieq_sd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = call i32 @llvm.x86.sse2.ucomieq.sd(<2 x double> [[TMP2]], <2 x double> [[TMP3]])
// X86-NEXT:    ret i32 [[TMP4]]
//
int test_mm_ucomieq_sd(__m128d A, __m128d B) {
  return _mm_ucomieq_sd(A, B);
}

//
// X86-LABEL: define i32 @test_mm_ucomige_sd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = call i32 @llvm.x86.sse2.ucomige.sd(<2 x double> [[TMP2]], <2 x double> [[TMP3]])
// X86-NEXT:    ret i32 [[TMP4]]
//
int test_mm_ucomige_sd(__m128d A, __m128d B) {
  return _mm_ucomige_sd(A, B);
}

//
// X86-LABEL: define i32 @test_mm_ucomigt_sd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = call i32 @llvm.x86.sse2.ucomigt.sd(<2 x double> [[TMP2]], <2 x double> [[TMP3]])
// X86-NEXT:    ret i32 [[TMP4]]
//
int test_mm_ucomigt_sd(__m128d A, __m128d B) {
  return _mm_ucomigt_sd(A, B);
}

//
// X86-LABEL: define i32 @test_mm_ucomile_sd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = call i32 @llvm.x86.sse2.ucomile.sd(<2 x double> [[TMP2]], <2 x double> [[TMP3]])
// X86-NEXT:    ret i32 [[TMP4]]
//
int test_mm_ucomile_sd(__m128d A, __m128d B) {
  return _mm_ucomile_sd(A, B);
}

//
// X86-LABEL: define i32 @test_mm_ucomilt_sd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = call i32 @llvm.x86.sse2.ucomilt.sd(<2 x double> [[TMP2]], <2 x double> [[TMP3]])
// X86-NEXT:    ret i32 [[TMP4]]
//
int test_mm_ucomilt_sd(__m128d A, __m128d B) {
  return _mm_ucomilt_sd(A, B);
}

//
// X86-LABEL: define i32 @test_mm_ucomineq_sd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = call i32 @llvm.x86.sse2.ucomineq.sd(<2 x double> [[TMP2]], <2 x double> [[TMP3]])
// X86-NEXT:    ret i32 [[TMP4]]
//
int test_mm_ucomineq_sd(__m128d A, __m128d B) {
  return _mm_ucomineq_sd(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_undefined_pd(
// X86-SAME: ) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[TMP0:%.*]] = freeze <2 x double> undef
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP2]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP3]]
//
__m128d test_mm_undefined_pd(void) {
  //
  return _mm_undefined_pd();
}

//
// X86-LABEL: define <2 x i64> @test_mm_undefined_si128(
// X86-SAME: ) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[TMP0:%.*]] = freeze <2 x double> undef
// X86-NEXT:    [[TMP1:%.*]] = bitcast <2 x double> [[TMP0]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP1]]
//
__m128i test_mm_undefined_si128(void) {
  return _mm_undefined_si128();
}

//
// X86-LABEL: define <2 x i64> @test_mm_unpackhi_epi8(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x i64> [[TMP2]] to <16 x i8>
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = bitcast <2 x i64> [[TMP4]] to <16 x i8>
// X86-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <16 x i8> [[TMP3]], <16 x i8> [[TMP5]], <16 x i32> <i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
// X86-NEXT:    [[TMP6:%.*]] = bitcast <16 x i8> [[SHUFFLE_I]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP6]]
//
__m128i test_mm_unpackhi_epi8(__m128i A, __m128i B) {
  return _mm_unpackhi_epi8(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_unpackhi_epi16(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x i64> [[TMP2]] to <8 x i16>
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = bitcast <2 x i64> [[TMP4]] to <8 x i16>
// X86-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <8 x i16> [[TMP3]], <8 x i16> [[TMP5]], <8 x i32> <i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
// X86-NEXT:    [[TMP6:%.*]] = bitcast <8 x i16> [[SHUFFLE_I]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP6]]
//
__m128i test_mm_unpackhi_epi16(__m128i A, __m128i B) {
  return _mm_unpackhi_epi16(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_unpackhi_epi32(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x i64> [[TMP2]] to <4 x i32>
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = bitcast <2 x i64> [[TMP4]] to <4 x i32>
// X86-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <4 x i32> [[TMP3]], <4 x i32> [[TMP5]], <4 x i32> <i32 2, i32 6, i32 3, i32 7>
// X86-NEXT:    [[TMP6:%.*]] = bitcast <4 x i32> [[SHUFFLE_I]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP6]]
//
__m128i test_mm_unpackhi_epi32(__m128i A, __m128i B) {
  return _mm_unpackhi_epi32(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_unpackhi_epi64(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <2 x i64> [[TMP2]], <2 x i64> [[TMP3]], <2 x i32> <i32 1, i32 3>
// X86-NEXT:    ret <2 x i64> [[SHUFFLE_I]]
//
__m128i test_mm_unpackhi_epi64(__m128i A, __m128i B) {
  return _mm_unpackhi_epi64(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_unpackhi_pd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <2 x double> [[TMP2]], <2 x double> [[TMP3]], <2 x i32> <i32 1, i32 3>
// X86-NEXT:    store <2 x double> [[SHUFFLE_I]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP4]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP5]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP6:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP6]]
//
__m128d test_mm_unpackhi_pd(__m128d A, __m128d B) {
  return _mm_unpackhi_pd(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_unpacklo_epi8(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x i64> [[TMP2]] to <16 x i8>
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = bitcast <2 x i64> [[TMP4]] to <16 x i8>
// X86-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <16 x i8> [[TMP3]], <16 x i8> [[TMP5]], <16 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23>
// X86-NEXT:    [[TMP6:%.*]] = bitcast <16 x i8> [[SHUFFLE_I]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP6]]
//
__m128i test_mm_unpacklo_epi8(__m128i A, __m128i B) {
  return _mm_unpacklo_epi8(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_unpacklo_epi16(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x i64> [[TMP2]] to <8 x i16>
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = bitcast <2 x i64> [[TMP4]] to <8 x i16>
// X86-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <8 x i16> [[TMP3]], <8 x i16> [[TMP5]], <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11>
// X86-NEXT:    [[TMP6:%.*]] = bitcast <8 x i16> [[SHUFFLE_I]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP6]]
//
__m128i test_mm_unpacklo_epi16(__m128i A, __m128i B) {
  return _mm_unpacklo_epi16(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_unpacklo_epi32(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x i64> [[TMP2]] to <4 x i32>
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = bitcast <2 x i64> [[TMP4]] to <4 x i32>
// X86-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <4 x i32> [[TMP3]], <4 x i32> [[TMP5]], <4 x i32> <i32 0, i32 4, i32 1, i32 5>
// X86-NEXT:    [[TMP6:%.*]] = bitcast <4 x i32> [[SHUFFLE_I]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP6]]
//
__m128i test_mm_unpacklo_epi32(__m128i A, __m128i B) {
  return _mm_unpacklo_epi32(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_unpacklo_epi64(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <2 x i64> [[TMP2]], <2 x i64> [[TMP3]], <2 x i32> <i32 0, i32 2>
// X86-NEXT:    ret <2 x i64> [[SHUFFLE_I]]
//
__m128i test_mm_unpacklo_epi64(__m128i A, __m128i B) {
  return _mm_unpacklo_epi64(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_unpacklo_pd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <2 x double> [[TMP2]], <2 x double> [[TMP3]], <2 x i32> <i32 0, i32 2>
// X86-NEXT:    store <2 x double> [[SHUFFLE_I]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP4]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP5]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP6:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP6]]
//
__m128d test_mm_unpacklo_pd(__m128d A, __m128d B) {
  return _mm_unpacklo_pd(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_xor_pd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x double> [[TMP2]] to <2 x i64>
// X86-NEXT:    [[TMP4:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = bitcast <2 x double> [[TMP4]] to <2 x i64>
// X86-NEXT:    [[XOR_I:%.*]] = xor <2 x i64> [[TMP3]], [[TMP5]]
// X86-NEXT:    [[TMP6:%.*]] = bitcast <2 x i64> [[XOR_I]] to <2 x double>
// X86-NEXT:    store <2 x double> [[TMP6]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP7:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP7]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP8:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP8]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP9:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP9]]
//
__m128d test_mm_xor_pd(__m128d A, __m128d B) {
  return _mm_xor_pd(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_xor_si128(
// X86-SAME: <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[XOR_I:%.*]] = xor <2 x i64> [[TMP2]], [[TMP3]]
// X86-NEXT:    ret <2 x i64> [[XOR_I]]
//
__m128i test_mm_xor_si128(__m128i A, __m128i B) {
  return _mm_xor_si128(A, B);
}
//.
// X86: [[META3]] = !{i32 1}
//.
//// NOTE: These prefixes are unused and the list is autogenerated. Do not add tests below this line:
// CHECK: {{.*}}
// X64: {{.*}}
