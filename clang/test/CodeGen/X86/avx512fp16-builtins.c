// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py UTC_ARGS: --version 4
// RUN: %clang_cc1 -ffreestanding -flax-vector-conversions=none %s -triple=x86_64-unknown-unknown -target-feature +avx512fp16 -emit-llvm -o - -Wall -Werror | FileCheck %s

#include <immintrin.h>

// CHECK-LABEL: define dso_local half @test_mm512_cvtsh_h(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]]) #[[ATTR0:[0-9]+]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[VECEXT_I:%.*]] = extractelement <32 x half> [[TMP1]], i32 0
// CHECK-NEXT:    ret half [[VECEXT_I]]
//
_Float16 test_mm512_cvtsh_h(__m512h __A) {
  return _mm512_cvtsh_h(__A);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_setzero_ph(
// CHECK-SAME: ) #[[ATTR1:[0-9]+]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    ret <8 x half> [[TMP0]]
//
__m128h test_mm_setzero_ph(void) {
  return _mm_setzero_ph();
}

// CHECK-LABEL: define dso_local <16 x half> @test_mm256_setzero_ph(
// CHECK-SAME: ) #[[ATTR2:[0-9]+]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    store <16 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 32
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 32
// CHECK-NEXT:    ret <16 x half> [[TMP0]]
//
__m256h test_mm256_setzero_ph(void) {
  return _mm256_setzero_ph();
}

// CHECK-LABEL: define dso_local <16 x half> @test_mm256_undefined_ph(
// CHECK-SAME: ) #[[ATTR2]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    ret <16 x half> zeroinitializer
//
__m256h test_mm256_undefined_ph(void) {
  return _mm256_undefined_ph();
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_setzero_ph(
// CHECK-SAME: ) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    ret <32 x half> [[TMP0]]
//
__m512h test_mm512_setzero_ph(void) {
  return _mm512_setzero_ph();
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_undefined_ph(
// CHECK-SAME: ) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    ret <8 x half> zeroinitializer
//
__m128h test_mm_undefined_ph(void) {
  return _mm_undefined_ph();
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_undefined_ph(
// CHECK-SAME: ) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    ret <32 x half> zeroinitializer
//
__m512h test_mm512_undefined_ph(void) {
  return _mm512_undefined_ph();
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_set1_ph(
// CHECK-SAME: half noundef [[H:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__H_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[H_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    store half [[H]], ptr [[H_ADDR]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = load half, ptr [[H_ADDR]], align 2
// CHECK-NEXT:    store half [[TMP0]], ptr [[__H_ADDR_I]], align 2
// CHECK-NEXT:    [[TMP1:%.*]] = load half, ptr [[__H_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT_I:%.*]] = insertelement <32 x half> undef, half [[TMP1]], i32 0
// CHECK-NEXT:    [[TMP2:%.*]] = load half, ptr [[__H_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT1_I:%.*]] = insertelement <32 x half> [[VECINIT_I]], half [[TMP2]], i32 1
// CHECK-NEXT:    [[TMP3:%.*]] = load half, ptr [[__H_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT2_I:%.*]] = insertelement <32 x half> [[VECINIT1_I]], half [[TMP3]], i32 2
// CHECK-NEXT:    [[TMP4:%.*]] = load half, ptr [[__H_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT3_I:%.*]] = insertelement <32 x half> [[VECINIT2_I]], half [[TMP4]], i32 3
// CHECK-NEXT:    [[TMP5:%.*]] = load half, ptr [[__H_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT4_I:%.*]] = insertelement <32 x half> [[VECINIT3_I]], half [[TMP5]], i32 4
// CHECK-NEXT:    [[TMP6:%.*]] = load half, ptr [[__H_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT5_I:%.*]] = insertelement <32 x half> [[VECINIT4_I]], half [[TMP6]], i32 5
// CHECK-NEXT:    [[TMP7:%.*]] = load half, ptr [[__H_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT6_I:%.*]] = insertelement <32 x half> [[VECINIT5_I]], half [[TMP7]], i32 6
// CHECK-NEXT:    [[TMP8:%.*]] = load half, ptr [[__H_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT7_I:%.*]] = insertelement <32 x half> [[VECINIT6_I]], half [[TMP8]], i32 7
// CHECK-NEXT:    [[TMP9:%.*]] = load half, ptr [[__H_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT8_I:%.*]] = insertelement <32 x half> [[VECINIT7_I]], half [[TMP9]], i32 8
// CHECK-NEXT:    [[TMP10:%.*]] = load half, ptr [[__H_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT9_I:%.*]] = insertelement <32 x half> [[VECINIT8_I]], half [[TMP10]], i32 9
// CHECK-NEXT:    [[TMP11:%.*]] = load half, ptr [[__H_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT10_I:%.*]] = insertelement <32 x half> [[VECINIT9_I]], half [[TMP11]], i32 10
// CHECK-NEXT:    [[TMP12:%.*]] = load half, ptr [[__H_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT11_I:%.*]] = insertelement <32 x half> [[VECINIT10_I]], half [[TMP12]], i32 11
// CHECK-NEXT:    [[TMP13:%.*]] = load half, ptr [[__H_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT12_I:%.*]] = insertelement <32 x half> [[VECINIT11_I]], half [[TMP13]], i32 12
// CHECK-NEXT:    [[TMP14:%.*]] = load half, ptr [[__H_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT13_I:%.*]] = insertelement <32 x half> [[VECINIT12_I]], half [[TMP14]], i32 13
// CHECK-NEXT:    [[TMP15:%.*]] = load half, ptr [[__H_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT14_I:%.*]] = insertelement <32 x half> [[VECINIT13_I]], half [[TMP15]], i32 14
// CHECK-NEXT:    [[TMP16:%.*]] = load half, ptr [[__H_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT15_I:%.*]] = insertelement <32 x half> [[VECINIT14_I]], half [[TMP16]], i32 15
// CHECK-NEXT:    [[TMP17:%.*]] = load half, ptr [[__H_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT16_I:%.*]] = insertelement <32 x half> [[VECINIT15_I]], half [[TMP17]], i32 16
// CHECK-NEXT:    [[TMP18:%.*]] = load half, ptr [[__H_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT17_I:%.*]] = insertelement <32 x half> [[VECINIT16_I]], half [[TMP18]], i32 17
// CHECK-NEXT:    [[TMP19:%.*]] = load half, ptr [[__H_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT18_I:%.*]] = insertelement <32 x half> [[VECINIT17_I]], half [[TMP19]], i32 18
// CHECK-NEXT:    [[TMP20:%.*]] = load half, ptr [[__H_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT19_I:%.*]] = insertelement <32 x half> [[VECINIT18_I]], half [[TMP20]], i32 19
// CHECK-NEXT:    [[TMP21:%.*]] = load half, ptr [[__H_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT20_I:%.*]] = insertelement <32 x half> [[VECINIT19_I]], half [[TMP21]], i32 20
// CHECK-NEXT:    [[TMP22:%.*]] = load half, ptr [[__H_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT21_I:%.*]] = insertelement <32 x half> [[VECINIT20_I]], half [[TMP22]], i32 21
// CHECK-NEXT:    [[TMP23:%.*]] = load half, ptr [[__H_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT22_I:%.*]] = insertelement <32 x half> [[VECINIT21_I]], half [[TMP23]], i32 22
// CHECK-NEXT:    [[TMP24:%.*]] = load half, ptr [[__H_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT23_I:%.*]] = insertelement <32 x half> [[VECINIT22_I]], half [[TMP24]], i32 23
// CHECK-NEXT:    [[TMP25:%.*]] = load half, ptr [[__H_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT24_I:%.*]] = insertelement <32 x half> [[VECINIT23_I]], half [[TMP25]], i32 24
// CHECK-NEXT:    [[TMP26:%.*]] = load half, ptr [[__H_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT25_I:%.*]] = insertelement <32 x half> [[VECINIT24_I]], half [[TMP26]], i32 25
// CHECK-NEXT:    [[TMP27:%.*]] = load half, ptr [[__H_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT26_I:%.*]] = insertelement <32 x half> [[VECINIT25_I]], half [[TMP27]], i32 26
// CHECK-NEXT:    [[TMP28:%.*]] = load half, ptr [[__H_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT27_I:%.*]] = insertelement <32 x half> [[VECINIT26_I]], half [[TMP28]], i32 27
// CHECK-NEXT:    [[TMP29:%.*]] = load half, ptr [[__H_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT28_I:%.*]] = insertelement <32 x half> [[VECINIT27_I]], half [[TMP29]], i32 28
// CHECK-NEXT:    [[TMP30:%.*]] = load half, ptr [[__H_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT29_I:%.*]] = insertelement <32 x half> [[VECINIT28_I]], half [[TMP30]], i32 29
// CHECK-NEXT:    [[TMP31:%.*]] = load half, ptr [[__H_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT30_I:%.*]] = insertelement <32 x half> [[VECINIT29_I]], half [[TMP31]], i32 30
// CHECK-NEXT:    [[TMP32:%.*]] = load half, ptr [[__H_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT31_I:%.*]] = insertelement <32 x half> [[VECINIT30_I]], half [[TMP32]], i32 31
// CHECK-NEXT:    store <32 x half> [[VECINIT31_I]], ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP33:%.*]] = load <32 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    ret <32 x half> [[TMP33]]
//
__m512h test_mm512_set1_ph(_Float16 h) {
  return _mm512_set1_ph(h);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_set1_pch(
// CHECK-SAME: <2 x half> noundef [[H_COERCE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca float, align 4
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <16 x float>, align 64
// CHECK-NEXT:    [[H_I:%.*]] = alloca { half, half }, align 2
// CHECK-NEXT:    [[H:%.*]] = alloca { half, half }, align 2
// CHECK-NEXT:    [[COERCE:%.*]] = alloca { half, half }, align 2
// CHECK-NEXT:    store <2 x half> [[H_COERCE]], ptr [[H]], align 2
// CHECK-NEXT:    [[H_REALP:%.*]] = getelementptr inbounds { half, half }, ptr [[H]], i32 0, i32 0
// CHECK-NEXT:    [[H_REAL:%.*]] = load half, ptr [[H_REALP]], align 2
// CHECK-NEXT:    [[H_IMAGP:%.*]] = getelementptr inbounds { half, half }, ptr [[H]], i32 0, i32 1
// CHECK-NEXT:    [[H_IMAG:%.*]] = load half, ptr [[H_IMAGP]], align 2
// CHECK-NEXT:    [[COERCE_REALP:%.*]] = getelementptr inbounds { half, half }, ptr [[COERCE]], i32 0, i32 0
// CHECK-NEXT:    [[COERCE_IMAGP:%.*]] = getelementptr inbounds { half, half }, ptr [[COERCE]], i32 0, i32 1
// CHECK-NEXT:    store half [[H_REAL]], ptr [[COERCE_REALP]], align 2
// CHECK-NEXT:    store half [[H_IMAG]], ptr [[COERCE_IMAGP]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x half>, ptr [[COERCE]], align 2
// CHECK-NEXT:    store <2 x half> [[TMP0]], ptr [[H_I]], align 2
// CHECK-NEXT:    [[TMP1:%.*]] = load float, ptr [[H_I]], align 2
// CHECK-NEXT:    store float [[TMP1]], ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT_I:%.*]] = insertelement <16 x float> undef, float [[TMP2]], i32 0
// CHECK-NEXT:    [[TMP3:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT1_I:%.*]] = insertelement <16 x float> [[VECINIT_I]], float [[TMP3]], i32 1
// CHECK-NEXT:    [[TMP4:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT2_I:%.*]] = insertelement <16 x float> [[VECINIT1_I]], float [[TMP4]], i32 2
// CHECK-NEXT:    [[TMP5:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT3_I:%.*]] = insertelement <16 x float> [[VECINIT2_I]], float [[TMP5]], i32 3
// CHECK-NEXT:    [[TMP6:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT4_I:%.*]] = insertelement <16 x float> [[VECINIT3_I]], float [[TMP6]], i32 4
// CHECK-NEXT:    [[TMP7:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT5_I:%.*]] = insertelement <16 x float> [[VECINIT4_I]], float [[TMP7]], i32 5
// CHECK-NEXT:    [[TMP8:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT6_I:%.*]] = insertelement <16 x float> [[VECINIT5_I]], float [[TMP8]], i32 6
// CHECK-NEXT:    [[TMP9:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT7_I:%.*]] = insertelement <16 x float> [[VECINIT6_I]], float [[TMP9]], i32 7
// CHECK-NEXT:    [[TMP10:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT8_I:%.*]] = insertelement <16 x float> [[VECINIT7_I]], float [[TMP10]], i32 8
// CHECK-NEXT:    [[TMP11:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT9_I:%.*]] = insertelement <16 x float> [[VECINIT8_I]], float [[TMP11]], i32 9
// CHECK-NEXT:    [[TMP12:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT10_I:%.*]] = insertelement <16 x float> [[VECINIT9_I]], float [[TMP12]], i32 10
// CHECK-NEXT:    [[TMP13:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT11_I:%.*]] = insertelement <16 x float> [[VECINIT10_I]], float [[TMP13]], i32 11
// CHECK-NEXT:    [[TMP14:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT12_I:%.*]] = insertelement <16 x float> [[VECINIT11_I]], float [[TMP14]], i32 12
// CHECK-NEXT:    [[TMP15:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT13_I:%.*]] = insertelement <16 x float> [[VECINIT12_I]], float [[TMP15]], i32 13
// CHECK-NEXT:    [[TMP16:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT14_I:%.*]] = insertelement <16 x float> [[VECINIT13_I]], float [[TMP16]], i32 14
// CHECK-NEXT:    [[TMP17:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT15_I:%.*]] = insertelement <16 x float> [[VECINIT14_I]], float [[TMP17]], i32 15
// CHECK-NEXT:    store <16 x float> [[VECINIT15_I]], ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP18:%.*]] = load <16 x float>, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP19:%.*]] = bitcast <16 x float> [[TMP18]] to <32 x half>
// CHECK-NEXT:    ret <32 x half> [[TMP19]]
//
__m512h test_mm512_set1_pch(_Float16 _Complex h) {
  return _mm512_set1_pch(h);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_set_ph(
// CHECK-SAME: half noundef [[__H1:%.*]], half noundef [[__H2:%.*]], half noundef [[__H3:%.*]], half noundef [[__H4:%.*]], half noundef [[__H5:%.*]], half noundef [[__H6:%.*]], half noundef [[__H7:%.*]], half noundef [[__H8:%.*]], half noundef [[__H9:%.*]], half noundef [[__H10:%.*]], half noundef [[__H11:%.*]], half noundef [[__H12:%.*]], half noundef [[__H13:%.*]], half noundef [[__H14:%.*]], half noundef [[__H15:%.*]], half noundef [[__H16:%.*]], half noundef [[__H17:%.*]], half noundef [[__H18:%.*]], half noundef [[__H19:%.*]], half noundef [[__H20:%.*]], half noundef [[__H21:%.*]], half noundef [[__H22:%.*]], half noundef [[__H23:%.*]], half noundef [[__H24:%.*]], half noundef [[__H25:%.*]], half noundef [[__H26:%.*]], half noundef [[__H27:%.*]], half noundef [[__H28:%.*]], half noundef [[__H29:%.*]], half noundef [[__H30:%.*]], half noundef [[__H31:%.*]], half noundef [[__H32:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__H1_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H2_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H3_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H4_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H5_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H6_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H7_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H8_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H9_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H10_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H11_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H12_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H13_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H14_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H15_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H16_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H17_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H18_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H19_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H20_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H21_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H22_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H23_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H24_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H25_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H26_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H27_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H28_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H29_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H30_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H31_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H32_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__H1_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H2_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H3_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H4_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H5_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H6_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H7_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H8_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H9_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H10_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H11_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H12_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H13_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H14_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H15_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H16_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H17_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H18_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H19_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H20_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H21_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H22_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H23_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H24_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H25_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H26_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H27_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H28_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H29_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H30_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H31_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H32_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    store half [[__H1]], ptr [[__H1_ADDR]], align 2
// CHECK-NEXT:    store half [[__H2]], ptr [[__H2_ADDR]], align 2
// CHECK-NEXT:    store half [[__H3]], ptr [[__H3_ADDR]], align 2
// CHECK-NEXT:    store half [[__H4]], ptr [[__H4_ADDR]], align 2
// CHECK-NEXT:    store half [[__H5]], ptr [[__H5_ADDR]], align 2
// CHECK-NEXT:    store half [[__H6]], ptr [[__H6_ADDR]], align 2
// CHECK-NEXT:    store half [[__H7]], ptr [[__H7_ADDR]], align 2
// CHECK-NEXT:    store half [[__H8]], ptr [[__H8_ADDR]], align 2
// CHECK-NEXT:    store half [[__H9]], ptr [[__H9_ADDR]], align 2
// CHECK-NEXT:    store half [[__H10]], ptr [[__H10_ADDR]], align 2
// CHECK-NEXT:    store half [[__H11]], ptr [[__H11_ADDR]], align 2
// CHECK-NEXT:    store half [[__H12]], ptr [[__H12_ADDR]], align 2
// CHECK-NEXT:    store half [[__H13]], ptr [[__H13_ADDR]], align 2
// CHECK-NEXT:    store half [[__H14]], ptr [[__H14_ADDR]], align 2
// CHECK-NEXT:    store half [[__H15]], ptr [[__H15_ADDR]], align 2
// CHECK-NEXT:    store half [[__H16]], ptr [[__H16_ADDR]], align 2
// CHECK-NEXT:    store half [[__H17]], ptr [[__H17_ADDR]], align 2
// CHECK-NEXT:    store half [[__H18]], ptr [[__H18_ADDR]], align 2
// CHECK-NEXT:    store half [[__H19]], ptr [[__H19_ADDR]], align 2
// CHECK-NEXT:    store half [[__H20]], ptr [[__H20_ADDR]], align 2
// CHECK-NEXT:    store half [[__H21]], ptr [[__H21_ADDR]], align 2
// CHECK-NEXT:    store half [[__H22]], ptr [[__H22_ADDR]], align 2
// CHECK-NEXT:    store half [[__H23]], ptr [[__H23_ADDR]], align 2
// CHECK-NEXT:    store half [[__H24]], ptr [[__H24_ADDR]], align 2
// CHECK-NEXT:    store half [[__H25]], ptr [[__H25_ADDR]], align 2
// CHECK-NEXT:    store half [[__H26]], ptr [[__H26_ADDR]], align 2
// CHECK-NEXT:    store half [[__H27]], ptr [[__H27_ADDR]], align 2
// CHECK-NEXT:    store half [[__H28]], ptr [[__H28_ADDR]], align 2
// CHECK-NEXT:    store half [[__H29]], ptr [[__H29_ADDR]], align 2
// CHECK-NEXT:    store half [[__H30]], ptr [[__H30_ADDR]], align 2
// CHECK-NEXT:    store half [[__H31]], ptr [[__H31_ADDR]], align 2
// CHECK-NEXT:    store half [[__H32]], ptr [[__H32_ADDR]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = load half, ptr [[__H1_ADDR]], align 2
// CHECK-NEXT:    [[TMP1:%.*]] = load half, ptr [[__H2_ADDR]], align 2
// CHECK-NEXT:    [[TMP2:%.*]] = load half, ptr [[__H3_ADDR]], align 2
// CHECK-NEXT:    [[TMP3:%.*]] = load half, ptr [[__H4_ADDR]], align 2
// CHECK-NEXT:    [[TMP4:%.*]] = load half, ptr [[__H5_ADDR]], align 2
// CHECK-NEXT:    [[TMP5:%.*]] = load half, ptr [[__H6_ADDR]], align 2
// CHECK-NEXT:    [[TMP6:%.*]] = load half, ptr [[__H7_ADDR]], align 2
// CHECK-NEXT:    [[TMP7:%.*]] = load half, ptr [[__H8_ADDR]], align 2
// CHECK-NEXT:    [[TMP8:%.*]] = load half, ptr [[__H9_ADDR]], align 2
// CHECK-NEXT:    [[TMP9:%.*]] = load half, ptr [[__H10_ADDR]], align 2
// CHECK-NEXT:    [[TMP10:%.*]] = load half, ptr [[__H11_ADDR]], align 2
// CHECK-NEXT:    [[TMP11:%.*]] = load half, ptr [[__H12_ADDR]], align 2
// CHECK-NEXT:    [[TMP12:%.*]] = load half, ptr [[__H13_ADDR]], align 2
// CHECK-NEXT:    [[TMP13:%.*]] = load half, ptr [[__H14_ADDR]], align 2
// CHECK-NEXT:    [[TMP14:%.*]] = load half, ptr [[__H15_ADDR]], align 2
// CHECK-NEXT:    [[TMP15:%.*]] = load half, ptr [[__H16_ADDR]], align 2
// CHECK-NEXT:    [[TMP16:%.*]] = load half, ptr [[__H17_ADDR]], align 2
// CHECK-NEXT:    [[TMP17:%.*]] = load half, ptr [[__H18_ADDR]], align 2
// CHECK-NEXT:    [[TMP18:%.*]] = load half, ptr [[__H19_ADDR]], align 2
// CHECK-NEXT:    [[TMP19:%.*]] = load half, ptr [[__H20_ADDR]], align 2
// CHECK-NEXT:    [[TMP20:%.*]] = load half, ptr [[__H21_ADDR]], align 2
// CHECK-NEXT:    [[TMP21:%.*]] = load half, ptr [[__H22_ADDR]], align 2
// CHECK-NEXT:    [[TMP22:%.*]] = load half, ptr [[__H23_ADDR]], align 2
// CHECK-NEXT:    [[TMP23:%.*]] = load half, ptr [[__H24_ADDR]], align 2
// CHECK-NEXT:    [[TMP24:%.*]] = load half, ptr [[__H25_ADDR]], align 2
// CHECK-NEXT:    [[TMP25:%.*]] = load half, ptr [[__H26_ADDR]], align 2
// CHECK-NEXT:    [[TMP26:%.*]] = load half, ptr [[__H27_ADDR]], align 2
// CHECK-NEXT:    [[TMP27:%.*]] = load half, ptr [[__H28_ADDR]], align 2
// CHECK-NEXT:    [[TMP28:%.*]] = load half, ptr [[__H29_ADDR]], align 2
// CHECK-NEXT:    [[TMP29:%.*]] = load half, ptr [[__H30_ADDR]], align 2
// CHECK-NEXT:    [[TMP30:%.*]] = load half, ptr [[__H31_ADDR]], align 2
// CHECK-NEXT:    [[TMP31:%.*]] = load half, ptr [[__H32_ADDR]], align 2
// CHECK-NEXT:    store half [[TMP0]], ptr [[__H1_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP1]], ptr [[__H2_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP2]], ptr [[__H3_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP3]], ptr [[__H4_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP4]], ptr [[__H5_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP5]], ptr [[__H6_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP6]], ptr [[__H7_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP7]], ptr [[__H8_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP8]], ptr [[__H9_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP9]], ptr [[__H10_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP10]], ptr [[__H11_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP11]], ptr [[__H12_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP12]], ptr [[__H13_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP13]], ptr [[__H14_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP14]], ptr [[__H15_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP15]], ptr [[__H16_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP16]], ptr [[__H17_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP17]], ptr [[__H18_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP18]], ptr [[__H19_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP19]], ptr [[__H20_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP20]], ptr [[__H21_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP21]], ptr [[__H22_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP22]], ptr [[__H23_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP23]], ptr [[__H24_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP24]], ptr [[__H25_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP25]], ptr [[__H26_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP26]], ptr [[__H27_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP27]], ptr [[__H28_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP28]], ptr [[__H29_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP29]], ptr [[__H30_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP30]], ptr [[__H31_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP31]], ptr [[__H32_ADDR_I]], align 2
// CHECK-NEXT:    [[TMP32:%.*]] = load half, ptr [[__H32_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT_I:%.*]] = insertelement <32 x half> undef, half [[TMP32]], i32 0
// CHECK-NEXT:    [[TMP33:%.*]] = load half, ptr [[__H31_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT1_I:%.*]] = insertelement <32 x half> [[VECINIT_I]], half [[TMP33]], i32 1
// CHECK-NEXT:    [[TMP34:%.*]] = load half, ptr [[__H30_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT2_I:%.*]] = insertelement <32 x half> [[VECINIT1_I]], half [[TMP34]], i32 2
// CHECK-NEXT:    [[TMP35:%.*]] = load half, ptr [[__H29_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT3_I:%.*]] = insertelement <32 x half> [[VECINIT2_I]], half [[TMP35]], i32 3
// CHECK-NEXT:    [[TMP36:%.*]] = load half, ptr [[__H28_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT4_I:%.*]] = insertelement <32 x half> [[VECINIT3_I]], half [[TMP36]], i32 4
// CHECK-NEXT:    [[TMP37:%.*]] = load half, ptr [[__H27_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT5_I:%.*]] = insertelement <32 x half> [[VECINIT4_I]], half [[TMP37]], i32 5
// CHECK-NEXT:    [[TMP38:%.*]] = load half, ptr [[__H26_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT6_I:%.*]] = insertelement <32 x half> [[VECINIT5_I]], half [[TMP38]], i32 6
// CHECK-NEXT:    [[TMP39:%.*]] = load half, ptr [[__H25_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT7_I:%.*]] = insertelement <32 x half> [[VECINIT6_I]], half [[TMP39]], i32 7
// CHECK-NEXT:    [[TMP40:%.*]] = load half, ptr [[__H24_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT8_I:%.*]] = insertelement <32 x half> [[VECINIT7_I]], half [[TMP40]], i32 8
// CHECK-NEXT:    [[TMP41:%.*]] = load half, ptr [[__H23_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT9_I:%.*]] = insertelement <32 x half> [[VECINIT8_I]], half [[TMP41]], i32 9
// CHECK-NEXT:    [[TMP42:%.*]] = load half, ptr [[__H22_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT10_I:%.*]] = insertelement <32 x half> [[VECINIT9_I]], half [[TMP42]], i32 10
// CHECK-NEXT:    [[TMP43:%.*]] = load half, ptr [[__H21_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT11_I:%.*]] = insertelement <32 x half> [[VECINIT10_I]], half [[TMP43]], i32 11
// CHECK-NEXT:    [[TMP44:%.*]] = load half, ptr [[__H20_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT12_I:%.*]] = insertelement <32 x half> [[VECINIT11_I]], half [[TMP44]], i32 12
// CHECK-NEXT:    [[TMP45:%.*]] = load half, ptr [[__H19_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT13_I:%.*]] = insertelement <32 x half> [[VECINIT12_I]], half [[TMP45]], i32 13
// CHECK-NEXT:    [[TMP46:%.*]] = load half, ptr [[__H18_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT14_I:%.*]] = insertelement <32 x half> [[VECINIT13_I]], half [[TMP46]], i32 14
// CHECK-NEXT:    [[TMP47:%.*]] = load half, ptr [[__H17_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT15_I:%.*]] = insertelement <32 x half> [[VECINIT14_I]], half [[TMP47]], i32 15
// CHECK-NEXT:    [[TMP48:%.*]] = load half, ptr [[__H16_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT16_I:%.*]] = insertelement <32 x half> [[VECINIT15_I]], half [[TMP48]], i32 16
// CHECK-NEXT:    [[TMP49:%.*]] = load half, ptr [[__H15_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT17_I:%.*]] = insertelement <32 x half> [[VECINIT16_I]], half [[TMP49]], i32 17
// CHECK-NEXT:    [[TMP50:%.*]] = load half, ptr [[__H14_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT18_I:%.*]] = insertelement <32 x half> [[VECINIT17_I]], half [[TMP50]], i32 18
// CHECK-NEXT:    [[TMP51:%.*]] = load half, ptr [[__H13_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT19_I:%.*]] = insertelement <32 x half> [[VECINIT18_I]], half [[TMP51]], i32 19
// CHECK-NEXT:    [[TMP52:%.*]] = load half, ptr [[__H12_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT20_I:%.*]] = insertelement <32 x half> [[VECINIT19_I]], half [[TMP52]], i32 20
// CHECK-NEXT:    [[TMP53:%.*]] = load half, ptr [[__H11_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT21_I:%.*]] = insertelement <32 x half> [[VECINIT20_I]], half [[TMP53]], i32 21
// CHECK-NEXT:    [[TMP54:%.*]] = load half, ptr [[__H10_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT22_I:%.*]] = insertelement <32 x half> [[VECINIT21_I]], half [[TMP54]], i32 22
// CHECK-NEXT:    [[TMP55:%.*]] = load half, ptr [[__H9_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT23_I:%.*]] = insertelement <32 x half> [[VECINIT22_I]], half [[TMP55]], i32 23
// CHECK-NEXT:    [[TMP56:%.*]] = load half, ptr [[__H8_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT24_I:%.*]] = insertelement <32 x half> [[VECINIT23_I]], half [[TMP56]], i32 24
// CHECK-NEXT:    [[TMP57:%.*]] = load half, ptr [[__H7_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT25_I:%.*]] = insertelement <32 x half> [[VECINIT24_I]], half [[TMP57]], i32 25
// CHECK-NEXT:    [[TMP58:%.*]] = load half, ptr [[__H6_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT26_I:%.*]] = insertelement <32 x half> [[VECINIT25_I]], half [[TMP58]], i32 26
// CHECK-NEXT:    [[TMP59:%.*]] = load half, ptr [[__H5_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT27_I:%.*]] = insertelement <32 x half> [[VECINIT26_I]], half [[TMP59]], i32 27
// CHECK-NEXT:    [[TMP60:%.*]] = load half, ptr [[__H4_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT28_I:%.*]] = insertelement <32 x half> [[VECINIT27_I]], half [[TMP60]], i32 28
// CHECK-NEXT:    [[TMP61:%.*]] = load half, ptr [[__H3_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT29_I:%.*]] = insertelement <32 x half> [[VECINIT28_I]], half [[TMP61]], i32 29
// CHECK-NEXT:    [[TMP62:%.*]] = load half, ptr [[__H2_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT30_I:%.*]] = insertelement <32 x half> [[VECINIT29_I]], half [[TMP62]], i32 30
// CHECK-NEXT:    [[TMP63:%.*]] = load half, ptr [[__H1_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT31_I:%.*]] = insertelement <32 x half> [[VECINIT30_I]], half [[TMP63]], i32 31
// CHECK-NEXT:    store <32 x half> [[VECINIT31_I]], ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP64:%.*]] = load <32 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    ret <32 x half> [[TMP64]]
//
__m512h test_mm512_set_ph(_Float16 __h1, _Float16 __h2, _Float16 __h3, _Float16 __h4,
                          _Float16 __h5, _Float16 __h6, _Float16 __h7, _Float16 __h8,
                          _Float16 __h9, _Float16 __h10, _Float16 __h11, _Float16 __h12,
                          _Float16 __h13, _Float16 __h14, _Float16 __h15, _Float16 __h16,
                          _Float16 __h17, _Float16 __h18, _Float16 __h19, _Float16 __h20,
                          _Float16 __h21, _Float16 __h22, _Float16 __h23, _Float16 __h24,
                          _Float16 __h25, _Float16 __h26, _Float16 __h27, _Float16 __h28,
                          _Float16 __h29, _Float16 __h30, _Float16 __h31, _Float16 __h32) {
  return _mm512_set_ph(__h1, __h2, __h3, __h4, __h5, __h6, __h7, __h8,
                       __h9, __h10, __h11, __h12, __h13, __h14, __h15, __h16,
                       __h17, __h18, __h19, __h20, __h21, __h22, __h23, __h24,
                       __h25, __h26, __h27, __h28, __h29, __h30, __h31, __h32);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_setr_ph(
// CHECK-SAME: half noundef [[__H1:%.*]], half noundef [[__H2:%.*]], half noundef [[__H3:%.*]], half noundef [[__H4:%.*]], half noundef [[__H5:%.*]], half noundef [[__H6:%.*]], half noundef [[__H7:%.*]], half noundef [[__H8:%.*]], half noundef [[__H9:%.*]], half noundef [[__H10:%.*]], half noundef [[__H11:%.*]], half noundef [[__H12:%.*]], half noundef [[__H13:%.*]], half noundef [[__H14:%.*]], half noundef [[__H15:%.*]], half noundef [[__H16:%.*]], half noundef [[__H17:%.*]], half noundef [[__H18:%.*]], half noundef [[__H19:%.*]], half noundef [[__H20:%.*]], half noundef [[__H21:%.*]], half noundef [[__H22:%.*]], half noundef [[__H23:%.*]], half noundef [[__H24:%.*]], half noundef [[__H25:%.*]], half noundef [[__H26:%.*]], half noundef [[__H27:%.*]], half noundef [[__H28:%.*]], half noundef [[__H29:%.*]], half noundef [[__H30:%.*]], half noundef [[__H31:%.*]], half noundef [[__H32:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__H1_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H2_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H3_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H4_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H5_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H6_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H7_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H8_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H9_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H10_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H11_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H12_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H13_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H14_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H15_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H16_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H17_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H18_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H19_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H20_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H21_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H22_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H23_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H24_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H25_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H26_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H27_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H28_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H29_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H30_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H31_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H32_ADDR_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__H1_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H2_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H3_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H4_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H5_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H6_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H7_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H8_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H9_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H10_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H11_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H12_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H13_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H14_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H15_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H16_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H17_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H18_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H19_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H20_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H21_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H22_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H23_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H24_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H25_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H26_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H27_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H28_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H29_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H30_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H31_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[__H32_ADDR:%.*]] = alloca half, align 2
// CHECK-NEXT:    store half [[__H1]], ptr [[__H1_ADDR]], align 2
// CHECK-NEXT:    store half [[__H2]], ptr [[__H2_ADDR]], align 2
// CHECK-NEXT:    store half [[__H3]], ptr [[__H3_ADDR]], align 2
// CHECK-NEXT:    store half [[__H4]], ptr [[__H4_ADDR]], align 2
// CHECK-NEXT:    store half [[__H5]], ptr [[__H5_ADDR]], align 2
// CHECK-NEXT:    store half [[__H6]], ptr [[__H6_ADDR]], align 2
// CHECK-NEXT:    store half [[__H7]], ptr [[__H7_ADDR]], align 2
// CHECK-NEXT:    store half [[__H8]], ptr [[__H8_ADDR]], align 2
// CHECK-NEXT:    store half [[__H9]], ptr [[__H9_ADDR]], align 2
// CHECK-NEXT:    store half [[__H10]], ptr [[__H10_ADDR]], align 2
// CHECK-NEXT:    store half [[__H11]], ptr [[__H11_ADDR]], align 2
// CHECK-NEXT:    store half [[__H12]], ptr [[__H12_ADDR]], align 2
// CHECK-NEXT:    store half [[__H13]], ptr [[__H13_ADDR]], align 2
// CHECK-NEXT:    store half [[__H14]], ptr [[__H14_ADDR]], align 2
// CHECK-NEXT:    store half [[__H15]], ptr [[__H15_ADDR]], align 2
// CHECK-NEXT:    store half [[__H16]], ptr [[__H16_ADDR]], align 2
// CHECK-NEXT:    store half [[__H17]], ptr [[__H17_ADDR]], align 2
// CHECK-NEXT:    store half [[__H18]], ptr [[__H18_ADDR]], align 2
// CHECK-NEXT:    store half [[__H19]], ptr [[__H19_ADDR]], align 2
// CHECK-NEXT:    store half [[__H20]], ptr [[__H20_ADDR]], align 2
// CHECK-NEXT:    store half [[__H21]], ptr [[__H21_ADDR]], align 2
// CHECK-NEXT:    store half [[__H22]], ptr [[__H22_ADDR]], align 2
// CHECK-NEXT:    store half [[__H23]], ptr [[__H23_ADDR]], align 2
// CHECK-NEXT:    store half [[__H24]], ptr [[__H24_ADDR]], align 2
// CHECK-NEXT:    store half [[__H25]], ptr [[__H25_ADDR]], align 2
// CHECK-NEXT:    store half [[__H26]], ptr [[__H26_ADDR]], align 2
// CHECK-NEXT:    store half [[__H27]], ptr [[__H27_ADDR]], align 2
// CHECK-NEXT:    store half [[__H28]], ptr [[__H28_ADDR]], align 2
// CHECK-NEXT:    store half [[__H29]], ptr [[__H29_ADDR]], align 2
// CHECK-NEXT:    store half [[__H30]], ptr [[__H30_ADDR]], align 2
// CHECK-NEXT:    store half [[__H31]], ptr [[__H31_ADDR]], align 2
// CHECK-NEXT:    store half [[__H32]], ptr [[__H32_ADDR]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = load half, ptr [[__H32_ADDR]], align 2
// CHECK-NEXT:    [[TMP1:%.*]] = load half, ptr [[__H31_ADDR]], align 2
// CHECK-NEXT:    [[TMP2:%.*]] = load half, ptr [[__H30_ADDR]], align 2
// CHECK-NEXT:    [[TMP3:%.*]] = load half, ptr [[__H29_ADDR]], align 2
// CHECK-NEXT:    [[TMP4:%.*]] = load half, ptr [[__H28_ADDR]], align 2
// CHECK-NEXT:    [[TMP5:%.*]] = load half, ptr [[__H27_ADDR]], align 2
// CHECK-NEXT:    [[TMP6:%.*]] = load half, ptr [[__H26_ADDR]], align 2
// CHECK-NEXT:    [[TMP7:%.*]] = load half, ptr [[__H25_ADDR]], align 2
// CHECK-NEXT:    [[TMP8:%.*]] = load half, ptr [[__H24_ADDR]], align 2
// CHECK-NEXT:    [[TMP9:%.*]] = load half, ptr [[__H23_ADDR]], align 2
// CHECK-NEXT:    [[TMP10:%.*]] = load half, ptr [[__H22_ADDR]], align 2
// CHECK-NEXT:    [[TMP11:%.*]] = load half, ptr [[__H21_ADDR]], align 2
// CHECK-NEXT:    [[TMP12:%.*]] = load half, ptr [[__H20_ADDR]], align 2
// CHECK-NEXT:    [[TMP13:%.*]] = load half, ptr [[__H19_ADDR]], align 2
// CHECK-NEXT:    [[TMP14:%.*]] = load half, ptr [[__H18_ADDR]], align 2
// CHECK-NEXT:    [[TMP15:%.*]] = load half, ptr [[__H17_ADDR]], align 2
// CHECK-NEXT:    [[TMP16:%.*]] = load half, ptr [[__H16_ADDR]], align 2
// CHECK-NEXT:    [[TMP17:%.*]] = load half, ptr [[__H15_ADDR]], align 2
// CHECK-NEXT:    [[TMP18:%.*]] = load half, ptr [[__H14_ADDR]], align 2
// CHECK-NEXT:    [[TMP19:%.*]] = load half, ptr [[__H13_ADDR]], align 2
// CHECK-NEXT:    [[TMP20:%.*]] = load half, ptr [[__H12_ADDR]], align 2
// CHECK-NEXT:    [[TMP21:%.*]] = load half, ptr [[__H11_ADDR]], align 2
// CHECK-NEXT:    [[TMP22:%.*]] = load half, ptr [[__H10_ADDR]], align 2
// CHECK-NEXT:    [[TMP23:%.*]] = load half, ptr [[__H9_ADDR]], align 2
// CHECK-NEXT:    [[TMP24:%.*]] = load half, ptr [[__H8_ADDR]], align 2
// CHECK-NEXT:    [[TMP25:%.*]] = load half, ptr [[__H7_ADDR]], align 2
// CHECK-NEXT:    [[TMP26:%.*]] = load half, ptr [[__H6_ADDR]], align 2
// CHECK-NEXT:    [[TMP27:%.*]] = load half, ptr [[__H5_ADDR]], align 2
// CHECK-NEXT:    [[TMP28:%.*]] = load half, ptr [[__H4_ADDR]], align 2
// CHECK-NEXT:    [[TMP29:%.*]] = load half, ptr [[__H3_ADDR]], align 2
// CHECK-NEXT:    [[TMP30:%.*]] = load half, ptr [[__H2_ADDR]], align 2
// CHECK-NEXT:    [[TMP31:%.*]] = load half, ptr [[__H1_ADDR]], align 2
// CHECK-NEXT:    store half [[TMP0]], ptr [[__H1_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP1]], ptr [[__H2_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP2]], ptr [[__H3_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP3]], ptr [[__H4_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP4]], ptr [[__H5_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP5]], ptr [[__H6_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP6]], ptr [[__H7_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP7]], ptr [[__H8_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP8]], ptr [[__H9_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP9]], ptr [[__H10_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP10]], ptr [[__H11_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP11]], ptr [[__H12_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP12]], ptr [[__H13_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP13]], ptr [[__H14_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP14]], ptr [[__H15_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP15]], ptr [[__H16_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP16]], ptr [[__H17_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP17]], ptr [[__H18_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP18]], ptr [[__H19_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP19]], ptr [[__H20_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP20]], ptr [[__H21_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP21]], ptr [[__H22_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP22]], ptr [[__H23_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP23]], ptr [[__H24_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP24]], ptr [[__H25_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP25]], ptr [[__H26_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP26]], ptr [[__H27_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP27]], ptr [[__H28_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP28]], ptr [[__H29_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP29]], ptr [[__H30_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP30]], ptr [[__H31_ADDR_I]], align 2
// CHECK-NEXT:    store half [[TMP31]], ptr [[__H32_ADDR_I]], align 2
// CHECK-NEXT:    [[TMP32:%.*]] = load half, ptr [[__H32_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT_I:%.*]] = insertelement <32 x half> undef, half [[TMP32]], i32 0
// CHECK-NEXT:    [[TMP33:%.*]] = load half, ptr [[__H31_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT1_I:%.*]] = insertelement <32 x half> [[VECINIT_I]], half [[TMP33]], i32 1
// CHECK-NEXT:    [[TMP34:%.*]] = load half, ptr [[__H30_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT2_I:%.*]] = insertelement <32 x half> [[VECINIT1_I]], half [[TMP34]], i32 2
// CHECK-NEXT:    [[TMP35:%.*]] = load half, ptr [[__H29_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT3_I:%.*]] = insertelement <32 x half> [[VECINIT2_I]], half [[TMP35]], i32 3
// CHECK-NEXT:    [[TMP36:%.*]] = load half, ptr [[__H28_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT4_I:%.*]] = insertelement <32 x half> [[VECINIT3_I]], half [[TMP36]], i32 4
// CHECK-NEXT:    [[TMP37:%.*]] = load half, ptr [[__H27_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT5_I:%.*]] = insertelement <32 x half> [[VECINIT4_I]], half [[TMP37]], i32 5
// CHECK-NEXT:    [[TMP38:%.*]] = load half, ptr [[__H26_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT6_I:%.*]] = insertelement <32 x half> [[VECINIT5_I]], half [[TMP38]], i32 6
// CHECK-NEXT:    [[TMP39:%.*]] = load half, ptr [[__H25_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT7_I:%.*]] = insertelement <32 x half> [[VECINIT6_I]], half [[TMP39]], i32 7
// CHECK-NEXT:    [[TMP40:%.*]] = load half, ptr [[__H24_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT8_I:%.*]] = insertelement <32 x half> [[VECINIT7_I]], half [[TMP40]], i32 8
// CHECK-NEXT:    [[TMP41:%.*]] = load half, ptr [[__H23_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT9_I:%.*]] = insertelement <32 x half> [[VECINIT8_I]], half [[TMP41]], i32 9
// CHECK-NEXT:    [[TMP42:%.*]] = load half, ptr [[__H22_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT10_I:%.*]] = insertelement <32 x half> [[VECINIT9_I]], half [[TMP42]], i32 10
// CHECK-NEXT:    [[TMP43:%.*]] = load half, ptr [[__H21_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT11_I:%.*]] = insertelement <32 x half> [[VECINIT10_I]], half [[TMP43]], i32 11
// CHECK-NEXT:    [[TMP44:%.*]] = load half, ptr [[__H20_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT12_I:%.*]] = insertelement <32 x half> [[VECINIT11_I]], half [[TMP44]], i32 12
// CHECK-NEXT:    [[TMP45:%.*]] = load half, ptr [[__H19_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT13_I:%.*]] = insertelement <32 x half> [[VECINIT12_I]], half [[TMP45]], i32 13
// CHECK-NEXT:    [[TMP46:%.*]] = load half, ptr [[__H18_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT14_I:%.*]] = insertelement <32 x half> [[VECINIT13_I]], half [[TMP46]], i32 14
// CHECK-NEXT:    [[TMP47:%.*]] = load half, ptr [[__H17_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT15_I:%.*]] = insertelement <32 x half> [[VECINIT14_I]], half [[TMP47]], i32 15
// CHECK-NEXT:    [[TMP48:%.*]] = load half, ptr [[__H16_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT16_I:%.*]] = insertelement <32 x half> [[VECINIT15_I]], half [[TMP48]], i32 16
// CHECK-NEXT:    [[TMP49:%.*]] = load half, ptr [[__H15_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT17_I:%.*]] = insertelement <32 x half> [[VECINIT16_I]], half [[TMP49]], i32 17
// CHECK-NEXT:    [[TMP50:%.*]] = load half, ptr [[__H14_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT18_I:%.*]] = insertelement <32 x half> [[VECINIT17_I]], half [[TMP50]], i32 18
// CHECK-NEXT:    [[TMP51:%.*]] = load half, ptr [[__H13_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT19_I:%.*]] = insertelement <32 x half> [[VECINIT18_I]], half [[TMP51]], i32 19
// CHECK-NEXT:    [[TMP52:%.*]] = load half, ptr [[__H12_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT20_I:%.*]] = insertelement <32 x half> [[VECINIT19_I]], half [[TMP52]], i32 20
// CHECK-NEXT:    [[TMP53:%.*]] = load half, ptr [[__H11_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT21_I:%.*]] = insertelement <32 x half> [[VECINIT20_I]], half [[TMP53]], i32 21
// CHECK-NEXT:    [[TMP54:%.*]] = load half, ptr [[__H10_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT22_I:%.*]] = insertelement <32 x half> [[VECINIT21_I]], half [[TMP54]], i32 22
// CHECK-NEXT:    [[TMP55:%.*]] = load half, ptr [[__H9_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT23_I:%.*]] = insertelement <32 x half> [[VECINIT22_I]], half [[TMP55]], i32 23
// CHECK-NEXT:    [[TMP56:%.*]] = load half, ptr [[__H8_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT24_I:%.*]] = insertelement <32 x half> [[VECINIT23_I]], half [[TMP56]], i32 24
// CHECK-NEXT:    [[TMP57:%.*]] = load half, ptr [[__H7_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT25_I:%.*]] = insertelement <32 x half> [[VECINIT24_I]], half [[TMP57]], i32 25
// CHECK-NEXT:    [[TMP58:%.*]] = load half, ptr [[__H6_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT26_I:%.*]] = insertelement <32 x half> [[VECINIT25_I]], half [[TMP58]], i32 26
// CHECK-NEXT:    [[TMP59:%.*]] = load half, ptr [[__H5_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT27_I:%.*]] = insertelement <32 x half> [[VECINIT26_I]], half [[TMP59]], i32 27
// CHECK-NEXT:    [[TMP60:%.*]] = load half, ptr [[__H4_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT28_I:%.*]] = insertelement <32 x half> [[VECINIT27_I]], half [[TMP60]], i32 28
// CHECK-NEXT:    [[TMP61:%.*]] = load half, ptr [[__H3_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT29_I:%.*]] = insertelement <32 x half> [[VECINIT28_I]], half [[TMP61]], i32 29
// CHECK-NEXT:    [[TMP62:%.*]] = load half, ptr [[__H2_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT30_I:%.*]] = insertelement <32 x half> [[VECINIT29_I]], half [[TMP62]], i32 30
// CHECK-NEXT:    [[TMP63:%.*]] = load half, ptr [[__H1_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT31_I:%.*]] = insertelement <32 x half> [[VECINIT30_I]], half [[TMP63]], i32 31
// CHECK-NEXT:    store <32 x half> [[VECINIT31_I]], ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP64:%.*]] = load <32 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    ret <32 x half> [[TMP64]]
//
__m512h test_mm512_setr_ph(_Float16 __h1, _Float16 __h2, _Float16 __h3, _Float16 __h4,
                           _Float16 __h5, _Float16 __h6, _Float16 __h7, _Float16 __h8,
                           _Float16 __h9, _Float16 __h10, _Float16 __h11, _Float16 __h12,
                           _Float16 __h13, _Float16 __h14, _Float16 __h15, _Float16 __h16,
                           _Float16 __h17, _Float16 __h18, _Float16 __h19, _Float16 __h20,
                           _Float16 __h21, _Float16 __h22, _Float16 __h23, _Float16 __h24,
                           _Float16 __h25, _Float16 __h26, _Float16 __h27, _Float16 __h28,
                           _Float16 __h29, _Float16 __h30, _Float16 __h31, _Float16 __h32) {
  return _mm512_setr_ph(__h1, __h2, __h3, __h4, __h5, __h6, __h7, __h8,
                        __h9, __h10, __h11, __h12, __h13, __h14, __h15, __h16,
                        __h17, __h18, __h19, __h20, __h21, __h22, __h23, __h24,
                        __h25, __h26, __h27, __h28, __h29, __h30, __h31, __h32);
}

// CHECK-LABEL: define dso_local <4 x float> @test_mm_castph_ps(
// CHECK-SAME: <8 x half> noundef [[A:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <8 x half> [[TMP1]] to <4 x float>
// CHECK-NEXT:    ret <4 x float> [[TMP2]]
//
__m128 test_mm_castph_ps(__m128h A) {
  return _mm_castph_ps(A);
}

// CHECK-LABEL: define dso_local <8 x float> @test_mm256_castph_ps(
// CHECK-SAME: <16 x half> noundef [[A:%.*]]) #[[ATTR2]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    store <16 x half> [[A]], ptr [[A_ADDR]], align 32
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x half>, ptr [[A_ADDR]], align 32
// CHECK-NEXT:    store <16 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 32
// CHECK-NEXT:    [[TMP1:%.*]] = load <16 x half>, ptr [[__A_ADDR_I]], align 32
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <16 x half> [[TMP1]] to <8 x float>
// CHECK-NEXT:    ret <8 x float> [[TMP2]]
//
__m256 test_mm256_castph_ps(__m256h A) {
  return _mm256_castph_ps(A);
}

// CHECK-LABEL: define dso_local <16 x float> @test_mm512_castph_ps(
// CHECK-SAME: <32 x half> noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <32 x half> [[TMP1]] to <16 x float>
// CHECK-NEXT:    ret <16 x float> [[TMP2]]
//
__m512 test_mm512_castph_ps(__m512h A) {
  return _mm512_castph_ps(A);
}

// CHECK-LABEL: define dso_local <2 x double> @test_mm_castph_pd(
// CHECK-SAME: <8 x half> noundef [[A:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <8 x half> [[TMP1]] to <2 x double>
// CHECK-NEXT:    ret <2 x double> [[TMP2]]
//
__m128d test_mm_castph_pd(__m128h A) {
  return _mm_castph_pd(A);
}

// CHECK-LABEL: define dso_local <4 x double> @test_mm256_castph_pd(
// CHECK-SAME: <16 x half> noundef [[A:%.*]]) #[[ATTR2]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    store <16 x half> [[A]], ptr [[A_ADDR]], align 32
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x half>, ptr [[A_ADDR]], align 32
// CHECK-NEXT:    store <16 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 32
// CHECK-NEXT:    [[TMP1:%.*]] = load <16 x half>, ptr [[__A_ADDR_I]], align 32
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <16 x half> [[TMP1]] to <4 x double>
// CHECK-NEXT:    ret <4 x double> [[TMP2]]
//
__m256d test_mm256_castph_pd(__m256h A) {
  return _mm256_castph_pd(A);
}

// CHECK-LABEL: define dso_local <8 x double> @test_mm512_castph_pd(
// CHECK-SAME: <32 x half> noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <32 x half> [[TMP1]] to <8 x double>
// CHECK-NEXT:    ret <8 x double> [[TMP2]]
//
__m512d test_mm512_castph_pd(__m512h A) {
  return _mm512_castph_pd(A);
}

// CHECK-LABEL: define dso_local <2 x i64> @test_mm_castph_si128(
// CHECK-SAME: <8 x half> noundef [[A:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <8 x half> [[TMP1]] to <2 x i64>
// CHECK-NEXT:    ret <2 x i64> [[TMP2]]
//
__m128i test_mm_castph_si128(__m128h A) {
  return _mm_castph_si128(A);
}

// CHECK-LABEL: define dso_local <4 x i64> @test_mm256_castph_si256(
// CHECK-SAME: <16 x half> noundef [[A:%.*]]) #[[ATTR2]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    store <16 x half> [[A]], ptr [[A_ADDR]], align 32
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x half>, ptr [[A_ADDR]], align 32
// CHECK-NEXT:    store <16 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 32
// CHECK-NEXT:    [[TMP1:%.*]] = load <16 x half>, ptr [[__A_ADDR_I]], align 32
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <16 x half> [[TMP1]] to <4 x i64>
// CHECK-NEXT:    ret <4 x i64> [[TMP2]]
//
__m256i test_mm256_castph_si256(__m256h A) {
  return _mm256_castph_si256(A);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_castph_si512(
// CHECK-SAME: <32 x half> noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <32 x half> [[TMP1]] to <8 x i64>
// CHECK-NEXT:    ret <8 x i64> [[TMP2]]
//
__m512i test_mm512_castph_si512(__m512h A) {
  return _mm512_castph_si512(A);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_castps_ph(
// CHECK-SAME: <4 x float> noundef [[A:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x float>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// CHECK-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store <4 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <4 x float> [[TMP1]] to <8 x half>
// CHECK-NEXT:    ret <8 x half> [[TMP2]]
//
__m128h test_mm_castps_ph(__m128 A) {
  return _mm_castps_ph(A);
}

// CHECK-LABEL: define dso_local <16 x half> @test_mm256_castps_ph(
// CHECK-SAME: <8 x float> noundef [[A:%.*]]) #[[ATTR2]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x float>, align 32
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// CHECK-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// CHECK-NEXT:    store <8 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 32
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[__A_ADDR_I]], align 32
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <8 x float> [[TMP1]] to <16 x half>
// CHECK-NEXT:    ret <16 x half> [[TMP2]]
//
__m256h test_mm256_castps_ph(__m256 A) {
  return _mm256_castps_ph(A);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_castps_ph(
// CHECK-SAME: <16 x float> noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <16 x float>, align 64
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <16 x float>, align 64
// CHECK-NEXT:    store <16 x float> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x float>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <16 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <16 x float>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <16 x float> [[TMP1]] to <32 x half>
// CHECK-NEXT:    ret <32 x half> [[TMP2]]
//
__m512h test_mm512_castps_ph(__m512 A) {
  return _mm512_castps_ph(A);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_castpd_ph(
// CHECK-SAME: <2 x double> noundef [[A:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// CHECK-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <2 x double> [[TMP1]] to <8 x half>
// CHECK-NEXT:    ret <8 x half> [[TMP2]]
//
__m128h test_mm_castpd_ph(__m128d A) {
  return _mm_castpd_ph(A);
}

// CHECK-LABEL: define dso_local <16 x half> @test_mm256_castpd_ph(
// CHECK-SAME: <4 x double> noundef [[A:%.*]]) #[[ATTR2]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x double>, align 32
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// CHECK-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// CHECK-NEXT:    store <4 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 32
// CHECK-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[__A_ADDR_I]], align 32
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <4 x double> [[TMP1]] to <16 x half>
// CHECK-NEXT:    ret <16 x half> [[TMP2]]
//
__m256h test_mm256_castpd_ph(__m256d A) {
  return _mm256_castpd_ph(A);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_castpd_ph(
// CHECK-SAME: <8 x double> noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x double>, align 64
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x double>, align 64
// CHECK-NEXT:    store <8 x double> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x double>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <8 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x double>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <8 x double> [[TMP1]] to <32 x half>
// CHECK-NEXT:    ret <32 x half> [[TMP2]]
//
__m512h test_mm512_castpd_ph(__m512d A) {
  return _mm512_castpd_ph(A);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_castsi128_ph(
// CHECK-SAME: <2 x i64> noundef [[A:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// CHECK-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <2 x i64> [[TMP1]] to <8 x half>
// CHECK-NEXT:    ret <8 x half> [[TMP2]]
//
__m128h test_mm_castsi128_ph(__m128i A) {
  return _mm_castsi128_ph(A);
}

// CHECK-LABEL: define dso_local <16 x half> @test_mm256_castsi256_ph(
// CHECK-SAME: <4 x i64> noundef [[A:%.*]]) #[[ATTR2]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x i64>, align 32
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <4 x i64>, align 32
// CHECK-NEXT:    store <4 x i64> [[A]], ptr [[A_ADDR]], align 32
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x i64>, ptr [[A_ADDR]], align 32
// CHECK-NEXT:    store <4 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 32
// CHECK-NEXT:    [[TMP1:%.*]] = load <4 x i64>, ptr [[__A_ADDR_I]], align 32
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <4 x i64> [[TMP1]] to <16 x half>
// CHECK-NEXT:    ret <16 x half> [[TMP2]]
//
__m256h test_mm256_castsi256_ph(__m256i A) {
  return _mm256_castsi256_ph(A);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_castsi512_ph(
// CHECK-SAME: <8 x i64> noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    store <8 x i64> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i64>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <8 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i64>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <8 x i64> [[TMP1]] to <32 x half>
// CHECK-NEXT:    ret <32 x half> [[TMP2]]
//
__m512h test_mm512_castsi512_ph(__m512i A) {
  return _mm512_castsi512_ph(A);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm256_castph256_ph128(
// CHECK-SAME: <16 x half> noundef [[__A:%.*]]) #[[ATTR2]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    store <16 x half> [[__A]], ptr [[__A_ADDR]], align 32
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x half>, ptr [[__A_ADDR]], align 32
// CHECK-NEXT:    store <16 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 32
// CHECK-NEXT:    [[TMP1:%.*]] = load <16 x half>, ptr [[__A_ADDR_I]], align 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <16 x half>, ptr [[__A_ADDR_I]], align 32
// CHECK-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <16 x half> [[TMP1]], <16 x half> [[TMP2]], <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
// CHECK-NEXT:    ret <8 x half> [[SHUFFLE_I]]
//
__m128h test_mm256_castph256_ph128(__m256h __a) {
  return _mm256_castph256_ph128(__a);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm512_castph512_ph128(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <32 x half> [[TMP1]], <32 x half> [[TMP2]], <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
// CHECK-NEXT:    ret <8 x half> [[SHUFFLE_I]]
//
__m128h test_mm512_castph512_ph128(__m512h __a) {
  return _mm512_castph512_ph128(__a);
}

// CHECK-LABEL: define dso_local <16 x half> @test_mm512_castph512_ph256(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <32 x half> [[TMP1]], <32 x half> [[TMP2]], <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
// CHECK-NEXT:    ret <16 x half> [[SHUFFLE_I]]
//
__m256h test_mm512_castph512_ph256(__m512h __a) {
  return _mm512_castph512_ph256(__a);
}

// CHECK-LABEL: define dso_local <16 x half> @test_mm256_castph128_ph256(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]]) #[[ATTR2]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <8 x half> [[TMP1]], <8 x half> [[TMP2]], <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
// CHECK-NEXT:    ret <16 x half> [[SHUFFLE_I]]
//
__m256h test_mm256_castph128_ph256(__m128h __a) {
  return _mm256_castph128_ph256(__a);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_castph128_ph512(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <8 x half> [[TMP1]], <8 x half> [[TMP2]], <32 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
// CHECK-NEXT:    ret <32 x half> [[SHUFFLE_I]]
//
__m512h test_mm512_castph128_ph512(__m128h __a) {
  return _mm512_castph128_ph512(__a);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_castph256_ph512(
// CHECK-SAME: <16 x half> noundef [[__A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    store <16 x half> [[__A]], ptr [[__A_ADDR]], align 32
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x half>, ptr [[__A_ADDR]], align 32
// CHECK-NEXT:    store <16 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 32
// CHECK-NEXT:    [[TMP1:%.*]] = load <16 x half>, ptr [[__A_ADDR_I]], align 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <16 x half>, ptr [[__A_ADDR_I]], align 32
// CHECK-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <16 x half> [[TMP1]], <16 x half> [[TMP2]], <32 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef, i32 undef>
// CHECK-NEXT:    ret <32 x half> [[SHUFFLE_I]]
//
__m512h test_mm512_castph256_ph512(__m256h __a) {
  return _mm512_castph256_ph512(__a);
}

// CHECK-LABEL: define dso_local <16 x half> @test_mm256_zextph128_ph256(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]]) #[[ATTR2]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <8 x half> [[TMP1]], <8 x half> [[TMP2]], <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
// CHECK-NEXT:    ret <16 x half> [[SHUFFLE_I]]
//
__m256h test_mm256_zextph128_ph256(__m128h __a) {
  return _mm256_zextph128_ph256(__a);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_zextph128_ph512(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <8 x half> [[TMP1]], <8 x half> [[TMP2]], <32 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
// CHECK-NEXT:    ret <32 x half> [[SHUFFLE_I]]
//
__m512h test_mm512_zextph128_ph512(__m128h __a) {
  return _mm512_zextph128_ph512(__a);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_zextph256_ph512(
// CHECK-SAME: <16 x half> noundef [[__A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    store <16 x half> [[__A]], ptr [[__A_ADDR]], align 32
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x half>, ptr [[__A_ADDR]], align 32
// CHECK-NEXT:    store <16 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 32
// CHECK-NEXT:    [[TMP1:%.*]] = load <16 x half>, ptr [[__A_ADDR_I]], align 32
// CHECK-NEXT:    store <16 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <16 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 32
// CHECK-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <16 x half> [[TMP1]], <16 x half> [[TMP2]], <32 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27, i32 28, i32 29, i32 30, i32 31>
// CHECK-NEXT:    ret <32 x half> [[SHUFFLE_I]]
//
__m512h test_mm512_zextph256_ph512(__m256h __a) {
  return _mm512_zextph256_ph512(__a);
}

// CHECK-LABEL: define dso_local i32 @test_mm_comi_round_sh(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.x86.avx512fp16.vcomi.sh(<8 x half> [[TMP0]], <8 x half> [[TMP1]], i32 0, i32 8)
// CHECK-NEXT:    ret i32 [[TMP2]]
//
int test_mm_comi_round_sh(__m128h __A, __m128h __B) {
  return _mm_comi_round_sh(__A, __B, 0, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local i32 @test_mm_comi_sh(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.x86.avx512fp16.vcomi.sh(<8 x half> [[TMP0]], <8 x half> [[TMP1]], i32 0, i32 4)
// CHECK-NEXT:    ret i32 [[TMP2]]
//
int test_mm_comi_sh(__m128h __A, __m128h __B) {
  return _mm_comi_sh(__A, __B, 0);
}

// CHECK-LABEL: define dso_local i32 @test_mm_comieq_sh(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = call i32 @llvm.x86.avx512fp16.vcomi.sh(<8 x half> [[TMP2]], <8 x half> [[TMP3]], i32 16, i32 4)
// CHECK-NEXT:    ret i32 [[TMP4]]
//
int test_mm_comieq_sh(__m128h __A, __m128h __B) {
  return _mm_comieq_sh(__A, __B);
}

// CHECK-LABEL: define dso_local i32 @test_mm_comilt_sh(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = call i32 @llvm.x86.avx512fp16.vcomi.sh(<8 x half> [[TMP2]], <8 x half> [[TMP3]], i32 1, i32 4)
// CHECK-NEXT:    ret i32 [[TMP4]]
//
int test_mm_comilt_sh(__m128h __A, __m128h __B) {
  return _mm_comilt_sh(__A, __B);
}

// CHECK-LABEL: define dso_local i32 @test_mm_comile_sh(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = call i32 @llvm.x86.avx512fp16.vcomi.sh(<8 x half> [[TMP2]], <8 x half> [[TMP3]], i32 2, i32 4)
// CHECK-NEXT:    ret i32 [[TMP4]]
//
int test_mm_comile_sh(__m128h __A, __m128h __B) {
  return _mm_comile_sh(__A, __B);
}

// CHECK-LABEL: define dso_local i32 @test_mm_comigt_sh(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = call i32 @llvm.x86.avx512fp16.vcomi.sh(<8 x half> [[TMP2]], <8 x half> [[TMP3]], i32 14, i32 4)
// CHECK-NEXT:    ret i32 [[TMP4]]
//
int test_mm_comigt_sh(__m128h __A, __m128h __B) {
  return _mm_comigt_sh(__A, __B);
}

// CHECK-LABEL: define dso_local i32 @test_mm_comige_sh(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = call i32 @llvm.x86.avx512fp16.vcomi.sh(<8 x half> [[TMP2]], <8 x half> [[TMP3]], i32 13, i32 4)
// CHECK-NEXT:    ret i32 [[TMP4]]
//
int test_mm_comige_sh(__m128h __A, __m128h __B) {
  return _mm_comige_sh(__A, __B);
}

// CHECK-LABEL: define dso_local i32 @test_mm_comineq_sh(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = call i32 @llvm.x86.avx512fp16.vcomi.sh(<8 x half> [[TMP2]], <8 x half> [[TMP3]], i32 20, i32 4)
// CHECK-NEXT:    ret i32 [[TMP4]]
//
int test_mm_comineq_sh(__m128h __A, __m128h __B) {
  return _mm_comineq_sh(__A, __B);
}

// CHECK-LABEL: define dso_local i32 @test_mm_ucomieq_sh(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = call i32 @llvm.x86.avx512fp16.vcomi.sh(<8 x half> [[TMP2]], <8 x half> [[TMP3]], i32 0, i32 4)
// CHECK-NEXT:    ret i32 [[TMP4]]
//
int test_mm_ucomieq_sh(__m128h __A, __m128h __B) {
  return _mm_ucomieq_sh(__A, __B);
}

// CHECK-LABEL: define dso_local i32 @test_mm_ucomilt_sh(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = call i32 @llvm.x86.avx512fp16.vcomi.sh(<8 x half> [[TMP2]], <8 x half> [[TMP3]], i32 17, i32 4)
// CHECK-NEXT:    ret i32 [[TMP4]]
//
int test_mm_ucomilt_sh(__m128h __A, __m128h __B) {
  return _mm_ucomilt_sh(__A, __B);
}

// CHECK-LABEL: define dso_local i32 @test_mm_ucomile_sh(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = call i32 @llvm.x86.avx512fp16.vcomi.sh(<8 x half> [[TMP2]], <8 x half> [[TMP3]], i32 18, i32 4)
// CHECK-NEXT:    ret i32 [[TMP4]]
//
int test_mm_ucomile_sh(__m128h __A, __m128h __B) {
  return _mm_ucomile_sh(__A, __B);
}

// CHECK-LABEL: define dso_local i32 @test_mm_ucomigt_sh(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = call i32 @llvm.x86.avx512fp16.vcomi.sh(<8 x half> [[TMP2]], <8 x half> [[TMP3]], i32 30, i32 4)
// CHECK-NEXT:    ret i32 [[TMP4]]
//
int test_mm_ucomigt_sh(__m128h __A, __m128h __B) {
  return _mm_ucomigt_sh(__A, __B);
}

// CHECK-LABEL: define dso_local i32 @test_mm_ucomige_sh(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = call i32 @llvm.x86.avx512fp16.vcomi.sh(<8 x half> [[TMP2]], <8 x half> [[TMP3]], i32 29, i32 4)
// CHECK-NEXT:    ret i32 [[TMP4]]
//
int test_mm_ucomige_sh(__m128h __A, __m128h __B) {
  return _mm_ucomige_sh(__A, __B);
}

// CHECK-LABEL: define dso_local i32 @test_mm_ucomineq_sh(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = call i32 @llvm.x86.avx512fp16.vcomi.sh(<8 x half> [[TMP2]], <8 x half> [[TMP3]], i32 4, i32 4)
// CHECK-NEXT:    ret i32 [[TMP4]]
//
int test_mm_ucomineq_sh(__m128h __A, __m128h __B) {
  return _mm_ucomineq_sh(__A, __B);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_add_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP1]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[ADD_I:%.*]] = fadd <32 x half> [[TMP2]], [[TMP3]]
// CHECK-NEXT:    ret <32 x half> [[ADD_I]]
//
__m512h test_mm512_add_ph(__m512h __A, __m512h __B) {
  return _mm512_add_ph(__A, __B);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask_add_ph(
// CHECK-SAME: <32 x half> noundef [[__W:%.*]], i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__W]], ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP3]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load i32, ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP5:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP6:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP5]], ptr [[__A_ADDR_I_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP6]], ptr [[__B_ADDR_I_I]], align 64
// CHECK-NEXT:    [[TMP7:%.*]] = load <32 x half>, ptr [[__A_ADDR_I_I]], align 64
// CHECK-NEXT:    [[TMP8:%.*]] = load <32 x half>, ptr [[__B_ADDR_I_I]], align 64
// CHECK-NEXT:    [[ADD_I_I:%.*]] = fadd <32 x half> [[TMP7]], [[TMP8]]
// CHECK-NEXT:    [[TMP9:%.*]] = load <32 x half>, ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP10:%.*]] = bitcast i32 [[TMP4]] to <32 x i1>
// CHECK-NEXT:    [[TMP11:%.*]] = select <32 x i1> [[TMP10]], <32 x half> [[ADD_I_I]], <32 x half> [[TMP9]]
// CHECK-NEXT:    ret <32 x half> [[TMP11]]
//
__m512h test_mm512_mask_add_ph(__m512h __W, __mmask32 __U, __m512h __A, __m512h __B) {
  return (__m512h)_mm512_mask_add_ph(__W, __U, __A, __B);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_maskz_add_ph(
// CHECK-SAME: i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store i32 [[TMP0]], ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    store <32 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP4]], ptr [[__A_ADDR_I_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP5]], ptr [[__B_ADDR_I_I]], align 64
// CHECK-NEXT:    [[TMP6:%.*]] = load <32 x half>, ptr [[__A_ADDR_I_I]], align 64
// CHECK-NEXT:    [[TMP7:%.*]] = load <32 x half>, ptr [[__B_ADDR_I_I]], align 64
// CHECK-NEXT:    [[ADD_I_I:%.*]] = fadd <32 x half> [[TMP6]], [[TMP7]]
// CHECK-NEXT:    store <32 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP8:%.*]] = load <32 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP9:%.*]] = bitcast i32 [[TMP3]] to <32 x i1>
// CHECK-NEXT:    [[TMP10:%.*]] = select <32 x i1> [[TMP9]], <32 x half> [[ADD_I_I]], <32 x half> [[TMP8]]
// CHECK-NEXT:    ret <32 x half> [[TMP10]]
//
__m512h test_mm512_maskz_add_ph(__mmask32 __U, __m512h __A, __m512h __B) {
  return _mm512_maskz_add_ph(__U, __A, __B);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_add_round_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = call <32 x half> @llvm.x86.avx512fp16.add.ph.512(<32 x half> [[TMP0]], <32 x half> [[TMP1]], i32 11)
// CHECK-NEXT:    ret <32 x half> [[TMP2]]
//
__m512h test_mm512_add_round_ph(__m512h __A, __m512h __B) {
  return _mm512_add_round_ph(__A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}
// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask_add_round_ph(
// CHECK-SAME: <32 x half> noundef [[__W:%.*]], i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__W]], ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = call <32 x half> @llvm.x86.avx512fp16.add.ph.512(<32 x half> [[TMP1]], <32 x half> [[TMP2]], i32 11)
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast i32 [[TMP0]] to <32 x i1>
// CHECK-NEXT:    [[TMP6:%.*]] = select <32 x i1> [[TMP5]], <32 x half> [[TMP3]], <32 x half> [[TMP4]]
// CHECK-NEXT:    ret <32 x half> [[TMP6]]
//
__m512h test_mm512_mask_add_round_ph(__m512h __W, __mmask32 __U, __m512h __A, __m512h __B) {
  return _mm512_mask_add_round_ph(__W, __U, __A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}
// CHECK-LABEL: define dso_local <32 x half> @test_mm512_maskz_add_round_ph(
// CHECK-SAME: i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = call <32 x half> @llvm.x86.avx512fp16.add.ph.512(<32 x half> [[TMP1]], <32 x half> [[TMP2]], i32 11)
// CHECK-NEXT:    store <32 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast i32 [[TMP0]] to <32 x i1>
// CHECK-NEXT:    [[TMP6:%.*]] = select <32 x i1> [[TMP5]], <32 x half> [[TMP3]], <32 x half> [[TMP4]]
// CHECK-NEXT:    ret <32 x half> [[TMP6]]
//
__m512h test_mm512_maskz_add_round_ph(__mmask32 __U, __m512h __A, __m512h __B) {
  return _mm512_maskz_add_round_ph(__U, __A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_sub_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP1]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[SUB_I:%.*]] = fsub <32 x half> [[TMP2]], [[TMP3]]
// CHECK-NEXT:    ret <32 x half> [[SUB_I]]
//
__m512h test_mm512_sub_ph(__m512h __A, __m512h __B) {
  return _mm512_sub_ph(__A, __B);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask_sub_ph(
// CHECK-SAME: <32 x half> noundef [[__W:%.*]], i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__W]], ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP3]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load i32, ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP5:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP6:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP5]], ptr [[__A_ADDR_I_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP6]], ptr [[__B_ADDR_I_I]], align 64
// CHECK-NEXT:    [[TMP7:%.*]] = load <32 x half>, ptr [[__A_ADDR_I_I]], align 64
// CHECK-NEXT:    [[TMP8:%.*]] = load <32 x half>, ptr [[__B_ADDR_I_I]], align 64
// CHECK-NEXT:    [[SUB_I_I:%.*]] = fsub <32 x half> [[TMP7]], [[TMP8]]
// CHECK-NEXT:    [[TMP9:%.*]] = load <32 x half>, ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP10:%.*]] = bitcast i32 [[TMP4]] to <32 x i1>
// CHECK-NEXT:    [[TMP11:%.*]] = select <32 x i1> [[TMP10]], <32 x half> [[SUB_I_I]], <32 x half> [[TMP9]]
// CHECK-NEXT:    ret <32 x half> [[TMP11]]
//
__m512h test_mm512_mask_sub_ph(__m512h __W, __mmask32 __U, __m512h __A, __m512h __B) {
  return (__m512h)_mm512_mask_sub_ph(__W, __U, __A, __B);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_maskz_sub_ph(
// CHECK-SAME: i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store i32 [[TMP0]], ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    store <32 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP4]], ptr [[__A_ADDR_I_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP5]], ptr [[__B_ADDR_I_I]], align 64
// CHECK-NEXT:    [[TMP6:%.*]] = load <32 x half>, ptr [[__A_ADDR_I_I]], align 64
// CHECK-NEXT:    [[TMP7:%.*]] = load <32 x half>, ptr [[__B_ADDR_I_I]], align 64
// CHECK-NEXT:    [[SUB_I_I:%.*]] = fsub <32 x half> [[TMP6]], [[TMP7]]
// CHECK-NEXT:    store <32 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP8:%.*]] = load <32 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP9:%.*]] = bitcast i32 [[TMP3]] to <32 x i1>
// CHECK-NEXT:    [[TMP10:%.*]] = select <32 x i1> [[TMP9]], <32 x half> [[SUB_I_I]], <32 x half> [[TMP8]]
// CHECK-NEXT:    ret <32 x half> [[TMP10]]
//
__m512h test_mm512_maskz_sub_ph(__mmask32 __U, __m512h __A, __m512h __B) {
  return _mm512_maskz_sub_ph(__U, __A, __B);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_sub_round_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = call <32 x half> @llvm.x86.avx512fp16.sub.ph.512(<32 x half> [[TMP0]], <32 x half> [[TMP1]], i32 11)
// CHECK-NEXT:    ret <32 x half> [[TMP2]]
//
__m512h test_mm512_sub_round_ph(__m512h __A, __m512h __B) {
  return _mm512_sub_round_ph(__A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}
// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask_sub_round_ph(
// CHECK-SAME: <32 x half> noundef [[__W:%.*]], i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__W]], ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = call <32 x half> @llvm.x86.avx512fp16.sub.ph.512(<32 x half> [[TMP1]], <32 x half> [[TMP2]], i32 11)
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast i32 [[TMP0]] to <32 x i1>
// CHECK-NEXT:    [[TMP6:%.*]] = select <32 x i1> [[TMP5]], <32 x half> [[TMP3]], <32 x half> [[TMP4]]
// CHECK-NEXT:    ret <32 x half> [[TMP6]]
//
__m512h test_mm512_mask_sub_round_ph(__m512h __W, __mmask32 __U, __m512h __A, __m512h __B) {
  return _mm512_mask_sub_round_ph(__W, __U, __A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}
// CHECK-LABEL: define dso_local <32 x half> @test_mm512_maskz_sub_round_ph(
// CHECK-SAME: i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = call <32 x half> @llvm.x86.avx512fp16.sub.ph.512(<32 x half> [[TMP1]], <32 x half> [[TMP2]], i32 11)
// CHECK-NEXT:    store <32 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast i32 [[TMP0]] to <32 x i1>
// CHECK-NEXT:    [[TMP6:%.*]] = select <32 x i1> [[TMP5]], <32 x half> [[TMP3]], <32 x half> [[TMP4]]
// CHECK-NEXT:    ret <32 x half> [[TMP6]]
//
__m512h test_mm512_maskz_sub_round_ph(__mmask32 __U, __m512h __A, __m512h __B) {
  return _mm512_maskz_sub_round_ph(__U, __A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mul_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP1]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[MUL_I:%.*]] = fmul <32 x half> [[TMP2]], [[TMP3]]
// CHECK-NEXT:    ret <32 x half> [[MUL_I]]
//
__m512h test_mm512_mul_ph(__m512h __A, __m512h __B) {
  return _mm512_mul_ph(__A, __B);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask_mul_ph(
// CHECK-SAME: <32 x half> noundef [[__W:%.*]], i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__W]], ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP3]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load i32, ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP5:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP6:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP5]], ptr [[__A_ADDR_I_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP6]], ptr [[__B_ADDR_I_I]], align 64
// CHECK-NEXT:    [[TMP7:%.*]] = load <32 x half>, ptr [[__A_ADDR_I_I]], align 64
// CHECK-NEXT:    [[TMP8:%.*]] = load <32 x half>, ptr [[__B_ADDR_I_I]], align 64
// CHECK-NEXT:    [[MUL_I_I:%.*]] = fmul <32 x half> [[TMP7]], [[TMP8]]
// CHECK-NEXT:    [[TMP9:%.*]] = load <32 x half>, ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP10:%.*]] = bitcast i32 [[TMP4]] to <32 x i1>
// CHECK-NEXT:    [[TMP11:%.*]] = select <32 x i1> [[TMP10]], <32 x half> [[MUL_I_I]], <32 x half> [[TMP9]]
// CHECK-NEXT:    ret <32 x half> [[TMP11]]
//
__m512h test_mm512_mask_mul_ph(__m512h __W, __mmask32 __U, __m512h __A, __m512h __B) {
  return (__m512h)_mm512_mask_mul_ph(__W, __U, __A, __B);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_maskz_mul_ph(
// CHECK-SAME: i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store i32 [[TMP0]], ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    store <32 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP4]], ptr [[__A_ADDR_I_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP5]], ptr [[__B_ADDR_I_I]], align 64
// CHECK-NEXT:    [[TMP6:%.*]] = load <32 x half>, ptr [[__A_ADDR_I_I]], align 64
// CHECK-NEXT:    [[TMP7:%.*]] = load <32 x half>, ptr [[__B_ADDR_I_I]], align 64
// CHECK-NEXT:    [[MUL_I_I:%.*]] = fmul <32 x half> [[TMP6]], [[TMP7]]
// CHECK-NEXT:    store <32 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP8:%.*]] = load <32 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP9:%.*]] = bitcast i32 [[TMP3]] to <32 x i1>
// CHECK-NEXT:    [[TMP10:%.*]] = select <32 x i1> [[TMP9]], <32 x half> [[MUL_I_I]], <32 x half> [[TMP8]]
// CHECK-NEXT:    ret <32 x half> [[TMP10]]
//
__m512h test_mm512_maskz_mul_ph(__mmask32 __U, __m512h __A, __m512h __B) {
  return _mm512_maskz_mul_ph(__U, __A, __B);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mul_round_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = call <32 x half> @llvm.x86.avx512fp16.mul.ph.512(<32 x half> [[TMP0]], <32 x half> [[TMP1]], i32 11)
// CHECK-NEXT:    ret <32 x half> [[TMP2]]
//
__m512h test_mm512_mul_round_ph(__m512h __A, __m512h __B) {
  return _mm512_mul_round_ph(__A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}
// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask_mul_round_ph(
// CHECK-SAME: <32 x half> noundef [[__W:%.*]], i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__W]], ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = call <32 x half> @llvm.x86.avx512fp16.mul.ph.512(<32 x half> [[TMP1]], <32 x half> [[TMP2]], i32 11)
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast i32 [[TMP0]] to <32 x i1>
// CHECK-NEXT:    [[TMP6:%.*]] = select <32 x i1> [[TMP5]], <32 x half> [[TMP3]], <32 x half> [[TMP4]]
// CHECK-NEXT:    ret <32 x half> [[TMP6]]
//
__m512h test_mm512_mask_mul_round_ph(__m512h __W, __mmask32 __U, __m512h __A, __m512h __B) {
  return _mm512_mask_mul_round_ph(__W, __U, __A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}
// CHECK-LABEL: define dso_local <32 x half> @test_mm512_maskz_mul_round_ph(
// CHECK-SAME: i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = call <32 x half> @llvm.x86.avx512fp16.mul.ph.512(<32 x half> [[TMP1]], <32 x half> [[TMP2]], i32 11)
// CHECK-NEXT:    store <32 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast i32 [[TMP0]] to <32 x i1>
// CHECK-NEXT:    [[TMP6:%.*]] = select <32 x i1> [[TMP5]], <32 x half> [[TMP3]], <32 x half> [[TMP4]]
// CHECK-NEXT:    ret <32 x half> [[TMP6]]
//
__m512h test_mm512_maskz_mul_round_ph(__mmask32 __U, __m512h __A, __m512h __B) {
  return _mm512_maskz_mul_round_ph(__U, __A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_div_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP1]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[DIV_I:%.*]] = fdiv <32 x half> [[TMP2]], [[TMP3]]
// CHECK-NEXT:    ret <32 x half> [[DIV_I]]
//
__m512h test_mm512_div_ph(__m512h __A, __m512h __B) {
  return _mm512_div_ph(__A, __B);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask_div_ph(
// CHECK-SAME: <32 x half> noundef [[__W:%.*]], i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__W]], ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP3]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load i32, ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP5:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP6:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP5]], ptr [[__A_ADDR_I_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP6]], ptr [[__B_ADDR_I_I]], align 64
// CHECK-NEXT:    [[TMP7:%.*]] = load <32 x half>, ptr [[__A_ADDR_I_I]], align 64
// CHECK-NEXT:    [[TMP8:%.*]] = load <32 x half>, ptr [[__B_ADDR_I_I]], align 64
// CHECK-NEXT:    [[DIV_I_I:%.*]] = fdiv <32 x half> [[TMP7]], [[TMP8]]
// CHECK-NEXT:    [[TMP9:%.*]] = load <32 x half>, ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP10:%.*]] = bitcast i32 [[TMP4]] to <32 x i1>
// CHECK-NEXT:    [[TMP11:%.*]] = select <32 x i1> [[TMP10]], <32 x half> [[DIV_I_I]], <32 x half> [[TMP9]]
// CHECK-NEXT:    ret <32 x half> [[TMP11]]
//
__m512h test_mm512_mask_div_ph(__m512h __W, __mmask32 __U, __m512h __A, __m512h __B) {
  return (__m512h)_mm512_mask_div_ph(__W, __U, __A, __B);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_maskz_div_ph(
// CHECK-SAME: i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store i32 [[TMP0]], ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    store <32 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP4]], ptr [[__A_ADDR_I_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP5]], ptr [[__B_ADDR_I_I]], align 64
// CHECK-NEXT:    [[TMP6:%.*]] = load <32 x half>, ptr [[__A_ADDR_I_I]], align 64
// CHECK-NEXT:    [[TMP7:%.*]] = load <32 x half>, ptr [[__B_ADDR_I_I]], align 64
// CHECK-NEXT:    [[DIV_I_I:%.*]] = fdiv <32 x half> [[TMP6]], [[TMP7]]
// CHECK-NEXT:    store <32 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP8:%.*]] = load <32 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP9:%.*]] = bitcast i32 [[TMP3]] to <32 x i1>
// CHECK-NEXT:    [[TMP10:%.*]] = select <32 x i1> [[TMP9]], <32 x half> [[DIV_I_I]], <32 x half> [[TMP8]]
// CHECK-NEXT:    ret <32 x half> [[TMP10]]
//
__m512h test_mm512_maskz_div_ph(__mmask32 __U, __m512h __A, __m512h __B) {
  return _mm512_maskz_div_ph(__U, __A, __B);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_div_round_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = call <32 x half> @llvm.x86.avx512fp16.div.ph.512(<32 x half> [[TMP0]], <32 x half> [[TMP1]], i32 11)
// CHECK-NEXT:    ret <32 x half> [[TMP2]]
//
__m512h test_mm512_div_round_ph(__m512h __A, __m512h __B) {
  return _mm512_div_round_ph(__A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}
// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask_div_round_ph(
// CHECK-SAME: <32 x half> noundef [[__W:%.*]], i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__W]], ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = call <32 x half> @llvm.x86.avx512fp16.div.ph.512(<32 x half> [[TMP1]], <32 x half> [[TMP2]], i32 11)
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast i32 [[TMP0]] to <32 x i1>
// CHECK-NEXT:    [[TMP6:%.*]] = select <32 x i1> [[TMP5]], <32 x half> [[TMP3]], <32 x half> [[TMP4]]
// CHECK-NEXT:    ret <32 x half> [[TMP6]]
//
__m512h test_mm512_mask_div_round_ph(__m512h __W, __mmask32 __U, __m512h __A, __m512h __B) {
  return _mm512_mask_div_round_ph(__W, __U, __A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}
// CHECK-LABEL: define dso_local <32 x half> @test_mm512_maskz_div_round_ph(
// CHECK-SAME: i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = call <32 x half> @llvm.x86.avx512fp16.div.ph.512(<32 x half> [[TMP1]], <32 x half> [[TMP2]], i32 11)
// CHECK-NEXT:    store <32 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast i32 [[TMP0]] to <32 x i1>
// CHECK-NEXT:    [[TMP6:%.*]] = select <32 x i1> [[TMP5]], <32 x half> [[TMP3]], <32 x half> [[TMP4]]
// CHECK-NEXT:    ret <32 x half> [[TMP6]]
//
__m512h test_mm512_maskz_div_round_ph(__mmask32 __U, __m512h __A, __m512h __B) {
  return _mm512_maskz_div_round_ph(__U, __A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_min_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP1]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = call <32 x half> @llvm.x86.avx512fp16.min.ph.512(<32 x half> [[TMP2]], <32 x half> [[TMP3]], i32 4)
// CHECK-NEXT:    ret <32 x half> [[TMP4]]
//
__m512h test_mm512_min_ph(__m512h __A, __m512h __B) {
  return _mm512_min_ph(__A, __B);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask_min_ph(
// CHECK-SAME: <32 x half> noundef [[__W:%.*]], i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__W]], ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP3]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load i32, ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP5:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP6:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP5]], ptr [[__A_ADDR_I_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP6]], ptr [[__B_ADDR_I_I]], align 64
// CHECK-NEXT:    [[TMP7:%.*]] = load <32 x half>, ptr [[__A_ADDR_I_I]], align 64
// CHECK-NEXT:    [[TMP8:%.*]] = load <32 x half>, ptr [[__B_ADDR_I_I]], align 64
// CHECK-NEXT:    [[TMP9:%.*]] = call <32 x half> @llvm.x86.avx512fp16.min.ph.512(<32 x half> [[TMP7]], <32 x half> [[TMP8]], i32 4)
// CHECK-NEXT:    [[TMP10:%.*]] = load <32 x half>, ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP11:%.*]] = bitcast i32 [[TMP4]] to <32 x i1>
// CHECK-NEXT:    [[TMP12:%.*]] = select <32 x i1> [[TMP11]], <32 x half> [[TMP9]], <32 x half> [[TMP10]]
// CHECK-NEXT:    ret <32 x half> [[TMP12]]
//
__m512h test_mm512_mask_min_ph(__m512h __W, __mmask32 __U, __m512h __A, __m512h __B) {
  return (__m512h)_mm512_mask_min_ph(__W, __U, __A, __B);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_maskz_min_ph(
// CHECK-SAME: i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store i32 [[TMP0]], ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    store <32 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP4]], ptr [[__A_ADDR_I_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP5]], ptr [[__B_ADDR_I_I]], align 64
// CHECK-NEXT:    [[TMP6:%.*]] = load <32 x half>, ptr [[__A_ADDR_I_I]], align 64
// CHECK-NEXT:    [[TMP7:%.*]] = load <32 x half>, ptr [[__B_ADDR_I_I]], align 64
// CHECK-NEXT:    [[TMP8:%.*]] = call <32 x half> @llvm.x86.avx512fp16.min.ph.512(<32 x half> [[TMP6]], <32 x half> [[TMP7]], i32 4)
// CHECK-NEXT:    store <32 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP9:%.*]] = load <32 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP10:%.*]] = bitcast i32 [[TMP3]] to <32 x i1>
// CHECK-NEXT:    [[TMP11:%.*]] = select <32 x i1> [[TMP10]], <32 x half> [[TMP8]], <32 x half> [[TMP9]]
// CHECK-NEXT:    ret <32 x half> [[TMP11]]
//
__m512h test_mm512_maskz_min_ph(__mmask32 __U, __m512h __A, __m512h __B) {
  return _mm512_maskz_min_ph(__U, __A, __B);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_min_round_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = call <32 x half> @llvm.x86.avx512fp16.min.ph.512(<32 x half> [[TMP0]], <32 x half> [[TMP1]], i32 8)
// CHECK-NEXT:    ret <32 x half> [[TMP2]]
//
__m512h test_mm512_min_round_ph(__m512h __A, __m512h __B) {
  return _mm512_min_round_ph(__A, __B, _MM_FROUND_NO_EXC);
}
// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask_min_round_ph(
// CHECK-SAME: <32 x half> noundef [[__W:%.*]], i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__W]], ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = call <32 x half> @llvm.x86.avx512fp16.min.ph.512(<32 x half> [[TMP1]], <32 x half> [[TMP2]], i32 8)
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast i32 [[TMP0]] to <32 x i1>
// CHECK-NEXT:    [[TMP6:%.*]] = select <32 x i1> [[TMP5]], <32 x half> [[TMP3]], <32 x half> [[TMP4]]
// CHECK-NEXT:    ret <32 x half> [[TMP6]]
//
__m512h test_mm512_mask_min_round_ph(__m512h __W, __mmask32 __U, __m512h __A, __m512h __B) {
  return _mm512_mask_min_round_ph(__W, __U, __A, __B, _MM_FROUND_NO_EXC);
}
// CHECK-LABEL: define dso_local <32 x half> @test_mm512_maskz_min_round_ph(
// CHECK-SAME: i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = call <32 x half> @llvm.x86.avx512fp16.min.ph.512(<32 x half> [[TMP1]], <32 x half> [[TMP2]], i32 8)
// CHECK-NEXT:    store <32 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast i32 [[TMP0]] to <32 x i1>
// CHECK-NEXT:    [[TMP6:%.*]] = select <32 x i1> [[TMP5]], <32 x half> [[TMP3]], <32 x half> [[TMP4]]
// CHECK-NEXT:    ret <32 x half> [[TMP6]]
//
__m512h test_mm512_maskz_min_round_ph(__mmask32 __U, __m512h __A, __m512h __B) {
  return _mm512_maskz_min_round_ph(__U, __A, __B, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_max_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP1]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = call <32 x half> @llvm.x86.avx512fp16.max.ph.512(<32 x half> [[TMP2]], <32 x half> [[TMP3]], i32 4)
// CHECK-NEXT:    ret <32 x half> [[TMP4]]
//
__m512h test_mm512_max_ph(__m512h __A, __m512h __B) {

  return _mm512_max_ph(__A, __B);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask_max_ph(
// CHECK-SAME: <32 x half> noundef [[__W:%.*]], i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__W]], ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP3]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load i32, ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP5:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP6:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP5]], ptr [[__A_ADDR_I_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP6]], ptr [[__B_ADDR_I_I]], align 64
// CHECK-NEXT:    [[TMP7:%.*]] = load <32 x half>, ptr [[__A_ADDR_I_I]], align 64
// CHECK-NEXT:    [[TMP8:%.*]] = load <32 x half>, ptr [[__B_ADDR_I_I]], align 64
// CHECK-NEXT:    [[TMP9:%.*]] = call <32 x half> @llvm.x86.avx512fp16.max.ph.512(<32 x half> [[TMP7]], <32 x half> [[TMP8]], i32 4)
// CHECK-NEXT:    [[TMP10:%.*]] = load <32 x half>, ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP11:%.*]] = bitcast i32 [[TMP4]] to <32 x i1>
// CHECK-NEXT:    [[TMP12:%.*]] = select <32 x i1> [[TMP11]], <32 x half> [[TMP9]], <32 x half> [[TMP10]]
// CHECK-NEXT:    ret <32 x half> [[TMP12]]
//
__m512h test_mm512_mask_max_ph(__m512h __W, __mmask32 __U, __m512h __A, __m512h __B) {
  return (__m512h)_mm512_mask_max_ph(__W, __U, __A, __B);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_maskz_max_ph(
// CHECK-SAME: i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store i32 [[TMP0]], ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    store <32 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP4]], ptr [[__A_ADDR_I_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP5]], ptr [[__B_ADDR_I_I]], align 64
// CHECK-NEXT:    [[TMP6:%.*]] = load <32 x half>, ptr [[__A_ADDR_I_I]], align 64
// CHECK-NEXT:    [[TMP7:%.*]] = load <32 x half>, ptr [[__B_ADDR_I_I]], align 64
// CHECK-NEXT:    [[TMP8:%.*]] = call <32 x half> @llvm.x86.avx512fp16.max.ph.512(<32 x half> [[TMP6]], <32 x half> [[TMP7]], i32 4)
// CHECK-NEXT:    store <32 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP9:%.*]] = load <32 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP10:%.*]] = bitcast i32 [[TMP3]] to <32 x i1>
// CHECK-NEXT:    [[TMP11:%.*]] = select <32 x i1> [[TMP10]], <32 x half> [[TMP8]], <32 x half> [[TMP9]]
// CHECK-NEXT:    ret <32 x half> [[TMP11]]
//
__m512h test_mm512_maskz_max_ph(__mmask32 __U, __m512h __A, __m512h __B) {
  return _mm512_maskz_max_ph(__U, __A, __B);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_max_round_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = call <32 x half> @llvm.x86.avx512fp16.max.ph.512(<32 x half> [[TMP0]], <32 x half> [[TMP1]], i32 8)
// CHECK-NEXT:    ret <32 x half> [[TMP2]]
//
__m512h test_mm512_max_round_ph(__m512h __A, __m512h __B) {
  return _mm512_max_round_ph(__A, __B, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask_max_round_ph(
// CHECK-SAME: <32 x half> noundef [[__W:%.*]], i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__W]], ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = call <32 x half> @llvm.x86.avx512fp16.max.ph.512(<32 x half> [[TMP1]], <32 x half> [[TMP2]], i32 8)
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast i32 [[TMP0]] to <32 x i1>
// CHECK-NEXT:    [[TMP6:%.*]] = select <32 x i1> [[TMP5]], <32 x half> [[TMP3]], <32 x half> [[TMP4]]
// CHECK-NEXT:    ret <32 x half> [[TMP6]]
//
__m512h test_mm512_mask_max_round_ph(__m512h __W, __mmask32 __U, __m512h __A, __m512h __B) {
  return _mm512_mask_max_round_ph(__W, __U, __A, __B, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_maskz_max_round_ph(
// CHECK-SAME: i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = call <32 x half> @llvm.x86.avx512fp16.max.ph.512(<32 x half> [[TMP1]], <32 x half> [[TMP2]], i32 8)
// CHECK-NEXT:    store <32 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast i32 [[TMP0]] to <32 x i1>
// CHECK-NEXT:    [[TMP6:%.*]] = select <32 x i1> [[TMP5]], <32 x half> [[TMP3]], <32 x half> [[TMP4]]
// CHECK-NEXT:    ret <32 x half> [[TMP6]]
//
__m512h test_mm512_maskz_max_round_ph(__mmask32 __U, __m512h __A, __m512h __B) {
  return _mm512_maskz_max_round_ph(__U, __A, __B, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_abs_ph(
// CHECK-SAME: <32 x half> noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__S_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <16 x i32>, align 64
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[__A_ADDR_I]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store i32 2147450879, ptr [[__S_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[__S_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT_I:%.*]] = insertelement <16 x i32> undef, i32 [[TMP1]], i32 0
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[__S_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT1_I:%.*]] = insertelement <16 x i32> [[VECINIT_I]], i32 [[TMP2]], i32 1
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[__S_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT2_I:%.*]] = insertelement <16 x i32> [[VECINIT1_I]], i32 [[TMP3]], i32 2
// CHECK-NEXT:    [[TMP4:%.*]] = load i32, ptr [[__S_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT3_I:%.*]] = insertelement <16 x i32> [[VECINIT2_I]], i32 [[TMP4]], i32 3
// CHECK-NEXT:    [[TMP5:%.*]] = load i32, ptr [[__S_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT4_I:%.*]] = insertelement <16 x i32> [[VECINIT3_I]], i32 [[TMP5]], i32 4
// CHECK-NEXT:    [[TMP6:%.*]] = load i32, ptr [[__S_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT5_I:%.*]] = insertelement <16 x i32> [[VECINIT4_I]], i32 [[TMP6]], i32 5
// CHECK-NEXT:    [[TMP7:%.*]] = load i32, ptr [[__S_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT6_I:%.*]] = insertelement <16 x i32> [[VECINIT5_I]], i32 [[TMP7]], i32 6
// CHECK-NEXT:    [[TMP8:%.*]] = load i32, ptr [[__S_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT7_I:%.*]] = insertelement <16 x i32> [[VECINIT6_I]], i32 [[TMP8]], i32 7
// CHECK-NEXT:    [[TMP9:%.*]] = load i32, ptr [[__S_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT8_I:%.*]] = insertelement <16 x i32> [[VECINIT7_I]], i32 [[TMP9]], i32 8
// CHECK-NEXT:    [[TMP10:%.*]] = load i32, ptr [[__S_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT9_I:%.*]] = insertelement <16 x i32> [[VECINIT8_I]], i32 [[TMP10]], i32 9
// CHECK-NEXT:    [[TMP11:%.*]] = load i32, ptr [[__S_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT10_I:%.*]] = insertelement <16 x i32> [[VECINIT9_I]], i32 [[TMP11]], i32 10
// CHECK-NEXT:    [[TMP12:%.*]] = load i32, ptr [[__S_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT11_I:%.*]] = insertelement <16 x i32> [[VECINIT10_I]], i32 [[TMP12]], i32 11
// CHECK-NEXT:    [[TMP13:%.*]] = load i32, ptr [[__S_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT12_I:%.*]] = insertelement <16 x i32> [[VECINIT11_I]], i32 [[TMP13]], i32 12
// CHECK-NEXT:    [[TMP14:%.*]] = load i32, ptr [[__S_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT13_I:%.*]] = insertelement <16 x i32> [[VECINIT12_I]], i32 [[TMP14]], i32 13
// CHECK-NEXT:    [[TMP15:%.*]] = load i32, ptr [[__S_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT14_I:%.*]] = insertelement <16 x i32> [[VECINIT13_I]], i32 [[TMP15]], i32 14
// CHECK-NEXT:    [[TMP16:%.*]] = load i32, ptr [[__S_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT15_I:%.*]] = insertelement <16 x i32> [[VECINIT14_I]], i32 [[TMP16]], i32 15
// CHECK-NEXT:    store <16 x i32> [[VECINIT15_I]], ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP17:%.*]] = load <16 x i32>, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP18:%.*]] = bitcast <16 x i32> [[TMP17]] to <8 x i64>
// CHECK-NEXT:    [[TMP19:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP20:%.*]] = bitcast <32 x half> [[TMP19]] to <8 x i64>
// CHECK-NEXT:    store <8 x i64> [[TMP18]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <8 x i64> [[TMP20]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP21:%.*]] = load <8 x i64>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP22:%.*]] = bitcast <8 x i64> [[TMP21]] to <16 x i32>
// CHECK-NEXT:    [[TMP23:%.*]] = load <8 x i64>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP24:%.*]] = bitcast <8 x i64> [[TMP23]] to <16 x i32>
// CHECK-NEXT:    [[AND_I:%.*]] = and <16 x i32> [[TMP22]], [[TMP24]]
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <16 x i32> [[AND_I]] to <8 x i64>
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <8 x i64> [[TMP25]] to <32 x half>
// CHECK-NEXT:    ret <32 x half> [[TMP26]]
//
__m512h test_mm512_abs_ph(__m512h a) {
  return _mm512_abs_ph(a);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_conj_pch(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I1:%.*]] = alloca <16 x float>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <16 x float>, align 64
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca float, align 4
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <16 x float>, align 64
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <32 x half> [[TMP1]] to <16 x float>
// CHECK-NEXT:    store float -0.000000e+00, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT_I:%.*]] = insertelement <16 x float> undef, float [[TMP3]], i32 0
// CHECK-NEXT:    [[TMP4:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT1_I:%.*]] = insertelement <16 x float> [[VECINIT_I]], float [[TMP4]], i32 1
// CHECK-NEXT:    [[TMP5:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT2_I:%.*]] = insertelement <16 x float> [[VECINIT1_I]], float [[TMP5]], i32 2
// CHECK-NEXT:    [[TMP6:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT3_I:%.*]] = insertelement <16 x float> [[VECINIT2_I]], float [[TMP6]], i32 3
// CHECK-NEXT:    [[TMP7:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT4_I:%.*]] = insertelement <16 x float> [[VECINIT3_I]], float [[TMP7]], i32 4
// CHECK-NEXT:    [[TMP8:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT5_I:%.*]] = insertelement <16 x float> [[VECINIT4_I]], float [[TMP8]], i32 5
// CHECK-NEXT:    [[TMP9:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT6_I:%.*]] = insertelement <16 x float> [[VECINIT5_I]], float [[TMP9]], i32 6
// CHECK-NEXT:    [[TMP10:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT7_I:%.*]] = insertelement <16 x float> [[VECINIT6_I]], float [[TMP10]], i32 7
// CHECK-NEXT:    [[TMP11:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT8_I:%.*]] = insertelement <16 x float> [[VECINIT7_I]], float [[TMP11]], i32 8
// CHECK-NEXT:    [[TMP12:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT9_I:%.*]] = insertelement <16 x float> [[VECINIT8_I]], float [[TMP12]], i32 9
// CHECK-NEXT:    [[TMP13:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT10_I:%.*]] = insertelement <16 x float> [[VECINIT9_I]], float [[TMP13]], i32 10
// CHECK-NEXT:    [[TMP14:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT11_I:%.*]] = insertelement <16 x float> [[VECINIT10_I]], float [[TMP14]], i32 11
// CHECK-NEXT:    [[TMP15:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT12_I:%.*]] = insertelement <16 x float> [[VECINIT11_I]], float [[TMP15]], i32 12
// CHECK-NEXT:    [[TMP16:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT13_I:%.*]] = insertelement <16 x float> [[VECINIT12_I]], float [[TMP16]], i32 13
// CHECK-NEXT:    [[TMP17:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT14_I:%.*]] = insertelement <16 x float> [[VECINIT13_I]], float [[TMP17]], i32 14
// CHECK-NEXT:    [[TMP18:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT15_I:%.*]] = insertelement <16 x float> [[VECINIT14_I]], float [[TMP18]], i32 15
// CHECK-NEXT:    store <16 x float> [[VECINIT15_I]], ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP19:%.*]] = load <16 x float>, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    store <16 x float> [[TMP2]], ptr [[__A_ADDR_I1]], align 64
// CHECK-NEXT:    store <16 x float> [[TMP19]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP20:%.*]] = load <16 x float>, ptr [[__A_ADDR_I1]], align 64
// CHECK-NEXT:    [[TMP21:%.*]] = bitcast <16 x float> [[TMP20]] to <16 x i32>
// CHECK-NEXT:    [[TMP22:%.*]] = load <16 x float>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP23:%.*]] = bitcast <16 x float> [[TMP22]] to <16 x i32>
// CHECK-NEXT:    [[XOR_I:%.*]] = xor <16 x i32> [[TMP21]], [[TMP23]]
// CHECK-NEXT:    [[TMP24:%.*]] = bitcast <16 x i32> [[XOR_I]] to <16 x float>
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <16 x float> [[TMP24]] to <32 x half>
// CHECK-NEXT:    ret <32 x half> [[TMP25]]
//
__m512h test_mm512_conj_pch(__m512h __A) {
  return _mm512_conj_pch(__A);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask_conj_pch(
// CHECK-SAME: <32 x half> noundef [[__W:%.*]], i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I1:%.*]] = alloca <16 x float>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <16 x float>, align 64
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca float, align 4
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <16 x float>, align 64
// CHECK-NEXT:    [[__A_ADDR_I_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__W_ADDR_I]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__W]], ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[CONV:%.*]] = trunc i32 [[TMP1]] to i16
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    store i16 [[CONV]], ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load i16, ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP4]], ptr [[__A_ADDR_I_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = load <32 x half>, ptr [[__A_ADDR_I_I]], align 64
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <32 x half> [[TMP5]] to <16 x float>
// CHECK-NEXT:    store float -0.000000e+00, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP7:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT_I:%.*]] = insertelement <16 x float> undef, float [[TMP7]], i32 0
// CHECK-NEXT:    [[TMP8:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT1_I:%.*]] = insertelement <16 x float> [[VECINIT_I]], float [[TMP8]], i32 1
// CHECK-NEXT:    [[TMP9:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT2_I:%.*]] = insertelement <16 x float> [[VECINIT1_I]], float [[TMP9]], i32 2
// CHECK-NEXT:    [[TMP10:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT3_I:%.*]] = insertelement <16 x float> [[VECINIT2_I]], float [[TMP10]], i32 3
// CHECK-NEXT:    [[TMP11:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT4_I:%.*]] = insertelement <16 x float> [[VECINIT3_I]], float [[TMP11]], i32 4
// CHECK-NEXT:    [[TMP12:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT5_I:%.*]] = insertelement <16 x float> [[VECINIT4_I]], float [[TMP12]], i32 5
// CHECK-NEXT:    [[TMP13:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT6_I:%.*]] = insertelement <16 x float> [[VECINIT5_I]], float [[TMP13]], i32 6
// CHECK-NEXT:    [[TMP14:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT7_I:%.*]] = insertelement <16 x float> [[VECINIT6_I]], float [[TMP14]], i32 7
// CHECK-NEXT:    [[TMP15:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT8_I:%.*]] = insertelement <16 x float> [[VECINIT7_I]], float [[TMP15]], i32 8
// CHECK-NEXT:    [[TMP16:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT9_I:%.*]] = insertelement <16 x float> [[VECINIT8_I]], float [[TMP16]], i32 9
// CHECK-NEXT:    [[TMP17:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT10_I:%.*]] = insertelement <16 x float> [[VECINIT9_I]], float [[TMP17]], i32 10
// CHECK-NEXT:    [[TMP18:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT11_I:%.*]] = insertelement <16 x float> [[VECINIT10_I]], float [[TMP18]], i32 11
// CHECK-NEXT:    [[TMP19:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT12_I:%.*]] = insertelement <16 x float> [[VECINIT11_I]], float [[TMP19]], i32 12
// CHECK-NEXT:    [[TMP20:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT13_I:%.*]] = insertelement <16 x float> [[VECINIT12_I]], float [[TMP20]], i32 13
// CHECK-NEXT:    [[TMP21:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT14_I:%.*]] = insertelement <16 x float> [[VECINIT13_I]], float [[TMP21]], i32 14
// CHECK-NEXT:    [[TMP22:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT15_I:%.*]] = insertelement <16 x float> [[VECINIT14_I]], float [[TMP22]], i32 15
// CHECK-NEXT:    store <16 x float> [[VECINIT15_I]], ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP23:%.*]] = load <16 x float>, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    store <16 x float> [[TMP6]], ptr [[__A_ADDR_I1]], align 64
// CHECK-NEXT:    store <16 x float> [[TMP23]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP24:%.*]] = load <16 x float>, ptr [[__A_ADDR_I1]], align 64
// CHECK-NEXT:    [[TMP25:%.*]] = bitcast <16 x float> [[TMP24]] to <16 x i32>
// CHECK-NEXT:    [[TMP26:%.*]] = load <16 x float>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP27:%.*]] = bitcast <16 x float> [[TMP26]] to <16 x i32>
// CHECK-NEXT:    [[XOR_I:%.*]] = xor <16 x i32> [[TMP25]], [[TMP27]]
// CHECK-NEXT:    [[TMP28:%.*]] = bitcast <16 x i32> [[XOR_I]] to <16 x float>
// CHECK-NEXT:    [[TMP29:%.*]] = bitcast <16 x float> [[TMP28]] to <32 x half>
// CHECK-NEXT:    [[TMP30:%.*]] = load <32 x half>, ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP31:%.*]] = bitcast <32 x half> [[TMP30]] to <16 x float>
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast i16 [[TMP3]] to <16 x i1>
// CHECK-NEXT:    [[TMP33:%.*]] = select <16 x i1> [[TMP32]], <16 x float> [[TMP28]], <16 x float> [[TMP31]]
// CHECK-NEXT:    [[TMP34:%.*]] = bitcast <16 x float> [[TMP33]] to <32 x half>
// CHECK-NEXT:    ret <32 x half> [[TMP34]]
//
__m512h test_mm512_mask_conj_pch(__m512h __W, __mmask32 __U, __m512h __A) {
  return _mm512_mask_conj_pch(__W, __U, __A);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_maskz_conj_pch(
// CHECK-SAME: i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I2:%.*]] = alloca <16 x float>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <16 x float>, align 64
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca float, align 4
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I1:%.*]] = alloca <16 x float>, align 64
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <16 x float>, align 64
// CHECK-NEXT:    [[__A_ADDR_I_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[CONV:%.*]] = trunc i32 [[TMP0]] to i16
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store i16 [[CONV]], ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    store <32 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i16, ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP3]], ptr [[__A_ADDR_I_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__A_ADDR_I_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <32 x half> [[TMP4]] to <16 x float>
// CHECK-NEXT:    store float -0.000000e+00, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP6:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT_I:%.*]] = insertelement <16 x float> undef, float [[TMP6]], i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT1_I:%.*]] = insertelement <16 x float> [[VECINIT_I]], float [[TMP7]], i32 1
// CHECK-NEXT:    [[TMP8:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT2_I:%.*]] = insertelement <16 x float> [[VECINIT1_I]], float [[TMP8]], i32 2
// CHECK-NEXT:    [[TMP9:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT3_I:%.*]] = insertelement <16 x float> [[VECINIT2_I]], float [[TMP9]], i32 3
// CHECK-NEXT:    [[TMP10:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT4_I:%.*]] = insertelement <16 x float> [[VECINIT3_I]], float [[TMP10]], i32 4
// CHECK-NEXT:    [[TMP11:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT5_I:%.*]] = insertelement <16 x float> [[VECINIT4_I]], float [[TMP11]], i32 5
// CHECK-NEXT:    [[TMP12:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT6_I:%.*]] = insertelement <16 x float> [[VECINIT5_I]], float [[TMP12]], i32 6
// CHECK-NEXT:    [[TMP13:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT7_I:%.*]] = insertelement <16 x float> [[VECINIT6_I]], float [[TMP13]], i32 7
// CHECK-NEXT:    [[TMP14:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT8_I:%.*]] = insertelement <16 x float> [[VECINIT7_I]], float [[TMP14]], i32 8
// CHECK-NEXT:    [[TMP15:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT9_I:%.*]] = insertelement <16 x float> [[VECINIT8_I]], float [[TMP15]], i32 9
// CHECK-NEXT:    [[TMP16:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT10_I:%.*]] = insertelement <16 x float> [[VECINIT9_I]], float [[TMP16]], i32 10
// CHECK-NEXT:    [[TMP17:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT11_I:%.*]] = insertelement <16 x float> [[VECINIT10_I]], float [[TMP17]], i32 11
// CHECK-NEXT:    [[TMP18:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT12_I:%.*]] = insertelement <16 x float> [[VECINIT11_I]], float [[TMP18]], i32 12
// CHECK-NEXT:    [[TMP19:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT13_I:%.*]] = insertelement <16 x float> [[VECINIT12_I]], float [[TMP19]], i32 13
// CHECK-NEXT:    [[TMP20:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT14_I:%.*]] = insertelement <16 x float> [[VECINIT13_I]], float [[TMP20]], i32 14
// CHECK-NEXT:    [[TMP21:%.*]] = load float, ptr [[__W_ADDR_I]], align 4
// CHECK-NEXT:    [[VECINIT15_I:%.*]] = insertelement <16 x float> [[VECINIT14_I]], float [[TMP21]], i32 15
// CHECK-NEXT:    store <16 x float> [[VECINIT15_I]], ptr [[DOTCOMPOUNDLITERAL_I1]], align 64
// CHECK-NEXT:    [[TMP22:%.*]] = load <16 x float>, ptr [[DOTCOMPOUNDLITERAL_I1]], align 64
// CHECK-NEXT:    store <16 x float> [[TMP5]], ptr [[__A_ADDR_I2]], align 64
// CHECK-NEXT:    store <16 x float> [[TMP22]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP23:%.*]] = load <16 x float>, ptr [[__A_ADDR_I2]], align 64
// CHECK-NEXT:    [[TMP24:%.*]] = bitcast <16 x float> [[TMP23]] to <16 x i32>
// CHECK-NEXT:    [[TMP25:%.*]] = load <16 x float>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP26:%.*]] = bitcast <16 x float> [[TMP25]] to <16 x i32>
// CHECK-NEXT:    [[XOR_I:%.*]] = xor <16 x i32> [[TMP24]], [[TMP26]]
// CHECK-NEXT:    [[TMP27:%.*]] = bitcast <16 x i32> [[XOR_I]] to <16 x float>
// CHECK-NEXT:    [[TMP28:%.*]] = bitcast <16 x float> [[TMP27]] to <32 x half>
// CHECK-NEXT:    store <16 x float> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP29:%.*]] = load <16 x float>, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP30:%.*]] = bitcast i16 [[TMP2]] to <16 x i1>
// CHECK-NEXT:    [[TMP31:%.*]] = select <16 x i1> [[TMP30]], <16 x float> [[TMP27]], <16 x float> [[TMP29]]
// CHECK-NEXT:    [[TMP32:%.*]] = bitcast <16 x float> [[TMP31]] to <32 x half>
// CHECK-NEXT:    ret <32 x half> [[TMP32]]
//
__m512h test_mm512_maskz_conj_pch(__mmask32 __U, __m512h __A) {
  return _mm512_maskz_conj_pch(__U, __A);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_add_round_sh(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.add.sh.round(<8 x half> [[TMP0]], <8 x half> [[TMP1]], <8 x half> [[TMP2]], i8 -1, i32 11)
// CHECK-NEXT:    ret <8 x half> [[TMP3]]
//
__m128h test_mm_add_round_sh(__m128h __A, __m128h __B) {
  return _mm_add_round_sh(__A, __B, _MM_FROUND_NO_EXC | _MM_FROUND_TO_ZERO);
}
// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask_add_round_sh(
// CHECK-SAME: <8 x half> noundef [[__W:%.*]], i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__W]], ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP4:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.add.sh.round(<8 x half> [[TMP0]], <8 x half> [[TMP1]], <8 x half> [[TMP2]], i8 [[TMP3]], i32 11)
// CHECK-NEXT:    ret <8 x half> [[TMP4]]
//
__m128h test_mm_mask_add_round_sh(__m128h __W, __mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_mask_add_round_sh(__W, __U, __A, __B, _MM_FROUND_NO_EXC | _MM_FROUND_TO_ZERO);
}
// CHECK-LABEL: define dso_local <8 x half> @test_mm_maskz_add_round_sh(
// CHECK-SAME: i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP4:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.add.sh.round(<8 x half> [[TMP0]], <8 x half> [[TMP1]], <8 x half> [[TMP2]], i8 [[TMP3]], i32 11)
// CHECK-NEXT:    ret <8 x half> [[TMP4]]
//
__m128h test_mm_maskz_add_round_sh(__mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_maskz_add_round_sh(__U, __A, __B, _MM_FROUND_NO_EXC | _MM_FROUND_TO_ZERO);
}
// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask_add_sh(
// CHECK-SAME: <8 x half> noundef [[__W:%.*]], i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I1:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I2:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__W]], ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    store i8 [[TMP1]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP3]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP4]], ptr [[__A_ADDR_I1]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP5]], ptr [[__B_ADDR_I2]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = load <8 x half>, ptr [[__B_ADDR_I2]], align 16
// CHECK-NEXT:    [[VECEXT_I:%.*]] = extractelement <8 x half> [[TMP6]], i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = load <8 x half>, ptr [[__A_ADDR_I1]], align 16
// CHECK-NEXT:    [[VECEXT1_I:%.*]] = extractelement <8 x half> [[TMP7]], i32 0
// CHECK-NEXT:    [[ADD_I:%.*]] = fadd half [[VECEXT1_I]], [[VECEXT_I]]
// CHECK-NEXT:    [[TMP8:%.*]] = load <8 x half>, ptr [[__A_ADDR_I1]], align 16
// CHECK-NEXT:    [[VECINS_I:%.*]] = insertelement <8 x half> [[TMP8]], half [[ADD_I]], i32 0
// CHECK-NEXT:    store <8 x half> [[VECINS_I]], ptr [[__A_ADDR_I1]], align 16
// CHECK-NEXT:    [[TMP9:%.*]] = load <8 x half>, ptr [[__A_ADDR_I1]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP9]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP10:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP11:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP12:%.*]] = load <8 x half>, ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP13:%.*]] = extractelement <8 x half> [[TMP11]], i64 0
// CHECK-NEXT:    [[TMP14:%.*]] = extractelement <8 x half> [[TMP12]], i64 0
// CHECK-NEXT:    [[TMP15:%.*]] = bitcast i8 [[TMP10]] to <8 x i1>
// CHECK-NEXT:    [[TMP16:%.*]] = extractelement <8 x i1> [[TMP15]], i64 0
// CHECK-NEXT:    [[TMP17:%.*]] = select i1 [[TMP16]], half [[TMP13]], half [[TMP14]]
// CHECK-NEXT:    [[TMP18:%.*]] = insertelement <8 x half> [[TMP11]], half [[TMP17]], i64 0
// CHECK-NEXT:    ret <8 x half> [[TMP18]]
//
__m128h test_mm_mask_add_sh(__m128h __W, __mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_mask_add_sh(__W, __U, __A, __B);
}
// CHECK-LABEL: define dso_local <8 x half> @test_mm_maskz_add_sh(
// CHECK-SAME: i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I1:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I2:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store i8 [[TMP0]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP3]], ptr [[__A_ADDR_I1]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP4]], ptr [[__B_ADDR_I2]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = load <8 x half>, ptr [[__B_ADDR_I2]], align 16
// CHECK-NEXT:    [[VECEXT_I:%.*]] = extractelement <8 x half> [[TMP5]], i32 0
// CHECK-NEXT:    [[TMP6:%.*]] = load <8 x half>, ptr [[__A_ADDR_I1]], align 16
// CHECK-NEXT:    [[VECEXT1_I:%.*]] = extractelement <8 x half> [[TMP6]], i32 0
// CHECK-NEXT:    [[ADD_I:%.*]] = fadd half [[VECEXT1_I]], [[VECEXT_I]]
// CHECK-NEXT:    [[TMP7:%.*]] = load <8 x half>, ptr [[__A_ADDR_I1]], align 16
// CHECK-NEXT:    [[VECINS_I:%.*]] = insertelement <8 x half> [[TMP7]], half [[ADD_I]], i32 0
// CHECK-NEXT:    store <8 x half> [[VECINS_I]], ptr [[__A_ADDR_I1]], align 16
// CHECK-NEXT:    [[TMP8:%.*]] = load <8 x half>, ptr [[__A_ADDR_I1]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP8]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP9:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP10:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP11:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP12:%.*]] = extractelement <8 x half> [[TMP10]], i64 0
// CHECK-NEXT:    [[TMP13:%.*]] = extractelement <8 x half> [[TMP11]], i64 0
// CHECK-NEXT:    [[TMP14:%.*]] = bitcast i8 [[TMP9]] to <8 x i1>
// CHECK-NEXT:    [[TMP15:%.*]] = extractelement <8 x i1> [[TMP14]], i64 0
// CHECK-NEXT:    [[TMP16:%.*]] = select i1 [[TMP15]], half [[TMP12]], half [[TMP13]]
// CHECK-NEXT:    [[TMP17:%.*]] = insertelement <8 x half> [[TMP10]], half [[TMP16]], i64 0
// CHECK-NEXT:    ret <8 x half> [[TMP17]]
//
__m128h test_mm_maskz_add_sh(__mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_maskz_add_sh(__U, __A, __B);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_add_sh(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[VECEXT_I:%.*]] = extractelement <8 x half> [[TMP2]], i32 0
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[VECEXT1_I:%.*]] = extractelement <8 x half> [[TMP3]], i32 0
// CHECK-NEXT:    [[ADD_I:%.*]] = fadd half [[VECEXT1_I]], [[VECEXT_I]]
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[VECINS_I:%.*]] = insertelement <8 x half> [[TMP4]], half [[ADD_I]], i32 0
// CHECK-NEXT:    store <8 x half> [[VECINS_I]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    ret <8 x half> [[TMP5]]
//
__m128h test_mm_add_sh(__m128h __A, __m128h __B) {
  return _mm_add_sh(__A, __B);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_sub_round_sh(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.sub.sh.round(<8 x half> [[TMP0]], <8 x half> [[TMP1]], <8 x half> [[TMP2]], i8 -1, i32 11)
// CHECK-NEXT:    ret <8 x half> [[TMP3]]
//
__m128h test_mm_sub_round_sh(__m128h __A, __m128h __B) {
  return _mm_sub_round_sh(__A, __B, _MM_FROUND_NO_EXC | _MM_FROUND_TO_ZERO);
}
// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask_sub_round_sh(
// CHECK-SAME: <8 x half> noundef [[__W:%.*]], i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__W]], ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP4:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.sub.sh.round(<8 x half> [[TMP0]], <8 x half> [[TMP1]], <8 x half> [[TMP2]], i8 [[TMP3]], i32 11)
// CHECK-NEXT:    ret <8 x half> [[TMP4]]
//
__m128h test_mm_mask_sub_round_sh(__m128h __W, __mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_mask_sub_round_sh(__W, __U, __A, __B, _MM_FROUND_NO_EXC | _MM_FROUND_TO_ZERO);
}
// CHECK-LABEL: define dso_local <8 x half> @test_mm_maskz_sub_round_sh(
// CHECK-SAME: i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP4:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.sub.sh.round(<8 x half> [[TMP0]], <8 x half> [[TMP1]], <8 x half> [[TMP2]], i8 [[TMP3]], i32 11)
// CHECK-NEXT:    ret <8 x half> [[TMP4]]
//
__m128h test_mm_maskz_sub_round_sh(__mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_maskz_sub_round_sh(__U, __A, __B, _MM_FROUND_NO_EXC | _MM_FROUND_TO_ZERO);
}
// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask_sub_sh(
// CHECK-SAME: <8 x half> noundef [[__W:%.*]], i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I1:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I2:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__W]], ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    store i8 [[TMP1]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP3]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP4]], ptr [[__A_ADDR_I1]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP5]], ptr [[__B_ADDR_I2]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = load <8 x half>, ptr [[__B_ADDR_I2]], align 16
// CHECK-NEXT:    [[VECEXT_I:%.*]] = extractelement <8 x half> [[TMP6]], i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = load <8 x half>, ptr [[__A_ADDR_I1]], align 16
// CHECK-NEXT:    [[VECEXT1_I:%.*]] = extractelement <8 x half> [[TMP7]], i32 0
// CHECK-NEXT:    [[SUB_I:%.*]] = fsub half [[VECEXT1_I]], [[VECEXT_I]]
// CHECK-NEXT:    [[TMP8:%.*]] = load <8 x half>, ptr [[__A_ADDR_I1]], align 16
// CHECK-NEXT:    [[VECINS_I:%.*]] = insertelement <8 x half> [[TMP8]], half [[SUB_I]], i32 0
// CHECK-NEXT:    store <8 x half> [[VECINS_I]], ptr [[__A_ADDR_I1]], align 16
// CHECK-NEXT:    [[TMP9:%.*]] = load <8 x half>, ptr [[__A_ADDR_I1]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP9]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP10:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP11:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP12:%.*]] = load <8 x half>, ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP13:%.*]] = extractelement <8 x half> [[TMP11]], i64 0
// CHECK-NEXT:    [[TMP14:%.*]] = extractelement <8 x half> [[TMP12]], i64 0
// CHECK-NEXT:    [[TMP15:%.*]] = bitcast i8 [[TMP10]] to <8 x i1>
// CHECK-NEXT:    [[TMP16:%.*]] = extractelement <8 x i1> [[TMP15]], i64 0
// CHECK-NEXT:    [[TMP17:%.*]] = select i1 [[TMP16]], half [[TMP13]], half [[TMP14]]
// CHECK-NEXT:    [[TMP18:%.*]] = insertelement <8 x half> [[TMP11]], half [[TMP17]], i64 0
// CHECK-NEXT:    ret <8 x half> [[TMP18]]
//
__m128h test_mm_mask_sub_sh(__m128h __W, __mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_mask_sub_sh(__W, __U, __A, __B);
}
// CHECK-LABEL: define dso_local <8 x half> @test_mm_maskz_sub_sh(
// CHECK-SAME: i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I1:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I2:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store i8 [[TMP0]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP3]], ptr [[__A_ADDR_I1]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP4]], ptr [[__B_ADDR_I2]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = load <8 x half>, ptr [[__B_ADDR_I2]], align 16
// CHECK-NEXT:    [[VECEXT_I:%.*]] = extractelement <8 x half> [[TMP5]], i32 0
// CHECK-NEXT:    [[TMP6:%.*]] = load <8 x half>, ptr [[__A_ADDR_I1]], align 16
// CHECK-NEXT:    [[VECEXT1_I:%.*]] = extractelement <8 x half> [[TMP6]], i32 0
// CHECK-NEXT:    [[SUB_I:%.*]] = fsub half [[VECEXT1_I]], [[VECEXT_I]]
// CHECK-NEXT:    [[TMP7:%.*]] = load <8 x half>, ptr [[__A_ADDR_I1]], align 16
// CHECK-NEXT:    [[VECINS_I:%.*]] = insertelement <8 x half> [[TMP7]], half [[SUB_I]], i32 0
// CHECK-NEXT:    store <8 x half> [[VECINS_I]], ptr [[__A_ADDR_I1]], align 16
// CHECK-NEXT:    [[TMP8:%.*]] = load <8 x half>, ptr [[__A_ADDR_I1]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP8]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP9:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP10:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP11:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP12:%.*]] = extractelement <8 x half> [[TMP10]], i64 0
// CHECK-NEXT:    [[TMP13:%.*]] = extractelement <8 x half> [[TMP11]], i64 0
// CHECK-NEXT:    [[TMP14:%.*]] = bitcast i8 [[TMP9]] to <8 x i1>
// CHECK-NEXT:    [[TMP15:%.*]] = extractelement <8 x i1> [[TMP14]], i64 0
// CHECK-NEXT:    [[TMP16:%.*]] = select i1 [[TMP15]], half [[TMP12]], half [[TMP13]]
// CHECK-NEXT:    [[TMP17:%.*]] = insertelement <8 x half> [[TMP10]], half [[TMP16]], i64 0
// CHECK-NEXT:    ret <8 x half> [[TMP17]]
//
__m128h test_mm_maskz_sub_sh(__mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_maskz_sub_sh(__U, __A, __B);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_sub_sh(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[VECEXT_I:%.*]] = extractelement <8 x half> [[TMP2]], i32 0
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[VECEXT1_I:%.*]] = extractelement <8 x half> [[TMP3]], i32 0
// CHECK-NEXT:    [[SUB_I:%.*]] = fsub half [[VECEXT1_I]], [[VECEXT_I]]
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[VECINS_I:%.*]] = insertelement <8 x half> [[TMP4]], half [[SUB_I]], i32 0
// CHECK-NEXT:    store <8 x half> [[VECINS_I]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    ret <8 x half> [[TMP5]]
//
__m128h test_mm_sub_sh(__m128h __A, __m128h __B) {
  return _mm_sub_sh(__A, __B);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_mul_round_sh(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.mul.sh.round(<8 x half> [[TMP0]], <8 x half> [[TMP1]], <8 x half> [[TMP2]], i8 -1, i32 11)
// CHECK-NEXT:    ret <8 x half> [[TMP3]]
//
__m128h test_mm_mul_round_sh(__m128h __A, __m128h __B) {
  return _mm_mul_round_sh(__A, __B, _MM_FROUND_NO_EXC | _MM_FROUND_TO_ZERO);
}
// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask_mul_round_sh(
// CHECK-SAME: <8 x half> noundef [[__W:%.*]], i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__W]], ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP4:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.mul.sh.round(<8 x half> [[TMP0]], <8 x half> [[TMP1]], <8 x half> [[TMP2]], i8 [[TMP3]], i32 11)
// CHECK-NEXT:    ret <8 x half> [[TMP4]]
//
__m128h test_mm_mask_mul_round_sh(__m128h __W, __mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_mask_mul_round_sh(__W, __U, __A, __B, _MM_FROUND_NO_EXC | _MM_FROUND_TO_ZERO);
}
// CHECK-LABEL: define dso_local <8 x half> @test_mm_maskz_mul_round_sh(
// CHECK-SAME: i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP4:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.mul.sh.round(<8 x half> [[TMP0]], <8 x half> [[TMP1]], <8 x half> [[TMP2]], i8 [[TMP3]], i32 11)
// CHECK-NEXT:    ret <8 x half> [[TMP4]]
//
__m128h test_mm_maskz_mul_round_sh(__mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_maskz_mul_round_sh(__U, __A, __B, _MM_FROUND_NO_EXC | _MM_FROUND_TO_ZERO);
}
// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask_mul_sh(
// CHECK-SAME: <8 x half> noundef [[__W:%.*]], i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I1:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I2:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__W]], ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    store i8 [[TMP1]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP3]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP4]], ptr [[__A_ADDR_I1]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP5]], ptr [[__B_ADDR_I2]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = load <8 x half>, ptr [[__B_ADDR_I2]], align 16
// CHECK-NEXT:    [[VECEXT_I:%.*]] = extractelement <8 x half> [[TMP6]], i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = load <8 x half>, ptr [[__A_ADDR_I1]], align 16
// CHECK-NEXT:    [[VECEXT1_I:%.*]] = extractelement <8 x half> [[TMP7]], i32 0
// CHECK-NEXT:    [[MUL_I:%.*]] = fmul half [[VECEXT1_I]], [[VECEXT_I]]
// CHECK-NEXT:    [[TMP8:%.*]] = load <8 x half>, ptr [[__A_ADDR_I1]], align 16
// CHECK-NEXT:    [[VECINS_I:%.*]] = insertelement <8 x half> [[TMP8]], half [[MUL_I]], i32 0
// CHECK-NEXT:    store <8 x half> [[VECINS_I]], ptr [[__A_ADDR_I1]], align 16
// CHECK-NEXT:    [[TMP9:%.*]] = load <8 x half>, ptr [[__A_ADDR_I1]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP9]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP10:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP11:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP12:%.*]] = load <8 x half>, ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP13:%.*]] = extractelement <8 x half> [[TMP11]], i64 0
// CHECK-NEXT:    [[TMP14:%.*]] = extractelement <8 x half> [[TMP12]], i64 0
// CHECK-NEXT:    [[TMP15:%.*]] = bitcast i8 [[TMP10]] to <8 x i1>
// CHECK-NEXT:    [[TMP16:%.*]] = extractelement <8 x i1> [[TMP15]], i64 0
// CHECK-NEXT:    [[TMP17:%.*]] = select i1 [[TMP16]], half [[TMP13]], half [[TMP14]]
// CHECK-NEXT:    [[TMP18:%.*]] = insertelement <8 x half> [[TMP11]], half [[TMP17]], i64 0
// CHECK-NEXT:    ret <8 x half> [[TMP18]]
//
__m128h test_mm_mask_mul_sh(__m128h __W, __mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_mask_mul_sh(__W, __U, __A, __B);
}
// CHECK-LABEL: define dso_local <8 x half> @test_mm_maskz_mul_sh(
// CHECK-SAME: i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I1:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I2:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store i8 [[TMP0]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP3]], ptr [[__A_ADDR_I1]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP4]], ptr [[__B_ADDR_I2]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = load <8 x half>, ptr [[__B_ADDR_I2]], align 16
// CHECK-NEXT:    [[VECEXT_I:%.*]] = extractelement <8 x half> [[TMP5]], i32 0
// CHECK-NEXT:    [[TMP6:%.*]] = load <8 x half>, ptr [[__A_ADDR_I1]], align 16
// CHECK-NEXT:    [[VECEXT1_I:%.*]] = extractelement <8 x half> [[TMP6]], i32 0
// CHECK-NEXT:    [[MUL_I:%.*]] = fmul half [[VECEXT1_I]], [[VECEXT_I]]
// CHECK-NEXT:    [[TMP7:%.*]] = load <8 x half>, ptr [[__A_ADDR_I1]], align 16
// CHECK-NEXT:    [[VECINS_I:%.*]] = insertelement <8 x half> [[TMP7]], half [[MUL_I]], i32 0
// CHECK-NEXT:    store <8 x half> [[VECINS_I]], ptr [[__A_ADDR_I1]], align 16
// CHECK-NEXT:    [[TMP8:%.*]] = load <8 x half>, ptr [[__A_ADDR_I1]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP8]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP9:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP10:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP11:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP12:%.*]] = extractelement <8 x half> [[TMP10]], i64 0
// CHECK-NEXT:    [[TMP13:%.*]] = extractelement <8 x half> [[TMP11]], i64 0
// CHECK-NEXT:    [[TMP14:%.*]] = bitcast i8 [[TMP9]] to <8 x i1>
// CHECK-NEXT:    [[TMP15:%.*]] = extractelement <8 x i1> [[TMP14]], i64 0
// CHECK-NEXT:    [[TMP16:%.*]] = select i1 [[TMP15]], half [[TMP12]], half [[TMP13]]
// CHECK-NEXT:    [[TMP17:%.*]] = insertelement <8 x half> [[TMP10]], half [[TMP16]], i64 0
// CHECK-NEXT:    ret <8 x half> [[TMP17]]
//
__m128h test_mm_maskz_mul_sh(__mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_maskz_mul_sh(__U, __A, __B);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_mul_sh(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[VECEXT_I:%.*]] = extractelement <8 x half> [[TMP2]], i32 0
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[VECEXT1_I:%.*]] = extractelement <8 x half> [[TMP3]], i32 0
// CHECK-NEXT:    [[MUL_I:%.*]] = fmul half [[VECEXT1_I]], [[VECEXT_I]]
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[VECINS_I:%.*]] = insertelement <8 x half> [[TMP4]], half [[MUL_I]], i32 0
// CHECK-NEXT:    store <8 x half> [[VECINS_I]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    ret <8 x half> [[TMP5]]
//
__m128h test_mm_mul_sh(__m128h __A, __m128h __B) {
  return _mm_mul_sh(__A, __B);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_div_round_sh(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.div.sh.round(<8 x half> [[TMP0]], <8 x half> [[TMP1]], <8 x half> [[TMP2]], i8 -1, i32 11)
// CHECK-NEXT:    ret <8 x half> [[TMP3]]
//
__m128h test_mm_div_round_sh(__m128h __A, __m128h __B) {
  return _mm_div_round_sh(__A, __B, _MM_FROUND_NO_EXC | _MM_FROUND_TO_ZERO);
}
// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask_div_round_sh(
// CHECK-SAME: <8 x half> noundef [[__W:%.*]], i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__W]], ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP4:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.div.sh.round(<8 x half> [[TMP0]], <8 x half> [[TMP1]], <8 x half> [[TMP2]], i8 [[TMP3]], i32 11)
// CHECK-NEXT:    ret <8 x half> [[TMP4]]
//
__m128h test_mm_mask_div_round_sh(__m128h __W, __mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_mask_div_round_sh(__W, __U, __A, __B, _MM_FROUND_NO_EXC | _MM_FROUND_TO_ZERO);
}
// CHECK-LABEL: define dso_local <8 x half> @test_mm_maskz_div_round_sh(
// CHECK-SAME: i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP4:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.div.sh.round(<8 x half> [[TMP0]], <8 x half> [[TMP1]], <8 x half> [[TMP2]], i8 [[TMP3]], i32 11)
// CHECK-NEXT:    ret <8 x half> [[TMP4]]
//
__m128h test_mm_maskz_div_round_sh(__mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_maskz_div_round_sh(__U, __A, __B, _MM_FROUND_NO_EXC | _MM_FROUND_TO_ZERO);
}
// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask_div_sh(
// CHECK-SAME: <8 x half> noundef [[__W:%.*]], i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I1:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I2:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__W]], ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    store i8 [[TMP1]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP3]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP4]], ptr [[__A_ADDR_I1]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP5]], ptr [[__B_ADDR_I2]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = load <8 x half>, ptr [[__B_ADDR_I2]], align 16
// CHECK-NEXT:    [[VECEXT_I:%.*]] = extractelement <8 x half> [[TMP6]], i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = load <8 x half>, ptr [[__A_ADDR_I1]], align 16
// CHECK-NEXT:    [[VECEXT1_I:%.*]] = extractelement <8 x half> [[TMP7]], i32 0
// CHECK-NEXT:    [[DIV_I:%.*]] = fdiv half [[VECEXT1_I]], [[VECEXT_I]]
// CHECK-NEXT:    [[TMP8:%.*]] = load <8 x half>, ptr [[__A_ADDR_I1]], align 16
// CHECK-NEXT:    [[VECINS_I:%.*]] = insertelement <8 x half> [[TMP8]], half [[DIV_I]], i32 0
// CHECK-NEXT:    store <8 x half> [[VECINS_I]], ptr [[__A_ADDR_I1]], align 16
// CHECK-NEXT:    [[TMP9:%.*]] = load <8 x half>, ptr [[__A_ADDR_I1]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP9]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP10:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP11:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP12:%.*]] = load <8 x half>, ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP13:%.*]] = extractelement <8 x half> [[TMP11]], i64 0
// CHECK-NEXT:    [[TMP14:%.*]] = extractelement <8 x half> [[TMP12]], i64 0
// CHECK-NEXT:    [[TMP15:%.*]] = bitcast i8 [[TMP10]] to <8 x i1>
// CHECK-NEXT:    [[TMP16:%.*]] = extractelement <8 x i1> [[TMP15]], i64 0
// CHECK-NEXT:    [[TMP17:%.*]] = select i1 [[TMP16]], half [[TMP13]], half [[TMP14]]
// CHECK-NEXT:    [[TMP18:%.*]] = insertelement <8 x half> [[TMP11]], half [[TMP17]], i64 0
// CHECK-NEXT:    ret <8 x half> [[TMP18]]
//
__m128h test_mm_mask_div_sh(__m128h __W, __mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_mask_div_sh(__W, __U, __A, __B);
}
// CHECK-LABEL: define dso_local <8 x half> @test_mm_maskz_div_sh(
// CHECK-SAME: i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I1:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I2:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store i8 [[TMP0]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP3]], ptr [[__A_ADDR_I1]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP4]], ptr [[__B_ADDR_I2]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = load <8 x half>, ptr [[__B_ADDR_I2]], align 16
// CHECK-NEXT:    [[VECEXT_I:%.*]] = extractelement <8 x half> [[TMP5]], i32 0
// CHECK-NEXT:    [[TMP6:%.*]] = load <8 x half>, ptr [[__A_ADDR_I1]], align 16
// CHECK-NEXT:    [[VECEXT1_I:%.*]] = extractelement <8 x half> [[TMP6]], i32 0
// CHECK-NEXT:    [[DIV_I:%.*]] = fdiv half [[VECEXT1_I]], [[VECEXT_I]]
// CHECK-NEXT:    [[TMP7:%.*]] = load <8 x half>, ptr [[__A_ADDR_I1]], align 16
// CHECK-NEXT:    [[VECINS_I:%.*]] = insertelement <8 x half> [[TMP7]], half [[DIV_I]], i32 0
// CHECK-NEXT:    store <8 x half> [[VECINS_I]], ptr [[__A_ADDR_I1]], align 16
// CHECK-NEXT:    [[TMP8:%.*]] = load <8 x half>, ptr [[__A_ADDR_I1]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP8]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP9:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP10:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP11:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP12:%.*]] = extractelement <8 x half> [[TMP10]], i64 0
// CHECK-NEXT:    [[TMP13:%.*]] = extractelement <8 x half> [[TMP11]], i64 0
// CHECK-NEXT:    [[TMP14:%.*]] = bitcast i8 [[TMP9]] to <8 x i1>
// CHECK-NEXT:    [[TMP15:%.*]] = extractelement <8 x i1> [[TMP14]], i64 0
// CHECK-NEXT:    [[TMP16:%.*]] = select i1 [[TMP15]], half [[TMP12]], half [[TMP13]]
// CHECK-NEXT:    [[TMP17:%.*]] = insertelement <8 x half> [[TMP10]], half [[TMP16]], i64 0
// CHECK-NEXT:    ret <8 x half> [[TMP17]]
//
__m128h test_mm_maskz_div_sh(__mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_maskz_div_sh(__U, __A, __B);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_div_sh(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[VECEXT_I:%.*]] = extractelement <8 x half> [[TMP2]], i32 0
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[VECEXT1_I:%.*]] = extractelement <8 x half> [[TMP3]], i32 0
// CHECK-NEXT:    [[DIV_I:%.*]] = fdiv half [[VECEXT1_I]], [[VECEXT_I]]
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[VECINS_I:%.*]] = insertelement <8 x half> [[TMP4]], half [[DIV_I]], i32 0
// CHECK-NEXT:    store <8 x half> [[VECINS_I]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    ret <8 x half> [[TMP5]]
//
__m128h test_mm_div_sh(__m128h __A, __m128h __B) {
  return _mm_div_sh(__A, __B);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_min_round_sh(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.min.sh.round(<8 x half> [[TMP0]], <8 x half> [[TMP1]], <8 x half> [[TMP2]], i8 -1, i32 8)
// CHECK-NEXT:    ret <8 x half> [[TMP3]]
//
__m128h test_mm_min_round_sh(__m128h __A, __m128h __B) {
  return _mm_min_round_sh(__A, __B, 0x08);
}
// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask_min_round_sh(
// CHECK-SAME: <8 x half> noundef [[__W:%.*]], i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__W]], ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP4:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.min.sh.round(<8 x half> [[TMP0]], <8 x half> [[TMP1]], <8 x half> [[TMP2]], i8 [[TMP3]], i32 8)
// CHECK-NEXT:    ret <8 x half> [[TMP4]]
//
__m128h test_mm_mask_min_round_sh(__m128h __W, __mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_mask_min_round_sh(__W, __U, __A, __B, 0x08);
}
// CHECK-LABEL: define dso_local <8 x half> @test_mm_maskz_min_round_sh(
// CHECK-SAME: i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP4:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.min.sh.round(<8 x half> [[TMP0]], <8 x half> [[TMP1]], <8 x half> [[TMP2]], i8 [[TMP3]], i32 8)
// CHECK-NEXT:    ret <8 x half> [[TMP4]]
//
__m128h test_mm_maskz_min_round_sh(__mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_maskz_min_round_sh(__U, __A, __B, 0x08);
}
// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask_min_sh(
// CHECK-SAME: <8 x half> noundef [[__W:%.*]], i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__W]], ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    store i8 [[TMP1]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP3]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = load <8 x half>, ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP7:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP8:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.min.sh.round(<8 x half> [[TMP4]], <8 x half> [[TMP5]], <8 x half> [[TMP6]], i8 [[TMP7]], i32 4)
// CHECK-NEXT:    ret <8 x half> [[TMP8]]
//
__m128h test_mm_mask_min_sh(__m128h __W, __mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_mask_min_sh(__W, __U, __A, __B);
}
// CHECK-LABEL: define dso_local <8 x half> @test_mm_maskz_min_sh(
// CHECK-SAME: i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store i8 [[TMP0]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP7:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.min.sh.round(<8 x half> [[TMP3]], <8 x half> [[TMP4]], <8 x half> [[TMP5]], i8 [[TMP6]], i32 4)
// CHECK-NEXT:    ret <8 x half> [[TMP7]]
//
__m128h test_mm_maskz_min_sh(__mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_maskz_min_sh(__U, __A, __B);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_min_sh(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.min.sh.round(<8 x half> [[TMP2]], <8 x half> [[TMP3]], <8 x half> [[TMP4]], i8 -1, i32 4)
// CHECK-NEXT:    ret <8 x half> [[TMP5]]
//
__m128h test_mm_min_sh(__m128h __A, __m128h __B) {
  return _mm_min_sh(__A, __B);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_max_round_sh(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.max.sh.round(<8 x half> [[TMP0]], <8 x half> [[TMP1]], <8 x half> [[TMP2]], i8 -1, i32 8)
// CHECK-NEXT:    ret <8 x half> [[TMP3]]
//
__m128h test_mm_max_round_sh(__m128h __A, __m128h __B) {
  return _mm_max_round_sh(__A, __B, 0x08);
}
// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask_max_round_sh(
// CHECK-SAME: <8 x half> noundef [[__W:%.*]], i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__W]], ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP4:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.max.sh.round(<8 x half> [[TMP0]], <8 x half> [[TMP1]], <8 x half> [[TMP2]], i8 [[TMP3]], i32 8)
// CHECK-NEXT:    ret <8 x half> [[TMP4]]
//
__m128h test_mm_mask_max_round_sh(__m128h __W, __mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_mask_max_round_sh(__W, __U, __A, __B, 0x08);
}
// CHECK-LABEL: define dso_local <8 x half> @test_mm_maskz_max_round_sh(
// CHECK-SAME: i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP4:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.max.sh.round(<8 x half> [[TMP0]], <8 x half> [[TMP1]], <8 x half> [[TMP2]], i8 [[TMP3]], i32 8)
// CHECK-NEXT:    ret <8 x half> [[TMP4]]
//
__m128h test_mm_maskz_max_round_sh(__mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_maskz_max_round_sh(__U, __A, __B, 0x08);
}
// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask_max_sh(
// CHECK-SAME: <8 x half> noundef [[__W:%.*]], i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__W]], ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    store i8 [[TMP1]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP3]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = load <8 x half>, ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP7:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP8:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.max.sh.round(<8 x half> [[TMP4]], <8 x half> [[TMP5]], <8 x half> [[TMP6]], i8 [[TMP7]], i32 4)
// CHECK-NEXT:    ret <8 x half> [[TMP8]]
//
__m128h test_mm_mask_max_sh(__m128h __W, __mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_mask_max_sh(__W, __U, __A, __B);
}
// CHECK-LABEL: define dso_local <8 x half> @test_mm_maskz_max_sh(
// CHECK-SAME: i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store i8 [[TMP0]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP7:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.max.sh.round(<8 x half> [[TMP3]], <8 x half> [[TMP4]], <8 x half> [[TMP5]], i8 [[TMP6]], i32 4)
// CHECK-NEXT:    ret <8 x half> [[TMP7]]
//
__m128h test_mm_maskz_max_sh(__mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_maskz_max_sh(__U, __A, __B);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_max_sh(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.max.sh.round(<8 x half> [[TMP2]], <8 x half> [[TMP3]], <8 x half> [[TMP4]], i8 -1, i32 4)
// CHECK-NEXT:    ret <8 x half> [[TMP5]]
//
__m128h test_mm_max_sh(__m128h __A, __m128h __B) {
  return _mm_max_sh(__A, __B);
}
// CHECK-LABEL: define dso_local i32 @test_mm512_cmp_round_ph_mask(
// CHECK-SAME: <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = fcmp oeq <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x i1> [[TMP2]] to i32
// CHECK-NEXT:    ret i32 [[TMP3]]
//
__mmask32 test_mm512_cmp_round_ph_mask(__m512h a, __m512h b) {
  return _mm512_cmp_round_ph_mask(a, b, 0, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_mask_cmp_round_ph_mask(
// CHECK-SAME: i32 noundef [[M:%.*]], <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[M_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[M]], ptr [[M_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[M_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = fcmp oeq <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast i32 [[TMP2]] to <32 x i1>
// CHECK-NEXT:    [[TMP5:%.*]] = and <32 x i1> [[TMP3]], [[TMP4]]
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <32 x i1> [[TMP5]] to i32
// CHECK-NEXT:    ret i32 [[TMP6]]
//
__mmask32 test_mm512_mask_cmp_round_ph_mask(__mmask32 m, __m512h a, __m512h b) {
  return _mm512_mask_cmp_round_ph_mask(m, a, b, 0, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_cmp_ph_mask_eq_oq(
// CHECK-SAME: <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = fcmp oeq <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x i1> [[TMP2]] to i32
// CHECK-NEXT:    ret i32 [[TMP3]]
//
__mmask32 test_mm512_cmp_ph_mask_eq_oq(__m512h a, __m512h b) {
  return _mm512_cmp_ph_mask(a, b, _CMP_EQ_OQ);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_cmp_ph_mask_lt_os(
// CHECK-SAME: <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = fcmp olt <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x i1> [[TMP2]] to i32
// CHECK-NEXT:    ret i32 [[TMP3]]
//
__mmask32 test_mm512_cmp_ph_mask_lt_os(__m512h a, __m512h b) {
  return _mm512_cmp_ph_mask(a, b, _CMP_LT_OS);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_cmp_ph_mask_le_os(
// CHECK-SAME: <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = fcmp ole <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x i1> [[TMP2]] to i32
// CHECK-NEXT:    ret i32 [[TMP3]]
//
__mmask32 test_mm512_cmp_ph_mask_le_os(__m512h a, __m512h b) {
  return _mm512_cmp_ph_mask(a, b, _CMP_LE_OS);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_cmp_ph_mask_unord_q(
// CHECK-SAME: <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = fcmp uno <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x i1> [[TMP2]] to i32
// CHECK-NEXT:    ret i32 [[TMP3]]
//
__mmask32 test_mm512_cmp_ph_mask_unord_q(__m512h a, __m512h b) {
  return _mm512_cmp_ph_mask(a, b, _CMP_UNORD_Q);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_cmp_ph_mask_neq_uq(
// CHECK-SAME: <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = fcmp une <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x i1> [[TMP2]] to i32
// CHECK-NEXT:    ret i32 [[TMP3]]
//
__mmask32 test_mm512_cmp_ph_mask_neq_uq(__m512h a, __m512h b) {
  return _mm512_cmp_ph_mask(a, b, _CMP_NEQ_UQ);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_cmp_ph_mask_nlt_us(
// CHECK-SAME: <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = fcmp uge <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x i1> [[TMP2]] to i32
// CHECK-NEXT:    ret i32 [[TMP3]]
//
__mmask32 test_mm512_cmp_ph_mask_nlt_us(__m512h a, __m512h b) {
  return _mm512_cmp_ph_mask(a, b, _CMP_NLT_US);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_cmp_ph_mask_nle_us(
// CHECK-SAME: <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = fcmp ugt <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x i1> [[TMP2]] to i32
// CHECK-NEXT:    ret i32 [[TMP3]]
//
__mmask32 test_mm512_cmp_ph_mask_nle_us(__m512h a, __m512h b) {
  return _mm512_cmp_ph_mask(a, b, _CMP_NLE_US);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_cmp_ph_mask_ord_q(
// CHECK-SAME: <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = fcmp ord <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x i1> [[TMP2]] to i32
// CHECK-NEXT:    ret i32 [[TMP3]]
//
__mmask32 test_mm512_cmp_ph_mask_ord_q(__m512h a, __m512h b) {
  return _mm512_cmp_ph_mask(a, b, _CMP_ORD_Q);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_cmp_ph_mask_eq_uq(
// CHECK-SAME: <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = fcmp ueq <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x i1> [[TMP2]] to i32
// CHECK-NEXT:    ret i32 [[TMP3]]
//
__mmask32 test_mm512_cmp_ph_mask_eq_uq(__m512h a, __m512h b) {
  return _mm512_cmp_ph_mask(a, b, _CMP_EQ_UQ);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_cmp_ph_mask_nge_us(
// CHECK-SAME: <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = fcmp ult <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x i1> [[TMP2]] to i32
// CHECK-NEXT:    ret i32 [[TMP3]]
//
__mmask32 test_mm512_cmp_ph_mask_nge_us(__m512h a, __m512h b) {
  return _mm512_cmp_ph_mask(a, b, _CMP_NGE_US);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_cmp_ph_mask_ngt_us(
// CHECK-SAME: <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = fcmp ule <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x i1> [[TMP2]] to i32
// CHECK-NEXT:    ret i32 [[TMP3]]
//
__mmask32 test_mm512_cmp_ph_mask_ngt_us(__m512h a, __m512h b) {
  return _mm512_cmp_ph_mask(a, b, _CMP_NGT_US);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_cmp_ph_mask_false_oq(
// CHECK-SAME: <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = fcmp false <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x i1> [[TMP2]] to i32
// CHECK-NEXT:    ret i32 [[TMP3]]
//
__mmask32 test_mm512_cmp_ph_mask_false_oq(__m512h a, __m512h b) {
  return _mm512_cmp_ph_mask(a, b, _CMP_FALSE_OQ);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_cmp_ph_mask_neq_oq(
// CHECK-SAME: <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = fcmp one <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x i1> [[TMP2]] to i32
// CHECK-NEXT:    ret i32 [[TMP3]]
//
__mmask32 test_mm512_cmp_ph_mask_neq_oq(__m512h a, __m512h b) {
  return _mm512_cmp_ph_mask(a, b, _CMP_NEQ_OQ);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_cmp_ph_mask_ge_os(
// CHECK-SAME: <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = fcmp oge <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x i1> [[TMP2]] to i32
// CHECK-NEXT:    ret i32 [[TMP3]]
//
__mmask32 test_mm512_cmp_ph_mask_ge_os(__m512h a, __m512h b) {
  return _mm512_cmp_ph_mask(a, b, _CMP_GE_OS);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_cmp_ph_mask_gt_os(
// CHECK-SAME: <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = fcmp ogt <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x i1> [[TMP2]] to i32
// CHECK-NEXT:    ret i32 [[TMP3]]
//
__mmask32 test_mm512_cmp_ph_mask_gt_os(__m512h a, __m512h b) {
  return _mm512_cmp_ph_mask(a, b, _CMP_GT_OS);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_cmp_ph_mask_true_uq(
// CHECK-SAME: <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = fcmp true <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x i1> [[TMP2]] to i32
// CHECK-NEXT:    ret i32 [[TMP3]]
//
__mmask32 test_mm512_cmp_ph_mask_true_uq(__m512h a, __m512h b) {
  return _mm512_cmp_ph_mask(a, b, _CMP_TRUE_UQ);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_cmp_ph_mask_eq_os(
// CHECK-SAME: <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = fcmp oeq <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x i1> [[TMP2]] to i32
// CHECK-NEXT:    ret i32 [[TMP3]]
//
__mmask32 test_mm512_cmp_ph_mask_eq_os(__m512h a, __m512h b) {
  return _mm512_cmp_ph_mask(a, b, _CMP_EQ_OS);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_cmp_ph_mask_lt_oq(
// CHECK-SAME: <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = fcmp olt <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x i1> [[TMP2]] to i32
// CHECK-NEXT:    ret i32 [[TMP3]]
//
__mmask32 test_mm512_cmp_ph_mask_lt_oq(__m512h a, __m512h b) {
  return _mm512_cmp_ph_mask(a, b, _CMP_LT_OQ);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_cmp_ph_mask_le_oq(
// CHECK-SAME: <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = fcmp ole <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x i1> [[TMP2]] to i32
// CHECK-NEXT:    ret i32 [[TMP3]]
//
__mmask32 test_mm512_cmp_ph_mask_le_oq(__m512h a, __m512h b) {
  return _mm512_cmp_ph_mask(a, b, _CMP_LE_OQ);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_cmp_ph_mask_unord_s(
// CHECK-SAME: <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = fcmp uno <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x i1> [[TMP2]] to i32
// CHECK-NEXT:    ret i32 [[TMP3]]
//
__mmask32 test_mm512_cmp_ph_mask_unord_s(__m512h a, __m512h b) {
  return _mm512_cmp_ph_mask(a, b, _CMP_UNORD_S);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_cmp_ph_mask_neq_us(
// CHECK-SAME: <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = fcmp une <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x i1> [[TMP2]] to i32
// CHECK-NEXT:    ret i32 [[TMP3]]
//
__mmask32 test_mm512_cmp_ph_mask_neq_us(__m512h a, __m512h b) {
  return _mm512_cmp_ph_mask(a, b, _CMP_NEQ_US);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_cmp_ph_mask_nlt_uq(
// CHECK-SAME: <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = fcmp uge <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x i1> [[TMP2]] to i32
// CHECK-NEXT:    ret i32 [[TMP3]]
//
__mmask32 test_mm512_cmp_ph_mask_nlt_uq(__m512h a, __m512h b) {
  return _mm512_cmp_ph_mask(a, b, _CMP_NLT_UQ);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_cmp_ph_mask_nle_uq(
// CHECK-SAME: <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = fcmp ugt <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x i1> [[TMP2]] to i32
// CHECK-NEXT:    ret i32 [[TMP3]]
//
__mmask32 test_mm512_cmp_ph_mask_nle_uq(__m512h a, __m512h b) {
  return _mm512_cmp_ph_mask(a, b, _CMP_NLE_UQ);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_cmp_ph_mask_ord_s(
// CHECK-SAME: <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = fcmp ord <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x i1> [[TMP2]] to i32
// CHECK-NEXT:    ret i32 [[TMP3]]
//
__mmask32 test_mm512_cmp_ph_mask_ord_s(__m512h a, __m512h b) {
  return _mm512_cmp_ph_mask(a, b, _CMP_ORD_S);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_cmp_ph_mask_eq_us(
// CHECK-SAME: <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = fcmp ueq <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x i1> [[TMP2]] to i32
// CHECK-NEXT:    ret i32 [[TMP3]]
//
__mmask32 test_mm512_cmp_ph_mask_eq_us(__m512h a, __m512h b) {
  return _mm512_cmp_ph_mask(a, b, _CMP_EQ_US);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_cmp_ph_mask_nge_uq(
// CHECK-SAME: <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = fcmp ult <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x i1> [[TMP2]] to i32
// CHECK-NEXT:    ret i32 [[TMP3]]
//
__mmask32 test_mm512_cmp_ph_mask_nge_uq(__m512h a, __m512h b) {
  return _mm512_cmp_ph_mask(a, b, _CMP_NGE_UQ);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_cmp_ph_mask_ngt_uq(
// CHECK-SAME: <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = fcmp ule <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x i1> [[TMP2]] to i32
// CHECK-NEXT:    ret i32 [[TMP3]]
//
__mmask32 test_mm512_cmp_ph_mask_ngt_uq(__m512h a, __m512h b) {
  return _mm512_cmp_ph_mask(a, b, _CMP_NGT_UQ);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_cmp_ph_mask_false_os(
// CHECK-SAME: <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = fcmp false <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x i1> [[TMP2]] to i32
// CHECK-NEXT:    ret i32 [[TMP3]]
//
__mmask32 test_mm512_cmp_ph_mask_false_os(__m512h a, __m512h b) {
  return _mm512_cmp_ph_mask(a, b, _CMP_FALSE_OS);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_cmp_ph_mask_neq_os(
// CHECK-SAME: <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = fcmp one <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x i1> [[TMP2]] to i32
// CHECK-NEXT:    ret i32 [[TMP3]]
//
__mmask32 test_mm512_cmp_ph_mask_neq_os(__m512h a, __m512h b) {
  return _mm512_cmp_ph_mask(a, b, _CMP_NEQ_OS);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_cmp_ph_mask_ge_oq(
// CHECK-SAME: <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = fcmp oge <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x i1> [[TMP2]] to i32
// CHECK-NEXT:    ret i32 [[TMP3]]
//
__mmask32 test_mm512_cmp_ph_mask_ge_oq(__m512h a, __m512h b) {
  return _mm512_cmp_ph_mask(a, b, _CMP_GE_OQ);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_cmp_ph_mask_gt_oq(
// CHECK-SAME: <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = fcmp ogt <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x i1> [[TMP2]] to i32
// CHECK-NEXT:    ret i32 [[TMP3]]
//
__mmask32 test_mm512_cmp_ph_mask_gt_oq(__m512h a, __m512h b) {
  return _mm512_cmp_ph_mask(a, b, _CMP_GT_OQ);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_cmp_ph_mask_true_us(
// CHECK-SAME: <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = fcmp true <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x i1> [[TMP2]] to i32
// CHECK-NEXT:    ret i32 [[TMP3]]
//
__mmask32 test_mm512_cmp_ph_mask_true_us(__m512h a, __m512h b) {
  return _mm512_cmp_ph_mask(a, b, _CMP_TRUE_US);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_mask_cmp_ph_mask_eq_oq(
// CHECK-SAME: i32 noundef [[M:%.*]], <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[M_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[M]], ptr [[M_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[M_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = fcmp oeq <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast i32 [[TMP2]] to <32 x i1>
// CHECK-NEXT:    [[TMP5:%.*]] = and <32 x i1> [[TMP3]], [[TMP4]]
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <32 x i1> [[TMP5]] to i32
// CHECK-NEXT:    ret i32 [[TMP6]]
//
__mmask32 test_mm512_mask_cmp_ph_mask_eq_oq(__mmask32 m, __m512h a, __m512h b) {
  return _mm512_mask_cmp_ph_mask(m, a, b, _CMP_EQ_OQ);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_mask_cmp_ph_mask_lt_os(
// CHECK-SAME: i32 noundef [[M:%.*]], <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[M_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[M]], ptr [[M_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[M_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = fcmp olt <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast i32 [[TMP2]] to <32 x i1>
// CHECK-NEXT:    [[TMP5:%.*]] = and <32 x i1> [[TMP3]], [[TMP4]]
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <32 x i1> [[TMP5]] to i32
// CHECK-NEXT:    ret i32 [[TMP6]]
//
__mmask32 test_mm512_mask_cmp_ph_mask_lt_os(__mmask32 m, __m512h a, __m512h b) {
  return _mm512_mask_cmp_ph_mask(m, a, b, _CMP_LT_OS);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_mask_cmp_ph_mask_le_os(
// CHECK-SAME: i32 noundef [[M:%.*]], <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[M_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[M]], ptr [[M_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[M_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = fcmp ole <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast i32 [[TMP2]] to <32 x i1>
// CHECK-NEXT:    [[TMP5:%.*]] = and <32 x i1> [[TMP3]], [[TMP4]]
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <32 x i1> [[TMP5]] to i32
// CHECK-NEXT:    ret i32 [[TMP6]]
//
__mmask32 test_mm512_mask_cmp_ph_mask_le_os(__mmask32 m, __m512h a, __m512h b) {
  return _mm512_mask_cmp_ph_mask(m, a, b, _CMP_LE_OS);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_mask_cmp_ph_mask_unord_q(
// CHECK-SAME: i32 noundef [[M:%.*]], <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[M_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[M]], ptr [[M_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[M_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = fcmp uno <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast i32 [[TMP2]] to <32 x i1>
// CHECK-NEXT:    [[TMP5:%.*]] = and <32 x i1> [[TMP3]], [[TMP4]]
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <32 x i1> [[TMP5]] to i32
// CHECK-NEXT:    ret i32 [[TMP6]]
//
__mmask32 test_mm512_mask_cmp_ph_mask_unord_q(__mmask32 m, __m512h a, __m512h b) {
  return _mm512_mask_cmp_ph_mask(m, a, b, _CMP_UNORD_Q);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_mask_cmp_ph_mask_neq_uq(
// CHECK-SAME: i32 noundef [[M:%.*]], <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[M_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[M]], ptr [[M_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[M_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = fcmp une <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast i32 [[TMP2]] to <32 x i1>
// CHECK-NEXT:    [[TMP5:%.*]] = and <32 x i1> [[TMP3]], [[TMP4]]
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <32 x i1> [[TMP5]] to i32
// CHECK-NEXT:    ret i32 [[TMP6]]
//
__mmask32 test_mm512_mask_cmp_ph_mask_neq_uq(__mmask32 m, __m512h a, __m512h b) {
  return _mm512_mask_cmp_ph_mask(m, a, b, _CMP_NEQ_UQ);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_mask_cmp_ph_mask_nlt_us(
// CHECK-SAME: i32 noundef [[M:%.*]], <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[M_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[M]], ptr [[M_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[M_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = fcmp uge <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast i32 [[TMP2]] to <32 x i1>
// CHECK-NEXT:    [[TMP5:%.*]] = and <32 x i1> [[TMP3]], [[TMP4]]
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <32 x i1> [[TMP5]] to i32
// CHECK-NEXT:    ret i32 [[TMP6]]
//
__mmask32 test_mm512_mask_cmp_ph_mask_nlt_us(__mmask32 m, __m512h a, __m512h b) {
  return _mm512_mask_cmp_ph_mask(m, a, b, _CMP_NLT_US);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_mask_cmp_ph_mask_nle_us(
// CHECK-SAME: i32 noundef [[M:%.*]], <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[M_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[M]], ptr [[M_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[M_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = fcmp ugt <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast i32 [[TMP2]] to <32 x i1>
// CHECK-NEXT:    [[TMP5:%.*]] = and <32 x i1> [[TMP3]], [[TMP4]]
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <32 x i1> [[TMP5]] to i32
// CHECK-NEXT:    ret i32 [[TMP6]]
//
__mmask32 test_mm512_mask_cmp_ph_mask_nle_us(__mmask32 m, __m512h a, __m512h b) {
  return _mm512_mask_cmp_ph_mask(m, a, b, _CMP_NLE_US);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_mask_cmp_ph_mask_ord_q(
// CHECK-SAME: i32 noundef [[M:%.*]], <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[M_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[M]], ptr [[M_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[M_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = fcmp ord <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast i32 [[TMP2]] to <32 x i1>
// CHECK-NEXT:    [[TMP5:%.*]] = and <32 x i1> [[TMP3]], [[TMP4]]
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <32 x i1> [[TMP5]] to i32
// CHECK-NEXT:    ret i32 [[TMP6]]
//
__mmask32 test_mm512_mask_cmp_ph_mask_ord_q(__mmask32 m, __m512h a, __m512h b) {
  return _mm512_mask_cmp_ph_mask(m, a, b, _CMP_ORD_Q);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_mask_cmp_ph_mask_eq_uq(
// CHECK-SAME: i32 noundef [[M:%.*]], <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[M_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[M]], ptr [[M_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[M_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = fcmp ueq <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast i32 [[TMP2]] to <32 x i1>
// CHECK-NEXT:    [[TMP5:%.*]] = and <32 x i1> [[TMP3]], [[TMP4]]
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <32 x i1> [[TMP5]] to i32
// CHECK-NEXT:    ret i32 [[TMP6]]
//
__mmask32 test_mm512_mask_cmp_ph_mask_eq_uq(__mmask32 m, __m512h a, __m512h b) {
  return _mm512_mask_cmp_ph_mask(m, a, b, _CMP_EQ_UQ);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_mask_cmp_ph_mask_nge_us(
// CHECK-SAME: i32 noundef [[M:%.*]], <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[M_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[M]], ptr [[M_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[M_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = fcmp ult <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast i32 [[TMP2]] to <32 x i1>
// CHECK-NEXT:    [[TMP5:%.*]] = and <32 x i1> [[TMP3]], [[TMP4]]
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <32 x i1> [[TMP5]] to i32
// CHECK-NEXT:    ret i32 [[TMP6]]
//
__mmask32 test_mm512_mask_cmp_ph_mask_nge_us(__mmask32 m, __m512h a, __m512h b) {
  return _mm512_mask_cmp_ph_mask(m, a, b, _CMP_NGE_US);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_mask_cmp_ph_mask_ngt_us(
// CHECK-SAME: i32 noundef [[M:%.*]], <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[M_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[M]], ptr [[M_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[M_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = fcmp ule <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast i32 [[TMP2]] to <32 x i1>
// CHECK-NEXT:    [[TMP5:%.*]] = and <32 x i1> [[TMP3]], [[TMP4]]
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <32 x i1> [[TMP5]] to i32
// CHECK-NEXT:    ret i32 [[TMP6]]
//
__mmask32 test_mm512_mask_cmp_ph_mask_ngt_us(__mmask32 m, __m512h a, __m512h b) {
  return _mm512_mask_cmp_ph_mask(m, a, b, _CMP_NGT_US);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_mask_cmp_ph_mask_false_oq(
// CHECK-SAME: i32 noundef [[M:%.*]], <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[M_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[M]], ptr [[M_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[M_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = fcmp false <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast i32 [[TMP2]] to <32 x i1>
// CHECK-NEXT:    [[TMP5:%.*]] = and <32 x i1> [[TMP3]], [[TMP4]]
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <32 x i1> [[TMP5]] to i32
// CHECK-NEXT:    ret i32 [[TMP6]]
//
__mmask32 test_mm512_mask_cmp_ph_mask_false_oq(__mmask32 m, __m512h a, __m512h b) {
  return _mm512_mask_cmp_ph_mask(m, a, b, _CMP_FALSE_OQ);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_mask_cmp_ph_mask_neq_oq(
// CHECK-SAME: i32 noundef [[M:%.*]], <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[M_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[M]], ptr [[M_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[M_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = fcmp one <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast i32 [[TMP2]] to <32 x i1>
// CHECK-NEXT:    [[TMP5:%.*]] = and <32 x i1> [[TMP3]], [[TMP4]]
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <32 x i1> [[TMP5]] to i32
// CHECK-NEXT:    ret i32 [[TMP6]]
//
__mmask32 test_mm512_mask_cmp_ph_mask_neq_oq(__mmask32 m, __m512h a, __m512h b) {
  return _mm512_mask_cmp_ph_mask(m, a, b, _CMP_NEQ_OQ);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_mask_cmp_ph_mask_ge_os(
// CHECK-SAME: i32 noundef [[M:%.*]], <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[M_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[M]], ptr [[M_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[M_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = fcmp oge <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast i32 [[TMP2]] to <32 x i1>
// CHECK-NEXT:    [[TMP5:%.*]] = and <32 x i1> [[TMP3]], [[TMP4]]
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <32 x i1> [[TMP5]] to i32
// CHECK-NEXT:    ret i32 [[TMP6]]
//
__mmask32 test_mm512_mask_cmp_ph_mask_ge_os(__mmask32 m, __m512h a, __m512h b) {
  return _mm512_mask_cmp_ph_mask(m, a, b, _CMP_GE_OS);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_mask_cmp_ph_mask_gt_os(
// CHECK-SAME: i32 noundef [[M:%.*]], <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[M_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[M]], ptr [[M_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[M_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = fcmp ogt <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast i32 [[TMP2]] to <32 x i1>
// CHECK-NEXT:    [[TMP5:%.*]] = and <32 x i1> [[TMP3]], [[TMP4]]
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <32 x i1> [[TMP5]] to i32
// CHECK-NEXT:    ret i32 [[TMP6]]
//
__mmask32 test_mm512_mask_cmp_ph_mask_gt_os(__mmask32 m, __m512h a, __m512h b) {
  return _mm512_mask_cmp_ph_mask(m, a, b, _CMP_GT_OS);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_mask_cmp_ph_mask_true_uq(
// CHECK-SAME: i32 noundef [[M:%.*]], <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[M_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[M]], ptr [[M_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[M_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = fcmp true <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast i32 [[TMP2]] to <32 x i1>
// CHECK-NEXT:    [[TMP5:%.*]] = and <32 x i1> [[TMP3]], [[TMP4]]
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <32 x i1> [[TMP5]] to i32
// CHECK-NEXT:    ret i32 [[TMP6]]
//
__mmask32 test_mm512_mask_cmp_ph_mask_true_uq(__mmask32 m, __m512h a, __m512h b) {
  return _mm512_mask_cmp_ph_mask(m, a, b, _CMP_TRUE_UQ);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_mask_cmp_ph_mask_eq_os(
// CHECK-SAME: i32 noundef [[M:%.*]], <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[M_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[M]], ptr [[M_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[M_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = fcmp oeq <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast i32 [[TMP2]] to <32 x i1>
// CHECK-NEXT:    [[TMP5:%.*]] = and <32 x i1> [[TMP3]], [[TMP4]]
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <32 x i1> [[TMP5]] to i32
// CHECK-NEXT:    ret i32 [[TMP6]]
//
__mmask32 test_mm512_mask_cmp_ph_mask_eq_os(__mmask32 m, __m512h a, __m512h b) {
  return _mm512_mask_cmp_ph_mask(m, a, b, _CMP_EQ_OS);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_mask_cmp_ph_mask_lt_oq(
// CHECK-SAME: i32 noundef [[M:%.*]], <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[M_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[M]], ptr [[M_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[M_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = fcmp olt <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast i32 [[TMP2]] to <32 x i1>
// CHECK-NEXT:    [[TMP5:%.*]] = and <32 x i1> [[TMP3]], [[TMP4]]
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <32 x i1> [[TMP5]] to i32
// CHECK-NEXT:    ret i32 [[TMP6]]
//
__mmask32 test_mm512_mask_cmp_ph_mask_lt_oq(__mmask32 m, __m512h a, __m512h b) {
  return _mm512_mask_cmp_ph_mask(m, a, b, _CMP_LT_OQ);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_mask_cmp_ph_mask_le_oq(
// CHECK-SAME: i32 noundef [[M:%.*]], <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[M_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[M]], ptr [[M_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[M_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = fcmp ole <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast i32 [[TMP2]] to <32 x i1>
// CHECK-NEXT:    [[TMP5:%.*]] = and <32 x i1> [[TMP3]], [[TMP4]]
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <32 x i1> [[TMP5]] to i32
// CHECK-NEXT:    ret i32 [[TMP6]]
//
__mmask32 test_mm512_mask_cmp_ph_mask_le_oq(__mmask32 m, __m512h a, __m512h b) {
  return _mm512_mask_cmp_ph_mask(m, a, b, _CMP_LE_OQ);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_mask_cmp_ph_mask_unord_s(
// CHECK-SAME: i32 noundef [[M:%.*]], <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[M_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[M]], ptr [[M_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[M_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = fcmp uno <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast i32 [[TMP2]] to <32 x i1>
// CHECK-NEXT:    [[TMP5:%.*]] = and <32 x i1> [[TMP3]], [[TMP4]]
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <32 x i1> [[TMP5]] to i32
// CHECK-NEXT:    ret i32 [[TMP6]]
//
__mmask32 test_mm512_mask_cmp_ph_mask_unord_s(__mmask32 m, __m512h a, __m512h b) {
  return _mm512_mask_cmp_ph_mask(m, a, b, _CMP_UNORD_S);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_mask_cmp_ph_mask_neq_us(
// CHECK-SAME: i32 noundef [[M:%.*]], <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[M_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[M]], ptr [[M_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[M_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = fcmp une <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast i32 [[TMP2]] to <32 x i1>
// CHECK-NEXT:    [[TMP5:%.*]] = and <32 x i1> [[TMP3]], [[TMP4]]
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <32 x i1> [[TMP5]] to i32
// CHECK-NEXT:    ret i32 [[TMP6]]
//
__mmask32 test_mm512_mask_cmp_ph_mask_neq_us(__mmask32 m, __m512h a, __m512h b) {
  return _mm512_mask_cmp_ph_mask(m, a, b, _CMP_NEQ_US);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_mask_cmp_ph_mask_nlt_uq(
// CHECK-SAME: i32 noundef [[M:%.*]], <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[M_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[M]], ptr [[M_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[M_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = fcmp uge <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast i32 [[TMP2]] to <32 x i1>
// CHECK-NEXT:    [[TMP5:%.*]] = and <32 x i1> [[TMP3]], [[TMP4]]
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <32 x i1> [[TMP5]] to i32
// CHECK-NEXT:    ret i32 [[TMP6]]
//
__mmask32 test_mm512_mask_cmp_ph_mask_nlt_uq(__mmask32 m, __m512h a, __m512h b) {
  return _mm512_mask_cmp_ph_mask(m, a, b, _CMP_NLT_UQ);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_mask_cmp_ph_mask_nle_uq(
// CHECK-SAME: i32 noundef [[M:%.*]], <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[M_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[M]], ptr [[M_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[M_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = fcmp ugt <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast i32 [[TMP2]] to <32 x i1>
// CHECK-NEXT:    [[TMP5:%.*]] = and <32 x i1> [[TMP3]], [[TMP4]]
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <32 x i1> [[TMP5]] to i32
// CHECK-NEXT:    ret i32 [[TMP6]]
//
__mmask32 test_mm512_mask_cmp_ph_mask_nle_uq(__mmask32 m, __m512h a, __m512h b) {
  return _mm512_mask_cmp_ph_mask(m, a, b, _CMP_NLE_UQ);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_mask_cmp_ph_mask_ord_s(
// CHECK-SAME: i32 noundef [[M:%.*]], <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[M_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[M]], ptr [[M_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[M_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = fcmp ord <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast i32 [[TMP2]] to <32 x i1>
// CHECK-NEXT:    [[TMP5:%.*]] = and <32 x i1> [[TMP3]], [[TMP4]]
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <32 x i1> [[TMP5]] to i32
// CHECK-NEXT:    ret i32 [[TMP6]]
//
__mmask32 test_mm512_mask_cmp_ph_mask_ord_s(__mmask32 m, __m512h a, __m512h b) {
  return _mm512_mask_cmp_ph_mask(m, a, b, _CMP_ORD_S);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_mask_cmp_ph_mask_eq_us(
// CHECK-SAME: i32 noundef [[M:%.*]], <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[M_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[M]], ptr [[M_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[M_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = fcmp ueq <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast i32 [[TMP2]] to <32 x i1>
// CHECK-NEXT:    [[TMP5:%.*]] = and <32 x i1> [[TMP3]], [[TMP4]]
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <32 x i1> [[TMP5]] to i32
// CHECK-NEXT:    ret i32 [[TMP6]]
//
__mmask32 test_mm512_mask_cmp_ph_mask_eq_us(__mmask32 m, __m512h a, __m512h b) {
  return _mm512_mask_cmp_ph_mask(m, a, b, _CMP_EQ_US);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_mask_cmp_ph_mask_nge_uq(
// CHECK-SAME: i32 noundef [[M:%.*]], <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[M_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[M]], ptr [[M_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[M_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = fcmp ult <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast i32 [[TMP2]] to <32 x i1>
// CHECK-NEXT:    [[TMP5:%.*]] = and <32 x i1> [[TMP3]], [[TMP4]]
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <32 x i1> [[TMP5]] to i32
// CHECK-NEXT:    ret i32 [[TMP6]]
//
__mmask32 test_mm512_mask_cmp_ph_mask_nge_uq(__mmask32 m, __m512h a, __m512h b) {
  return _mm512_mask_cmp_ph_mask(m, a, b, _CMP_NGE_UQ);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_mask_cmp_ph_mask_ngt_uq(
// CHECK-SAME: i32 noundef [[M:%.*]], <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[M_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[M]], ptr [[M_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[M_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = fcmp ule <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast i32 [[TMP2]] to <32 x i1>
// CHECK-NEXT:    [[TMP5:%.*]] = and <32 x i1> [[TMP3]], [[TMP4]]
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <32 x i1> [[TMP5]] to i32
// CHECK-NEXT:    ret i32 [[TMP6]]
//
__mmask32 test_mm512_mask_cmp_ph_mask_ngt_uq(__mmask32 m, __m512h a, __m512h b) {
  return _mm512_mask_cmp_ph_mask(m, a, b, _CMP_NGT_UQ);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_mask_cmp_ph_mask_false_os(
// CHECK-SAME: i32 noundef [[M:%.*]], <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[M_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[M]], ptr [[M_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[M_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = fcmp false <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast i32 [[TMP2]] to <32 x i1>
// CHECK-NEXT:    [[TMP5:%.*]] = and <32 x i1> [[TMP3]], [[TMP4]]
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <32 x i1> [[TMP5]] to i32
// CHECK-NEXT:    ret i32 [[TMP6]]
//
__mmask32 test_mm512_mask_cmp_ph_mask_false_os(__mmask32 m, __m512h a, __m512h b) {
  return _mm512_mask_cmp_ph_mask(m, a, b, _CMP_FALSE_OS);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_mask_cmp_ph_mask_neq_os(
// CHECK-SAME: i32 noundef [[M:%.*]], <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[M_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[M]], ptr [[M_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[M_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = fcmp one <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast i32 [[TMP2]] to <32 x i1>
// CHECK-NEXT:    [[TMP5:%.*]] = and <32 x i1> [[TMP3]], [[TMP4]]
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <32 x i1> [[TMP5]] to i32
// CHECK-NEXT:    ret i32 [[TMP6]]
//
__mmask32 test_mm512_mask_cmp_ph_mask_neq_os(__mmask32 m, __m512h a, __m512h b) {
  return _mm512_mask_cmp_ph_mask(m, a, b, _CMP_NEQ_OS);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_mask_cmp_ph_mask_ge_oq(
// CHECK-SAME: i32 noundef [[M:%.*]], <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[M_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[M]], ptr [[M_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[M_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = fcmp oge <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast i32 [[TMP2]] to <32 x i1>
// CHECK-NEXT:    [[TMP5:%.*]] = and <32 x i1> [[TMP3]], [[TMP4]]
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <32 x i1> [[TMP5]] to i32
// CHECK-NEXT:    ret i32 [[TMP6]]
//
__mmask32 test_mm512_mask_cmp_ph_mask_ge_oq(__mmask32 m, __m512h a, __m512h b) {
  return _mm512_mask_cmp_ph_mask(m, a, b, _CMP_GE_OQ);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_mask_cmp_ph_mask_gt_oq(
// CHECK-SAME: i32 noundef [[M:%.*]], <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[M_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[M]], ptr [[M_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[M_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = fcmp ogt <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast i32 [[TMP2]] to <32 x i1>
// CHECK-NEXT:    [[TMP5:%.*]] = and <32 x i1> [[TMP3]], [[TMP4]]
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <32 x i1> [[TMP5]] to i32
// CHECK-NEXT:    ret i32 [[TMP6]]
//
__mmask32 test_mm512_mask_cmp_ph_mask_gt_oq(__mmask32 m, __m512h a, __m512h b) {
  return _mm512_mask_cmp_ph_mask(m, a, b, _CMP_GT_OQ);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_mask_cmp_ph_mask_true_us(
// CHECK-SAME: i32 noundef [[M:%.*]], <32 x half> noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[M_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[M]], ptr [[M_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[M_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = fcmp true <32 x half> [[TMP0]], [[TMP1]]
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast i32 [[TMP2]] to <32 x i1>
// CHECK-NEXT:    [[TMP5:%.*]] = and <32 x i1> [[TMP3]], [[TMP4]]
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <32 x i1> [[TMP5]] to i32
// CHECK-NEXT:    ret i32 [[TMP6]]
//
__mmask32 test_mm512_mask_cmp_ph_mask_true_us(__mmask32 m, __m512h a, __m512h b) {
  return _mm512_mask_cmp_ph_mask(m, a, b, _CMP_TRUE_US);
}

// CHECK-LABEL: define dso_local zeroext i8 @test_mm_cmp_round_sh_mask(
// CHECK-SAME: <8 x half> noundef [[__X:%.*]], <8 x half> noundef [[__Y:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__X_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__Y_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__X]], ptr [[__X_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__Y]], ptr [[__Y_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__X_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__Y_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = call i8 @llvm.x86.avx512fp16.mask.cmp.sh(<8 x half> [[TMP0]], <8 x half> [[TMP1]], i32 5, i8 -1, i32 8)
// CHECK-NEXT:    ret i8 [[TMP2]]
//
__mmask8 test_mm_cmp_round_sh_mask(__m128h __X, __m128h __Y) {
  return _mm_cmp_round_sh_mask(__X, __Y, _CMP_NLT_US, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local zeroext i8 @test_mm_mask_cmp_round_sh_mask(
// CHECK-SAME: i8 noundef zeroext [[__M:%.*]], <8 x half> noundef [[__X:%.*]], <8 x half> noundef [[__Y:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__M_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__X_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__Y_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[__M]], ptr [[__M_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__X]], ptr [[__X_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__Y]], ptr [[__Y_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__X_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__Y_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load i8, ptr [[__M_ADDR]], align 1
// CHECK-NEXT:    [[TMP3:%.*]] = call i8 @llvm.x86.avx512fp16.mask.cmp.sh(<8 x half> [[TMP0]], <8 x half> [[TMP1]], i32 5, i8 [[TMP2]], i32 8)
// CHECK-NEXT:    ret i8 [[TMP3]]
//
__mmask8 test_mm_mask_cmp_round_sh_mask(__mmask8 __M, __m128h __X, __m128h __Y) {
  return _mm_mask_cmp_round_sh_mask(__M, __X, __Y, _CMP_NLT_US, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local zeroext i8 @test_mm_cmp_sh_mask(
// CHECK-SAME: <8 x half> noundef [[__X:%.*]], <8 x half> noundef [[__Y:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__X_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__Y_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__X]], ptr [[__X_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__Y]], ptr [[__Y_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__X_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__Y_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = call i8 @llvm.x86.avx512fp16.mask.cmp.sh(<8 x half> [[TMP0]], <8 x half> [[TMP1]], i32 5, i8 -1, i32 4)
// CHECK-NEXT:    ret i8 [[TMP2]]
//
__mmask8 test_mm_cmp_sh_mask(__m128h __X, __m128h __Y) {
  return _mm_cmp_sh_mask(__X, __Y, _CMP_NLT_US);
}

// CHECK-LABEL: define dso_local zeroext i8 @test_mm_mask_cmp_sh_mask(
// CHECK-SAME: i8 noundef zeroext [[__M:%.*]], <8 x half> noundef [[__X:%.*]], <8 x half> noundef [[__Y:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__M_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__X_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__Y_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[__M]], ptr [[__M_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__X]], ptr [[__X_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__Y]], ptr [[__Y_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__X_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__Y_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load i8, ptr [[__M_ADDR]], align 1
// CHECK-NEXT:    [[TMP3:%.*]] = call i8 @llvm.x86.avx512fp16.mask.cmp.sh(<8 x half> [[TMP0]], <8 x half> [[TMP1]], i32 5, i8 [[TMP2]], i32 4)
// CHECK-NEXT:    ret i8 [[TMP3]]
//
__mmask8 test_mm_mask_cmp_sh_mask(__mmask8 __M, __m128h __X, __m128h __Y) {
  return _mm_mask_cmp_sh_mask(__M, __X, __Y, _CMP_NLT_US);
}

// VMOVSH

// CHECK-LABEL: define dso_local <8 x half> @test_mm_load_sh(
// CHECK-SAME: ptr noundef [[A:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__DP_ADDR_I:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[__U_I:%.*]] = alloca half, align 2
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    store ptr [[TMP0]], ptr [[__DP_ADDR_I]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[__DP_ADDR_I]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = load half, ptr [[TMP1]], align 1
// CHECK-NEXT:    store half [[TMP2]], ptr [[__U_I]], align 2
// CHECK-NEXT:    [[TMP3:%.*]] = load half, ptr [[__U_I]], align 2
// CHECK-NEXT:    [[VECINIT_I:%.*]] = insertelement <8 x half> undef, half [[TMP3]], i32 0
// CHECK-NEXT:    [[VECINIT2_I:%.*]] = insertelement <8 x half> [[VECINIT_I]], half 0xH0000, i32 1
// CHECK-NEXT:    [[VECINIT3_I:%.*]] = insertelement <8 x half> [[VECINIT2_I]], half 0xH0000, i32 2
// CHECK-NEXT:    [[VECINIT4_I:%.*]] = insertelement <8 x half> [[VECINIT3_I]], half 0xH0000, i32 3
// CHECK-NEXT:    [[VECINIT5_I:%.*]] = insertelement <8 x half> [[VECINIT4_I]], half 0xH0000, i32 4
// CHECK-NEXT:    [[VECINIT6_I:%.*]] = insertelement <8 x half> [[VECINIT5_I]], half 0xH0000, i32 5
// CHECK-NEXT:    [[VECINIT7_I:%.*]] = insertelement <8 x half> [[VECINIT6_I]], half 0xH0000, i32 6
// CHECK-NEXT:    [[VECINIT8_I:%.*]] = insertelement <8 x half> [[VECINIT7_I]], half 0xH0000, i32 7
// CHECK-NEXT:    store <8 x half> [[VECINIT8_I]], ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    ret <8 x half> [[TMP4]]
//
__m128h test_mm_load_sh(void const *A) {
  return _mm_load_sh(A);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask_load_sh(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], i8 noundef zeroext [[__U:%.*]], ptr noundef [[__W:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[SRC_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store ptr [[__W]], ptr [[__W_ADDR]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[__W_ADDR]], align 8
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    store i8 [[TMP1]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store ptr [[TMP2]], ptr [[__A_ADDR_I]], align 8
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <8 x half> [[TMP3]], <8 x half> [[TMP4]], <8 x i32> <i32 0, i32 8, i32 8, i32 8, i32 8, i32 8, i32 8, i32 8>
// CHECK-NEXT:    store <8 x half> [[SHUFFLE_I]], ptr [[SRC_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = load ptr, ptr [[__A_ADDR_I]], align 8
// CHECK-NEXT:    [[TMP6:%.*]] = load <8 x half>, ptr [[SRC_I]], align 16
// CHECK-NEXT:    [[TMP7:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[CONV_I:%.*]] = zext i8 [[TMP7]] to i32
// CHECK-NEXT:    [[AND_I:%.*]] = and i32 [[CONV_I]], 1
// CHECK-NEXT:    [[CONV1_I:%.*]] = trunc i32 [[AND_I]] to i8
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast i8 [[CONV1_I]] to <8 x i1>
// CHECK-NEXT:    [[TMP9:%.*]] = call <8 x half> @llvm.masked.load.v8f16.p0(ptr [[TMP5]], i32 1, <8 x i1> [[TMP8]], <8 x half> [[TMP6]])
// CHECK-NEXT:    ret <8 x half> [[TMP9]]
//
__m128h test_mm_mask_load_sh(__m128h __A, __mmask8 __U, const void *__W) {
  return _mm_mask_load_sh(__A, __U, __W);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_maskz_load_sh(
// CHECK-SAME: i8 noundef zeroext [[__U:%.*]], ptr noundef [[__W:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store ptr [[__W]], ptr [[__W_ADDR]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[__W_ADDR]], align 8
// CHECK-NEXT:    store i8 [[TMP0]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store ptr [[TMP1]], ptr [[__A_ADDR_I]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[__A_ADDR_I]], align 8
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[CONV_I:%.*]] = zext i8 [[TMP4]] to i32
// CHECK-NEXT:    [[AND_I:%.*]] = and i32 [[CONV_I]], 1
// CHECK-NEXT:    [[CONV1_I:%.*]] = trunc i32 [[AND_I]] to i8
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast i8 [[CONV1_I]] to <8 x i1>
// CHECK-NEXT:    [[TMP6:%.*]] = call <8 x half> @llvm.masked.load.v8f16.p0(ptr [[TMP2]], i32 1, <8 x i1> [[TMP5]], <8 x half> [[TMP3]])
// CHECK-NEXT:    ret <8 x half> [[TMP6]]
//
__m128h test_mm_maskz_load_sh(__mmask8 __U, const void *__W) {
  return _mm_maskz_load_sh(__U, __W);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_load_ph(
// CHECK-SAME: ptr noundef [[P:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__P_ADDR_I:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[P_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    store ptr [[P]], ptr [[P_ADDR]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[P_ADDR]], align 8
// CHECK-NEXT:    store ptr [[TMP0]], ptr [[__P_ADDR_I]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[__P_ADDR_I]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[TMP1]], align 64
// CHECK-NEXT:    ret <32 x half> [[TMP2]]
//
__m512h test_mm512_load_ph(void *p) {
  return _mm512_load_ph(p);
}

// CHECK-LABEL: define dso_local <16 x half> @test_mm256_load_ph(
// CHECK-SAME: ptr noundef [[P:%.*]]) #[[ATTR2]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__P_ADDR_I:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[P_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    store ptr [[P]], ptr [[P_ADDR]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[P_ADDR]], align 8
// CHECK-NEXT:    store ptr [[TMP0]], ptr [[__P_ADDR_I]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[__P_ADDR_I]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = load <16 x half>, ptr [[TMP1]], align 32
// CHECK-NEXT:    ret <16 x half> [[TMP2]]
//
__m256h test_mm256_load_ph(void *p) {
  return _mm256_load_ph(p);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_load_ph(
// CHECK-SAME: ptr noundef [[P:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__P_ADDR_I:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[P_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    store ptr [[P]], ptr [[P_ADDR]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[P_ADDR]], align 8
// CHECK-NEXT:    store ptr [[TMP0]], ptr [[__P_ADDR_I]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[__P_ADDR_I]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[TMP1]], align 16
// CHECK-NEXT:    ret <8 x half> [[TMP2]]
//
__m128h test_mm_load_ph(void *p) {
  return _mm_load_ph(p);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_loadu_ph(
// CHECK-SAME: ptr noundef [[P:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__P_ADDR_I:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[P_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    store ptr [[P]], ptr [[P_ADDR]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[P_ADDR]], align 8
// CHECK-NEXT:    store ptr [[TMP0]], ptr [[__P_ADDR_I]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[__P_ADDR_I]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[TMP1]], align 1
// CHECK-NEXT:    ret <32 x half> [[TMP2]]
//
__m512h test_mm512_loadu_ph(void *p) {
  return _mm512_loadu_ph(p);
}

// CHECK-LABEL: define dso_local <16 x half> @test_mm256_loadu_ph(
// CHECK-SAME: ptr noundef [[P:%.*]]) #[[ATTR2]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__P_ADDR_I:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[P_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    store ptr [[P]], ptr [[P_ADDR]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[P_ADDR]], align 8
// CHECK-NEXT:    store ptr [[TMP0]], ptr [[__P_ADDR_I]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[__P_ADDR_I]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = load <16 x half>, ptr [[TMP1]], align 1
// CHECK-NEXT:    ret <16 x half> [[TMP2]]
//
__m256h test_mm256_loadu_ph(void *p) {
  return _mm256_loadu_ph(p);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_loadu_ph(
// CHECK-SAME: ptr noundef [[P:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__P_ADDR_I:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[P_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    store ptr [[P]], ptr [[P_ADDR]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[P_ADDR]], align 8
// CHECK-NEXT:    store ptr [[TMP0]], ptr [[__P_ADDR_I]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[__P_ADDR_I]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[TMP1]], align 1
// CHECK-NEXT:    ret <8 x half> [[TMP2]]
//
__m128h test_mm_loadu_ph(void *p) {
  return _mm_loadu_ph(p);
}

// CHECK-LABEL: define dso_local void @test_mm_store_sh(
// CHECK-SAME: ptr noundef [[A:%.*]], <8 x half> noundef [[B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__DP_ADDR_I:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 8
// CHECK-NEXT:    store <8 x half> [[B]], ptr [[B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[B_ADDR]], align 16
// CHECK-NEXT:    store ptr [[TMP0]], ptr [[__DP_ADDR_I]], align 8
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[VECEXT_I:%.*]] = extractelement <8 x half> [[TMP2]], i32 0
// CHECK-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[__DP_ADDR_I]], align 8
// CHECK-NEXT:    store half [[VECEXT_I]], ptr [[TMP3]], align 1
// CHECK-NEXT:    ret void
//
void test_mm_store_sh(void *A, __m128h B) {
  _mm_store_sh(A, B);
}

// CHECK-LABEL: define dso_local void @test_mm_mask_store_sh(
// CHECK-SAME: ptr noundef [[__P:%.*]], i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__P_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store ptr [[__P]], ptr [[__P_ADDR]], align 8
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[__P_ADDR]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store ptr [[TMP0]], ptr [[__W_ADDR_I]], align 8
// CHECK-NEXT:    store i8 [[TMP1]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[__W_ADDR_I]], align 8
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[CONV_I:%.*]] = zext i8 [[TMP5]] to i32
// CHECK-NEXT:    [[AND_I:%.*]] = and i32 [[CONV_I]], 1
// CHECK-NEXT:    [[CONV1_I:%.*]] = trunc i32 [[AND_I]] to i8
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast i8 [[CONV1_I]] to <8 x i1>
// CHECK-NEXT:    call void @llvm.masked.store.v8f16.p0(<8 x half> [[TMP4]], ptr [[TMP3]], i32 1, <8 x i1> [[TMP6]])
// CHECK-NEXT:    ret void
//
void test_mm_mask_store_sh(void *__P, __mmask8 __U, __m128h __A) {
  _mm_mask_store_sh(__P, __U, __A);
}

// CHECK-LABEL: define dso_local void @test_mm512_store_ph(
// CHECK-SAME: ptr noundef [[P:%.*]], <32 x half> noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__P_ADDR_I:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[P_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store ptr [[P]], ptr [[P_ADDR]], align 8
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[P_ADDR]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store ptr [[TMP0]], ptr [[__P_ADDR_I]], align 8
// CHECK-NEXT:    store <32 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[__P_ADDR_I]], align 8
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[TMP3]], align 64
// CHECK-NEXT:    ret void
//
void test_mm512_store_ph(void *p, __m512h a) {
  _mm512_store_ph(p, a);
}

// CHECK-LABEL: define dso_local void @test_mm256_store_ph(
// CHECK-SAME: ptr noundef [[P:%.*]], <16 x half> noundef [[A:%.*]]) #[[ATTR2]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__P_ADDR_I:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    [[P_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    store ptr [[P]], ptr [[P_ADDR]], align 8
// CHECK-NEXT:    store <16 x half> [[A]], ptr [[A_ADDR]], align 32
// CHECK-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[P_ADDR]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <16 x half>, ptr [[A_ADDR]], align 32
// CHECK-NEXT:    store ptr [[TMP0]], ptr [[__P_ADDR_I]], align 8
// CHECK-NEXT:    store <16 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <16 x half>, ptr [[__A_ADDR_I]], align 32
// CHECK-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[__P_ADDR_I]], align 8
// CHECK-NEXT:    store <16 x half> [[TMP2]], ptr [[TMP3]], align 32
// CHECK-NEXT:    ret void
//
void test_mm256_store_ph(void *p, __m256h a) {
  _mm256_store_ph(p, a);
}

// CHECK-LABEL: define dso_local void @test_mm_store_ph(
// CHECK-SAME: ptr noundef [[P:%.*]], <8 x half> noundef [[A:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__P_ADDR_I:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[P_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store ptr [[P]], ptr [[P_ADDR]], align 8
// CHECK-NEXT:    store <8 x half> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[P_ADDR]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store ptr [[TMP0]], ptr [[__P_ADDR_I]], align 8
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[__P_ADDR_I]], align 8
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[TMP3]], align 16
// CHECK-NEXT:    ret void
//
void test_mm_store_ph(void *p, __m128h a) {
  _mm_store_ph(p, a);
}

// CHECK-LABEL: define dso_local void @test_mm512_storeu_ph(
// CHECK-SAME: ptr noundef [[P:%.*]], <32 x half> noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__P_ADDR_I:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[P_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store ptr [[P]], ptr [[P_ADDR]], align 8
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[P_ADDR]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store ptr [[TMP0]], ptr [[__P_ADDR_I]], align 8
// CHECK-NEXT:    store <32 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[__P_ADDR_I]], align 8
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[TMP3]], align 1
// CHECK-NEXT:    ret void
//
void test_mm512_storeu_ph(void *p, __m512h a) {
  _mm512_storeu_ph(p, a);
}

// CHECK-LABEL: define dso_local void @test_mm256_storeu_ph(
// CHECK-SAME: ptr noundef [[P:%.*]], <16 x half> noundef [[A:%.*]]) #[[ATTR2]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__P_ADDR_I:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    [[P_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    store ptr [[P]], ptr [[P_ADDR]], align 8
// CHECK-NEXT:    store <16 x half> [[A]], ptr [[A_ADDR]], align 32
// CHECK-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[P_ADDR]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <16 x half>, ptr [[A_ADDR]], align 32
// CHECK-NEXT:    store ptr [[TMP0]], ptr [[__P_ADDR_I]], align 8
// CHECK-NEXT:    store <16 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <16 x half>, ptr [[__A_ADDR_I]], align 32
// CHECK-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[__P_ADDR_I]], align 8
// CHECK-NEXT:    store <16 x half> [[TMP2]], ptr [[TMP3]], align 1
// CHECK-NEXT:    ret void
//
void test_mm256_storeu_ph(void *p, __m256h a) {
  _mm256_storeu_ph(p, a);
}

// CHECK-LABEL: define dso_local void @test_mm_storeu_ph(
// CHECK-SAME: ptr noundef [[P:%.*]], <8 x half> noundef [[A:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__P_ADDR_I:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[P_ADDR:%.*]] = alloca ptr, align 8
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store ptr [[P]], ptr [[P_ADDR]], align 8
// CHECK-NEXT:    store <8 x half> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[P_ADDR]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store ptr [[TMP0]], ptr [[__P_ADDR_I]], align 8
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[__P_ADDR_I]], align 8
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[TMP3]], align 1
// CHECK-NEXT:    ret void
//
void test_mm_storeu_ph(void *p, __m128h a) {
  _mm_storeu_ph(p, a);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_move_sh(
// CHECK-SAME: <8 x half> noundef [[A:%.*]], <8 x half> noundef [[B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[B]], ptr [[B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[VECEXT_I:%.*]] = extractelement <8 x half> [[TMP2]], i32 0
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[VECINS_I:%.*]] = insertelement <8 x half> [[TMP3]], half [[VECEXT_I]], i32 0
// CHECK-NEXT:    store <8 x half> [[VECINS_I]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    ret <8 x half> [[TMP4]]
//
__m128h test_mm_move_sh(__m128h A, __m128h B) {
  return _mm_move_sh(A, B);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask_move_sh(
// CHECK-SAME: <8 x half> noundef [[__W:%.*]], i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__W]], ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    store i8 [[TMP1]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP3]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP5:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP5]], ptr [[__A_ADDR_I_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP6]], ptr [[__B_ADDR_I_I]], align 16
// CHECK-NEXT:    [[TMP7:%.*]] = load <8 x half>, ptr [[__B_ADDR_I_I]], align 16
// CHECK-NEXT:    [[VECEXT_I_I:%.*]] = extractelement <8 x half> [[TMP7]], i32 0
// CHECK-NEXT:    [[TMP8:%.*]] = load <8 x half>, ptr [[__A_ADDR_I_I]], align 16
// CHECK-NEXT:    [[VECINS_I_I:%.*]] = insertelement <8 x half> [[TMP8]], half [[VECEXT_I_I]], i32 0
// CHECK-NEXT:    store <8 x half> [[VECINS_I_I]], ptr [[__A_ADDR_I_I]], align 16
// CHECK-NEXT:    [[TMP9:%.*]] = load <8 x half>, ptr [[__A_ADDR_I_I]], align 16
// CHECK-NEXT:    [[TMP10:%.*]] = load <8 x half>, ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP11:%.*]] = extractelement <8 x half> [[TMP9]], i64 0
// CHECK-NEXT:    [[TMP12:%.*]] = extractelement <8 x half> [[TMP10]], i64 0
// CHECK-NEXT:    [[TMP13:%.*]] = bitcast i8 [[TMP4]] to <8 x i1>
// CHECK-NEXT:    [[TMP14:%.*]] = extractelement <8 x i1> [[TMP13]], i64 0
// CHECK-NEXT:    [[TMP15:%.*]] = select i1 [[TMP14]], half [[TMP11]], half [[TMP12]]
// CHECK-NEXT:    [[TMP16:%.*]] = insertelement <8 x half> [[TMP9]], half [[TMP15]], i64 0
// CHECK-NEXT:    ret <8 x half> [[TMP16]]
//
__m128h test_mm_mask_move_sh(__m128h __W, __mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_mask_move_sh(__W, __U, __A, __B);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_maskz_move_sh(
// CHECK-SAME: i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store i8 [[TMP0]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP4]], ptr [[__A_ADDR_I_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP5]], ptr [[__B_ADDR_I_I]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = load <8 x half>, ptr [[__B_ADDR_I_I]], align 16
// CHECK-NEXT:    [[VECEXT_I_I:%.*]] = extractelement <8 x half> [[TMP6]], i32 0
// CHECK-NEXT:    [[TMP7:%.*]] = load <8 x half>, ptr [[__A_ADDR_I_I]], align 16
// CHECK-NEXT:    [[VECINS_I_I:%.*]] = insertelement <8 x half> [[TMP7]], half [[VECEXT_I_I]], i32 0
// CHECK-NEXT:    store <8 x half> [[VECINS_I_I]], ptr [[__A_ADDR_I_I]], align 16
// CHECK-NEXT:    [[TMP8:%.*]] = load <8 x half>, ptr [[__A_ADDR_I_I]], align 16
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP9:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP10:%.*]] = extractelement <8 x half> [[TMP8]], i64 0
// CHECK-NEXT:    [[TMP11:%.*]] = extractelement <8 x half> [[TMP9]], i64 0
// CHECK-NEXT:    [[TMP12:%.*]] = bitcast i8 [[TMP3]] to <8 x i1>
// CHECK-NEXT:    [[TMP13:%.*]] = extractelement <8 x i1> [[TMP12]], i64 0
// CHECK-NEXT:    [[TMP14:%.*]] = select i1 [[TMP13]], half [[TMP10]], half [[TMP11]]
// CHECK-NEXT:    [[TMP15:%.*]] = insertelement <8 x half> [[TMP8]], half [[TMP14]], i64 0
// CHECK-NEXT:    ret <8 x half> [[TMP15]]
//
__m128h test_mm_maskz_move_sh(__mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_maskz_move_sh(__U, __A, __B);
}

// CHECK-LABEL: define dso_local signext i16 @test_mm_cvtsi128_si16(
// CHECK-SAME: <2 x i64> noundef [[A:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// CHECK-NEXT:    [[__B_I:%.*]] = alloca <8 x i16>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// CHECK-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <2 x i64> [[TMP1]] to <8 x i16>
// CHECK-NEXT:    store <8 x i16> [[TMP2]], ptr [[__B_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x i16>, ptr [[__B_I]], align 16
// CHECK-NEXT:    [[VECEXT_I:%.*]] = extractelement <8 x i16> [[TMP3]], i32 0
// CHECK-NEXT:    ret i16 [[VECEXT_I]]
//
short test_mm_cvtsi128_si16(__m128i A) {
  return _mm_cvtsi128_si16(A);
}

// CHECK-LABEL: define dso_local <2 x i64> @test_mm_cvtsi16_si128(
// CHECK-SAME: i16 noundef signext [[A:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x i16>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    store i16 [[A]], ptr [[A_ADDR]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = load i16, ptr [[A_ADDR]], align 2
// CHECK-NEXT:    store i16 [[TMP0]], ptr [[__A_ADDR_I]], align 2
// CHECK-NEXT:    [[TMP1:%.*]] = load i16, ptr [[__A_ADDR_I]], align 2
// CHECK-NEXT:    [[VECINIT_I:%.*]] = insertelement <8 x i16> undef, i16 [[TMP1]], i32 0
// CHECK-NEXT:    [[VECINIT1_I:%.*]] = insertelement <8 x i16> [[VECINIT_I]], i16 0, i32 1
// CHECK-NEXT:    [[VECINIT2_I:%.*]] = insertelement <8 x i16> [[VECINIT1_I]], i16 0, i32 2
// CHECK-NEXT:    [[VECINIT3_I:%.*]] = insertelement <8 x i16> [[VECINIT2_I]], i16 0, i32 3
// CHECK-NEXT:    [[VECINIT4_I:%.*]] = insertelement <8 x i16> [[VECINIT3_I]], i16 0, i32 4
// CHECK-NEXT:    [[VECINIT5_I:%.*]] = insertelement <8 x i16> [[VECINIT4_I]], i16 0, i32 5
// CHECK-NEXT:    [[VECINIT6_I:%.*]] = insertelement <8 x i16> [[VECINIT5_I]], i16 0, i32 6
// CHECK-NEXT:    [[VECINIT7_I:%.*]] = insertelement <8 x i16> [[VECINIT6_I]], i16 0, i32 7
// CHECK-NEXT:    store <8 x i16> [[VECINIT7_I]], ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x i16>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <8 x i16> [[TMP2]] to <2 x i64>
// CHECK-NEXT:    ret <2 x i64> [[TMP3]]
//
__m128i test_mm_cvtsi16_si128(short A) {
  return _mm_cvtsi16_si128(A);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_rcp_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = call <32 x half> @llvm.x86.avx512fp16.mask.rcp.ph.512(<32 x half> [[TMP1]], <32 x half> zeroinitializer, i32 -1)
// CHECK-NEXT:    ret <32 x half> [[TMP2]]
//
__m512h test_mm512_rcp_ph(__m512h __A) {
  return _mm512_rcp_ph(__A);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask_rcp_ph(
// CHECK-SAME: <32 x half> noundef [[__W:%.*]], i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__W]], ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = load i32, ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP6:%.*]] = call <32 x half> @llvm.x86.avx512fp16.mask.rcp.ph.512(<32 x half> [[TMP3]], <32 x half> [[TMP4]], i32 [[TMP5]])
// CHECK-NEXT:    ret <32 x half> [[TMP6]]
//
__m512h test_mm512_mask_rcp_ph(__m512h __W, __mmask32 __U, __m512h __A) {
  return (__m512h)_mm512_mask_rcp_ph(__W, __U, __A);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_maskz_rcp_ph(
// CHECK-SAME: i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store i32 [[TMP0]], ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    store <32 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load i32, ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP5:%.*]] = call <32 x half> @llvm.x86.avx512fp16.mask.rcp.ph.512(<32 x half> [[TMP2]], <32 x half> [[TMP3]], i32 [[TMP4]])
// CHECK-NEXT:    ret <32 x half> [[TMP5]]
//
__m512h test_mm512_maskz_rcp_ph(__mmask32 __U, __m512h __A) {
  return _mm512_maskz_rcp_ph(__U, __A);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_rsqrt_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = call <32 x half> @llvm.x86.avx512fp16.mask.rsqrt.ph.512(<32 x half> [[TMP1]], <32 x half> zeroinitializer, i32 -1)
// CHECK-NEXT:    ret <32 x half> [[TMP2]]
//
__m512h test_mm512_rsqrt_ph(__m512h __A) {
  return _mm512_rsqrt_ph(__A);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask_rsqrt_ph(
// CHECK-SAME: <32 x half> noundef [[__W:%.*]], i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__W]], ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = load i32, ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP6:%.*]] = call <32 x half> @llvm.x86.avx512fp16.mask.rsqrt.ph.512(<32 x half> [[TMP3]], <32 x half> [[TMP4]], i32 [[TMP5]])
// CHECK-NEXT:    ret <32 x half> [[TMP6]]
//
__m512h test_mm512_mask_rsqrt_ph(__m512h __W, __mmask32 __U, __m512h __A) {
  return (__m512h)_mm512_mask_rsqrt_ph(__W, __U, __A);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_maskz_rsqrt_ph(
// CHECK-SAME: i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store i32 [[TMP0]], ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    store <32 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load i32, ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP5:%.*]] = call <32 x half> @llvm.x86.avx512fp16.mask.rsqrt.ph.512(<32 x half> [[TMP2]], <32 x half> [[TMP3]], i32 [[TMP4]])
// CHECK-NEXT:    ret <32 x half> [[TMP5]]
//
__m512h test_mm512_maskz_rsqrt_ph(__mmask32 __U, __m512h __A) {
  return _mm512_maskz_rsqrt_ph(__U, __A);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_getmant_round_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = call <32 x half> @llvm.x86.avx512fp16.mask.getmant.ph.512(<32 x half> [[TMP0]], i32 9, <32 x half> zeroinitializer, i32 -1, i32 8)
// CHECK-NEXT:    ret <32 x half> [[TMP1]]
//
__m512h test_mm512_getmant_round_ph(__m512h __A) {
  return _mm512_getmant_round_ph(__A, _MM_MANT_NORM_p5_2, _MM_MANT_SIGN_nan, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask_getmant_round_ph(
// CHECK-SAME: <32 x half> noundef [[__W:%.*]], i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__W]], ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = call <32 x half> @llvm.x86.avx512fp16.mask.getmant.ph.512(<32 x half> [[TMP0]], i32 9, <32 x half> [[TMP1]], i32 [[TMP2]], i32 8)
// CHECK-NEXT:    ret <32 x half> [[TMP3]]
//
__m512h test_mm512_mask_getmant_round_ph(__m512h __W, __mmask32 __U, __m512h __A) {
  return _mm512_mask_getmant_round_ph(__W, __U, __A, _MM_MANT_NORM_p5_2, _MM_MANT_SIGN_nan, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_maskz_getmant_round_ph(
// CHECK-SAME: i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = call <32 x half> @llvm.x86.avx512fp16.mask.getmant.ph.512(<32 x half> [[TMP0]], i32 9, <32 x half> [[TMP1]], i32 [[TMP2]], i32 8)
// CHECK-NEXT:    ret <32 x half> [[TMP3]]
//
__m512h test_mm512_maskz_getmant_round_ph(__mmask32 __U, __m512h __A) {
  return _mm512_maskz_getmant_round_ph(__U, __A, _MM_MANT_NORM_p5_2, _MM_MANT_SIGN_nan, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_getmant_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = call <32 x half> @llvm.x86.avx512fp16.mask.getmant.ph.512(<32 x half> [[TMP0]], i32 9, <32 x half> zeroinitializer, i32 -1, i32 4)
// CHECK-NEXT:    ret <32 x half> [[TMP1]]
//
__m512h test_mm512_getmant_ph(__m512h __A) {
  return _mm512_getmant_ph(__A, _MM_MANT_NORM_p5_2, _MM_MANT_SIGN_nan);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask_getmant_ph(
// CHECK-SAME: <32 x half> noundef [[__W:%.*]], i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__W]], ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = call <32 x half> @llvm.x86.avx512fp16.mask.getmant.ph.512(<32 x half> [[TMP0]], i32 9, <32 x half> [[TMP1]], i32 [[TMP2]], i32 4)
// CHECK-NEXT:    ret <32 x half> [[TMP3]]
//
__m512h test_mm512_mask_getmant_ph(__m512h __W, __mmask32 __U, __m512h __A) {
  return _mm512_mask_getmant_ph(__W, __U, __A, _MM_MANT_NORM_p5_2, _MM_MANT_SIGN_nan);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_maskz_getmant_ph(
// CHECK-SAME: i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = call <32 x half> @llvm.x86.avx512fp16.mask.getmant.ph.512(<32 x half> [[TMP0]], i32 9, <32 x half> [[TMP1]], i32 [[TMP2]], i32 4)
// CHECK-NEXT:    ret <32 x half> [[TMP3]]
//
__m512h test_mm512_maskz_getmant_ph(__mmask32 __U, __m512h __A) {
  return _mm512_maskz_getmant_ph(__U, __A, _MM_MANT_NORM_p5_2, _MM_MANT_SIGN_nan);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_scalef_round_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = call <32 x half> @llvm.x86.avx512fp16.mask.scalef.ph.512(<32 x half> [[TMP0]], <32 x half> [[TMP1]], <32 x half> zeroinitializer, i32 -1, i32 11)
// CHECK-NEXT:    ret <32 x half> [[TMP2]]
//
__m512h test_mm512_scalef_round_ph(__m512h __A, __m512h __B) {
  return _mm512_scalef_round_ph(__A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask_scalef_round_ph(
// CHECK-SAME: <32 x half> noundef [[__W:%.*]], i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__W]], ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = call <32 x half> @llvm.x86.avx512fp16.mask.scalef.ph.512(<32 x half> [[TMP0]], <32 x half> [[TMP1]], <32 x half> [[TMP2]], i32 [[TMP3]], i32 11)
// CHECK-NEXT:    ret <32 x half> [[TMP4]]
//
__m512h test_mm512_mask_scalef_round_ph(__m512h __W, __mmask32 __U, __m512h __A, __m512h __B) {
  return _mm512_mask_scalef_round_ph(__W, __U, __A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_maskz_scalef_round_ph(
// CHECK-SAME: i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = call <32 x half> @llvm.x86.avx512fp16.mask.scalef.ph.512(<32 x half> [[TMP0]], <32 x half> [[TMP1]], <32 x half> [[TMP2]], i32 [[TMP3]], i32 11)
// CHECK-NEXT:    ret <32 x half> [[TMP4]]
//
__m512h test_mm512_maskz_scalef_round_ph(__mmask32 __U, __m512h __A, __m512h __B) {
  return _mm512_maskz_scalef_round_ph(__U, __A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_scalef_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP1]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = call <32 x half> @llvm.x86.avx512fp16.mask.scalef.ph.512(<32 x half> [[TMP2]], <32 x half> [[TMP3]], <32 x half> zeroinitializer, i32 -1, i32 4)
// CHECK-NEXT:    ret <32 x half> [[TMP4]]
//
__m512h test_mm512_scalef_ph(__m512h __A, __m512h __B) {
  return _mm512_scalef_ph(__A, __B);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask_scalef_ph(
// CHECK-SAME: <32 x half> noundef [[__W:%.*]], i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__W]], ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP3]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP6:%.*]] = load <32 x half>, ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP7:%.*]] = load i32, ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP8:%.*]] = call <32 x half> @llvm.x86.avx512fp16.mask.scalef.ph.512(<32 x half> [[TMP4]], <32 x half> [[TMP5]], <32 x half> [[TMP6]], i32 [[TMP7]], i32 4)
// CHECK-NEXT:    ret <32 x half> [[TMP8]]
//
__m512h test_mm512_mask_scalef_ph(__m512h __W, __mmask32 __U, __m512h __A, __m512h __B) {
  return _mm512_mask_scalef_ph(__W, __U, __A, __B);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_maskz_scalef_ph(
// CHECK-SAME: i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store i32 [[TMP0]], ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    store <32 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = load <32 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP6:%.*]] = load i32, ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP7:%.*]] = call <32 x half> @llvm.x86.avx512fp16.mask.scalef.ph.512(<32 x half> [[TMP3]], <32 x half> [[TMP4]], <32 x half> [[TMP5]], i32 [[TMP6]], i32 4)
// CHECK-NEXT:    ret <32 x half> [[TMP7]]
//
__m512h test_mm512_maskz_scalef_ph(__mmask32 __U, __m512h __A, __m512h __B) {
  return _mm512_maskz_scalef_ph(__U, __A, __B);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask_roundscale_ph(
// CHECK-SAME: <32 x half> noundef [[__W:%.*]], i16 noundef zeroext [[__U:%.*]], <32 x half> noundef [[__A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__W]], ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    store i16 [[__U]], ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i16, ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    [[CONV:%.*]] = zext i16 [[TMP2]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call <32 x half> @llvm.x86.avx512fp16.mask.rndscale.ph.512(<32 x half> [[TMP0]], i32 1, <32 x half> [[TMP1]], i32 [[CONV]], i32 4)
// CHECK-NEXT:    ret <32 x half> [[TMP3]]
//
__m512h test_mm512_mask_roundscale_ph(__m512h __W, __mmask16 __U, __m512h __A) {
  return _mm512_mask_roundscale_ph(__W, __U, __A, 1);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_maskz_roundscale_ph(
// CHECK-SAME: i16 noundef zeroext [[__U:%.*]], <32 x half> noundef [[__A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i16 [[__U]], ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i16, ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    [[CONV:%.*]] = zext i16 [[TMP2]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call <32 x half> @llvm.x86.avx512fp16.mask.rndscale.ph.512(<32 x half> [[TMP0]], i32 1, <32 x half> [[TMP1]], i32 [[CONV]], i32 4)
// CHECK-NEXT:    ret <32 x half> [[TMP3]]
//
__m512h test_mm512_maskz_roundscale_ph(__mmask16 __U, __m512h __A) {
  return _mm512_maskz_roundscale_ph(__U, __A, 1);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask_roundscale_round_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], i16 noundef zeroext [[__U:%.*]], <32 x half> noundef [[__C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store i16 [[__U]], ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i16, ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    [[CONV:%.*]] = zext i16 [[TMP2]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call <32 x half> @llvm.x86.avx512fp16.mask.rndscale.ph.512(<32 x half> [[TMP0]], i32 3, <32 x half> [[TMP1]], i32 [[CONV]], i32 8)
// CHECK-NEXT:    ret <32 x half> [[TMP3]]
//
__m512h test_mm512_mask_roundscale_round_ph(__m512h __A, __mmask16 __U, __m512h __C) {
  return _mm512_mask_roundscale_round_ph(__A, __U, __C, 3, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_maskz_roundscale_round_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], i16 noundef zeroext [[__U:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store i16 [[__U]], ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i16, ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    [[CONV:%.*]] = zext i16 [[TMP2]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call <32 x half> @llvm.x86.avx512fp16.mask.rndscale.ph.512(<32 x half> [[TMP0]], i32 3, <32 x half> [[TMP1]], i32 [[CONV]], i32 8)
// CHECK-NEXT:    ret <32 x half> [[TMP3]]
//
__m512h test_mm512_maskz_roundscale_round_ph(__m512h __A, __mmask16 __U) {
  return _mm512_maskz_roundscale_round_ph(__U, __A, 3, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_roundscale_round_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = call <32 x half> @llvm.x86.avx512fp16.mask.rndscale.ph.512(<32 x half> [[TMP0]], i32 3, <32 x half> zeroinitializer, i32 -1, i32 8)
// CHECK-NEXT:    ret <32 x half> [[TMP1]]
//
__m512h test_mm512_roundscale_round_ph(__m512h __A) {
  return _mm512_roundscale_round_ph(__A, 3, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_roundscale_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = call <32 x half> @llvm.x86.avx512fp16.mask.rndscale.ph.512(<32 x half> [[TMP0]], i32 3, <32 x half> [[TMP1]], i32 -1, i32 4)
// CHECK-NEXT:    ret <32 x half> [[TMP2]]
//
__m512h test_mm512_roundscale_ph(__m512h __A) {
  return _mm512_roundscale_ph(__A, 3);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_getexp_round_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = call <32 x half> @llvm.x86.avx512fp16.mask.getexp.ph.512(<32 x half> [[TMP0]], <32 x half> zeroinitializer, i32 -1, i32 8)
// CHECK-NEXT:    ret <32 x half> [[TMP1]]
//
__m512h test_mm512_getexp_round_ph(__m512h __A) {
  return _mm512_getexp_round_ph(__A, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask_getexp_round_ph(
// CHECK-SAME: <32 x half> noundef [[__W:%.*]], i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__W]], ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = call <32 x half> @llvm.x86.avx512fp16.mask.getexp.ph.512(<32 x half> [[TMP0]], <32 x half> [[TMP1]], i32 [[TMP2]], i32 8)
// CHECK-NEXT:    ret <32 x half> [[TMP3]]
//
__m512h test_mm512_mask_getexp_round_ph(__m512h __W, __mmask32 __U, __m512h __A) {
  return _mm512_mask_getexp_round_ph(__W, __U, __A, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_maskz_getexp_round_ph(
// CHECK-SAME: i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = call <32 x half> @llvm.x86.avx512fp16.mask.getexp.ph.512(<32 x half> [[TMP0]], <32 x half> [[TMP1]], i32 [[TMP2]], i32 8)
// CHECK-NEXT:    ret <32 x half> [[TMP3]]
//
__m512h test_mm512_maskz_getexp_round_ph(__mmask32 __U, __m512h __A) {
  return _mm512_maskz_getexp_round_ph(__U, __A, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_getexp_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = call <32 x half> @llvm.x86.avx512fp16.mask.getexp.ph.512(<32 x half> [[TMP1]], <32 x half> zeroinitializer, i32 -1, i32 4)
// CHECK-NEXT:    ret <32 x half> [[TMP2]]
//
__m512h test_mm512_getexp_ph(__m512h __A) {
  return _mm512_getexp_ph(__A);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask_getexp_ph(
// CHECK-SAME: <32 x half> noundef [[__W:%.*]], i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__W]], ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = load i32, ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP6:%.*]] = call <32 x half> @llvm.x86.avx512fp16.mask.getexp.ph.512(<32 x half> [[TMP3]], <32 x half> [[TMP4]], i32 [[TMP5]], i32 4)
// CHECK-NEXT:    ret <32 x half> [[TMP6]]
//
__m512h test_mm512_mask_getexp_ph(__m512h __W, __mmask32 __U, __m512h __A) {
  return _mm512_mask_getexp_ph(__W, __U, __A);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_maskz_getexp_ph(
// CHECK-SAME: i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store i32 [[TMP0]], ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    store <32 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load i32, ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP5:%.*]] = call <32 x half> @llvm.x86.avx512fp16.mask.getexp.ph.512(<32 x half> [[TMP2]], <32 x half> [[TMP3]], i32 [[TMP4]], i32 4)
// CHECK-NEXT:    ret <32 x half> [[TMP5]]
//
__m512h test_mm512_maskz_getexp_ph(__mmask32 __U, __m512h __A) {
  return _mm512_maskz_getexp_ph(__U, __A);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask_reduce_ph(
// CHECK-SAME: <32 x half> noundef [[__W:%.*]], i16 noundef zeroext [[__U:%.*]], <32 x half> noundef [[__A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__W]], ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    store i16 [[__U]], ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i16, ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    [[CONV:%.*]] = zext i16 [[TMP2]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call <32 x half> @llvm.x86.avx512fp16.mask.reduce.ph.512(<32 x half> [[TMP0]], i32 1, <32 x half> [[TMP1]], i32 [[CONV]], i32 4)
// CHECK-NEXT:    ret <32 x half> [[TMP3]]
//
__m512h test_mm512_mask_reduce_ph(__m512h __W, __mmask16 __U, __m512h __A) {
  return _mm512_mask_reduce_ph(__W, __U, __A, 1);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_maskz_reduce_ph(
// CHECK-SAME: i16 noundef zeroext [[__U:%.*]], <32 x half> noundef [[__A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i16 [[__U]], ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i16, ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    [[CONV:%.*]] = zext i16 [[TMP2]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call <32 x half> @llvm.x86.avx512fp16.mask.reduce.ph.512(<32 x half> [[TMP0]], i32 1, <32 x half> [[TMP1]], i32 [[CONV]], i32 4)
// CHECK-NEXT:    ret <32 x half> [[TMP3]]
//
__m512h test_mm512_maskz_reduce_ph(__mmask16 __U, __m512h __A) {
  return _mm512_maskz_reduce_ph(__U, __A, 1);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask_reduce_round_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], i16 noundef zeroext [[__U:%.*]], <32 x half> noundef [[__C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store i16 [[__U]], ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i16, ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    [[CONV:%.*]] = zext i16 [[TMP2]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call <32 x half> @llvm.x86.avx512fp16.mask.reduce.ph.512(<32 x half> [[TMP0]], i32 3, <32 x half> [[TMP1]], i32 [[CONV]], i32 8)
// CHECK-NEXT:    ret <32 x half> [[TMP3]]
//
__m512h test_mm512_mask_reduce_round_ph(__m512h __A, __mmask16 __U, __m512h __C) {
  return _mm512_mask_reduce_round_ph(__A, __U, __C, 3, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_maskz_reduce_round_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], i16 noundef zeroext [[__U:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store i16 [[__U]], ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i16, ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    [[CONV:%.*]] = zext i16 [[TMP2]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = call <32 x half> @llvm.x86.avx512fp16.mask.reduce.ph.512(<32 x half> [[TMP0]], i32 3, <32 x half> [[TMP1]], i32 [[CONV]], i32 8)
// CHECK-NEXT:    ret <32 x half> [[TMP3]]
//
__m512h test_mm512_maskz_reduce_round_ph(__m512h __A, __mmask16 __U) {
  return _mm512_maskz_reduce_round_ph(__U, __A, 3, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_reduce_round_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = call <32 x half> @llvm.x86.avx512fp16.mask.reduce.ph.512(<32 x half> [[TMP0]], i32 3, <32 x half> zeroinitializer, i32 -1, i32 8)
// CHECK-NEXT:    ret <32 x half> [[TMP1]]
//
__m512h test_mm512_reduce_round_ph(__m512h __A) {
  return _mm512_reduce_round_ph(__A, 3, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_reduce_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = call <32 x half> @llvm.x86.avx512fp16.mask.reduce.ph.512(<32 x half> [[TMP0]], i32 3, <32 x half> zeroinitializer, i32 -1, i32 4)
// CHECK-NEXT:    ret <32 x half> [[TMP1]]
//
__m512h test_mm512_reduce_ph(__m512h __A) {
  return _mm512_reduce_ph(__A, 3);
}
// CHECK-LABEL: define dso_local <8 x half> @test_mm_rcp_sh(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.rcp.sh(<8 x half> [[TMP2]], <8 x half> [[TMP3]], <8 x half> [[TMP4]], i8 -1)
// CHECK-NEXT:    ret <8 x half> [[TMP5]]
//
__m128h test_mm_rcp_sh(__m128h __A, __m128h __B) {
  return _mm_rcp_sh(__A, __B);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask_rcp_sh(
// CHECK-SAME: <8 x half> noundef [[__W:%.*]], i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__W]], ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    store i8 [[TMP1]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP3]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = load <8 x half>, ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP7:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP8:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.rcp.sh(<8 x half> [[TMP4]], <8 x half> [[TMP5]], <8 x half> [[TMP6]], i8 [[TMP7]])
// CHECK-NEXT:    ret <8 x half> [[TMP8]]
//
__m128h test_mm_mask_rcp_sh(__m128h __W, __mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_mask_rcp_sh(__W, __U, __A, __B);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_maskz_rcp_sh(
// CHECK-SAME: i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store i8 [[TMP0]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP7:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.rcp.sh(<8 x half> [[TMP3]], <8 x half> [[TMP4]], <8 x half> [[TMP5]], i8 [[TMP6]])
// CHECK-NEXT:    ret <8 x half> [[TMP7]]
//
__m128h test_mm_maskz_rcp_sh(__mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_maskz_rcp_sh(__U, __A, __B);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_rsqrt_sh(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.rsqrt.sh(<8 x half> [[TMP2]], <8 x half> [[TMP3]], <8 x half> [[TMP4]], i8 -1)
// CHECK-NEXT:    ret <8 x half> [[TMP5]]
//
__m128h test_mm_rsqrt_sh(__m128h __A, __m128h __B) {
  return _mm_rsqrt_sh(__A, __B);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask_rsqrt_sh(
// CHECK-SAME: <8 x half> noundef [[__W:%.*]], i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__W]], ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    store i8 [[TMP1]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP3]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = load <8 x half>, ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP7:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP8:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.rsqrt.sh(<8 x half> [[TMP4]], <8 x half> [[TMP5]], <8 x half> [[TMP6]], i8 [[TMP7]])
// CHECK-NEXT:    ret <8 x half> [[TMP8]]
//
__m128h test_mm_mask_rsqrt_sh(__m128h __W, __mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_mask_rsqrt_sh(__W, __U, __A, __B);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_maskz_rsqrt_sh(
// CHECK-SAME: i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store i8 [[TMP0]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP7:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.rsqrt.sh(<8 x half> [[TMP3]], <8 x half> [[TMP4]], <8 x half> [[TMP5]], i8 [[TMP6]])
// CHECK-NEXT:    ret <8 x half> [[TMP7]]
//
__m128h test_mm_maskz_rsqrt_sh(__mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_maskz_rsqrt_sh(__U, __A, __B);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_getmant_round_sh(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.getmant.sh(<8 x half> [[TMP0]], <8 x half> [[TMP1]], i32 0, <8 x half> [[TMP2]], i8 -1, i32 8)
// CHECK-NEXT:    ret <8 x half> [[TMP3]]
//
__m128h test_mm_getmant_round_sh(__m128h __A, __m128h __B) {
  return _mm_getmant_round_sh(__A, __B, _MM_MANT_NORM_1_2, _MM_MANT_SIGN_src, 8);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_getmant_sh(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.getmant.sh(<8 x half> [[TMP0]], <8 x half> [[TMP1]], i32 0, <8 x half> [[TMP2]], i8 -1, i32 4)
// CHECK-NEXT:    ret <8 x half> [[TMP3]]
//
__m128h test_mm_getmant_sh(__m128h __A, __m128h __B) {
  return _mm_getmant_sh(__A, __B, _MM_MANT_NORM_1_2, _MM_MANT_SIGN_src);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask_getmant_sh(
// CHECK-SAME: <8 x half> noundef [[__W:%.*]], i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__W]], ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP4:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.getmant.sh(<8 x half> [[TMP0]], <8 x half> [[TMP1]], i32 9, <8 x half> [[TMP2]], i8 [[TMP3]], i32 4)
// CHECK-NEXT:    ret <8 x half> [[TMP4]]
//
__m128h test_mm_mask_getmant_sh(__m128h __W, __mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_mask_getmant_sh(__W, __U, __A, __B, 1, 2);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask_getmant_round_sh(
// CHECK-SAME: <8 x half> noundef [[__W:%.*]], i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__W]], ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP4:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.getmant.sh(<8 x half> [[TMP0]], <8 x half> [[TMP1]], i32 9, <8 x half> [[TMP2]], i8 [[TMP3]], i32 8)
// CHECK-NEXT:    ret <8 x half> [[TMP4]]
//
__m128h test_mm_mask_getmant_round_sh(__m128h __W, __mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_mask_getmant_round_sh(__W, __U, __A, __B, 1, 2, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_maskz_getmant_sh(
// CHECK-SAME: i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP4:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.getmant.sh(<8 x half> [[TMP0]], <8 x half> [[TMP1]], i32 9, <8 x half> [[TMP2]], i8 [[TMP3]], i32 4)
// CHECK-NEXT:    ret <8 x half> [[TMP4]]
//
__m128h test_mm_maskz_getmant_sh(__mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_maskz_getmant_sh(__U, __A, __B, 1, 2);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_maskz_getmant_round_sh(
// CHECK-SAME: i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP4:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.getmant.sh(<8 x half> [[TMP0]], <8 x half> [[TMP1]], i32 9, <8 x half> [[TMP2]], i8 [[TMP3]], i32 8)
// CHECK-NEXT:    ret <8 x half> [[TMP4]]
//
__m128h test_mm_maskz_getmant_round_sh(__mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_maskz_getmant_round_sh(__U, __A, __B, 1, 2, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_getexp_round_sh(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.getexp.sh(<8 x half> [[TMP0]], <8 x half> [[TMP1]], <8 x half> [[TMP2]], i8 -1, i32 8)
// CHECK-NEXT:    ret <8 x half> [[TMP3]]
//
__m128h test_mm_getexp_round_sh(__m128h __A, __m128h __B) {
  return _mm_getexp_round_sh(__A, __B, 8);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_getexp_sh(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.getexp.sh(<8 x half> [[TMP2]], <8 x half> [[TMP3]], <8 x half> [[TMP4]], i8 -1, i32 4)
// CHECK-NEXT:    ret <8 x half> [[TMP5]]
//
__m128h test_mm_getexp_sh(__m128h __A, __m128h __B) {
  return _mm_getexp_sh(__A, __B);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask_getexp_sh(
// CHECK-SAME: <8 x half> noundef [[__W:%.*]], i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__W]], ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    store i8 [[TMP1]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP3]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = load <8 x half>, ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP7:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP8:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.getexp.sh(<8 x half> [[TMP4]], <8 x half> [[TMP5]], <8 x half> [[TMP6]], i8 [[TMP7]], i32 4)
// CHECK-NEXT:    ret <8 x half> [[TMP8]]
//
__m128h test_mm_mask_getexp_sh(__m128h __W, __mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_mask_getexp_sh(__W, __U, __A, __B);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask_getexp_round_sh(
// CHECK-SAME: <8 x half> noundef [[__W:%.*]], i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__W]], ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP4:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.getexp.sh(<8 x half> [[TMP0]], <8 x half> [[TMP1]], <8 x half> [[TMP2]], i8 [[TMP3]], i32 8)
// CHECK-NEXT:    ret <8 x half> [[TMP4]]
//
__m128h test_mm_mask_getexp_round_sh(__m128h __W, __mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_mask_getexp_round_sh(__W, __U, __A, __B, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_maskz_getexp_sh(
// CHECK-SAME: i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store i8 [[TMP0]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP7:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.getexp.sh(<8 x half> [[TMP3]], <8 x half> [[TMP4]], <8 x half> [[TMP5]], i8 [[TMP6]], i32 4)
// CHECK-NEXT:    ret <8 x half> [[TMP7]]
//
__m128h test_mm_maskz_getexp_sh(__mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_maskz_getexp_sh(__U, __A, __B);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_maskz_getexp_round_sh(
// CHECK-SAME: i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP4:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.getexp.sh(<8 x half> [[TMP0]], <8 x half> [[TMP1]], <8 x half> [[TMP2]], i8 [[TMP3]], i32 8)
// CHECK-NEXT:    ret <8 x half> [[TMP4]]
//
__m128h test_mm_maskz_getexp_round_sh(__mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_maskz_getexp_round_sh(__U, __A, __B, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_scalef_round_sh(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.scalef.sh(<8 x half> [[TMP0]], <8 x half> [[TMP1]], <8 x half> [[TMP2]], i8 -1, i32 11)
// CHECK-NEXT:    ret <8 x half> [[TMP3]]
//
__m128h test_mm_scalef_round_sh(__m128h __A, __m128h __B) {
  return _mm_scalef_round_sh(__A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_scalef_sh(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.scalef.sh(<8 x half> [[TMP2]], <8 x half> [[TMP3]], <8 x half> [[TMP4]], i8 -1, i32 4)
// CHECK-NEXT:    ret <8 x half> [[TMP5]]
//
__m128h test_mm_scalef_sh(__m128h __A, __m128h __B) {
  return _mm_scalef_sh(__A, __B);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask_scalef_sh(
// CHECK-SAME: <8 x half> noundef [[__W:%.*]], i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__W]], ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    store i8 [[TMP1]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP3]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = load <8 x half>, ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP7:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP8:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.scalef.sh(<8 x half> [[TMP4]], <8 x half> [[TMP5]], <8 x half> [[TMP6]], i8 [[TMP7]], i32 4)
// CHECK-NEXT:    ret <8 x half> [[TMP8]]
//
__m128h test_mm_mask_scalef_sh(__m128h __W, __mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_mask_scalef_sh(__W, __U, __A, __B);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask_scalef_round_sh(
// CHECK-SAME: <8 x half> noundef [[__W:%.*]], i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__W]], ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP4:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.scalef.sh(<8 x half> [[TMP0]], <8 x half> [[TMP1]], <8 x half> [[TMP2]], i8 [[TMP3]], i32 11)
// CHECK-NEXT:    ret <8 x half> [[TMP4]]
//
__m128h test_mm_mask_scalef_round_sh(__m128h __W, __mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_mask_scalef_round_sh(__W, __U, __A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_maskz_scalef_sh(
// CHECK-SAME: i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store i8 [[TMP0]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP7:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.scalef.sh(<8 x half> [[TMP3]], <8 x half> [[TMP4]], <8 x half> [[TMP5]], i8 [[TMP6]], i32 4)
// CHECK-NEXT:    ret <8 x half> [[TMP7]]
//
__m128h test_mm_maskz_scalef_sh(__mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_maskz_scalef_sh(__U, __A, __B);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_maskz_scalef_round_sh(
// CHECK-SAME: i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP4:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.scalef.sh(<8 x half> [[TMP0]], <8 x half> [[TMP1]], <8 x half> [[TMP2]], i8 [[TMP3]], i32 11)
// CHECK-NEXT:    ret <8 x half> [[TMP4]]
//
__m128h test_mm_maskz_scalef_round_sh(__mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_maskz_scalef_round_sh(__U, __A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_roundscale_round_sh(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.rndscale.sh(<8 x half> [[TMP0]], <8 x half> [[TMP1]], <8 x half> [[TMP2]], i8 -1, i32 3, i32 8)
// CHECK-NEXT:    ret <8 x half> [[TMP3]]
//
__m128h test_mm_roundscale_round_sh(__m128h __A, __m128h __B) {
  return _mm_roundscale_round_sh(__A, __B, 3, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_roundscale_sh(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.rndscale.sh(<8 x half> [[TMP0]], <8 x half> [[TMP1]], <8 x half> [[TMP2]], i8 -1, i32 3, i32 4)
// CHECK-NEXT:    ret <8 x half> [[TMP3]]
//
__m128h test_mm_roundscale_sh(__m128h __A, __m128h __B) {
  return _mm_roundscale_sh(__A, __B, 3);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask_roundscale_sh(
// CHECK-SAME: <8 x half> noundef [[__W:%.*]], i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__W]], ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP4:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.rndscale.sh(<8 x half> [[TMP0]], <8 x half> [[TMP1]], <8 x half> [[TMP2]], i8 [[TMP3]], i32 3, i32 4)
// CHECK-NEXT:    ret <8 x half> [[TMP4]]
//
__m128h test_mm_mask_roundscale_sh(__m128h __W, __mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_mask_roundscale_sh(__W, __U, __A, __B, 3);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask_roundscale_round_sh(
// CHECK-SAME: <8 x half> noundef [[__W:%.*]], i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__W]], ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP4:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.rndscale.sh(<8 x half> [[TMP0]], <8 x half> [[TMP1]], <8 x half> [[TMP2]], i8 [[TMP3]], i32 3, i32 8)
// CHECK-NEXT:    ret <8 x half> [[TMP4]]
//
__m128h test_mm_mask_roundscale_round_sh(__m128h __W, __mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_mask_roundscale_round_sh(__W, __U, __A, __B, 3, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_maskz_roundscale_round_sh(
// CHECK-SAME: i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP4:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.rndscale.sh(<8 x half> [[TMP0]], <8 x half> [[TMP1]], <8 x half> [[TMP2]], i8 [[TMP3]], i32 3, i32 8)
// CHECK-NEXT:    ret <8 x half> [[TMP4]]
//
__m128h test_mm_maskz_roundscale_round_sh(__mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_maskz_roundscale_round_sh(__U, __A, __B, 3, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_maskz_roundscale_sh(
// CHECK-SAME: i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP4:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.rndscale.sh(<8 x half> [[TMP0]], <8 x half> [[TMP1]], <8 x half> [[TMP2]], i8 [[TMP3]], i32 3, i32 4)
// CHECK-NEXT:    ret <8 x half> [[TMP4]]
//
__m128h test_mm_maskz_roundscale_sh(__mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_maskz_roundscale_sh(__U, __A, __B, 3);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_reduce_sh(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.reduce.sh(<8 x half> [[TMP0]], <8 x half> [[TMP1]], <8 x half> [[TMP2]], i8 -1, i32 4, i32 4)
// CHECK-NEXT:    ret <8 x half> [[TMP3]]
//
__m128h test_mm_reduce_sh(__m128h __A, __m128h __B) {
  return _mm_reduce_sh(__A, __B, 4);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask_reduce_sh(
// CHECK-SAME: <8 x half> noundef [[__W:%.*]], i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__W]], ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP4:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.reduce.sh(<8 x half> [[TMP0]], <8 x half> [[TMP1]], <8 x half> [[TMP2]], i8 [[TMP3]], i32 4, i32 4)
// CHECK-NEXT:    ret <8 x half> [[TMP4]]
//
__m128h test_mm_mask_reduce_sh(__m128h __W, __mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_mask_reduce_sh(__W, __U, __A, __B, 4);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_maskz_reduce_sh(
// CHECK-SAME: i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP4:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.reduce.sh(<8 x half> [[TMP0]], <8 x half> [[TMP1]], <8 x half> [[TMP2]], i8 [[TMP3]], i32 4, i32 4)
// CHECK-NEXT:    ret <8 x half> [[TMP4]]
//
__m128h test_mm_maskz_reduce_sh(__mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_maskz_reduce_sh(__U, __A, __B, 4);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_reduce_round_sh(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.reduce.sh(<8 x half> [[TMP0]], <8 x half> [[TMP1]], <8 x half> [[TMP2]], i8 -1, i32 4, i32 8)
// CHECK-NEXT:    ret <8 x half> [[TMP3]]
//
__m128h test_mm_reduce_round_sh(__m128h __A, __m128h __B) {
  return _mm_reduce_round_sh(__A, __B, 4, 8);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask_reduce_round_sh(
// CHECK-SAME: <8 x half> noundef [[__W:%.*]], i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__W]], ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP4:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.reduce.sh(<8 x half> [[TMP0]], <8 x half> [[TMP1]], <8 x half> [[TMP2]], i8 [[TMP3]], i32 4, i32 8)
// CHECK-NEXT:    ret <8 x half> [[TMP4]]
//
__m128h test_mm_mask_reduce_round_sh(__m128h __W, __mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_mask_reduce_round_sh(__W, __U, __A, __B, 4, 8);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_maskz_reduce_round_sh(
// CHECK-SAME: i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP4:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.reduce.sh(<8 x half> [[TMP0]], <8 x half> [[TMP1]], <8 x half> [[TMP2]], i8 [[TMP3]], i32 4, i32 8)
// CHECK-NEXT:    ret <8 x half> [[TMP4]]
//
__m128h test_mm_maskz_reduce_round_sh(__mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_maskz_reduce_round_sh(__U, __A, __B, 4, 8);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_sqrt_round_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = call <32 x half> @llvm.x86.avx512fp16.sqrt.ph.512(<32 x half> [[TMP0]], i32 11)
// CHECK-NEXT:    ret <32 x half> [[TMP1]]
//
__m512h test_mm512_sqrt_round_ph(__m512h __A) {
  return _mm512_sqrt_round_ph(__A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask_sqrt_round_ph(
// CHECK-SAME: <32 x half> noundef [[__W:%.*]], i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__W]], ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = call <32 x half> @llvm.x86.avx512fp16.sqrt.ph.512(<32 x half> [[TMP1]], i32 11)
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast i32 [[TMP0]] to <32 x i1>
// CHECK-NEXT:    [[TMP5:%.*]] = select <32 x i1> [[TMP4]], <32 x half> [[TMP2]], <32 x half> [[TMP3]]
// CHECK-NEXT:    ret <32 x half> [[TMP5]]
//
__m512h test_mm512_mask_sqrt_round_ph(__m512h __W, __mmask32 __U, __m512h __A) {
  return _mm512_mask_sqrt_round_ph(__W, __U, __A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_maskz_sqrt_round_ph(
// CHECK-SAME: i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = call <32 x half> @llvm.x86.avx512fp16.sqrt.ph.512(<32 x half> [[TMP1]], i32 11)
// CHECK-NEXT:    store <32 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast i32 [[TMP0]] to <32 x i1>
// CHECK-NEXT:    [[TMP5:%.*]] = select <32 x i1> [[TMP4]], <32 x half> [[TMP2]], <32 x half> [[TMP3]]
// CHECK-NEXT:    ret <32 x half> [[TMP5]]
//
__m512h test_mm512_maskz_sqrt_round_ph(__mmask32 __U, __m512h __A) {
  return _mm512_maskz_sqrt_round_ph(__U, __A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_sqrt_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = call <32 x half> @llvm.sqrt.v32f16(<32 x half> [[TMP1]])
// CHECK-NEXT:    ret <32 x half> [[TMP2]]
//
__m512h test_mm512_sqrt_ph(__m512h __A) {
  return _mm512_sqrt_ph(__A);
}
// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask_sqrt_ph(
// CHECK-SAME: <32 x half> noundef [[__W:%.*]], i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__W]], ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = call <32 x half> @llvm.sqrt.v32f16(<32 x half> [[TMP4]])
// CHECK-NEXT:    [[TMP6:%.*]] = load <32 x half>, ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast i32 [[TMP3]] to <32 x i1>
// CHECK-NEXT:    [[TMP8:%.*]] = select <32 x i1> [[TMP7]], <32 x half> [[TMP5]], <32 x half> [[TMP6]]
// CHECK-NEXT:    ret <32 x half> [[TMP8]]
//
__m512h test_mm512_mask_sqrt_ph(__m512h __W, __mmask32 __U, __m512h __A) {
  return _mm512_mask_sqrt_ph(__W, __U, __A);
}
// CHECK-LABEL: define dso_local <32 x half> @test_mm512_maskz_sqrt_ph(
// CHECK-SAME: i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store i32 [[TMP0]], ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    store <32 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = call <32 x half> @llvm.sqrt.v32f16(<32 x half> [[TMP3]])
// CHECK-NEXT:    store <32 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = load <32 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast i32 [[TMP2]] to <32 x i1>
// CHECK-NEXT:    [[TMP7:%.*]] = select <32 x i1> [[TMP6]], <32 x half> [[TMP4]], <32 x half> [[TMP5]]
// CHECK-NEXT:    ret <32 x half> [[TMP7]]
//
__m512h test_mm512_maskz_sqrt_ph(__mmask32 __U, __m512h __A) {
  return _mm512_maskz_sqrt_ph(__U, __A);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_sqrt_round_sh(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.sqrt.sh(<8 x half> [[TMP0]], <8 x half> [[TMP1]], <8 x half> [[TMP2]], i8 -1, i32 11)
// CHECK-NEXT:    ret <8 x half> [[TMP3]]
//
__m128h test_mm_sqrt_round_sh(__m128h __A, __m128h __B) {
  return _mm_sqrt_round_sh(__A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask_sqrt_round_sh(
// CHECK-SAME: <8 x half> noundef [[__W:%.*]], i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__W]], ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP4:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.sqrt.sh(<8 x half> [[TMP0]], <8 x half> [[TMP1]], <8 x half> [[TMP2]], i8 [[TMP3]], i32 11)
// CHECK-NEXT:    ret <8 x half> [[TMP4]]
//
__m128h test_mm_mask_sqrt_round_sh(__m128h __W, __mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_mask_sqrt_round_sh(__W, __U, __A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_maskz_sqrt_round_sh(
// CHECK-SAME: i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP4:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.sqrt.sh(<8 x half> [[TMP0]], <8 x half> [[TMP1]], <8 x half> [[TMP2]], i8 [[TMP3]], i32 11)
// CHECK-NEXT:    ret <8 x half> [[TMP4]]
//
__m128h test_mm_maskz_sqrt_round_sh(__mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_maskz_sqrt_round_sh(__U, __A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_sqrt_sh(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = extractelement <8 x half> [[TMP3]], i64 0
// CHECK-NEXT:    [[TMP6:%.*]] = call half @llvm.sqrt.f16(half [[TMP5]])
// CHECK-NEXT:    [[TMP7:%.*]] = extractelement <8 x half> [[TMP4]], i64 0
// CHECK-NEXT:    [[TMP8:%.*]] = insertelement <8 x half> [[TMP2]], half [[TMP6]], i64 0
// CHECK-NEXT:    ret <8 x half> [[TMP8]]
//
__m128h test_mm_sqrt_sh(__m128h __A, __m128h __B) {
  return _mm_sqrt_sh(__A, __B);
}
// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask_sqrt_sh(
// CHECK-SAME: <8 x half> noundef [[__W:%.*]], i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__W]], ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[CONV:%.*]] = zext i8 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    store i32 [[CONV]], ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP3]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = load <8 x half>, ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP7:%.*]] = load i32, ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[CONV_I:%.*]] = trunc i32 [[TMP7]] to i8
// CHECK-NEXT:    [[TMP8:%.*]] = extractelement <8 x half> [[TMP5]], i64 0
// CHECK-NEXT:    [[TMP9:%.*]] = call half @llvm.sqrt.f16(half [[TMP8]])
// CHECK-NEXT:    [[TMP10:%.*]] = extractelement <8 x half> [[TMP6]], i64 0
// CHECK-NEXT:    [[TMP11:%.*]] = bitcast i8 [[CONV_I]] to <8 x i1>
// CHECK-NEXT:    [[TMP12:%.*]] = extractelement <8 x i1> [[TMP11]], i64 0
// CHECK-NEXT:    [[TMP13:%.*]] = select i1 [[TMP12]], half [[TMP9]], half [[TMP10]]
// CHECK-NEXT:    [[TMP14:%.*]] = insertelement <8 x half> [[TMP4]], half [[TMP13]], i64 0
// CHECK-NEXT:    ret <8 x half> [[TMP14]]
//
__m128h test_mm_mask_sqrt_sh(__m128h __W, __mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_mask_sqrt_sh(__W, __U, __A, __B);
}
// CHECK-LABEL: define dso_local <8 x half> @test_mm_maskz_sqrt_sh(
// CHECK-SAME: i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[CONV:%.*]] = zext i8 [[TMP0]] to i32
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store i32 [[CONV]], ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = load i32, ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[CONV_I:%.*]] = trunc i32 [[TMP6]] to i8
// CHECK-NEXT:    [[TMP7:%.*]] = extractelement <8 x half> [[TMP4]], i64 0
// CHECK-NEXT:    [[TMP8:%.*]] = call half @llvm.sqrt.f16(half [[TMP7]])
// CHECK-NEXT:    [[TMP9:%.*]] = extractelement <8 x half> [[TMP5]], i64 0
// CHECK-NEXT:    [[TMP10:%.*]] = bitcast i8 [[CONV_I]] to <8 x i1>
// CHECK-NEXT:    [[TMP11:%.*]] = extractelement <8 x i1> [[TMP10]], i64 0
// CHECK-NEXT:    [[TMP12:%.*]] = select i1 [[TMP11]], half [[TMP8]], half [[TMP9]]
// CHECK-NEXT:    [[TMP13:%.*]] = insertelement <8 x half> [[TMP3]], half [[TMP12]], i64 0
// CHECK-NEXT:    ret <8 x half> [[TMP13]]
//
__m128h test_mm_maskz_sqrt_sh(__mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_maskz_sqrt_sh(__U, __A, __B);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_mask_fpclass_ph_mask(
// CHECK-SAME: i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call <32 x i1> @llvm.x86.avx512fp16.fpclass.ph.512(<32 x half> [[TMP0]], i32 4)
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast i32 [[TMP1]] to <32 x i1>
// CHECK-NEXT:    [[TMP4:%.*]] = and <32 x i1> [[TMP2]], [[TMP3]]
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <32 x i1> [[TMP4]] to i32
// CHECK-NEXT:    ret i32 [[TMP5]]
//
__mmask32 test_mm512_mask_fpclass_ph_mask(__mmask32 __U, __m512h __A) {
  return _mm512_mask_fpclass_ph_mask(__U, __A, 4);
}

// CHECK-LABEL: define dso_local i32 @test_mm512_fpclass_ph_mask(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = call <32 x i1> @llvm.x86.avx512fp16.fpclass.ph.512(<32 x half> [[TMP0]], i32 4)
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <32 x i1> [[TMP1]] to i32
// CHECK-NEXT:    ret i32 [[TMP2]]
//
__mmask32 test_mm512_fpclass_ph_mask(__m512h __A) {
  return _mm512_fpclass_ph_mask(__A, 4);
}

// CHECK-LABEL: define dso_local zeroext i8 @test_mm_fpclash_sh_mask(
// CHECK-SAME: <4 x float> noundef [[__A:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <4 x float>, align 16
// CHECK-NEXT:    store <4 x float> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <4 x float> [[TMP0]] to <8 x half>
// CHECK-NEXT:    [[TMP2:%.*]] = call i8 @llvm.x86.avx512fp16.mask.fpclass.sh(<8 x half> [[TMP1]], i32 2, i8 -1)
// CHECK-NEXT:    ret i8 [[TMP2]]
//
__mmask8 test_mm_fpclash_sh_mask(__m128 __A) {
  return _mm_fpclass_sh_mask(__A, 2);
}

// CHECK-LABEL: define dso_local zeroext i8 @test_mm_mask_fpclash_sh_mask(
// CHECK-SAME: i8 noundef zeroext [[__U:%.*]], <4 x float> noundef [[__A:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <4 x float>, align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <4 x float> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <4 x float> [[TMP0]] to <8 x half>
// CHECK-NEXT:    [[TMP2:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP3:%.*]] = call i8 @llvm.x86.avx512fp16.mask.fpclass.sh(<8 x half> [[TMP1]], i32 2, i8 [[TMP2]])
// CHECK-NEXT:    ret i8 [[TMP3]]
//
__mmask8 test_mm_mask_fpclash_sh_mask(__mmask8 __U, __m128 __A) {
  return _mm_mask_fpclass_sh_mask(__U, __A, 2);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm512_cvt_roundpd_ph(
// CHECK-SAME: <8 x double> noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x double>, align 64
// CHECK-NEXT:    store <8 x double> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x double>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.vcvtpd2ph.512(<8 x double> [[TMP0]], <8 x half> zeroinitializer, i8 -1, i32 11)
// CHECK-NEXT:    ret <8 x half> [[TMP1]]
//
__m128h test_mm512_cvt_roundpd_ph(__m512d A) {
  return _mm512_cvt_roundpd_ph(A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm512_mask_cvt_roundpd_ph(
// CHECK-SAME: <8 x half> noundef [[A:%.*]], i8 noundef zeroext [[B:%.*]], <8 x double> noundef [[C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <8 x double>, align 64
// CHECK-NEXT:    store <8 x half> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store i8 [[B]], ptr [[B_ADDR]], align 1
// CHECK-NEXT:    store <8 x double> [[C]], ptr [[C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x double>, ptr [[C_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load i8, ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP3:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.vcvtpd2ph.512(<8 x double> [[TMP0]], <8 x half> [[TMP1]], i8 [[TMP2]], i32 11)
// CHECK-NEXT:    ret <8 x half> [[TMP3]]
//
__m128h test_mm512_mask_cvt_roundpd_ph(__m128h A, __mmask8 B, __m512d C) {
  return _mm512_mask_cvt_roundpd_ph(A, B, C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm512_maskz_cvt_roundpd_ph(
// CHECK-SAME: i8 noundef zeroext [[A:%.*]], <8 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <8 x double>, align 64
// CHECK-NEXT:    store i8 [[A]], ptr [[A_ADDR]], align 1
// CHECK-NEXT:    store <8 x double> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x double>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load i8, ptr [[A_ADDR]], align 1
// CHECK-NEXT:    [[TMP3:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.vcvtpd2ph.512(<8 x double> [[TMP0]], <8 x half> [[TMP1]], i8 [[TMP2]], i32 11)
// CHECK-NEXT:    ret <8 x half> [[TMP3]]
//
__m128h test_mm512_maskz_cvt_roundpd_ph(__mmask8 A, __m512d B) {
  return _mm512_maskz_cvt_roundpd_ph(A, B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm512_cvtpd_ph(
// CHECK-SAME: <8 x double> noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x double>, align 64
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x double>, align 64
// CHECK-NEXT:    store <8 x double> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x double>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <8 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x double>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.vcvtpd2ph.512(<8 x double> [[TMP1]], <8 x half> [[TMP2]], i8 -1, i32 4)
// CHECK-NEXT:    ret <8 x half> [[TMP3]]
//
__m128h test_mm512_cvtpd_ph(__m512d A) {
  return _mm512_cvtpd_ph(A);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm512_mask_cvtpd_ph(
// CHECK-SAME: <8 x half> noundef [[A:%.*]], i8 noundef zeroext [[B:%.*]], <8 x double> noundef [[C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x double>, align 64
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <8 x double>, align 64
// CHECK-NEXT:    store <8 x half> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store i8 [[B]], ptr [[B_ADDR]], align 1
// CHECK-NEXT:    store <8 x double> [[C]], ptr [[C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x double>, ptr [[C_ADDR]], align 64
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    store i8 [[TMP1]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x double> [[TMP2]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x double>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP6:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.vcvtpd2ph.512(<8 x double> [[TMP3]], <8 x half> [[TMP4]], i8 [[TMP5]], i32 4)
// CHECK-NEXT:    ret <8 x half> [[TMP6]]
//
__m128h test_mm512_mask_cvtpd_ph(__m128h A, __mmask8 B, __m512d C) {
  return _mm512_mask_cvtpd_ph(A, B, C);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm512_maskz_cvtpd_ph(
// CHECK-SAME: i8 noundef zeroext [[A:%.*]], <8 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x double>, align 64
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <8 x double>, align 64
// CHECK-NEXT:    store i8 [[A]], ptr [[A_ADDR]], align 1
// CHECK-NEXT:    store <8 x double> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load i8, ptr [[A_ADDR]], align 1
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x double>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    store i8 [[TMP0]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x double> [[TMP1]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x double>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP5:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.vcvtpd2ph.512(<8 x double> [[TMP2]], <8 x half> [[TMP3]], i8 [[TMP4]], i32 4)
// CHECK-NEXT:    ret <8 x half> [[TMP5]]
//
__m128h test_mm512_maskz_cvtpd_ph(__mmask8 A, __m512d B) {
  return _mm512_maskz_cvtpd_ph(A, B);
}

// CHECK-LABEL: define dso_local <8 x double> @test_mm512_cvt_roundph_pd(
// CHECK-SAME: <8 x half> noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = call <8 x double> @llvm.x86.avx512fp16.mask.vcvtph2pd.512(<8 x half> [[TMP0]], <8 x double> zeroinitializer, i8 -1, i32 8)
// CHECK-NEXT:    ret <8 x double> [[TMP1]]
//
__m512d test_mm512_cvt_roundph_pd(__m128h A) {
  return _mm512_cvt_roundph_pd(A, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x double> @test_mm512_mask_cvt_roundph_pd(
// CHECK-SAME: <8 x double> noundef [[A:%.*]], i8 noundef zeroext [[B:%.*]], <8 x half> noundef [[C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x double>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x double> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store i8 [[B]], ptr [[B_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[C]], ptr [[C_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[C_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x double>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i8, ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP3:%.*]] = call <8 x double> @llvm.x86.avx512fp16.mask.vcvtph2pd.512(<8 x half> [[TMP0]], <8 x double> [[TMP1]], i8 [[TMP2]], i32 8)
// CHECK-NEXT:    ret <8 x double> [[TMP3]]
//
__m512d test_mm512_mask_cvt_roundph_pd(__m512d A, __mmask8 B, __m128h C) {
  return _mm512_mask_cvt_roundph_pd(A, B, C, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x double> @test_mm512_maskz_cvt_roundph_pd(
// CHECK-SAME: i8 noundef zeroext [[A:%.*]], <8 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x double>, align 64
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[A]], ptr [[A_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[B]], ptr [[B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[B_ADDR]], align 16
// CHECK-NEXT:    store <8 x double> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x double>, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i8, ptr [[A_ADDR]], align 1
// CHECK-NEXT:    [[TMP3:%.*]] = call <8 x double> @llvm.x86.avx512fp16.mask.vcvtph2pd.512(<8 x half> [[TMP0]], <8 x double> [[TMP1]], i8 [[TMP2]], i32 8)
// CHECK-NEXT:    ret <8 x double> [[TMP3]]
//
__m512d test_mm512_maskz_cvt_roundph_pd(__mmask8 A, __m128h B) {
  return _mm512_maskz_cvt_roundph_pd(A, B, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x double> @test_mm512_cvtph_pd(
// CHECK-SAME: <8 x half> noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x double>, align 64
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x double> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x double>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = call <8 x double> @llvm.x86.avx512fp16.mask.vcvtph2pd.512(<8 x half> [[TMP1]], <8 x double> [[TMP2]], i8 -1, i32 4)
// CHECK-NEXT:    ret <8 x double> [[TMP3]]
//
__m512d test_mm512_cvtph_pd(__m128h A) {
  return _mm512_cvtph_pd(A);
}

// CHECK-LABEL: define dso_local <8 x double> @test_mm512_mask_cvtph_pd(
// CHECK-SAME: <8 x double> noundef [[A:%.*]], i8 noundef zeroext [[B:%.*]], <8 x half> noundef [[C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <8 x double>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x double>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x double> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store i8 [[B]], ptr [[B_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[C]], ptr [[C_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x double>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[C_ADDR]], align 16
// CHECK-NEXT:    store <8 x double> [[TMP0]], ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    store i8 [[TMP1]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x double>, ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP6:%.*]] = call <8 x double> @llvm.x86.avx512fp16.mask.vcvtph2pd.512(<8 x half> [[TMP3]], <8 x double> [[TMP4]], i8 [[TMP5]], i32 4)
// CHECK-NEXT:    ret <8 x double> [[TMP6]]
//
__m512d test_mm512_mask_cvtph_pd(__m512d A, __mmask8 B, __m128h C) {
  return _mm512_mask_cvtph_pd(A, B, C);
}

// CHECK-LABEL: define dso_local <8 x double> @test_mm512_maskz_cvtph_pd(
// CHECK-SAME: i8 noundef zeroext [[A:%.*]], <8 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x double>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[A]], ptr [[A_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[B]], ptr [[B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load i8, ptr [[A_ADDR]], align 1
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[B_ADDR]], align 16
// CHECK-NEXT:    store i8 [[TMP0]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x double> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x double>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP5:%.*]] = call <8 x double> @llvm.x86.avx512fp16.mask.vcvtph2pd.512(<8 x half> [[TMP2]], <8 x double> [[TMP3]], i8 [[TMP4]], i32 4)
// CHECK-NEXT:    ret <8 x double> [[TMP5]]
//
__m512d test_mm512_maskz_cvtph_pd(__mmask8 A, __m128h B) {
  return _mm512_maskz_cvtph_pd(A, B);
}

// CHECK-LABEL: define dso_local <4 x float> @test_mm_cvt_roundsh_ss(
// CHECK-SAME: <4 x float> noundef [[A:%.*]], <8 x half> noundef [[B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[B]], ptr [[B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[B_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = call <4 x float> @llvm.x86.avx512fp16.mask.vcvtsh2ss.round(<4 x float> [[TMP0]], <8 x half> [[TMP1]], <4 x float> zeroinitializer, i8 -1, i32 8)
// CHECK-NEXT:    ret <4 x float> [[TMP2]]
//
__m128 test_mm_cvt_roundsh_ss(__m128 A, __m128h B) {
  return _mm_cvt_roundsh_ss(A, B, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <4 x float> @test_mm_mask_cvt_roundsh_ss(
// CHECK-SAME: <4 x float> noundef [[A:%.*]], i8 noundef zeroext [[B:%.*]], <4 x float> noundef [[C:%.*]], <8 x half> noundef [[D:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <4 x float>, align 16
// CHECK-NEXT:    [[D_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store i8 [[B]], ptr [[B_ADDR]], align 1
// CHECK-NEXT:    store <4 x float> [[C]], ptr [[C_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[D]], ptr [[D_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[C_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[D_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load i8, ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP4:%.*]] = call <4 x float> @llvm.x86.avx512fp16.mask.vcvtsh2ss.round(<4 x float> [[TMP0]], <8 x half> [[TMP1]], <4 x float> [[TMP2]], i8 [[TMP3]], i32 8)
// CHECK-NEXT:    ret <4 x float> [[TMP4]]
//
__m128 test_mm_mask_cvt_roundsh_ss(__m128 A, __mmask8 B, __m128 C, __m128h D) {
  return _mm_mask_cvt_roundsh_ss(A, B, C, D, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <4 x float> @test_mm_maskz_cvt_roundsh_ss(
// CHECK-SAME: i8 noundef zeroext [[A:%.*]], <4 x float> noundef [[B:%.*]], <8 x half> noundef [[C:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <4 x float>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <4 x float>, align 16
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[A]], ptr [[A_ADDR]], align 1
// CHECK-NEXT:    store <4 x float> [[B]], ptr [[B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[C]], ptr [[C_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[B_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[C_ADDR]], align 16
// CHECK-NEXT:    store <4 x float> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <4 x float>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load i8, ptr [[A_ADDR]], align 1
// CHECK-NEXT:    [[TMP4:%.*]] = call <4 x float> @llvm.x86.avx512fp16.mask.vcvtsh2ss.round(<4 x float> [[TMP0]], <8 x half> [[TMP1]], <4 x float> [[TMP2]], i8 [[TMP3]], i32 8)
// CHECK-NEXT:    ret <4 x float> [[TMP4]]
//
__m128 test_mm_maskz_cvt_roundsh_ss(__mmask8 A, __m128 B, __m128h C) {
  return _mm_maskz_cvt_roundsh_ss(A, B, C, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <4 x float> @test_mm_cvtsh_ss(
// CHECK-SAME: <4 x float> noundef [[A:%.*]], <8 x half> noundef [[B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x float>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[B]], ptr [[B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[B_ADDR]], align 16
// CHECK-NEXT:    store <4 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <4 x float>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = call <4 x float> @llvm.x86.avx512fp16.mask.vcvtsh2ss.round(<4 x float> [[TMP2]], <8 x half> [[TMP3]], <4 x float> zeroinitializer, i8 -1, i32 4)
// CHECK-NEXT:    ret <4 x float> [[TMP4]]
//
__m128 test_mm_cvtsh_ss(__m128 A, __m128h B) {
  return _mm_cvtsh_ss(A, B);
}

// CHECK-LABEL: define dso_local <4 x float> @test_mm_mask_cvtsh_ss(
// CHECK-SAME: <4 x float> noundef [[A:%.*]], i8 noundef zeroext [[B:%.*]], <4 x float> noundef [[C:%.*]], <8 x half> noundef [[D:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <4 x float>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x float>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <4 x float>, align 16
// CHECK-NEXT:    [[D_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store i8 [[B]], ptr [[B_ADDR]], align 1
// CHECK-NEXT:    store <4 x float> [[C]], ptr [[C_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[D]], ptr [[D_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = load <4 x float>, ptr [[C_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[D_ADDR]], align 16
// CHECK-NEXT:    store <4 x float> [[TMP0]], ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    store i8 [[TMP1]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <4 x float> [[TMP2]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP3]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <4 x float>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = load <4 x float>, ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP7:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP8:%.*]] = call <4 x float> @llvm.x86.avx512fp16.mask.vcvtsh2ss.round(<4 x float> [[TMP4]], <8 x half> [[TMP5]], <4 x float> [[TMP6]], i8 [[TMP7]], i32 4)
// CHECK-NEXT:    ret <4 x float> [[TMP8]]
//
__m128 test_mm_mask_cvtsh_ss(__m128 A, __mmask8 B, __m128 C, __m128h D) {
  return _mm_mask_cvtsh_ss(A, B, C, D);
}

// CHECK-LABEL: define dso_local <4 x float> @test_mm_maskz_cvtsh_ss(
// CHECK-SAME: i8 noundef zeroext [[A:%.*]], <4 x float> noundef [[B:%.*]], <8 x half> noundef [[C:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <4 x float>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x float>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <4 x float>, align 16
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[A]], ptr [[A_ADDR]], align 1
// CHECK-NEXT:    store <4 x float> [[B]], ptr [[B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[C]], ptr [[C_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load i8, ptr [[A_ADDR]], align 1
// CHECK-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[B_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[C_ADDR]], align 16
// CHECK-NEXT:    store i8 [[TMP0]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <4 x float> [[TMP1]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <4 x float>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    store <4 x float> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = load <4 x float>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP7:%.*]] = call <4 x float> @llvm.x86.avx512fp16.mask.vcvtsh2ss.round(<4 x float> [[TMP3]], <8 x half> [[TMP4]], <4 x float> [[TMP5]], i8 [[TMP6]], i32 4)
// CHECK-NEXT:    ret <4 x float> [[TMP7]]
//
__m128 test_mm_maskz_cvtsh_ss(__mmask8 A, __m128 B, __m128h C) {
  return _mm_maskz_cvtsh_ss(A, B, C);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_cvt_roundss_sh(
// CHECK-SAME: <8 x half> noundef [[A:%.*]], <4 x float> noundef [[B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <4 x float>, align 16
// CHECK-NEXT:    store <8 x half> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store <4 x float> [[B]], ptr [[B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[B_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.vcvtss2sh.round(<8 x half> [[TMP0]], <4 x float> [[TMP1]], <8 x half> zeroinitializer, i8 -1, i32 11)
// CHECK-NEXT:    ret <8 x half> [[TMP2]]
//
__m128h test_mm_cvt_roundss_sh(__m128h A, __m128 B) {
  return _mm_cvt_roundss_sh(A, B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask_cvt_roundss_sh(
// CHECK-SAME: <8 x half> noundef [[A:%.*]], i8 noundef zeroext [[B:%.*]], <8 x half> noundef [[C:%.*]], <4 x float> noundef [[D:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[D_ADDR:%.*]] = alloca <4 x float>, align 16
// CHECK-NEXT:    store <8 x half> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store i8 [[B]], ptr [[B_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[C]], ptr [[C_ADDR]], align 16
// CHECK-NEXT:    store <4 x float> [[D]], ptr [[D_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[C_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[D_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load i8, ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP4:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.vcvtss2sh.round(<8 x half> [[TMP0]], <4 x float> [[TMP1]], <8 x half> [[TMP2]], i8 [[TMP3]], i32 11)
// CHECK-NEXT:    ret <8 x half> [[TMP4]]
//
__m128h test_mm_mask_cvt_roundss_sh(__m128h A, __mmask8 B, __m128h C, __m128 D) {
  return _mm_mask_cvt_roundss_sh(A, B, C, D, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_maskz_cvt_roundss_sh(
// CHECK-SAME: i8 noundef zeroext [[A:%.*]], <8 x half> noundef [[B:%.*]], <4 x float> noundef [[C:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <4 x float>, align 16
// CHECK-NEXT:    store i8 [[A]], ptr [[A_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[B]], ptr [[B_ADDR]], align 16
// CHECK-NEXT:    store <4 x float> [[C]], ptr [[C_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[B_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[C_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load i8, ptr [[A_ADDR]], align 1
// CHECK-NEXT:    [[TMP4:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.vcvtss2sh.round(<8 x half> [[TMP0]], <4 x float> [[TMP1]], <8 x half> [[TMP2]], i8 [[TMP3]], i32 11)
// CHECK-NEXT:    ret <8 x half> [[TMP4]]
//
__m128h test_mm_maskz_cvt_roundss_sh(__mmask8 A, __m128h B, __m128 C) {
  return _mm_maskz_cvt_roundss_sh(A, B, C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_cvtss_sh(
// CHECK-SAME: <8 x half> noundef [[A:%.*]], <4 x float> noundef [[B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <4 x float>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <4 x float>, align 16
// CHECK-NEXT:    store <8 x half> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store <4 x float> [[B]], ptr [[B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <4 x float> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <4 x float>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.vcvtss2sh.round(<8 x half> [[TMP2]], <4 x float> [[TMP3]], <8 x half> zeroinitializer, i8 -1, i32 4)
// CHECK-NEXT:    ret <8 x half> [[TMP4]]
//
__m128h test_mm_cvtss_sh(__m128h A, __m128 B) {
  return _mm_cvtss_sh(A, B);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask_cvtss_sh(
// CHECK-SAME: <8 x half> noundef [[A:%.*]], i8 noundef zeroext [[B:%.*]], <8 x half> noundef [[C:%.*]], <4 x float> noundef [[D:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <4 x float>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[D_ADDR:%.*]] = alloca <4 x float>, align 16
// CHECK-NEXT:    store <8 x half> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store i8 [[B]], ptr [[B_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[C]], ptr [[C_ADDR]], align 16
// CHECK-NEXT:    store <4 x float> [[D]], ptr [[D_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[C_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <4 x float>, ptr [[D_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    store i8 [[TMP1]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <4 x float> [[TMP3]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = load <4 x float>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = load <8 x half>, ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP7:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP8:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.vcvtss2sh.round(<8 x half> [[TMP4]], <4 x float> [[TMP5]], <8 x half> [[TMP6]], i8 [[TMP7]], i32 4)
// CHECK-NEXT:    ret <8 x half> [[TMP8]]
//
__m128h test_mm_mask_cvtss_sh(__m128h A, __mmask8 B, __m128h C, __m128 D) {
  return _mm_mask_cvtss_sh(A, B, C, D);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_maskz_cvtss_sh(
// CHECK-SAME: i8 noundef zeroext [[A:%.*]], <8 x half> noundef [[B:%.*]], <4 x float> noundef [[C:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <4 x float>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <4 x float>, align 16
// CHECK-NEXT:    store i8 [[A]], ptr [[A_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[B]], ptr [[B_ADDR]], align 16
// CHECK-NEXT:    store <4 x float> [[C]], ptr [[C_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load i8, ptr [[A_ADDR]], align 1
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[B_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <4 x float>, ptr [[C_ADDR]], align 16
// CHECK-NEXT:    store i8 [[TMP0]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <4 x float> [[TMP2]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <4 x float>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP7:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.vcvtss2sh.round(<8 x half> [[TMP3]], <4 x float> [[TMP4]], <8 x half> [[TMP5]], i8 [[TMP6]], i32 4)
// CHECK-NEXT:    ret <8 x half> [[TMP7]]
//
__m128h test_mm_maskz_cvtss_sh(__mmask8 A, __m128h B, __m128 C) {
  return _mm_maskz_cvtss_sh(A, B, C);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_cvt_roundsd_sh(
// CHECK-SAME: <8 x half> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// CHECK-NEXT:    store <8 x half> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.vcvtsd2sh.round(<8 x half> [[TMP0]], <2 x double> [[TMP1]], <8 x half> zeroinitializer, i8 -1, i32 11)
// CHECK-NEXT:    ret <8 x half> [[TMP2]]
//
__m128h test_mm_cvt_roundsd_sh(__m128h A, __m128d B) {
  return _mm_cvt_roundsd_sh(A, B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask_cvt_roundsd_sh(
// CHECK-SAME: <8 x half> noundef [[A:%.*]], i8 noundef zeroext [[B:%.*]], <8 x half> noundef [[C:%.*]], <2 x double> noundef [[D:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[D_ADDR:%.*]] = alloca <2 x double>, align 16
// CHECK-NEXT:    store <8 x half> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store i8 [[B]], ptr [[B_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[C]], ptr [[C_ADDR]], align 16
// CHECK-NEXT:    store <2 x double> [[D]], ptr [[D_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[C_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[D_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load i8, ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP4:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.vcvtsd2sh.round(<8 x half> [[TMP0]], <2 x double> [[TMP1]], <8 x half> [[TMP2]], i8 [[TMP3]], i32 11)
// CHECK-NEXT:    ret <8 x half> [[TMP4]]
//
__m128h test_mm_mask_cvt_roundsd_sh(__m128h A, __mmask8 B, __m128h C, __m128d D) {
  return _mm_mask_cvt_roundsd_sh(A, B, C, D, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_maskz_cvt_roundsd_sh(
// CHECK-SAME: i8 noundef zeroext [[A:%.*]], <8 x half> noundef [[B:%.*]], <2 x double> noundef [[C:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <2 x double>, align 16
// CHECK-NEXT:    store i8 [[A]], ptr [[A_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[B]], ptr [[B_ADDR]], align 16
// CHECK-NEXT:    store <2 x double> [[C]], ptr [[C_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[B_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[C_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load i8, ptr [[A_ADDR]], align 1
// CHECK-NEXT:    [[TMP4:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.vcvtsd2sh.round(<8 x half> [[TMP0]], <2 x double> [[TMP1]], <8 x half> [[TMP2]], i8 [[TMP3]], i32 11)
// CHECK-NEXT:    ret <8 x half> [[TMP4]]
//
__m128h test_mm_maskz_cvt_roundsd_sh(__mmask8 A, __m128h B, __m128d C) {
  return _mm_maskz_cvt_roundsd_sh(A, B, C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_cvtsd_sh(
// CHECK-SAME: <8 x half> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// CHECK-NEXT:    store <8 x half> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.vcvtsd2sh.round(<8 x half> [[TMP2]], <2 x double> [[TMP3]], <8 x half> zeroinitializer, i8 -1, i32 4)
// CHECK-NEXT:    ret <8 x half> [[TMP4]]
//
__m128h test_mm_cvtsd_sh(__m128h A, __m128d B) {
  return _mm_cvtsd_sh(A, B);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask_cvtsd_sh(
// CHECK-SAME: <8 x half> noundef [[A:%.*]], i8 noundef zeroext [[B:%.*]], <8 x half> noundef [[C:%.*]], <2 x double> noundef [[D:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[D_ADDR:%.*]] = alloca <2 x double>, align 16
// CHECK-NEXT:    store <8 x half> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store i8 [[B]], ptr [[B_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[C]], ptr [[C_ADDR]], align 16
// CHECK-NEXT:    store <2 x double> [[D]], ptr [[D_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[C_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[D_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    store i8 [[TMP1]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <2 x double> [[TMP3]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = load <8 x half>, ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP7:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP8:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.vcvtsd2sh.round(<8 x half> [[TMP4]], <2 x double> [[TMP5]], <8 x half> [[TMP6]], i8 [[TMP7]], i32 4)
// CHECK-NEXT:    ret <8 x half> [[TMP8]]
//
__m128h test_mm_mask_cvtsd_sh(__m128h A, __mmask8 B, __m128h C, __m128d D) {
  return _mm_mask_cvtsd_sh(A, B, C, D);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_maskz_cvtsd_sh(
// CHECK-SAME: i8 noundef zeroext [[A:%.*]], <8 x half> noundef [[B:%.*]], <2 x double> noundef [[C:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <2 x double>, align 16
// CHECK-NEXT:    store i8 [[A]], ptr [[A_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[B]], ptr [[B_ADDR]], align 16
// CHECK-NEXT:    store <2 x double> [[C]], ptr [[C_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load i8, ptr [[A_ADDR]], align 1
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[B_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[C_ADDR]], align 16
// CHECK-NEXT:    store i8 [[TMP0]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <2 x double> [[TMP2]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP7:%.*]] = call <8 x half> @llvm.x86.avx512fp16.mask.vcvtsd2sh.round(<8 x half> [[TMP3]], <2 x double> [[TMP4]], <8 x half> [[TMP5]], i8 [[TMP6]], i32 4)
// CHECK-NEXT:    ret <8 x half> [[TMP7]]
//
__m128h test_mm_maskz_cvtsd_sh(__mmask8 A, __m128h B, __m128d C) {
  return _mm_maskz_cvtsd_sh(A, B, C);
}

// CHECK-LABEL: define dso_local <2 x double> @test_mm_cvt_roundsh_sd(
// CHECK-SAME: <2 x double> noundef [[A:%.*]], <8 x half> noundef [[B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[B]], ptr [[B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[B_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = call <2 x double> @llvm.x86.avx512fp16.mask.vcvtsh2sd.round(<2 x double> [[TMP0]], <8 x half> [[TMP1]], <2 x double> zeroinitializer, i8 -1, i32 8)
// CHECK-NEXT:    ret <2 x double> [[TMP2]]
//
__m128d test_mm_cvt_roundsh_sd(__m128d A, __m128h B) {
  return _mm_cvt_roundsh_sd(A, B, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <2 x double> @test_mm_mask_cvt_roundsh_sd(
// CHECK-SAME: <2 x double> noundef [[A:%.*]], i8 noundef zeroext [[B:%.*]], <2 x double> noundef [[C:%.*]], <8 x half> noundef [[D:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <2 x double>, align 16
// CHECK-NEXT:    [[D_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store i8 [[B]], ptr [[B_ADDR]], align 1
// CHECK-NEXT:    store <2 x double> [[C]], ptr [[C_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[D]], ptr [[D_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[C_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[D_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load i8, ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP4:%.*]] = call <2 x double> @llvm.x86.avx512fp16.mask.vcvtsh2sd.round(<2 x double> [[TMP0]], <8 x half> [[TMP1]], <2 x double> [[TMP2]], i8 [[TMP3]], i32 8)
// CHECK-NEXT:    ret <2 x double> [[TMP4]]
//
__m128d test_mm_mask_cvt_roundsh_sd(__m128d A, __mmask8 B, __m128d C, __m128h D) {
  return _mm_mask_cvt_roundsh_sd(A, B, C, D, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <2 x double> @test_mm_maskz_cvt_roundsh_sd(
// CHECK-SAME: i8 noundef zeroext [[A:%.*]], <2 x double> noundef [[B:%.*]], <8 x half> noundef [[C:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <2 x double>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[A]], ptr [[A_ADDR]], align 1
// CHECK-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[C]], ptr [[C_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[C_ADDR]], align 16
// CHECK-NEXT:    store <2 x double> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load i8, ptr [[A_ADDR]], align 1
// CHECK-NEXT:    [[TMP4:%.*]] = call <2 x double> @llvm.x86.avx512fp16.mask.vcvtsh2sd.round(<2 x double> [[TMP0]], <8 x half> [[TMP1]], <2 x double> [[TMP2]], i8 [[TMP3]], i32 8)
// CHECK-NEXT:    ret <2 x double> [[TMP4]]
//
__m128d test_mm_maskz_cvt_roundsh_sd(__mmask8 A, __m128d B, __m128h C) {
  return _mm_maskz_cvt_roundsh_sd(A, B, C, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <2 x double> @test_mm_cvtsh_sd(
// CHECK-SAME: <2 x double> noundef [[A:%.*]], <8 x half> noundef [[B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[B]], ptr [[B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[B_ADDR]], align 16
// CHECK-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = call <2 x double> @llvm.x86.avx512fp16.mask.vcvtsh2sd.round(<2 x double> [[TMP2]], <8 x half> [[TMP3]], <2 x double> zeroinitializer, i8 -1, i32 4)
// CHECK-NEXT:    ret <2 x double> [[TMP4]]
//
__m128d test_mm_cvtsh_sd(__m128d A, __m128h B) {
  return _mm_cvtsh_sd(A, B);
}

// CHECK-LABEL: define dso_local <2 x double> @test_mm_mask_cvtsh_sd(
// CHECK-SAME: <2 x double> noundef [[A:%.*]], i8 noundef zeroext [[B:%.*]], <2 x double> noundef [[C:%.*]], <8 x half> noundef [[D:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <2 x double>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <2 x double>, align 16
// CHECK-NEXT:    [[D_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store i8 [[B]], ptr [[B_ADDR]], align 1
// CHECK-NEXT:    store <2 x double> [[C]], ptr [[C_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[D]], ptr [[D_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[C_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[D_ADDR]], align 16
// CHECK-NEXT:    store <2 x double> [[TMP0]], ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    store i8 [[TMP1]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <2 x double> [[TMP2]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP3]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = load <2 x double>, ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP7:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP8:%.*]] = call <2 x double> @llvm.x86.avx512fp16.mask.vcvtsh2sd.round(<2 x double> [[TMP4]], <8 x half> [[TMP5]], <2 x double> [[TMP6]], i8 [[TMP7]], i32 4)
// CHECK-NEXT:    ret <2 x double> [[TMP8]]
//
__m128d test_mm_mask_cvtsh_sd(__m128d A, __mmask8 B, __m128d C, __m128h D) {
  return _mm_mask_cvtsh_sd(A, B, C, D);
}

// CHECK-LABEL: define dso_local <2 x double> @test_mm_maskz_cvtsh_sd(
// CHECK-SAME: i8 noundef zeroext [[A:%.*]], <2 x double> noundef [[B:%.*]], <8 x half> noundef [[C:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <2 x double>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[A]], ptr [[A_ADDR]], align 1
// CHECK-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[C]], ptr [[C_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load i8, ptr [[A_ADDR]], align 1
// CHECK-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[C_ADDR]], align 16
// CHECK-NEXT:    store i8 [[TMP0]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <2 x double> [[TMP1]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    store <2 x double> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = load <2 x double>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP7:%.*]] = call <2 x double> @llvm.x86.avx512fp16.mask.vcvtsh2sd.round(<2 x double> [[TMP3]], <8 x half> [[TMP4]], <2 x double> [[TMP5]], i8 [[TMP6]], i32 4)
// CHECK-NEXT:    ret <2 x double> [[TMP7]]
//
__m128d test_mm_maskz_cvtsh_sd(__mmask8 A, __m128d B, __m128h C) {
  return _mm_maskz_cvtsh_sd(A, B, C);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_cvt_roundph_epi16(
// CHECK-SAME: <32 x half> noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <8 x i64> zeroinitializer to <32 x i16>
// CHECK-NEXT:    [[TMP2:%.*]] = call <32 x i16> @llvm.x86.avx512fp16.mask.vcvtph2w.512(<32 x half> [[TMP0]], <32 x i16> [[TMP1]], i32 -1, i32 11)
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x i16> [[TMP2]] to <8 x i64>
// CHECK-NEXT:    ret <8 x i64> [[TMP3]]
//
__m512i test_mm512_cvt_roundph_epi16(__m512h A) {
  return _mm512_cvt_roundph_epi16(A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_mask_cvt_roundph_epi16(
// CHECK-SAME: <8 x i64> noundef [[A:%.*]], i32 noundef [[B:%.*]], <32 x half> noundef [[C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <8 x i64> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store i32 [[B]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[C]], ptr [[C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[C_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i64>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <8 x i64> [[TMP1]] to <32 x i16>
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = call <32 x i16> @llvm.x86.avx512fp16.mask.vcvtph2w.512(<32 x half> [[TMP0]], <32 x i16> [[TMP2]], i32 [[TMP3]], i32 11)
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <32 x i16> [[TMP4]] to <8 x i64>
// CHECK-NEXT:    ret <8 x i64> [[TMP5]]
//
__m512i test_mm512_mask_cvt_roundph_epi16(__m512i A, __mmask32 B, __m512h C) {
  return _mm512_mask_cvt_roundph_epi16(A, B, C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_maskz_cvt_roundph_epi16(
// CHECK-SAME: i32 noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[A]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    store <8 x i64> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i64>, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <8 x i64> [[TMP1]] to <32 x i16>
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = call <32 x i16> @llvm.x86.avx512fp16.mask.vcvtph2w.512(<32 x half> [[TMP0]], <32 x i16> [[TMP2]], i32 [[TMP3]], i32 11)
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <32 x i16> [[TMP4]] to <8 x i64>
// CHECK-NEXT:    ret <8 x i64> [[TMP5]]
//
__m512i test_mm512_maskz_cvt_roundph_epi16(__mmask32 A, __m512h B) {
  return _mm512_maskz_cvt_roundph_epi16(A, B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_cvtph_epi16(
// CHECK-SAME: <32 x half> noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <8 x i64> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x i64>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <8 x i64> [[TMP2]] to <32 x i16>
// CHECK-NEXT:    [[TMP4:%.*]] = call <32 x i16> @llvm.x86.avx512fp16.mask.vcvtph2w.512(<32 x half> [[TMP1]], <32 x i16> [[TMP3]], i32 -1, i32 4)
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <32 x i16> [[TMP4]] to <8 x i64>
// CHECK-NEXT:    ret <8 x i64> [[TMP5]]
//
__m512i test_mm512_cvtph_epi16(__m512h A) {
  return _mm512_cvtph_epi16(A);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_mask_cvtph_epi16(
// CHECK-SAME: <8 x i64> noundef [[A:%.*]], i32 noundef [[B:%.*]], <32 x half> noundef [[C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <8 x i64> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store i32 [[B]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[C]], ptr [[C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i64>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[C_ADDR]], align 64
// CHECK-NEXT:    store <8 x i64> [[TMP0]], ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x i64>, ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <8 x i64> [[TMP4]] to <32 x i16>
// CHECK-NEXT:    [[TMP6:%.*]] = load i32, ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP7:%.*]] = call <32 x i16> @llvm.x86.avx512fp16.mask.vcvtph2w.512(<32 x half> [[TMP3]], <32 x i16> [[TMP5]], i32 [[TMP6]], i32 4)
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast <32 x i16> [[TMP7]] to <8 x i64>
// CHECK-NEXT:    ret <8 x i64> [[TMP8]]
//
__m512i test_mm512_mask_cvtph_epi16(__m512i A, __mmask32 B, __m512h C) {
  return _mm512_mask_cvtph_epi16(A, B, C);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_maskz_cvtph_epi16(
// CHECK-SAME: i32 noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[A]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    store i32 [[TMP0]], ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    store <32 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <8 x i64> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x i64>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <8 x i64> [[TMP3]] to <32 x i16>
// CHECK-NEXT:    [[TMP5:%.*]] = load i32, ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP6:%.*]] = call <32 x i16> @llvm.x86.avx512fp16.mask.vcvtph2w.512(<32 x half> [[TMP2]], <32 x i16> [[TMP4]], i32 [[TMP5]], i32 4)
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast <32 x i16> [[TMP6]] to <8 x i64>
// CHECK-NEXT:    ret <8 x i64> [[TMP7]]
//
__m512i test_mm512_maskz_cvtph_epi16(__mmask32 A, __m512h B) {
  return _mm512_maskz_cvtph_epi16(A, B);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_cvtt_roundph_epi16(
// CHECK-SAME: <32 x half> noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <8 x i64> zeroinitializer to <32 x i16>
// CHECK-NEXT:    [[TMP2:%.*]] = call <32 x i16> @llvm.x86.avx512fp16.mask.vcvttph2w.512(<32 x half> [[TMP0]], <32 x i16> [[TMP1]], i32 -1, i32 8)
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x i16> [[TMP2]] to <8 x i64>
// CHECK-NEXT:    ret <8 x i64> [[TMP3]]
//
__m512i test_mm512_cvtt_roundph_epi16(__m512h A) {
  return _mm512_cvtt_roundph_epi16(A, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_mask_cvtt_roundph_epi16(
// CHECK-SAME: <8 x i64> noundef [[A:%.*]], i32 noundef [[B:%.*]], <32 x half> noundef [[C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <8 x i64> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store i32 [[B]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[C]], ptr [[C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[C_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i64>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <8 x i64> [[TMP1]] to <32 x i16>
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = call <32 x i16> @llvm.x86.avx512fp16.mask.vcvttph2w.512(<32 x half> [[TMP0]], <32 x i16> [[TMP2]], i32 [[TMP3]], i32 8)
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <32 x i16> [[TMP4]] to <8 x i64>
// CHECK-NEXT:    ret <8 x i64> [[TMP5]]
//
__m512i test_mm512_mask_cvtt_roundph_epi16(__m512i A, __mmask32 B, __m512h C) {
  return _mm512_mask_cvtt_roundph_epi16(A, B, C, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_maskz_cvtt_roundph_epi16(
// CHECK-SAME: i32 noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[A]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    store <8 x i64> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i64>, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <8 x i64> [[TMP1]] to <32 x i16>
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = call <32 x i16> @llvm.x86.avx512fp16.mask.vcvttph2w.512(<32 x half> [[TMP0]], <32 x i16> [[TMP2]], i32 [[TMP3]], i32 8)
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <32 x i16> [[TMP4]] to <8 x i64>
// CHECK-NEXT:    ret <8 x i64> [[TMP5]]
//
__m512i test_mm512_maskz_cvtt_roundph_epi16(__mmask32 A, __m512h B) {
  return _mm512_maskz_cvtt_roundph_epi16(A, B, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_cvttph_epi16(
// CHECK-SAME: <32 x half> noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <8 x i64> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x i64>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <8 x i64> [[TMP2]] to <32 x i16>
// CHECK-NEXT:    [[TMP4:%.*]] = call <32 x i16> @llvm.x86.avx512fp16.mask.vcvttph2w.512(<32 x half> [[TMP1]], <32 x i16> [[TMP3]], i32 -1, i32 4)
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <32 x i16> [[TMP4]] to <8 x i64>
// CHECK-NEXT:    ret <8 x i64> [[TMP5]]
//
__m512i test_mm512_cvttph_epi16(__m512h A) {
  return _mm512_cvttph_epi16(A);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_mask_cvttph_epi16(
// CHECK-SAME: <8 x i64> noundef [[A:%.*]], i32 noundef [[B:%.*]], <32 x half> noundef [[C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <8 x i64> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store i32 [[B]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[C]], ptr [[C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i64>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[C_ADDR]], align 64
// CHECK-NEXT:    store <8 x i64> [[TMP0]], ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x i64>, ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <8 x i64> [[TMP4]] to <32 x i16>
// CHECK-NEXT:    [[TMP6:%.*]] = load i32, ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP7:%.*]] = call <32 x i16> @llvm.x86.avx512fp16.mask.vcvttph2w.512(<32 x half> [[TMP3]], <32 x i16> [[TMP5]], i32 [[TMP6]], i32 4)
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast <32 x i16> [[TMP7]] to <8 x i64>
// CHECK-NEXT:    ret <8 x i64> [[TMP8]]
//
__m512i test_mm512_mask_cvttph_epi16(__m512i A, __mmask32 B, __m512h C) {
  return _mm512_mask_cvttph_epi16(A, B, C);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_maskz_cvttph_epi16(
// CHECK-SAME: i32 noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[A]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    store i32 [[TMP0]], ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    store <32 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <8 x i64> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x i64>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <8 x i64> [[TMP3]] to <32 x i16>
// CHECK-NEXT:    [[TMP5:%.*]] = load i32, ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP6:%.*]] = call <32 x i16> @llvm.x86.avx512fp16.mask.vcvttph2w.512(<32 x half> [[TMP2]], <32 x i16> [[TMP4]], i32 [[TMP5]], i32 4)
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast <32 x i16> [[TMP6]] to <8 x i64>
// CHECK-NEXT:    ret <8 x i64> [[TMP7]]
//
__m512i test_mm512_maskz_cvttph_epi16(__mmask32 A, __m512h B) {
  return _mm512_maskz_cvttph_epi16(A, B);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_cvt_roundepi16_ph(
// CHECK-SAME: <8 x i64> noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    store <8 x i64> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i64>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <8 x i64> [[TMP0]] to <32 x i16>
// CHECK-NEXT:    [[TMP2:%.*]] = call <32 x half> @llvm.x86.avx512.sitofp.round.v32f16.v32i16(<32 x i16> [[TMP1]], i32 11)
// CHECK-NEXT:    ret <32 x half> [[TMP2]]
//
__m512h test_mm512_cvt_roundepi16_ph(__m512i A) {
  return _mm512_cvt_roundepi16_ph(A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask_cvt_roundepi16_ph(
// CHECK-SAME: <32 x half> noundef [[A:%.*]], i32 noundef [[B:%.*]], <8 x i64> noundef [[C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store i32 [[B]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    store <8 x i64> [[C]], ptr [[C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i64>, ptr [[C_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <8 x i64> [[TMP0]] to <32 x i16>
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = call <32 x half> @llvm.x86.avx512.sitofp.round.v32f16.v32i16(<32 x i16> [[TMP1]], i32 11)
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast i32 [[TMP3]] to <32 x i1>
// CHECK-NEXT:    [[TMP6:%.*]] = select <32 x i1> [[TMP5]], <32 x half> [[TMP4]], <32 x half> [[TMP2]]
// CHECK-NEXT:    ret <32 x half> [[TMP6]]
//
__m512h test_mm512_mask_cvt_roundepi16_ph(__m512h A, __mmask32 B, __m512i C) {
  return _mm512_mask_cvt_roundepi16_ph(A, B, C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_maskz_cvt_roundepi16_ph(
// CHECK-SAME: i32 noundef [[A:%.*]], <8 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    store i32 [[A]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store <8 x i64> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i64>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <8 x i64> [[TMP0]] to <32 x i16>
// CHECK-NEXT:    store <32 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = call <32 x half> @llvm.x86.avx512.sitofp.round.v32f16.v32i16(<32 x i16> [[TMP1]], i32 11)
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast i32 [[TMP3]] to <32 x i1>
// CHECK-NEXT:    [[TMP6:%.*]] = select <32 x i1> [[TMP5]], <32 x half> [[TMP4]], <32 x half> [[TMP2]]
// CHECK-NEXT:    ret <32 x half> [[TMP6]]
//
__m512h test_mm512_maskz_cvt_roundepi16_ph(__mmask32 A, __m512i B) {
  return _mm512_maskz_cvt_roundepi16_ph(A, B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_cvtepi16_ph(
// CHECK-SAME: <8 x i64> noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    store <8 x i64> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i64>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <8 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i64>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <8 x i64> [[TMP1]] to <32 x i16>
// CHECK-NEXT:    store <32 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = sitofp <32 x i16> [[TMP2]] to <32 x half>
// CHECK-NEXT:    ret <32 x half> [[TMP4]]
//
__m512h test_mm512_cvtepi16_ph(__m512i A) {
  return _mm512_cvtepi16_ph(A);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask_cvtepi16_ph(
// CHECK-SAME: <32 x half> noundef [[A:%.*]], i32 noundef [[B:%.*]], <8 x i64> noundef [[C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store i32 [[B]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    store <8 x i64> [[C]], ptr [[C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x i64>, ptr [[C_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    store <8 x i64> [[TMP2]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x i64>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <8 x i64> [[TMP3]] to <32 x i16>
// CHECK-NEXT:    [[TMP5:%.*]] = load <32 x half>, ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP6:%.*]] = load i32, ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP7:%.*]] = sitofp <32 x i16> [[TMP4]] to <32 x half>
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast i32 [[TMP6]] to <32 x i1>
// CHECK-NEXT:    [[TMP9:%.*]] = select <32 x i1> [[TMP8]], <32 x half> [[TMP7]], <32 x half> [[TMP5]]
// CHECK-NEXT:    ret <32 x half> [[TMP9]]
//
__m512h test_mm512_mask_cvtepi16_ph(__m512h A, __mmask32 B, __m512i C) {
  return _mm512_mask_cvtepi16_ph(A, B, C);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_maskz_cvtepi16_ph(
// CHECK-SAME: i32 noundef [[A:%.*]], <8 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    store i32 [[A]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store <8 x i64> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i64>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    store i32 [[TMP0]], ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    store <8 x i64> [[TMP1]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x i64>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <8 x i64> [[TMP2]] to <32 x i16>
// CHECK-NEXT:    store <32 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = load i32, ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP6:%.*]] = sitofp <32 x i16> [[TMP3]] to <32 x half>
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast i32 [[TMP5]] to <32 x i1>
// CHECK-NEXT:    [[TMP8:%.*]] = select <32 x i1> [[TMP7]], <32 x half> [[TMP6]], <32 x half> [[TMP4]]
// CHECK-NEXT:    ret <32 x half> [[TMP8]]
//
__m512h test_mm512_maskz_cvtepi16_ph(__mmask32 A, __m512i B) {
  return _mm512_maskz_cvtepi16_ph(A, B);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_cvt_roundph_epu16(
// CHECK-SAME: <32 x half> noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <8 x i64> zeroinitializer to <32 x i16>
// CHECK-NEXT:    [[TMP2:%.*]] = call <32 x i16> @llvm.x86.avx512fp16.mask.vcvtph2uw.512(<32 x half> [[TMP0]], <32 x i16> [[TMP1]], i32 -1, i32 11)
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x i16> [[TMP2]] to <8 x i64>
// CHECK-NEXT:    ret <8 x i64> [[TMP3]]
//
__m512i test_mm512_cvt_roundph_epu16(__m512h A) {
  return _mm512_cvt_roundph_epu16(A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_mask_cvt_roundph_epu16(
// CHECK-SAME: <8 x i64> noundef [[A:%.*]], i32 noundef [[B:%.*]], <32 x half> noundef [[C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <8 x i64> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store i32 [[B]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[C]], ptr [[C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[C_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i64>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <8 x i64> [[TMP1]] to <32 x i16>
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = call <32 x i16> @llvm.x86.avx512fp16.mask.vcvtph2uw.512(<32 x half> [[TMP0]], <32 x i16> [[TMP2]], i32 [[TMP3]], i32 11)
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <32 x i16> [[TMP4]] to <8 x i64>
// CHECK-NEXT:    ret <8 x i64> [[TMP5]]
//
__m512i test_mm512_mask_cvt_roundph_epu16(__m512i A, __mmask32 B, __m512h C) {
  return _mm512_mask_cvt_roundph_epu16(A, B, C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_maskz_cvt_roundph_epu16(
// CHECK-SAME: i32 noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[A]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    store <8 x i64> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i64>, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <8 x i64> [[TMP1]] to <32 x i16>
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = call <32 x i16> @llvm.x86.avx512fp16.mask.vcvtph2uw.512(<32 x half> [[TMP0]], <32 x i16> [[TMP2]], i32 [[TMP3]], i32 11)
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <32 x i16> [[TMP4]] to <8 x i64>
// CHECK-NEXT:    ret <8 x i64> [[TMP5]]
//
__m512i test_mm512_maskz_cvt_roundph_epu16(__mmask32 A, __m512h B) {
  return _mm512_maskz_cvt_roundph_epu16(A, B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_cvtph_epu16(
// CHECK-SAME: <32 x half> noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <8 x i64> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x i64>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <8 x i64> [[TMP2]] to <32 x i16>
// CHECK-NEXT:    [[TMP4:%.*]] = call <32 x i16> @llvm.x86.avx512fp16.mask.vcvtph2uw.512(<32 x half> [[TMP1]], <32 x i16> [[TMP3]], i32 -1, i32 4)
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <32 x i16> [[TMP4]] to <8 x i64>
// CHECK-NEXT:    ret <8 x i64> [[TMP5]]
//
__m512i test_mm512_cvtph_epu16(__m512h A) {
  return _mm512_cvtph_epu16(A);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_mask_cvtph_epu16(
// CHECK-SAME: <8 x i64> noundef [[A:%.*]], i32 noundef [[B:%.*]], <32 x half> noundef [[C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <8 x i64> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store i32 [[B]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[C]], ptr [[C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i64>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[C_ADDR]], align 64
// CHECK-NEXT:    store <8 x i64> [[TMP0]], ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x i64>, ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <8 x i64> [[TMP4]] to <32 x i16>
// CHECK-NEXT:    [[TMP6:%.*]] = load i32, ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP7:%.*]] = call <32 x i16> @llvm.x86.avx512fp16.mask.vcvtph2uw.512(<32 x half> [[TMP3]], <32 x i16> [[TMP5]], i32 [[TMP6]], i32 4)
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast <32 x i16> [[TMP7]] to <8 x i64>
// CHECK-NEXT:    ret <8 x i64> [[TMP8]]
//
__m512i test_mm512_mask_cvtph_epu16(__m512i A, __mmask32 B, __m512h C) {
  return _mm512_mask_cvtph_epu16(A, B, C);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_maskz_cvtph_epu16(
// CHECK-SAME: i32 noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[A]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    store i32 [[TMP0]], ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    store <32 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <8 x i64> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x i64>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <8 x i64> [[TMP3]] to <32 x i16>
// CHECK-NEXT:    [[TMP5:%.*]] = load i32, ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP6:%.*]] = call <32 x i16> @llvm.x86.avx512fp16.mask.vcvtph2uw.512(<32 x half> [[TMP2]], <32 x i16> [[TMP4]], i32 [[TMP5]], i32 4)
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast <32 x i16> [[TMP6]] to <8 x i64>
// CHECK-NEXT:    ret <8 x i64> [[TMP7]]
//
__m512i test_mm512_maskz_cvtph_epu16(__mmask32 A, __m512h B) {
  return _mm512_maskz_cvtph_epu16(A, B);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_cvtt_roundph_epu16(
// CHECK-SAME: <32 x half> noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <8 x i64> zeroinitializer to <32 x i16>
// CHECK-NEXT:    [[TMP2:%.*]] = call <32 x i16> @llvm.x86.avx512fp16.mask.vcvttph2uw.512(<32 x half> [[TMP0]], <32 x i16> [[TMP1]], i32 -1, i32 8)
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x i16> [[TMP2]] to <8 x i64>
// CHECK-NEXT:    ret <8 x i64> [[TMP3]]
//
__m512i test_mm512_cvtt_roundph_epu16(__m512h A) {
  return _mm512_cvtt_roundph_epu16(A, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_mask_cvtt_roundph_epu16(
// CHECK-SAME: <8 x i64> noundef [[A:%.*]], i32 noundef [[B:%.*]], <32 x half> noundef [[C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <8 x i64> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store i32 [[B]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[C]], ptr [[C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[C_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i64>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <8 x i64> [[TMP1]] to <32 x i16>
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = call <32 x i16> @llvm.x86.avx512fp16.mask.vcvttph2uw.512(<32 x half> [[TMP0]], <32 x i16> [[TMP2]], i32 [[TMP3]], i32 8)
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <32 x i16> [[TMP4]] to <8 x i64>
// CHECK-NEXT:    ret <8 x i64> [[TMP5]]
//
__m512i test_mm512_mask_cvtt_roundph_epu16(__m512i A, __mmask32 B, __m512h C) {
  return _mm512_mask_cvtt_roundph_epu16(A, B, C, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_maskz_cvtt_roundph_epu16(
// CHECK-SAME: i32 noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[A]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    store <8 x i64> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i64>, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <8 x i64> [[TMP1]] to <32 x i16>
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = call <32 x i16> @llvm.x86.avx512fp16.mask.vcvttph2uw.512(<32 x half> [[TMP0]], <32 x i16> [[TMP2]], i32 [[TMP3]], i32 8)
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <32 x i16> [[TMP4]] to <8 x i64>
// CHECK-NEXT:    ret <8 x i64> [[TMP5]]
//
__m512i test_mm512_maskz_cvtt_roundph_epu16(__mmask32 A, __m512h B) {
  return _mm512_maskz_cvtt_roundph_epu16(A, B, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_cvttph_epu16(
// CHECK-SAME: <32 x half> noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <8 x i64> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x i64>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <8 x i64> [[TMP2]] to <32 x i16>
// CHECK-NEXT:    [[TMP4:%.*]] = call <32 x i16> @llvm.x86.avx512fp16.mask.vcvttph2uw.512(<32 x half> [[TMP1]], <32 x i16> [[TMP3]], i32 -1, i32 4)
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <32 x i16> [[TMP4]] to <8 x i64>
// CHECK-NEXT:    ret <8 x i64> [[TMP5]]
//
__m512i test_mm512_cvttph_epu16(__m512h A) {
  return _mm512_cvttph_epu16(A);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_mask_cvttph_epu16(
// CHECK-SAME: <8 x i64> noundef [[A:%.*]], i32 noundef [[B:%.*]], <32 x half> noundef [[C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <8 x i64> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store i32 [[B]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[C]], ptr [[C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i64>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[C_ADDR]], align 64
// CHECK-NEXT:    store <8 x i64> [[TMP0]], ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x i64>, ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <8 x i64> [[TMP4]] to <32 x i16>
// CHECK-NEXT:    [[TMP6:%.*]] = load i32, ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP7:%.*]] = call <32 x i16> @llvm.x86.avx512fp16.mask.vcvttph2uw.512(<32 x half> [[TMP3]], <32 x i16> [[TMP5]], i32 [[TMP6]], i32 4)
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast <32 x i16> [[TMP7]] to <8 x i64>
// CHECK-NEXT:    ret <8 x i64> [[TMP8]]
//
__m512i test_mm512_mask_cvttph_epu16(__m512i A, __mmask32 B, __m512h C) {
  return _mm512_mask_cvttph_epu16(A, B, C);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_maskz_cvttph_epu16(
// CHECK-SAME: i32 noundef [[A:%.*]], <32 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[A]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    store i32 [[TMP0]], ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    store <32 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <8 x i64> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x i64>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <8 x i64> [[TMP3]] to <32 x i16>
// CHECK-NEXT:    [[TMP5:%.*]] = load i32, ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP6:%.*]] = call <32 x i16> @llvm.x86.avx512fp16.mask.vcvttph2uw.512(<32 x half> [[TMP2]], <32 x i16> [[TMP4]], i32 [[TMP5]], i32 4)
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast <32 x i16> [[TMP6]] to <8 x i64>
// CHECK-NEXT:    ret <8 x i64> [[TMP7]]
//
__m512i test_mm512_maskz_cvttph_epu16(__mmask32 A, __m512h B) {
  return _mm512_maskz_cvttph_epu16(A, B);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_cvt_roundepu16_ph(
// CHECK-SAME: <8 x i64> noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    store <8 x i64> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i64>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <8 x i64> [[TMP0]] to <32 x i16>
// CHECK-NEXT:    [[TMP2:%.*]] = call <32 x half> @llvm.x86.avx512.uitofp.round.v32f16.v32i16(<32 x i16> [[TMP1]], i32 11)
// CHECK-NEXT:    ret <32 x half> [[TMP2]]
//
__m512h test_mm512_cvt_roundepu16_ph(__m512i A) {
  return _mm512_cvt_roundepu16_ph(A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask_cvt_roundepu16_ph(
// CHECK-SAME: <32 x half> noundef [[A:%.*]], i32 noundef [[B:%.*]], <8 x i64> noundef [[C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store i32 [[B]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    store <8 x i64> [[C]], ptr [[C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i64>, ptr [[C_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <8 x i64> [[TMP0]] to <32 x i16>
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = call <32 x half> @llvm.x86.avx512.uitofp.round.v32f16.v32i16(<32 x i16> [[TMP1]], i32 11)
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast i32 [[TMP3]] to <32 x i1>
// CHECK-NEXT:    [[TMP6:%.*]] = select <32 x i1> [[TMP5]], <32 x half> [[TMP4]], <32 x half> [[TMP2]]
// CHECK-NEXT:    ret <32 x half> [[TMP6]]
//
__m512h test_mm512_mask_cvt_roundepu16_ph(__m512h A, __mmask32 B, __m512i C) {
  return _mm512_mask_cvt_roundepu16_ph(A, B, C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_maskz_cvt_roundepu16_ph(
// CHECK-SAME: i32 noundef [[A:%.*]], <8 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    store i32 [[A]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store <8 x i64> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i64>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <8 x i64> [[TMP0]] to <32 x i16>
// CHECK-NEXT:    store <32 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = call <32 x half> @llvm.x86.avx512.uitofp.round.v32f16.v32i16(<32 x i16> [[TMP1]], i32 11)
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast i32 [[TMP3]] to <32 x i1>
// CHECK-NEXT:    [[TMP6:%.*]] = select <32 x i1> [[TMP5]], <32 x half> [[TMP4]], <32 x half> [[TMP2]]
// CHECK-NEXT:    ret <32 x half> [[TMP6]]
//
__m512h test_mm512_maskz_cvt_roundepu16_ph(__mmask32 A, __m512i B) {
  return _mm512_maskz_cvt_roundepu16_ph(A, B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_cvtepu16_ph(
// CHECK-SAME: <8 x i64> noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    store <8 x i64> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i64>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <8 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i64>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <8 x i64> [[TMP1]] to <32 x i16>
// CHECK-NEXT:    store <32 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = uitofp <32 x i16> [[TMP2]] to <32 x half>
// CHECK-NEXT:    ret <32 x half> [[TMP4]]
//
__m512h test_mm512_cvtepu16_ph(__m512i A) {
  return _mm512_cvtepu16_ph(A);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask_cvtepu16_ph(
// CHECK-SAME: <32 x half> noundef [[A:%.*]], i32 noundef [[B:%.*]], <8 x i64> noundef [[C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    store <32 x half> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store i32 [[B]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    store <8 x i64> [[C]], ptr [[C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x i64>, ptr [[C_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    store <8 x i64> [[TMP2]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x i64>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <8 x i64> [[TMP3]] to <32 x i16>
// CHECK-NEXT:    [[TMP5:%.*]] = load <32 x half>, ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP6:%.*]] = load i32, ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP7:%.*]] = uitofp <32 x i16> [[TMP4]] to <32 x half>
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast i32 [[TMP6]] to <32 x i1>
// CHECK-NEXT:    [[TMP9:%.*]] = select <32 x i1> [[TMP8]], <32 x half> [[TMP7]], <32 x half> [[TMP5]]
// CHECK-NEXT:    ret <32 x half> [[TMP9]]
//
__m512h test_mm512_mask_cvtepu16_ph(__m512h A, __mmask32 B, __m512i C) {
  return _mm512_mask_cvtepu16_ph(A, B, C);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_maskz_cvtepu16_ph(
// CHECK-SAME: i32 noundef [[A:%.*]], <8 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    store i32 [[A]], ptr [[A_ADDR]], align 4
// CHECK-NEXT:    store <8 x i64> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i64>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    store i32 [[TMP0]], ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    store <8 x i64> [[TMP1]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x i64>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <8 x i64> [[TMP2]] to <32 x i16>
// CHECK-NEXT:    store <32 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = load i32, ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP6:%.*]] = uitofp <32 x i16> [[TMP3]] to <32 x half>
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast i32 [[TMP5]] to <32 x i1>
// CHECK-NEXT:    [[TMP8:%.*]] = select <32 x i1> [[TMP7]], <32 x half> [[TMP6]], <32 x half> [[TMP4]]
// CHECK-NEXT:    ret <32 x half> [[TMP8]]
//
__m512h test_mm512_maskz_cvtepu16_ph(__mmask32 A, __m512i B) {
  return _mm512_maskz_cvtepu16_ph(A, B);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_cvt_roundph_epi32(
// CHECK-SAME: <16 x half> noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    store <16 x half> [[A]], ptr [[A_ADDR]], align 32
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x half>, ptr [[A_ADDR]], align 32
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <8 x i64> zeroinitializer to <16 x i32>
// CHECK-NEXT:    [[TMP2:%.*]] = call <16 x i32> @llvm.x86.avx512fp16.mask.vcvtph2dq.512(<16 x half> [[TMP0]], <16 x i32> [[TMP1]], i16 -1, i32 11)
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <16 x i32> [[TMP2]] to <8 x i64>
// CHECK-NEXT:    ret <8 x i64> [[TMP3]]
//
__m512i test_mm512_cvt_roundph_epi32(__m256h A) {
  return _mm512_cvt_roundph_epi32(A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_mask_cvt_roundph_epi32(
// CHECK-SAME: <8 x i64> noundef [[A:%.*]], i16 noundef zeroext [[B:%.*]], <16 x half> noundef [[C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    store <8 x i64> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store i16 [[B]], ptr [[B_ADDR]], align 2
// CHECK-NEXT:    store <16 x half> [[C]], ptr [[C_ADDR]], align 32
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x half>, ptr [[C_ADDR]], align 32
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i64>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <8 x i64> [[TMP1]] to <16 x i32>
// CHECK-NEXT:    [[TMP3:%.*]] = load i16, ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP4:%.*]] = call <16 x i32> @llvm.x86.avx512fp16.mask.vcvtph2dq.512(<16 x half> [[TMP0]], <16 x i32> [[TMP2]], i16 [[TMP3]], i32 11)
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <16 x i32> [[TMP4]] to <8 x i64>
// CHECK-NEXT:    ret <8 x i64> [[TMP5]]
//
__m512i test_mm512_mask_cvt_roundph_epi32(__m512i A, __mmask16 B, __m256h C) {
  return _mm512_mask_cvt_roundph_epi32(A, B, C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_maskz_cvt_roundph_epi32(
// CHECK-SAME: i16 noundef zeroext [[A:%.*]], <16 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    store i16 [[A]], ptr [[A_ADDR]], align 2
// CHECK-NEXT:    store <16 x half> [[B]], ptr [[B_ADDR]], align 32
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x half>, ptr [[B_ADDR]], align 32
// CHECK-NEXT:    store <8 x i64> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i64>, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <8 x i64> [[TMP1]] to <16 x i32>
// CHECK-NEXT:    [[TMP3:%.*]] = load i16, ptr [[A_ADDR]], align 2
// CHECK-NEXT:    [[TMP4:%.*]] = call <16 x i32> @llvm.x86.avx512fp16.mask.vcvtph2dq.512(<16 x half> [[TMP0]], <16 x i32> [[TMP2]], i16 [[TMP3]], i32 11)
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <16 x i32> [[TMP4]] to <8 x i64>
// CHECK-NEXT:    ret <8 x i64> [[TMP5]]
//
__m512i test_mm512_maskz_cvt_roundph_epi32(__mmask16 A, __m256h B) {
  return _mm512_maskz_cvt_roundph_epi32(A, B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_cvtph_epi32(
// CHECK-SAME: <16 x half> noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    store <16 x half> [[A]], ptr [[A_ADDR]], align 32
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x half>, ptr [[A_ADDR]], align 32
// CHECK-NEXT:    store <16 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 32
// CHECK-NEXT:    [[TMP1:%.*]] = load <16 x half>, ptr [[__A_ADDR_I]], align 32
// CHECK-NEXT:    store <8 x i64> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x i64>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <8 x i64> [[TMP2]] to <16 x i32>
// CHECK-NEXT:    [[TMP4:%.*]] = call <16 x i32> @llvm.x86.avx512fp16.mask.vcvtph2dq.512(<16 x half> [[TMP1]], <16 x i32> [[TMP3]], i16 -1, i32 4)
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <16 x i32> [[TMP4]] to <8 x i64>
// CHECK-NEXT:    ret <8 x i64> [[TMP5]]
//
__m512i test_mm512_cvtph_epi32(__m256h A) {
  return _mm512_cvtph_epi32(A);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_mask_cvtph_epi32(
// CHECK-SAME: <8 x i64> noundef [[A:%.*]], i16 noundef zeroext [[B:%.*]], <16 x half> noundef [[C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    store <8 x i64> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store i16 [[B]], ptr [[B_ADDR]], align 2
// CHECK-NEXT:    store <16 x half> [[C]], ptr [[C_ADDR]], align 32
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i64>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load i16, ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP2:%.*]] = load <16 x half>, ptr [[C_ADDR]], align 32
// CHECK-NEXT:    store <8 x i64> [[TMP0]], ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    store i16 [[TMP1]], ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    store <16 x half> [[TMP2]], ptr [[__A_ADDR_I]], align 32
// CHECK-NEXT:    [[TMP3:%.*]] = load <16 x half>, ptr [[__A_ADDR_I]], align 32
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x i64>, ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <8 x i64> [[TMP4]] to <16 x i32>
// CHECK-NEXT:    [[TMP6:%.*]] = load i16, ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    [[TMP7:%.*]] = call <16 x i32> @llvm.x86.avx512fp16.mask.vcvtph2dq.512(<16 x half> [[TMP3]], <16 x i32> [[TMP5]], i16 [[TMP6]], i32 4)
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast <16 x i32> [[TMP7]] to <8 x i64>
// CHECK-NEXT:    ret <8 x i64> [[TMP8]]
//
__m512i test_mm512_mask_cvtph_epi32(__m512i A, __mmask16 B, __m256h C) {
  return _mm512_mask_cvtph_epi32(A, B, C);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_maskz_cvtph_epi32(
// CHECK-SAME: i16 noundef zeroext [[A:%.*]], <16 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    store i16 [[A]], ptr [[A_ADDR]], align 2
// CHECK-NEXT:    store <16 x half> [[B]], ptr [[B_ADDR]], align 32
// CHECK-NEXT:    [[TMP0:%.*]] = load i16, ptr [[A_ADDR]], align 2
// CHECK-NEXT:    [[TMP1:%.*]] = load <16 x half>, ptr [[B_ADDR]], align 32
// CHECK-NEXT:    store i16 [[TMP0]], ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    store <16 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <16 x half>, ptr [[__A_ADDR_I]], align 32
// CHECK-NEXT:    store <8 x i64> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x i64>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <8 x i64> [[TMP3]] to <16 x i32>
// CHECK-NEXT:    [[TMP5:%.*]] = load i16, ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    [[TMP6:%.*]] = call <16 x i32> @llvm.x86.avx512fp16.mask.vcvtph2dq.512(<16 x half> [[TMP2]], <16 x i32> [[TMP4]], i16 [[TMP5]], i32 4)
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast <16 x i32> [[TMP6]] to <8 x i64>
// CHECK-NEXT:    ret <8 x i64> [[TMP7]]
//
__m512i test_mm512_maskz_cvtph_epi32(__mmask16 A, __m256h B) {
  return _mm512_maskz_cvtph_epi32(A, B);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_cvt_roundph_epu32(
// CHECK-SAME: <16 x half> noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    store <16 x half> [[A]], ptr [[A_ADDR]], align 32
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x half>, ptr [[A_ADDR]], align 32
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <8 x i64> zeroinitializer to <16 x i32>
// CHECK-NEXT:    [[TMP2:%.*]] = call <16 x i32> @llvm.x86.avx512fp16.mask.vcvtph2udq.512(<16 x half> [[TMP0]], <16 x i32> [[TMP1]], i16 -1, i32 11)
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <16 x i32> [[TMP2]] to <8 x i64>
// CHECK-NEXT:    ret <8 x i64> [[TMP3]]
//
__m512i test_mm512_cvt_roundph_epu32(__m256h A) {
  return _mm512_cvt_roundph_epu32(A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_mask_cvt_roundph_epu32(
// CHECK-SAME: <8 x i64> noundef [[A:%.*]], i16 noundef zeroext [[B:%.*]], <16 x half> noundef [[C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    store <8 x i64> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store i16 [[B]], ptr [[B_ADDR]], align 2
// CHECK-NEXT:    store <16 x half> [[C]], ptr [[C_ADDR]], align 32
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x half>, ptr [[C_ADDR]], align 32
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i64>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <8 x i64> [[TMP1]] to <16 x i32>
// CHECK-NEXT:    [[TMP3:%.*]] = load i16, ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP4:%.*]] = call <16 x i32> @llvm.x86.avx512fp16.mask.vcvtph2udq.512(<16 x half> [[TMP0]], <16 x i32> [[TMP2]], i16 [[TMP3]], i32 11)
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <16 x i32> [[TMP4]] to <8 x i64>
// CHECK-NEXT:    ret <8 x i64> [[TMP5]]
//
__m512i test_mm512_mask_cvt_roundph_epu32(__m512i A, __mmask16 B, __m256h C) {
  return _mm512_mask_cvt_roundph_epu32(A, B, C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_maskz_cvt_roundph_epu32(
// CHECK-SAME: i16 noundef zeroext [[A:%.*]], <16 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    store i16 [[A]], ptr [[A_ADDR]], align 2
// CHECK-NEXT:    store <16 x half> [[B]], ptr [[B_ADDR]], align 32
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x half>, ptr [[B_ADDR]], align 32
// CHECK-NEXT:    store <8 x i64> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i64>, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <8 x i64> [[TMP1]] to <16 x i32>
// CHECK-NEXT:    [[TMP3:%.*]] = load i16, ptr [[A_ADDR]], align 2
// CHECK-NEXT:    [[TMP4:%.*]] = call <16 x i32> @llvm.x86.avx512fp16.mask.vcvtph2udq.512(<16 x half> [[TMP0]], <16 x i32> [[TMP2]], i16 [[TMP3]], i32 11)
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <16 x i32> [[TMP4]] to <8 x i64>
// CHECK-NEXT:    ret <8 x i64> [[TMP5]]
//
__m512i test_mm512_maskz_cvt_roundph_epu32(__mmask16 A, __m256h B) {
  return _mm512_maskz_cvt_roundph_epu32(A, B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_cvtph_epu32(
// CHECK-SAME: <16 x half> noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    store <16 x half> [[A]], ptr [[A_ADDR]], align 32
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x half>, ptr [[A_ADDR]], align 32
// CHECK-NEXT:    store <16 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 32
// CHECK-NEXT:    [[TMP1:%.*]] = load <16 x half>, ptr [[__A_ADDR_I]], align 32
// CHECK-NEXT:    store <8 x i64> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x i64>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <8 x i64> [[TMP2]] to <16 x i32>
// CHECK-NEXT:    [[TMP4:%.*]] = call <16 x i32> @llvm.x86.avx512fp16.mask.vcvtph2udq.512(<16 x half> [[TMP1]], <16 x i32> [[TMP3]], i16 -1, i32 4)
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <16 x i32> [[TMP4]] to <8 x i64>
// CHECK-NEXT:    ret <8 x i64> [[TMP5]]
//
__m512i test_mm512_cvtph_epu32(__m256h A) {
  return _mm512_cvtph_epu32(A);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_mask_cvtph_epu32(
// CHECK-SAME: <8 x i64> noundef [[A:%.*]], i16 noundef zeroext [[B:%.*]], <16 x half> noundef [[C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    store <8 x i64> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store i16 [[B]], ptr [[B_ADDR]], align 2
// CHECK-NEXT:    store <16 x half> [[C]], ptr [[C_ADDR]], align 32
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i64>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load i16, ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP2:%.*]] = load <16 x half>, ptr [[C_ADDR]], align 32
// CHECK-NEXT:    store <8 x i64> [[TMP0]], ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    store i16 [[TMP1]], ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    store <16 x half> [[TMP2]], ptr [[__A_ADDR_I]], align 32
// CHECK-NEXT:    [[TMP3:%.*]] = load <16 x half>, ptr [[__A_ADDR_I]], align 32
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x i64>, ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <8 x i64> [[TMP4]] to <16 x i32>
// CHECK-NEXT:    [[TMP6:%.*]] = load i16, ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    [[TMP7:%.*]] = call <16 x i32> @llvm.x86.avx512fp16.mask.vcvtph2udq.512(<16 x half> [[TMP3]], <16 x i32> [[TMP5]], i16 [[TMP6]], i32 4)
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast <16 x i32> [[TMP7]] to <8 x i64>
// CHECK-NEXT:    ret <8 x i64> [[TMP8]]
//
__m512i test_mm512_mask_cvtph_epu32(__m512i A, __mmask16 B, __m256h C) {
  return _mm512_mask_cvtph_epu32(A, B, C);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_maskz_cvtph_epu32(
// CHECK-SAME: i16 noundef zeroext [[A:%.*]], <16 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    store i16 [[A]], ptr [[A_ADDR]], align 2
// CHECK-NEXT:    store <16 x half> [[B]], ptr [[B_ADDR]], align 32
// CHECK-NEXT:    [[TMP0:%.*]] = load i16, ptr [[A_ADDR]], align 2
// CHECK-NEXT:    [[TMP1:%.*]] = load <16 x half>, ptr [[B_ADDR]], align 32
// CHECK-NEXT:    store i16 [[TMP0]], ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    store <16 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <16 x half>, ptr [[__A_ADDR_I]], align 32
// CHECK-NEXT:    store <8 x i64> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x i64>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <8 x i64> [[TMP3]] to <16 x i32>
// CHECK-NEXT:    [[TMP5:%.*]] = load i16, ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    [[TMP6:%.*]] = call <16 x i32> @llvm.x86.avx512fp16.mask.vcvtph2udq.512(<16 x half> [[TMP2]], <16 x i32> [[TMP4]], i16 [[TMP5]], i32 4)
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast <16 x i32> [[TMP6]] to <8 x i64>
// CHECK-NEXT:    ret <8 x i64> [[TMP7]]
//
__m512i test_mm512_maskz_cvtph_epu32(__mmask16 A, __m256h B) {
  return _mm512_maskz_cvtph_epu32(A, B);
}

// CHECK-LABEL: define dso_local <16 x half> @test_mm512_cvt_roundepi32_ph(
// CHECK-SAME: <8 x i64> noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    store <8 x i64> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i64>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <8 x i64> [[TMP0]] to <16 x i32>
// CHECK-NEXT:    [[TMP2:%.*]] = call <16 x half> @llvm.x86.avx512.sitofp.round.v16f16.v16i32(<16 x i32> [[TMP1]], i32 11)
// CHECK-NEXT:    ret <16 x half> [[TMP2]]
//
__m256h test_mm512_cvt_roundepi32_ph(__m512i A) {
  return _mm512_cvt_roundepi32_ph(A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <16 x half> @test_mm512_mask_cvt_roundepi32_ph(
// CHECK-SAME: <16 x half> noundef [[A:%.*]], i16 noundef zeroext [[B:%.*]], <8 x i64> noundef [[C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    store <16 x half> [[A]], ptr [[A_ADDR]], align 32
// CHECK-NEXT:    store i16 [[B]], ptr [[B_ADDR]], align 2
// CHECK-NEXT:    store <8 x i64> [[C]], ptr [[C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i64>, ptr [[C_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <8 x i64> [[TMP0]] to <16 x i32>
// CHECK-NEXT:    [[TMP2:%.*]] = load <16 x half>, ptr [[A_ADDR]], align 32
// CHECK-NEXT:    [[TMP3:%.*]] = load i16, ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP4:%.*]] = call <16 x half> @llvm.x86.avx512.sitofp.round.v16f16.v16i32(<16 x i32> [[TMP1]], i32 11)
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast i16 [[TMP3]] to <16 x i1>
// CHECK-NEXT:    [[TMP6:%.*]] = select <16 x i1> [[TMP5]], <16 x half> [[TMP4]], <16 x half> [[TMP2]]
// CHECK-NEXT:    ret <16 x half> [[TMP6]]
//
__m256h test_mm512_mask_cvt_roundepi32_ph(__m256h A, __mmask16 B, __m512i C) {
  return _mm512_mask_cvt_roundepi32_ph(A, B, C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <16 x half> @test_mm512_maskz_cvt_roundepi32_ph(
// CHECK-SAME: i16 noundef zeroext [[A:%.*]], <8 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    store i16 [[A]], ptr [[A_ADDR]], align 2
// CHECK-NEXT:    store <8 x i64> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i64>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <8 x i64> [[TMP0]] to <16 x i32>
// CHECK-NEXT:    store <16 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <16 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 32
// CHECK-NEXT:    [[TMP3:%.*]] = load i16, ptr [[A_ADDR]], align 2
// CHECK-NEXT:    [[TMP4:%.*]] = call <16 x half> @llvm.x86.avx512.sitofp.round.v16f16.v16i32(<16 x i32> [[TMP1]], i32 11)
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast i16 [[TMP3]] to <16 x i1>
// CHECK-NEXT:    [[TMP6:%.*]] = select <16 x i1> [[TMP5]], <16 x half> [[TMP4]], <16 x half> [[TMP2]]
// CHECK-NEXT:    ret <16 x half> [[TMP6]]
//
__m256h test_mm512_maskz_cvt_roundepi32_ph(__mmask16 A, __m512i B) {
  return _mm512_maskz_cvt_roundepi32_ph(A, B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <16 x half> @test_mm512_cvtepi32_ph(
// CHECK-SAME: <8 x i64> noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    store <8 x i64> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i64>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <8 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i64>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <8 x i64> [[TMP1]] to <16 x i32>
// CHECK-NEXT:    store <16 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 32
// CHECK-NEXT:    [[TMP3:%.*]] = load <16 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 32
// CHECK-NEXT:    [[TMP4:%.*]] = sitofp <16 x i32> [[TMP2]] to <16 x half>
// CHECK-NEXT:    ret <16 x half> [[TMP4]]
//
__m256h test_mm512_cvtepi32_ph(__m512i A) {
  return _mm512_cvtepi32_ph(A);
}

// CHECK-LABEL: define dso_local <16 x half> @test_mm512_mask_cvtepi32_ph(
// CHECK-SAME: <16 x half> noundef [[A:%.*]], i16 noundef zeroext [[B:%.*]], <8 x i64> noundef [[C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    store <16 x half> [[A]], ptr [[A_ADDR]], align 32
// CHECK-NEXT:    store i16 [[B]], ptr [[B_ADDR]], align 2
// CHECK-NEXT:    store <8 x i64> [[C]], ptr [[C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x half>, ptr [[A_ADDR]], align 32
// CHECK-NEXT:    [[TMP1:%.*]] = load i16, ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x i64>, ptr [[C_ADDR]], align 64
// CHECK-NEXT:    store <16 x half> [[TMP0]], ptr [[__W_ADDR_I]], align 32
// CHECK-NEXT:    store i16 [[TMP1]], ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    store <8 x i64> [[TMP2]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x i64>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <8 x i64> [[TMP3]] to <16 x i32>
// CHECK-NEXT:    [[TMP5:%.*]] = load <16 x half>, ptr [[__W_ADDR_I]], align 32
// CHECK-NEXT:    [[TMP6:%.*]] = load i16, ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    [[TMP7:%.*]] = sitofp <16 x i32> [[TMP4]] to <16 x half>
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast i16 [[TMP6]] to <16 x i1>
// CHECK-NEXT:    [[TMP9:%.*]] = select <16 x i1> [[TMP8]], <16 x half> [[TMP7]], <16 x half> [[TMP5]]
// CHECK-NEXT:    ret <16 x half> [[TMP9]]
//
__m256h test_mm512_mask_cvtepi32_ph(__m256h A, __mmask16 B, __m512i C) {
  return _mm512_mask_cvtepi32_ph(A, B, C);
}

// CHECK-LABEL: define dso_local <16 x half> @test_mm512_maskz_cvtepi32_ph(
// CHECK-SAME: i16 noundef zeroext [[A:%.*]], <8 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    store i16 [[A]], ptr [[A_ADDR]], align 2
// CHECK-NEXT:    store <8 x i64> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load i16, ptr [[A_ADDR]], align 2
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i64>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    store i16 [[TMP0]], ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    store <8 x i64> [[TMP1]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x i64>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <8 x i64> [[TMP2]] to <16 x i32>
// CHECK-NEXT:    store <16 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 32
// CHECK-NEXT:    [[TMP4:%.*]] = load <16 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 32
// CHECK-NEXT:    [[TMP5:%.*]] = load i16, ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    [[TMP6:%.*]] = sitofp <16 x i32> [[TMP3]] to <16 x half>
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast i16 [[TMP5]] to <16 x i1>
// CHECK-NEXT:    [[TMP8:%.*]] = select <16 x i1> [[TMP7]], <16 x half> [[TMP6]], <16 x half> [[TMP4]]
// CHECK-NEXT:    ret <16 x half> [[TMP8]]
//
__m256h test_mm512_maskz_cvtepi32_ph(__mmask16 A, __m512i B) {
  return _mm512_maskz_cvtepi32_ph(A, B);
}

// CHECK-LABEL: define dso_local <16 x half> @test_mm512_cvt_roundepu32_ph(
// CHECK-SAME: <8 x i64> noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    store <8 x i64> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i64>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <8 x i64> [[TMP0]] to <16 x i32>
// CHECK-NEXT:    [[TMP2:%.*]] = call <16 x half> @llvm.x86.avx512.uitofp.round.v16f16.v16i32(<16 x i32> [[TMP1]], i32 11)
// CHECK-NEXT:    ret <16 x half> [[TMP2]]
//
__m256h test_mm512_cvt_roundepu32_ph(__m512i A) {
  return _mm512_cvt_roundepu32_ph(A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <16 x half> @test_mm512_mask_cvt_roundepu32_ph(
// CHECK-SAME: <16 x half> noundef [[A:%.*]], i16 noundef zeroext [[B:%.*]], <8 x i64> noundef [[C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    store <16 x half> [[A]], ptr [[A_ADDR]], align 32
// CHECK-NEXT:    store i16 [[B]], ptr [[B_ADDR]], align 2
// CHECK-NEXT:    store <8 x i64> [[C]], ptr [[C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i64>, ptr [[C_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <8 x i64> [[TMP0]] to <16 x i32>
// CHECK-NEXT:    [[TMP2:%.*]] = load <16 x half>, ptr [[A_ADDR]], align 32
// CHECK-NEXT:    [[TMP3:%.*]] = load i16, ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP4:%.*]] = call <16 x half> @llvm.x86.avx512.uitofp.round.v16f16.v16i32(<16 x i32> [[TMP1]], i32 11)
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast i16 [[TMP3]] to <16 x i1>
// CHECK-NEXT:    [[TMP6:%.*]] = select <16 x i1> [[TMP5]], <16 x half> [[TMP4]], <16 x half> [[TMP2]]
// CHECK-NEXT:    ret <16 x half> [[TMP6]]
//
__m256h test_mm512_mask_cvt_roundepu32_ph(__m256h A, __mmask16 B, __m512i C) {
  return _mm512_mask_cvt_roundepu32_ph(A, B, C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <16 x half> @test_mm512_maskz_cvt_roundepu32_ph(
// CHECK-SAME: i16 noundef zeroext [[A:%.*]], <8 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    store i16 [[A]], ptr [[A_ADDR]], align 2
// CHECK-NEXT:    store <8 x i64> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i64>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <8 x i64> [[TMP0]] to <16 x i32>
// CHECK-NEXT:    store <16 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <16 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 32
// CHECK-NEXT:    [[TMP3:%.*]] = load i16, ptr [[A_ADDR]], align 2
// CHECK-NEXT:    [[TMP4:%.*]] = call <16 x half> @llvm.x86.avx512.uitofp.round.v16f16.v16i32(<16 x i32> [[TMP1]], i32 11)
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast i16 [[TMP3]] to <16 x i1>
// CHECK-NEXT:    [[TMP6:%.*]] = select <16 x i1> [[TMP5]], <16 x half> [[TMP4]], <16 x half> [[TMP2]]
// CHECK-NEXT:    ret <16 x half> [[TMP6]]
//
__m256h test_mm512_maskz_cvt_roundepu32_ph(__mmask16 A, __m512i B) {
  return _mm512_maskz_cvt_roundepu32_ph(A, B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <16 x half> @test_mm512_cvtepu32_ph(
// CHECK-SAME: <8 x i64> noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    store <8 x i64> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i64>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <8 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i64>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <8 x i64> [[TMP1]] to <16 x i32>
// CHECK-NEXT:    store <16 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 32
// CHECK-NEXT:    [[TMP3:%.*]] = load <16 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 32
// CHECK-NEXT:    [[TMP4:%.*]] = uitofp <16 x i32> [[TMP2]] to <16 x half>
// CHECK-NEXT:    ret <16 x half> [[TMP4]]
//
__m256h test_mm512_cvtepu32_ph(__m512i A) {
  return _mm512_cvtepu32_ph(A);
}

// CHECK-LABEL: define dso_local <16 x half> @test_mm512_mask_cvtepu32_ph(
// CHECK-SAME: <16 x half> noundef [[A:%.*]], i16 noundef zeroext [[B:%.*]], <8 x i64> noundef [[C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    store <16 x half> [[A]], ptr [[A_ADDR]], align 32
// CHECK-NEXT:    store i16 [[B]], ptr [[B_ADDR]], align 2
// CHECK-NEXT:    store <8 x i64> [[C]], ptr [[C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x half>, ptr [[A_ADDR]], align 32
// CHECK-NEXT:    [[TMP1:%.*]] = load i16, ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x i64>, ptr [[C_ADDR]], align 64
// CHECK-NEXT:    store <16 x half> [[TMP0]], ptr [[__W_ADDR_I]], align 32
// CHECK-NEXT:    store i16 [[TMP1]], ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    store <8 x i64> [[TMP2]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x i64>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <8 x i64> [[TMP3]] to <16 x i32>
// CHECK-NEXT:    [[TMP5:%.*]] = load <16 x half>, ptr [[__W_ADDR_I]], align 32
// CHECK-NEXT:    [[TMP6:%.*]] = load i16, ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    [[TMP7:%.*]] = uitofp <16 x i32> [[TMP4]] to <16 x half>
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast i16 [[TMP6]] to <16 x i1>
// CHECK-NEXT:    [[TMP9:%.*]] = select <16 x i1> [[TMP8]], <16 x half> [[TMP7]], <16 x half> [[TMP5]]
// CHECK-NEXT:    ret <16 x half> [[TMP9]]
//
__m256h test_mm512_mask_cvtepu32_ph(__m256h A, __mmask16 B, __m512i C) {
  return _mm512_mask_cvtepu32_ph(A, B, C);
}

// CHECK-LABEL: define dso_local <16 x half> @test_mm512_maskz_cvtepu32_ph(
// CHECK-SAME: i16 noundef zeroext [[A:%.*]], <8 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    store i16 [[A]], ptr [[A_ADDR]], align 2
// CHECK-NEXT:    store <8 x i64> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load i16, ptr [[A_ADDR]], align 2
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i64>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    store i16 [[TMP0]], ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    store <8 x i64> [[TMP1]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x i64>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <8 x i64> [[TMP2]] to <16 x i32>
// CHECK-NEXT:    store <16 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 32
// CHECK-NEXT:    [[TMP4:%.*]] = load <16 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 32
// CHECK-NEXT:    [[TMP5:%.*]] = load i16, ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    [[TMP6:%.*]] = uitofp <16 x i32> [[TMP3]] to <16 x half>
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast i16 [[TMP5]] to <16 x i1>
// CHECK-NEXT:    [[TMP8:%.*]] = select <16 x i1> [[TMP7]], <16 x half> [[TMP6]], <16 x half> [[TMP4]]
// CHECK-NEXT:    ret <16 x half> [[TMP8]]
//
__m256h test_mm512_maskz_cvtepu32_ph(__mmask16 A, __m512i B) {
  return _mm512_maskz_cvtepu32_ph(A, B);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_cvtt_roundph_epi32(
// CHECK-SAME: <16 x half> noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    store <16 x half> [[A]], ptr [[A_ADDR]], align 32
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x half>, ptr [[A_ADDR]], align 32
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <8 x i64> zeroinitializer to <16 x i32>
// CHECK-NEXT:    [[TMP2:%.*]] = call <16 x i32> @llvm.x86.avx512fp16.mask.vcvttph2dq.512(<16 x half> [[TMP0]], <16 x i32> [[TMP1]], i16 -1, i32 8)
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <16 x i32> [[TMP2]] to <8 x i64>
// CHECK-NEXT:    ret <8 x i64> [[TMP3]]
//
__m512i test_mm512_cvtt_roundph_epi32(__m256h A) {
  return _mm512_cvtt_roundph_epi32(A, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_mask_cvtt_roundph_epi32(
// CHECK-SAME: <8 x i64> noundef [[A:%.*]], i16 noundef zeroext [[B:%.*]], <16 x half> noundef [[C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    store <8 x i64> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store i16 [[B]], ptr [[B_ADDR]], align 2
// CHECK-NEXT:    store <16 x half> [[C]], ptr [[C_ADDR]], align 32
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x half>, ptr [[C_ADDR]], align 32
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i64>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <8 x i64> [[TMP1]] to <16 x i32>
// CHECK-NEXT:    [[TMP3:%.*]] = load i16, ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP4:%.*]] = call <16 x i32> @llvm.x86.avx512fp16.mask.vcvttph2dq.512(<16 x half> [[TMP0]], <16 x i32> [[TMP2]], i16 [[TMP3]], i32 8)
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <16 x i32> [[TMP4]] to <8 x i64>
// CHECK-NEXT:    ret <8 x i64> [[TMP5]]
//
__m512i test_mm512_mask_cvtt_roundph_epi32(__m512i A, __mmask16 B, __m256h C) {
  return _mm512_mask_cvtt_roundph_epi32(A, B, C, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_maskz_cvtt_roundph_epi32(
// CHECK-SAME: i16 noundef zeroext [[A:%.*]], <16 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    store i16 [[A]], ptr [[A_ADDR]], align 2
// CHECK-NEXT:    store <16 x half> [[B]], ptr [[B_ADDR]], align 32
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x half>, ptr [[B_ADDR]], align 32
// CHECK-NEXT:    store <8 x i64> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i64>, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <8 x i64> [[TMP1]] to <16 x i32>
// CHECK-NEXT:    [[TMP3:%.*]] = load i16, ptr [[A_ADDR]], align 2
// CHECK-NEXT:    [[TMP4:%.*]] = call <16 x i32> @llvm.x86.avx512fp16.mask.vcvttph2dq.512(<16 x half> [[TMP0]], <16 x i32> [[TMP2]], i16 [[TMP3]], i32 8)
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <16 x i32> [[TMP4]] to <8 x i64>
// CHECK-NEXT:    ret <8 x i64> [[TMP5]]
//
__m512i test_mm512_maskz_cvtt_roundph_epi32(__mmask16 A, __m256h B) {
  return _mm512_maskz_cvtt_roundph_epi32(A, B, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_cvttph_epi32(
// CHECK-SAME: <16 x half> noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    store <16 x half> [[A]], ptr [[A_ADDR]], align 32
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x half>, ptr [[A_ADDR]], align 32
// CHECK-NEXT:    store <16 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 32
// CHECK-NEXT:    [[TMP1:%.*]] = load <16 x half>, ptr [[__A_ADDR_I]], align 32
// CHECK-NEXT:    store <8 x i64> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x i64>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <8 x i64> [[TMP2]] to <16 x i32>
// CHECK-NEXT:    [[TMP4:%.*]] = call <16 x i32> @llvm.x86.avx512fp16.mask.vcvttph2dq.512(<16 x half> [[TMP1]], <16 x i32> [[TMP3]], i16 -1, i32 4)
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <16 x i32> [[TMP4]] to <8 x i64>
// CHECK-NEXT:    ret <8 x i64> [[TMP5]]
//
__m512i test_mm512_cvttph_epi32(__m256h A) {
  return _mm512_cvttph_epi32(A);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_mask_cvttph_epi32(
// CHECK-SAME: <8 x i64> noundef [[A:%.*]], i16 noundef zeroext [[B:%.*]], <16 x half> noundef [[C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    store <8 x i64> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store i16 [[B]], ptr [[B_ADDR]], align 2
// CHECK-NEXT:    store <16 x half> [[C]], ptr [[C_ADDR]], align 32
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i64>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load i16, ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP2:%.*]] = load <16 x half>, ptr [[C_ADDR]], align 32
// CHECK-NEXT:    store <8 x i64> [[TMP0]], ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    store i16 [[TMP1]], ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    store <16 x half> [[TMP2]], ptr [[__A_ADDR_I]], align 32
// CHECK-NEXT:    [[TMP3:%.*]] = load <16 x half>, ptr [[__A_ADDR_I]], align 32
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x i64>, ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <8 x i64> [[TMP4]] to <16 x i32>
// CHECK-NEXT:    [[TMP6:%.*]] = load i16, ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    [[TMP7:%.*]] = call <16 x i32> @llvm.x86.avx512fp16.mask.vcvttph2dq.512(<16 x half> [[TMP3]], <16 x i32> [[TMP5]], i16 [[TMP6]], i32 4)
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast <16 x i32> [[TMP7]] to <8 x i64>
// CHECK-NEXT:    ret <8 x i64> [[TMP8]]
//
__m512i test_mm512_mask_cvttph_epi32(__m512i A, __mmask16 B, __m256h C) {
  return _mm512_mask_cvttph_epi32(A, B, C);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_maskz_cvttph_epi32(
// CHECK-SAME: i16 noundef zeroext [[A:%.*]], <16 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    store i16 [[A]], ptr [[A_ADDR]], align 2
// CHECK-NEXT:    store <16 x half> [[B]], ptr [[B_ADDR]], align 32
// CHECK-NEXT:    [[TMP0:%.*]] = load i16, ptr [[A_ADDR]], align 2
// CHECK-NEXT:    [[TMP1:%.*]] = load <16 x half>, ptr [[B_ADDR]], align 32
// CHECK-NEXT:    store i16 [[TMP0]], ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    store <16 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <16 x half>, ptr [[__A_ADDR_I]], align 32
// CHECK-NEXT:    store <8 x i64> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x i64>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <8 x i64> [[TMP3]] to <16 x i32>
// CHECK-NEXT:    [[TMP5:%.*]] = load i16, ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    [[TMP6:%.*]] = call <16 x i32> @llvm.x86.avx512fp16.mask.vcvttph2dq.512(<16 x half> [[TMP2]], <16 x i32> [[TMP4]], i16 [[TMP5]], i32 4)
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast <16 x i32> [[TMP6]] to <8 x i64>
// CHECK-NEXT:    ret <8 x i64> [[TMP7]]
//
__m512i test_mm512_maskz_cvttph_epi32(__mmask16 A, __m256h B) {
  return _mm512_maskz_cvttph_epi32(A, B);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_cvtt_roundph_epu32(
// CHECK-SAME: <16 x half> noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    store <16 x half> [[A]], ptr [[A_ADDR]], align 32
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x half>, ptr [[A_ADDR]], align 32
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <8 x i64> zeroinitializer to <16 x i32>
// CHECK-NEXT:    [[TMP2:%.*]] = call <16 x i32> @llvm.x86.avx512fp16.mask.vcvttph2udq.512(<16 x half> [[TMP0]], <16 x i32> [[TMP1]], i16 -1, i32 8)
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <16 x i32> [[TMP2]] to <8 x i64>
// CHECK-NEXT:    ret <8 x i64> [[TMP3]]
//
__m512i test_mm512_cvtt_roundph_epu32(__m256h A) {
  return _mm512_cvtt_roundph_epu32(A, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_mask_cvtt_roundph_epu32(
// CHECK-SAME: <8 x i64> noundef [[A:%.*]], i16 noundef zeroext [[B:%.*]], <16 x half> noundef [[C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    store <8 x i64> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store i16 [[B]], ptr [[B_ADDR]], align 2
// CHECK-NEXT:    store <16 x half> [[C]], ptr [[C_ADDR]], align 32
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x half>, ptr [[C_ADDR]], align 32
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i64>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <8 x i64> [[TMP1]] to <16 x i32>
// CHECK-NEXT:    [[TMP3:%.*]] = load i16, ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP4:%.*]] = call <16 x i32> @llvm.x86.avx512fp16.mask.vcvttph2udq.512(<16 x half> [[TMP0]], <16 x i32> [[TMP2]], i16 [[TMP3]], i32 8)
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <16 x i32> [[TMP4]] to <8 x i64>
// CHECK-NEXT:    ret <8 x i64> [[TMP5]]
//
__m512i test_mm512_mask_cvtt_roundph_epu32(__m512i A, __mmask16 B, __m256h C) {
  return _mm512_mask_cvtt_roundph_epu32(A, B, C, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_maskz_cvtt_roundph_epu32(
// CHECK-SAME: i16 noundef zeroext [[A:%.*]], <16 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    store i16 [[A]], ptr [[A_ADDR]], align 2
// CHECK-NEXT:    store <16 x half> [[B]], ptr [[B_ADDR]], align 32
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x half>, ptr [[B_ADDR]], align 32
// CHECK-NEXT:    store <8 x i64> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i64>, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <8 x i64> [[TMP1]] to <16 x i32>
// CHECK-NEXT:    [[TMP3:%.*]] = load i16, ptr [[A_ADDR]], align 2
// CHECK-NEXT:    [[TMP4:%.*]] = call <16 x i32> @llvm.x86.avx512fp16.mask.vcvttph2udq.512(<16 x half> [[TMP0]], <16 x i32> [[TMP2]], i16 [[TMP3]], i32 8)
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <16 x i32> [[TMP4]] to <8 x i64>
// CHECK-NEXT:    ret <8 x i64> [[TMP5]]
//
__m512i test_mm512_maskz_cvtt_roundph_epu32(__mmask16 A, __m256h B) {
  return _mm512_maskz_cvtt_roundph_epu32(A, B, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_cvttph_epu32(
// CHECK-SAME: <16 x half> noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    store <16 x half> [[A]], ptr [[A_ADDR]], align 32
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x half>, ptr [[A_ADDR]], align 32
// CHECK-NEXT:    store <16 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 32
// CHECK-NEXT:    [[TMP1:%.*]] = load <16 x half>, ptr [[__A_ADDR_I]], align 32
// CHECK-NEXT:    store <8 x i64> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x i64>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <8 x i64> [[TMP2]] to <16 x i32>
// CHECK-NEXT:    [[TMP4:%.*]] = call <16 x i32> @llvm.x86.avx512fp16.mask.vcvttph2udq.512(<16 x half> [[TMP1]], <16 x i32> [[TMP3]], i16 -1, i32 4)
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <16 x i32> [[TMP4]] to <8 x i64>
// CHECK-NEXT:    ret <8 x i64> [[TMP5]]
//
__m512i test_mm512_cvttph_epu32(__m256h A) {
  return _mm512_cvttph_epu32(A);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_mask_cvttph_epu32(
// CHECK-SAME: <8 x i64> noundef [[A:%.*]], i16 noundef zeroext [[B:%.*]], <16 x half> noundef [[C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    store <8 x i64> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store i16 [[B]], ptr [[B_ADDR]], align 2
// CHECK-NEXT:    store <16 x half> [[C]], ptr [[C_ADDR]], align 32
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i64>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load i16, ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP2:%.*]] = load <16 x half>, ptr [[C_ADDR]], align 32
// CHECK-NEXT:    store <8 x i64> [[TMP0]], ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    store i16 [[TMP1]], ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    store <16 x half> [[TMP2]], ptr [[__A_ADDR_I]], align 32
// CHECK-NEXT:    [[TMP3:%.*]] = load <16 x half>, ptr [[__A_ADDR_I]], align 32
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x i64>, ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <8 x i64> [[TMP4]] to <16 x i32>
// CHECK-NEXT:    [[TMP6:%.*]] = load i16, ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    [[TMP7:%.*]] = call <16 x i32> @llvm.x86.avx512fp16.mask.vcvttph2udq.512(<16 x half> [[TMP3]], <16 x i32> [[TMP5]], i16 [[TMP6]], i32 4)
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast <16 x i32> [[TMP7]] to <8 x i64>
// CHECK-NEXT:    ret <8 x i64> [[TMP8]]
//
__m512i test_mm512_mask_cvttph_epu32(__m512i A, __mmask16 B, __m256h C) {
  return _mm512_mask_cvttph_epu32(A, B, C);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_maskz_cvttph_epu32(
// CHECK-SAME: i16 noundef zeroext [[A:%.*]], <16 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    store i16 [[A]], ptr [[A_ADDR]], align 2
// CHECK-NEXT:    store <16 x half> [[B]], ptr [[B_ADDR]], align 32
// CHECK-NEXT:    [[TMP0:%.*]] = load i16, ptr [[A_ADDR]], align 2
// CHECK-NEXT:    [[TMP1:%.*]] = load <16 x half>, ptr [[B_ADDR]], align 32
// CHECK-NEXT:    store i16 [[TMP0]], ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    store <16 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <16 x half>, ptr [[__A_ADDR_I]], align 32
// CHECK-NEXT:    store <8 x i64> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x i64>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <8 x i64> [[TMP3]] to <16 x i32>
// CHECK-NEXT:    [[TMP5:%.*]] = load i16, ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    [[TMP6:%.*]] = call <16 x i32> @llvm.x86.avx512fp16.mask.vcvttph2udq.512(<16 x half> [[TMP2]], <16 x i32> [[TMP4]], i16 [[TMP5]], i32 4)
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast <16 x i32> [[TMP6]] to <8 x i64>
// CHECK-NEXT:    ret <8 x i64> [[TMP7]]
//
__m512i test_mm512_maskz_cvttph_epu32(__mmask16 A, __m256h B) {
  return _mm512_maskz_cvttph_epu32(A, B);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm512_cvt_roundepi64_ph(
// CHECK-SAME: <8 x i64> noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    store <8 x i64> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i64>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = call <8 x half> @llvm.x86.avx512.sitofp.round.v8f16.v8i64(<8 x i64> [[TMP0]], i32 11)
// CHECK-NEXT:    ret <8 x half> [[TMP1]]
//
__m128h test_mm512_cvt_roundepi64_ph(__m512i A) {
  return _mm512_cvt_roundepi64_ph(A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm512_mask_cvt_roundepi64_ph(
// CHECK-SAME: <8 x half> noundef [[A:%.*]], i8 noundef zeroext [[B:%.*]], <8 x i64> noundef [[C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    store <8 x half> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store i8 [[B]], ptr [[B_ADDR]], align 1
// CHECK-NEXT:    store <8 x i64> [[C]], ptr [[C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i64>, ptr [[C_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load i8, ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP3:%.*]] = call <8 x half> @llvm.x86.avx512.sitofp.round.v8f16.v8i64(<8 x i64> [[TMP0]], i32 11)
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast i8 [[TMP2]] to <8 x i1>
// CHECK-NEXT:    [[TMP5:%.*]] = select <8 x i1> [[TMP4]], <8 x half> [[TMP3]], <8 x half> [[TMP1]]
// CHECK-NEXT:    ret <8 x half> [[TMP5]]
//
__m128h test_mm512_mask_cvt_roundepi64_ph(__m128h A, __mmask8 B, __m512i C) {
  return _mm512_mask_cvt_roundepi64_ph(A, B, C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm512_maskz_cvt_roundepi64_ph(
// CHECK-SAME: i8 noundef zeroext [[A:%.*]], <8 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    store i8 [[A]], ptr [[A_ADDR]], align 1
// CHECK-NEXT:    store <8 x i64> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i64>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load i8, ptr [[A_ADDR]], align 1
// CHECK-NEXT:    [[TMP3:%.*]] = call <8 x half> @llvm.x86.avx512.sitofp.round.v8f16.v8i64(<8 x i64> [[TMP0]], i32 11)
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast i8 [[TMP2]] to <8 x i1>
// CHECK-NEXT:    [[TMP5:%.*]] = select <8 x i1> [[TMP4]], <8 x half> [[TMP3]], <8 x half> [[TMP1]]
// CHECK-NEXT:    ret <8 x half> [[TMP5]]
//
__m128h test_mm512_maskz_cvt_roundepi64_ph(__mmask8 A, __m512i B) {
  return _mm512_maskz_cvt_roundepi64_ph(A, B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm512_cvtepi64_ph(
// CHECK-SAME: <8 x i64> noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    store <8 x i64> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i64>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <8 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i64>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = sitofp <8 x i64> [[TMP1]] to <8 x half>
// CHECK-NEXT:    ret <8 x half> [[TMP3]]
//
__m128h test_mm512_cvtepi64_ph(__m512i A) {
  return _mm512_cvtepi64_ph(A);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm512_mask_cvtepi64_ph(
// CHECK-SAME: <8 x half> noundef [[A:%.*]], i8 noundef zeroext [[B:%.*]], <8 x i64> noundef [[C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    store <8 x half> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store i8 [[B]], ptr [[B_ADDR]], align 1
// CHECK-NEXT:    store <8 x i64> [[C]], ptr [[C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x i64>, ptr [[C_ADDR]], align 64
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    store i8 [[TMP1]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x i64> [[TMP2]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x i64>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP6:%.*]] = sitofp <8 x i64> [[TMP3]] to <8 x half>
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast i8 [[TMP5]] to <8 x i1>
// CHECK-NEXT:    [[TMP8:%.*]] = select <8 x i1> [[TMP7]], <8 x half> [[TMP6]], <8 x half> [[TMP4]]
// CHECK-NEXT:    ret <8 x half> [[TMP8]]
//
__m128h test_mm512_mask_cvtepi64_ph(__m128h A, __mmask8 B, __m512i C) {
  return _mm512_mask_cvtepi64_ph(A, B, C);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm512_maskz_cvtepi64_ph(
// CHECK-SAME: i8 noundef zeroext [[A:%.*]], <8 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    store i8 [[A]], ptr [[A_ADDR]], align 1
// CHECK-NEXT:    store <8 x i64> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load i8, ptr [[A_ADDR]], align 1
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i64>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    store i8 [[TMP0]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x i64> [[TMP1]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x i64>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP5:%.*]] = sitofp <8 x i64> [[TMP2]] to <8 x half>
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast i8 [[TMP4]] to <8 x i1>
// CHECK-NEXT:    [[TMP7:%.*]] = select <8 x i1> [[TMP6]], <8 x half> [[TMP5]], <8 x half> [[TMP3]]
// CHECK-NEXT:    ret <8 x half> [[TMP7]]
//
__m128h test_mm512_maskz_cvtepi64_ph(__mmask8 A, __m512i B) {
  return _mm512_maskz_cvtepi64_ph(A, B);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_cvt_roundph_epi64(
// CHECK-SAME: <8 x half> noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = call <8 x i64> @llvm.x86.avx512fp16.mask.vcvtph2qq.512(<8 x half> [[TMP0]], <8 x i64> zeroinitializer, i8 -1, i32 11)
// CHECK-NEXT:    ret <8 x i64> [[TMP1]]
//
__m512i test_mm512_cvt_roundph_epi64(__m128h A) {
  return _mm512_cvt_roundph_epi64(A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_mask_cvt_roundph_epi64(
// CHECK-SAME: <8 x i64> noundef [[A:%.*]], i8 noundef zeroext [[B:%.*]], <8 x half> noundef [[C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x i64> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store i8 [[B]], ptr [[B_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[C]], ptr [[C_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[C_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i64>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i8, ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP3:%.*]] = call <8 x i64> @llvm.x86.avx512fp16.mask.vcvtph2qq.512(<8 x half> [[TMP0]], <8 x i64> [[TMP1]], i8 [[TMP2]], i32 11)
// CHECK-NEXT:    ret <8 x i64> [[TMP3]]
//
__m512i test_mm512_mask_cvt_roundph_epi64(__m512i A, __mmask8 B, __m128h C) {
  return _mm512_mask_cvt_roundph_epi64(A, B, C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_maskz_cvt_roundph_epi64(
// CHECK-SAME: i8 noundef zeroext [[A:%.*]], <8 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[A]], ptr [[A_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[B]], ptr [[B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[B_ADDR]], align 16
// CHECK-NEXT:    store <8 x i64> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i64>, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i8, ptr [[A_ADDR]], align 1
// CHECK-NEXT:    [[TMP3:%.*]] = call <8 x i64> @llvm.x86.avx512fp16.mask.vcvtph2qq.512(<8 x half> [[TMP0]], <8 x i64> [[TMP1]], i8 [[TMP2]], i32 11)
// CHECK-NEXT:    ret <8 x i64> [[TMP3]]
//
__m512i test_mm512_maskz_cvt_roundph_epi64(__mmask8 A, __m128h B) {
  return _mm512_maskz_cvt_roundph_epi64(A, B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_cvtph_epi64(
// CHECK-SAME: <8 x half> noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x i64> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x i64>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = call <8 x i64> @llvm.x86.avx512fp16.mask.vcvtph2qq.512(<8 x half> [[TMP1]], <8 x i64> [[TMP2]], i8 -1, i32 4)
// CHECK-NEXT:    ret <8 x i64> [[TMP3]]
//
__m512i test_mm512_cvtph_epi64(__m128h A) {
  return _mm512_cvtph_epi64(A);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_mask_cvtph_epi64(
// CHECK-SAME: <8 x i64> noundef [[A:%.*]], i8 noundef zeroext [[B:%.*]], <8 x half> noundef [[C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x i64> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store i8 [[B]], ptr [[B_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[C]], ptr [[C_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i64>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[C_ADDR]], align 16
// CHECK-NEXT:    store <8 x i64> [[TMP0]], ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    store i8 [[TMP1]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x i64>, ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP6:%.*]] = call <8 x i64> @llvm.x86.avx512fp16.mask.vcvtph2qq.512(<8 x half> [[TMP3]], <8 x i64> [[TMP4]], i8 [[TMP5]], i32 4)
// CHECK-NEXT:    ret <8 x i64> [[TMP6]]
//
__m512i test_mm512_mask_cvtph_epi64(__m512i A, __mmask8 B, __m128h C) {
  return _mm512_mask_cvtph_epi64(A, B, C);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_maskz_cvtph_epi64(
// CHECK-SAME: i8 noundef zeroext [[A:%.*]], <8 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[A]], ptr [[A_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[B]], ptr [[B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load i8, ptr [[A_ADDR]], align 1
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[B_ADDR]], align 16
// CHECK-NEXT:    store i8 [[TMP0]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x i64> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x i64>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP5:%.*]] = call <8 x i64> @llvm.x86.avx512fp16.mask.vcvtph2qq.512(<8 x half> [[TMP2]], <8 x i64> [[TMP3]], i8 [[TMP4]], i32 4)
// CHECK-NEXT:    ret <8 x i64> [[TMP5]]
//
__m512i test_mm512_maskz_cvtph_epi64(__mmask8 A, __m128h B) {
  return _mm512_maskz_cvtph_epi64(A, B);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm512_cvt_roundepu64_ph(
// CHECK-SAME: <8 x i64> noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    store <8 x i64> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i64>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = call <8 x half> @llvm.x86.avx512.uitofp.round.v8f16.v8i64(<8 x i64> [[TMP0]], i32 11)
// CHECK-NEXT:    ret <8 x half> [[TMP1]]
//
__m128h test_mm512_cvt_roundepu64_ph(__m512i A) {
  return _mm512_cvt_roundepu64_ph(A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm512_mask_cvt_roundepu64_ph(
// CHECK-SAME: <8 x half> noundef [[A:%.*]], i8 noundef zeroext [[B:%.*]], <8 x i64> noundef [[C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    store <8 x half> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store i8 [[B]], ptr [[B_ADDR]], align 1
// CHECK-NEXT:    store <8 x i64> [[C]], ptr [[C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i64>, ptr [[C_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load i8, ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP3:%.*]] = call <8 x half> @llvm.x86.avx512.uitofp.round.v8f16.v8i64(<8 x i64> [[TMP0]], i32 11)
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast i8 [[TMP2]] to <8 x i1>
// CHECK-NEXT:    [[TMP5:%.*]] = select <8 x i1> [[TMP4]], <8 x half> [[TMP3]], <8 x half> [[TMP1]]
// CHECK-NEXT:    ret <8 x half> [[TMP5]]
//
__m128h test_mm512_mask_cvt_roundepu64_ph(__m128h A, __mmask8 B, __m512i C) {
  return _mm512_mask_cvt_roundepu64_ph(A, B, C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm512_maskz_cvt_roundepu64_ph(
// CHECK-SAME: i8 noundef zeroext [[A:%.*]], <8 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    store i8 [[A]], ptr [[A_ADDR]], align 1
// CHECK-NEXT:    store <8 x i64> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i64>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load i8, ptr [[A_ADDR]], align 1
// CHECK-NEXT:    [[TMP3:%.*]] = call <8 x half> @llvm.x86.avx512.uitofp.round.v8f16.v8i64(<8 x i64> [[TMP0]], i32 11)
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast i8 [[TMP2]] to <8 x i1>
// CHECK-NEXT:    [[TMP5:%.*]] = select <8 x i1> [[TMP4]], <8 x half> [[TMP3]], <8 x half> [[TMP1]]
// CHECK-NEXT:    ret <8 x half> [[TMP5]]
//
__m128h test_mm512_maskz_cvt_roundepu64_ph(__mmask8 A, __m512i B) {
  return _mm512_maskz_cvt_roundepu64_ph(A, B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm512_cvtepu64_ph(
// CHECK-SAME: <8 x i64> noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    store <8 x i64> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i64>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <8 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i64>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = uitofp <8 x i64> [[TMP1]] to <8 x half>
// CHECK-NEXT:    ret <8 x half> [[TMP3]]
//
__m128h test_mm512_cvtepu64_ph(__m512i A) {
  return _mm512_cvtepu64_ph(A);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm512_mask_cvtepu64_ph(
// CHECK-SAME: <8 x half> noundef [[A:%.*]], i8 noundef zeroext [[B:%.*]], <8 x i64> noundef [[C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    store <8 x half> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store i8 [[B]], ptr [[B_ADDR]], align 1
// CHECK-NEXT:    store <8 x i64> [[C]], ptr [[C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x i64>, ptr [[C_ADDR]], align 64
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    store i8 [[TMP1]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x i64> [[TMP2]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x i64>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP6:%.*]] = uitofp <8 x i64> [[TMP3]] to <8 x half>
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast i8 [[TMP5]] to <8 x i1>
// CHECK-NEXT:    [[TMP8:%.*]] = select <8 x i1> [[TMP7]], <8 x half> [[TMP6]], <8 x half> [[TMP4]]
// CHECK-NEXT:    ret <8 x half> [[TMP8]]
//
__m128h test_mm512_mask_cvtepu64_ph(__m128h A, __mmask8 B, __m512i C) {
  return _mm512_mask_cvtepu64_ph(A, B, C);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm512_maskz_cvtepu64_ph(
// CHECK-SAME: i8 noundef zeroext [[A:%.*]], <8 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    store i8 [[A]], ptr [[A_ADDR]], align 1
// CHECK-NEXT:    store <8 x i64> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load i8, ptr [[A_ADDR]], align 1
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i64>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    store i8 [[TMP0]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x i64> [[TMP1]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x i64>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP5:%.*]] = uitofp <8 x i64> [[TMP2]] to <8 x half>
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast i8 [[TMP4]] to <8 x i1>
// CHECK-NEXT:    [[TMP7:%.*]] = select <8 x i1> [[TMP6]], <8 x half> [[TMP5]], <8 x half> [[TMP3]]
// CHECK-NEXT:    ret <8 x half> [[TMP7]]
//
__m128h test_mm512_maskz_cvtepu64_ph(__mmask8 A, __m512i B) {
  return _mm512_maskz_cvtepu64_ph(A, B);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_cvt_roundph_epu64(
// CHECK-SAME: <8 x half> noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = call <8 x i64> @llvm.x86.avx512fp16.mask.vcvtph2uqq.512(<8 x half> [[TMP0]], <8 x i64> zeroinitializer, i8 -1, i32 11)
// CHECK-NEXT:    ret <8 x i64> [[TMP1]]
//
__m512i test_mm512_cvt_roundph_epu64(__m128h A) {
  return _mm512_cvt_roundph_epu64(A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_mask_cvt_roundph_epu64(
// CHECK-SAME: <8 x i64> noundef [[A:%.*]], i8 noundef zeroext [[B:%.*]], <8 x half> noundef [[C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x i64> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store i8 [[B]], ptr [[B_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[C]], ptr [[C_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[C_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i64>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i8, ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP3:%.*]] = call <8 x i64> @llvm.x86.avx512fp16.mask.vcvtph2uqq.512(<8 x half> [[TMP0]], <8 x i64> [[TMP1]], i8 [[TMP2]], i32 11)
// CHECK-NEXT:    ret <8 x i64> [[TMP3]]
//
__m512i test_mm512_mask_cvt_roundph_epu64(__m512i A, __mmask8 B, __m128h C) {
  return _mm512_mask_cvt_roundph_epu64(A, B, C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_maskz_cvt_roundph_epu64(
// CHECK-SAME: i8 noundef zeroext [[A:%.*]], <8 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[A]], ptr [[A_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[B]], ptr [[B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[B_ADDR]], align 16
// CHECK-NEXT:    store <8 x i64> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i64>, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i8, ptr [[A_ADDR]], align 1
// CHECK-NEXT:    [[TMP3:%.*]] = call <8 x i64> @llvm.x86.avx512fp16.mask.vcvtph2uqq.512(<8 x half> [[TMP0]], <8 x i64> [[TMP1]], i8 [[TMP2]], i32 11)
// CHECK-NEXT:    ret <8 x i64> [[TMP3]]
//
__m512i test_mm512_maskz_cvt_roundph_epu64(__mmask8 A, __m128h B) {
  return _mm512_maskz_cvt_roundph_epu64(A, B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_cvtph_epu64(
// CHECK-SAME: <8 x half> noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x i64> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x i64>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = call <8 x i64> @llvm.x86.avx512fp16.mask.vcvtph2uqq.512(<8 x half> [[TMP1]], <8 x i64> [[TMP2]], i8 -1, i32 4)
// CHECK-NEXT:    ret <8 x i64> [[TMP3]]
//
__m512i test_mm512_cvtph_epu64(__m128h A) {
  return _mm512_cvtph_epu64(A);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_mask_cvtph_epu64(
// CHECK-SAME: <8 x i64> noundef [[A:%.*]], i8 noundef zeroext [[B:%.*]], <8 x half> noundef [[C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x i64> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store i8 [[B]], ptr [[B_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[C]], ptr [[C_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i64>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[C_ADDR]], align 16
// CHECK-NEXT:    store <8 x i64> [[TMP0]], ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    store i8 [[TMP1]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x i64>, ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP6:%.*]] = call <8 x i64> @llvm.x86.avx512fp16.mask.vcvtph2uqq.512(<8 x half> [[TMP3]], <8 x i64> [[TMP4]], i8 [[TMP5]], i32 4)
// CHECK-NEXT:    ret <8 x i64> [[TMP6]]
//
__m512i test_mm512_mask_cvtph_epu64(__m512i A, __mmask8 B, __m128h C) {
  return _mm512_mask_cvtph_epu64(A, B, C);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_maskz_cvtph_epu64(
// CHECK-SAME: i8 noundef zeroext [[A:%.*]], <8 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[A]], ptr [[A_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[B]], ptr [[B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load i8, ptr [[A_ADDR]], align 1
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[B_ADDR]], align 16
// CHECK-NEXT:    store i8 [[TMP0]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x i64> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x i64>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP5:%.*]] = call <8 x i64> @llvm.x86.avx512fp16.mask.vcvtph2uqq.512(<8 x half> [[TMP2]], <8 x i64> [[TMP3]], i8 [[TMP4]], i32 4)
// CHECK-NEXT:    ret <8 x i64> [[TMP5]]
//
__m512i test_mm512_maskz_cvtph_epu64(__mmask8 A, __m128h B) {
  return _mm512_maskz_cvtph_epu64(A, B);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_cvtt_roundph_epi64(
// CHECK-SAME: <8 x half> noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = call <8 x i64> @llvm.x86.avx512fp16.mask.vcvttph2qq.512(<8 x half> [[TMP0]], <8 x i64> zeroinitializer, i8 -1, i32 8)
// CHECK-NEXT:    ret <8 x i64> [[TMP1]]
//
__m512i test_mm512_cvtt_roundph_epi64(__m128h A) {
  return _mm512_cvtt_roundph_epi64(A, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_mask_cvtt_roundph_epi64(
// CHECK-SAME: <8 x i64> noundef [[A:%.*]], i8 noundef zeroext [[B:%.*]], <8 x half> noundef [[C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x i64> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store i8 [[B]], ptr [[B_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[C]], ptr [[C_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[C_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i64>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i8, ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP3:%.*]] = call <8 x i64> @llvm.x86.avx512fp16.mask.vcvttph2qq.512(<8 x half> [[TMP0]], <8 x i64> [[TMP1]], i8 [[TMP2]], i32 8)
// CHECK-NEXT:    ret <8 x i64> [[TMP3]]
//
__m512i test_mm512_mask_cvtt_roundph_epi64(__m512i A, __mmask8 B, __m128h C) {
  return _mm512_mask_cvtt_roundph_epi64(A, B, C, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_maskz_cvtt_roundph_epi64(
// CHECK-SAME: i8 noundef zeroext [[A:%.*]], <8 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[A]], ptr [[A_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[B]], ptr [[B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[B_ADDR]], align 16
// CHECK-NEXT:    store <8 x i64> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i64>, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i8, ptr [[A_ADDR]], align 1
// CHECK-NEXT:    [[TMP3:%.*]] = call <8 x i64> @llvm.x86.avx512fp16.mask.vcvttph2qq.512(<8 x half> [[TMP0]], <8 x i64> [[TMP1]], i8 [[TMP2]], i32 8)
// CHECK-NEXT:    ret <8 x i64> [[TMP3]]
//
__m512i test_mm512_maskz_cvtt_roundph_epi64(__mmask8 A, __m128h B) {
  return _mm512_maskz_cvtt_roundph_epi64(A, B, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_cvttph_epi64(
// CHECK-SAME: <8 x half> noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x i64> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x i64>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = call <8 x i64> @llvm.x86.avx512fp16.mask.vcvttph2qq.512(<8 x half> [[TMP1]], <8 x i64> [[TMP2]], i8 -1, i32 4)
// CHECK-NEXT:    ret <8 x i64> [[TMP3]]
//
__m512i test_mm512_cvttph_epi64(__m128h A) {
  return _mm512_cvttph_epi64(A);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_mask_cvttph_epi64(
// CHECK-SAME: <8 x i64> noundef [[A:%.*]], i8 noundef zeroext [[B:%.*]], <8 x half> noundef [[C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x i64> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store i8 [[B]], ptr [[B_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[C]], ptr [[C_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i64>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[C_ADDR]], align 16
// CHECK-NEXT:    store <8 x i64> [[TMP0]], ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    store i8 [[TMP1]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x i64>, ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP6:%.*]] = call <8 x i64> @llvm.x86.avx512fp16.mask.vcvttph2qq.512(<8 x half> [[TMP3]], <8 x i64> [[TMP4]], i8 [[TMP5]], i32 4)
// CHECK-NEXT:    ret <8 x i64> [[TMP6]]
//
__m512i test_mm512_mask_cvttph_epi64(__m512i A, __mmask8 B, __m128h C) {
  return _mm512_mask_cvttph_epi64(A, B, C);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_maskz_cvttph_epi64(
// CHECK-SAME: i8 noundef zeroext [[A:%.*]], <8 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[A]], ptr [[A_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[B]], ptr [[B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load i8, ptr [[A_ADDR]], align 1
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[B_ADDR]], align 16
// CHECK-NEXT:    store i8 [[TMP0]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x i64> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x i64>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP5:%.*]] = call <8 x i64> @llvm.x86.avx512fp16.mask.vcvttph2qq.512(<8 x half> [[TMP2]], <8 x i64> [[TMP3]], i8 [[TMP4]], i32 4)
// CHECK-NEXT:    ret <8 x i64> [[TMP5]]
//
__m512i test_mm512_maskz_cvttph_epi64(__mmask8 A, __m128h B) {
  return _mm512_maskz_cvttph_epi64(A, B);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_cvtt_roundph_epu64(
// CHECK-SAME: <8 x half> noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = call <8 x i64> @llvm.x86.avx512fp16.mask.vcvttph2uqq.512(<8 x half> [[TMP0]], <8 x i64> zeroinitializer, i8 -1, i32 8)
// CHECK-NEXT:    ret <8 x i64> [[TMP1]]
//
__m512i test_mm512_cvtt_roundph_epu64(__m128h A) {
  return _mm512_cvtt_roundph_epu64(A, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_mask_cvtt_roundph_epu64(
// CHECK-SAME: <8 x i64> noundef [[A:%.*]], i8 noundef zeroext [[B:%.*]], <8 x half> noundef [[C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x i64> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store i8 [[B]], ptr [[B_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[C]], ptr [[C_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[C_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i64>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i8, ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP3:%.*]] = call <8 x i64> @llvm.x86.avx512fp16.mask.vcvttph2uqq.512(<8 x half> [[TMP0]], <8 x i64> [[TMP1]], i8 [[TMP2]], i32 8)
// CHECK-NEXT:    ret <8 x i64> [[TMP3]]
//
__m512i test_mm512_mask_cvtt_roundph_epu64(__m512i A, __mmask8 B, __m128h C) {
  return _mm512_mask_cvtt_roundph_epu64(A, B, C, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_maskz_cvtt_roundph_epu64(
// CHECK-SAME: i8 noundef zeroext [[A:%.*]], <8 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[A]], ptr [[A_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[B]], ptr [[B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[B_ADDR]], align 16
// CHECK-NEXT:    store <8 x i64> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i64>, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i8, ptr [[A_ADDR]], align 1
// CHECK-NEXT:    [[TMP3:%.*]] = call <8 x i64> @llvm.x86.avx512fp16.mask.vcvttph2uqq.512(<8 x half> [[TMP0]], <8 x i64> [[TMP1]], i8 [[TMP2]], i32 8)
// CHECK-NEXT:    ret <8 x i64> [[TMP3]]
//
__m512i test_mm512_maskz_cvtt_roundph_epu64(__mmask8 A, __m128h B) {
  return _mm512_maskz_cvtt_roundph_epu64(A, B, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_cvttph_epu64(
// CHECK-SAME: <8 x half> noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x i64> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x i64>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = call <8 x i64> @llvm.x86.avx512fp16.mask.vcvttph2uqq.512(<8 x half> [[TMP1]], <8 x i64> [[TMP2]], i8 -1, i32 4)
// CHECK-NEXT:    ret <8 x i64> [[TMP3]]
//
__m512i test_mm512_cvttph_epu64(__m128h A) {
  return _mm512_cvttph_epu64(A);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_mask_cvttph_epu64(
// CHECK-SAME: <8 x i64> noundef [[A:%.*]], i8 noundef zeroext [[B:%.*]], <8 x half> noundef [[C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x i64> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store i8 [[B]], ptr [[B_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[C]], ptr [[C_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i64>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[B_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[C_ADDR]], align 16
// CHECK-NEXT:    store <8 x i64> [[TMP0]], ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    store i8 [[TMP1]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x i64>, ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP6:%.*]] = call <8 x i64> @llvm.x86.avx512fp16.mask.vcvttph2uqq.512(<8 x half> [[TMP3]], <8 x i64> [[TMP4]], i8 [[TMP5]], i32 4)
// CHECK-NEXT:    ret <8 x i64> [[TMP6]]
//
__m512i test_mm512_mask_cvttph_epu64(__m512i A, __mmask8 B, __m128h C) {
  return _mm512_mask_cvttph_epu64(A, B, C);
}

// CHECK-LABEL: define dso_local <8 x i64> @test_mm512_maskz_cvttph_epu64(
// CHECK-SAME: i8 noundef zeroext [[A:%.*]], <8 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[A]], ptr [[A_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[B]], ptr [[B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load i8, ptr [[A_ADDR]], align 1
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[B_ADDR]], align 16
// CHECK-NEXT:    store i8 [[TMP0]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x i64> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x i64>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP5:%.*]] = call <8 x i64> @llvm.x86.avx512fp16.mask.vcvttph2uqq.512(<8 x half> [[TMP2]], <8 x i64> [[TMP3]], i8 [[TMP4]], i32 4)
// CHECK-NEXT:    ret <8 x i64> [[TMP5]]
//
__m512i test_mm512_maskz_cvttph_epu64(__mmask8 A, __m128h B) {
  return _mm512_maskz_cvttph_epu64(A, B);
}

// CHECK-LABEL: define dso_local i32 @test_mm_cvt_roundsh_i32(
// CHECK-SAME: <8 x half> noundef [[A:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.x86.avx512fp16.vcvtsh2si32(<8 x half> [[TMP0]], i32 11)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
int test_mm_cvt_roundsh_i32(__m128h A) {
  return _mm_cvt_roundsh_i32(A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local i32 @test_mm_cvtsh_i32(
// CHECK-SAME: <8 x half> noundef [[A:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.x86.avx512fp16.vcvtsh2si32(<8 x half> [[TMP1]], i32 4)
// CHECK-NEXT:    ret i32 [[TMP2]]
//
int test_mm_cvtsh_i32(__m128h A) {
  return _mm_cvtsh_i32(A);
}

// CHECK-LABEL: define dso_local i32 @test_mm_cvt_roundsh_u32(
// CHECK-SAME: <8 x half> noundef [[A:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.x86.avx512fp16.vcvtsh2usi32(<8 x half> [[TMP0]], i32 11)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
unsigned int test_mm_cvt_roundsh_u32(__m128h A) {
  return _mm_cvt_roundsh_u32(A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local i32 @test_mm_cvtsh_u32(
// CHECK-SAME: <8 x half> noundef [[A:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.x86.avx512fp16.vcvtsh2usi32(<8 x half> [[TMP1]], i32 4)
// CHECK-NEXT:    ret i32 [[TMP2]]
//
unsigned int test_mm_cvtsh_u32(__m128h A) {
  return _mm_cvtsh_u32(A);
}

#ifdef __x86_64__
// CHECK-LABEL: define dso_local i64 @test_mm_cvt_roundsh_i64(
// CHECK-SAME: <8 x half> noundef [[A:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = call i64 @llvm.x86.avx512fp16.vcvtsh2si64(<8 x half> [[TMP0]], i32 11)
// CHECK-NEXT:    ret i64 [[TMP1]]
//
long long test_mm_cvt_roundsh_i64(__m128h A) {
  return _mm_cvt_roundsh_i64(A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local i64 @test_mm_cvtsh_i64(
// CHECK-SAME: <8 x half> noundef [[A:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = call i64 @llvm.x86.avx512fp16.vcvtsh2si64(<8 x half> [[TMP1]], i32 4)
// CHECK-NEXT:    ret i64 [[TMP2]]
//
long long test_mm_cvtsh_i64(__m128h A) {
  return _mm_cvtsh_i64(A);
}

// CHECK-LABEL: define dso_local i64 @test_mm_cvt_roundsh_u64(
// CHECK-SAME: <8 x half> noundef [[A:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = call i64 @llvm.x86.avx512fp16.vcvtsh2usi64(<8 x half> [[TMP0]], i32 11)
// CHECK-NEXT:    ret i64 [[TMP1]]
//
unsigned long long test_mm_cvt_roundsh_u64(__m128h A) {
  return _mm_cvt_roundsh_u64(A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local i64 @test_mm_cvtsh_u64(
// CHECK-SAME: <8 x half> noundef [[A:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = call i64 @llvm.x86.avx512fp16.vcvtsh2usi64(<8 x half> [[TMP1]], i32 4)
// CHECK-NEXT:    ret i64 [[TMP2]]
//
unsigned long long test_mm_cvtsh_u64(__m128h A) {
  return _mm_cvtsh_u64(A);
}
#endif

// CHECK-LABEL: define dso_local <8 x half> @test_mm_cvt_roundu32_sh(
// CHECK-SAME: <8 x half> noundef [[A:%.*]], i32 noundef [[B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store <8 x half> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store i32 [[B]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call <8 x half> @llvm.x86.avx512fp16.vcvtusi2sh(<8 x half> [[TMP0]], i32 [[TMP1]], i32 11)
// CHECK-NEXT:    ret <8 x half> [[TMP2]]
//
__m128h test_mm_cvt_roundu32_sh(__m128h A, unsigned int B) {
  return _mm_cvt_roundu32_sh(A, B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_cvtu32_sh(
// CHECK-SAME: <8 x half> noundef [[A:%.*]], i32 noundef [[B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store <8 x half> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store i32 [[B]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[__B_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[__B_ADDR_I]], align 4
// CHECK-NEXT:    [[CONV_I:%.*]] = uitofp i32 [[TMP2]] to half
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[VECINS_I:%.*]] = insertelement <8 x half> [[TMP3]], half [[CONV_I]], i32 0
// CHECK-NEXT:    store <8 x half> [[VECINS_I]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    ret <8 x half> [[TMP4]]
//
__m128h test_mm_cvtu32_sh(__m128h A, unsigned int B) {
  return _mm_cvtu32_sh(A, B);
}

#ifdef __x86_64__
// CHECK-LABEL: define dso_local <8 x half> @test_mm_cvt_roundu64_sh(
// CHECK-SAME: <8 x half> noundef [[A:%.*]], i64 noundef [[B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i64, align 8
// CHECK-NEXT:    store <8 x half> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store i64 [[B]], ptr [[B_ADDR]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load i64, ptr [[B_ADDR]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = call <8 x half> @llvm.x86.avx512fp16.vcvtusi642sh(<8 x half> [[TMP0]], i64 [[TMP1]], i32 11)
// CHECK-NEXT:    ret <8 x half> [[TMP2]]
//
__m128h test_mm_cvt_roundu64_sh(__m128h A, unsigned long long B) {
  return _mm_cvt_roundu64_sh(A, B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_cvtu64_sh(
// CHECK-SAME: <8 x half> noundef [[A:%.*]], i64 noundef [[B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca i64, align 8
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i64, align 8
// CHECK-NEXT:    store <8 x half> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store i64 [[B]], ptr [[B_ADDR]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load i64, ptr [[B_ADDR]], align 8
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store i64 [[TMP1]], ptr [[__B_ADDR_I]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = load i64, ptr [[__B_ADDR_I]], align 8
// CHECK-NEXT:    [[CONV_I:%.*]] = uitofp i64 [[TMP2]] to half
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[VECINS_I:%.*]] = insertelement <8 x half> [[TMP3]], half [[CONV_I]], i32 0
// CHECK-NEXT:    store <8 x half> [[VECINS_I]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    ret <8 x half> [[TMP4]]
//
__m128h test_mm_cvtu64_sh(__m128h A, unsigned long long B) {
  return _mm_cvtu64_sh(A, B);
}
#endif

// CHECK-LABEL: define dso_local <8 x half> @test_mm_cvt_roundi32_sh(
// CHECK-SAME: <8 x half> noundef [[A:%.*]], i32 noundef [[B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store <8 x half> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store i32 [[B]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call <8 x half> @llvm.x86.avx512fp16.vcvtsi2sh(<8 x half> [[TMP0]], i32 [[TMP1]], i32 11)
// CHECK-NEXT:    ret <8 x half> [[TMP2]]
//
__m128h test_mm_cvt_roundi32_sh(__m128h A, int B) {
  return _mm_cvt_roundi32_sh(A, B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_cvti32_sh(
// CHECK-SAME: <8 x half> noundef [[A:%.*]], i32 noundef [[B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store <8 x half> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store i32 [[B]], ptr [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[B_ADDR]], align 4
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[__B_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[__B_ADDR_I]], align 4
// CHECK-NEXT:    [[CONV_I:%.*]] = sitofp i32 [[TMP2]] to half
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[VECINS_I:%.*]] = insertelement <8 x half> [[TMP3]], half [[CONV_I]], i32 0
// CHECK-NEXT:    store <8 x half> [[VECINS_I]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    ret <8 x half> [[TMP4]]
//
__m128h test_mm_cvti32_sh(__m128h A, int B) {
  return _mm_cvti32_sh(A, B);
}

#ifdef __x86_64__
// CHECK-LABEL: define dso_local <8 x half> @test_mm_cvt_roundi64_sh(
// CHECK-SAME: <8 x half> noundef [[A:%.*]], i64 noundef [[B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i64, align 8
// CHECK-NEXT:    store <8 x half> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store i64 [[B]], ptr [[B_ADDR]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load i64, ptr [[B_ADDR]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = call <8 x half> @llvm.x86.avx512fp16.vcvtsi642sh(<8 x half> [[TMP0]], i64 [[TMP1]], i32 11)
// CHECK-NEXT:    ret <8 x half> [[TMP2]]
//
__m128h test_mm_cvt_roundi64_sh(__m128h A, long long B) {
  return _mm_cvt_roundi64_sh(A, B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_cvti64_sh(
// CHECK-SAME: <8 x half> noundef [[A:%.*]], i64 noundef [[B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca i64, align 8
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i64, align 8
// CHECK-NEXT:    store <8 x half> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store i64 [[B]], ptr [[B_ADDR]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load i64, ptr [[B_ADDR]], align 8
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store i64 [[TMP1]], ptr [[__B_ADDR_I]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = load i64, ptr [[__B_ADDR_I]], align 8
// CHECK-NEXT:    [[CONV_I:%.*]] = sitofp i64 [[TMP2]] to half
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[VECINS_I:%.*]] = insertelement <8 x half> [[TMP3]], half [[CONV_I]], i32 0
// CHECK-NEXT:    store <8 x half> [[VECINS_I]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    ret <8 x half> [[TMP4]]
//
__m128h test_mm_cvti64_sh(__m128h A, long long B) {
  return _mm_cvti64_sh(A, B);
}
#endif

// CHECK-LABEL: define dso_local i32 @test_mm_cvtt_roundsh_i32(
// CHECK-SAME: <8 x half> noundef [[A:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.x86.avx512fp16.vcvttsh2si32(<8 x half> [[TMP0]], i32 8)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
int test_mm_cvtt_roundsh_i32(__m128h A) {
  return _mm_cvtt_roundsh_i32(A, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local i32 @test_mm_cvttsh_i32(
// CHECK-SAME: <8 x half> noundef [[A:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.x86.avx512fp16.vcvttsh2si32(<8 x half> [[TMP1]], i32 4)
// CHECK-NEXT:    ret i32 [[TMP2]]
//
int test_mm_cvttsh_i32(__m128h A) {
  return _mm_cvttsh_i32(A);
}

#ifdef __x86_64__
// CHECK-LABEL: define dso_local i64 @test_mm_cvtt_roundsh_i64(
// CHECK-SAME: <8 x half> noundef [[A:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = call i64 @llvm.x86.avx512fp16.vcvttsh2si64(<8 x half> [[TMP0]], i32 8)
// CHECK-NEXT:    ret i64 [[TMP1]]
//
long long test_mm_cvtt_roundsh_i64(__m128h A) {
  return _mm_cvtt_roundsh_i64(A, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local i64 @test_mm_cvttsh_i64(
// CHECK-SAME: <8 x half> noundef [[A:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = call i64 @llvm.x86.avx512fp16.vcvttsh2si64(<8 x half> [[TMP1]], i32 4)
// CHECK-NEXT:    ret i64 [[TMP2]]
//
long long test_mm_cvttsh_i64(__m128h A) {
  return _mm_cvttsh_i64(A);
}
#endif

// CHECK-LABEL: define dso_local i32 @test_mm_cvtt_roundsh_u32(
// CHECK-SAME: <8 x half> noundef [[A:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.x86.avx512fp16.vcvttsh2usi32(<8 x half> [[TMP0]], i32 8)
// CHECK-NEXT:    ret i32 [[TMP1]]
//
unsigned int test_mm_cvtt_roundsh_u32(__m128h A) {
  return _mm_cvtt_roundsh_u32(A, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local i32 @test_mm_cvttsh_u32(
// CHECK-SAME: <8 x half> noundef [[A:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.x86.avx512fp16.vcvttsh2usi32(<8 x half> [[TMP1]], i32 4)
// CHECK-NEXT:    ret i32 [[TMP2]]
//
unsigned int test_mm_cvttsh_u32(__m128h A) {
  return _mm_cvttsh_u32(A);
}

#ifdef __x86_64__
// CHECK-LABEL: define dso_local i64 @test_mm_cvtt_roundsh_u64(
// CHECK-SAME: <8 x half> noundef [[A:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = call i64 @llvm.x86.avx512fp16.vcvttsh2usi64(<8 x half> [[TMP0]], i32 8)
// CHECK-NEXT:    ret i64 [[TMP1]]
//
unsigned long long test_mm_cvtt_roundsh_u64(__m128h A) {
  return _mm_cvtt_roundsh_u64(A, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local i64 @test_mm_cvttsh_u64(
// CHECK-SAME: <8 x half> noundef [[A:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[A]], ptr [[A_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = call i64 @llvm.x86.avx512fp16.vcvttsh2usi64(<8 x half> [[TMP1]], i32 4)
// CHECK-NEXT:    ret i64 [[TMP2]]
//
unsigned long long test_mm_cvttsh_u64(__m128h A) {
  return _mm_cvttsh_u64(A);
}
#endif

// CHECK-LABEL: define dso_local <16 x float> @test_mm512_cvtx_roundph_ps(
// CHECK-SAME: <16 x half> noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    store <16 x half> [[A]], ptr [[A_ADDR]], align 32
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x half>, ptr [[A_ADDR]], align 32
// CHECK-NEXT:    [[TMP1:%.*]] = call <16 x float> @llvm.x86.avx512fp16.mask.vcvtph2psx.512(<16 x half> [[TMP0]], <16 x float> zeroinitializer, i16 -1, i32 8)
// CHECK-NEXT:    ret <16 x float> [[TMP1]]
//
__m512 test_mm512_cvtx_roundph_ps(__m256h A) {
  return _mm512_cvtx_roundph_ps(A, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <16 x float> @test_mm512_mask_cvtx_roundph_ps(
// CHECK-SAME: <16 x float> noundef [[A:%.*]], i16 noundef zeroext [[B:%.*]], <16 x half> noundef [[C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <16 x float>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    store <16 x float> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store i16 [[B]], ptr [[B_ADDR]], align 2
// CHECK-NEXT:    store <16 x half> [[C]], ptr [[C_ADDR]], align 32
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x half>, ptr [[C_ADDR]], align 32
// CHECK-NEXT:    [[TMP1:%.*]] = load <16 x float>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i16, ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP3:%.*]] = call <16 x float> @llvm.x86.avx512fp16.mask.vcvtph2psx.512(<16 x half> [[TMP0]], <16 x float> [[TMP1]], i16 [[TMP2]], i32 8)
// CHECK-NEXT:    ret <16 x float> [[TMP3]]
//
__m512 test_mm512_mask_cvtx_roundph_ps(__m512 A, __mmask16 B, __m256h C) {
  return _mm512_mask_cvtx_roundph_ps(A, B, C, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <16 x float> @test_mm512_maskz_cvtx_roundph_ps(
// CHECK-SAME: i16 noundef zeroext [[A:%.*]], <16 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <16 x float>, align 64
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    store i16 [[A]], ptr [[A_ADDR]], align 2
// CHECK-NEXT:    store <16 x half> [[B]], ptr [[B_ADDR]], align 32
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x half>, ptr [[B_ADDR]], align 32
// CHECK-NEXT:    store <16 x float> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <16 x float>, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load i16, ptr [[A_ADDR]], align 2
// CHECK-NEXT:    [[TMP3:%.*]] = call <16 x float> @llvm.x86.avx512fp16.mask.vcvtph2psx.512(<16 x half> [[TMP0]], <16 x float> [[TMP1]], i16 [[TMP2]], i32 8)
// CHECK-NEXT:    ret <16 x float> [[TMP3]]
//
__m512 test_mm512_maskz_cvtx_roundph_ps(__mmask16 A, __m256h B) {
  return _mm512_maskz_cvtx_roundph_ps(A, B, _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <16 x float> @test_mm512_cvtxph_ps(
// CHECK-SAME: <16 x half> noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <16 x float>, align 64
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    store <16 x half> [[A]], ptr [[A_ADDR]], align 32
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x half>, ptr [[A_ADDR]], align 32
// CHECK-NEXT:    store <16 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 32
// CHECK-NEXT:    [[TMP1:%.*]] = load <16 x half>, ptr [[__A_ADDR_I]], align 32
// CHECK-NEXT:    store <16 x float> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <16 x float>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = call <16 x float> @llvm.x86.avx512fp16.mask.vcvtph2psx.512(<16 x half> [[TMP1]], <16 x float> [[TMP2]], i16 -1, i32 4)
// CHECK-NEXT:    ret <16 x float> [[TMP3]]
//
__m512 test_mm512_cvtxph_ps(__m256h A) {
  return _mm512_cvtxph_ps(A);
}

// CHECK-LABEL: define dso_local <16 x float> @test_mm512_mask_cvtxph_ps(
// CHECK-SAME: <16 x float> noundef [[A:%.*]], i16 noundef zeroext [[B:%.*]], <16 x half> noundef [[C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <16 x float>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <16 x float>, align 64
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    store <16 x float> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store i16 [[B]], ptr [[B_ADDR]], align 2
// CHECK-NEXT:    store <16 x half> [[C]], ptr [[C_ADDR]], align 32
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x float>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load i16, ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP2:%.*]] = load <16 x half>, ptr [[C_ADDR]], align 32
// CHECK-NEXT:    store <16 x float> [[TMP0]], ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    store i16 [[TMP1]], ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    store <16 x half> [[TMP2]], ptr [[__A_ADDR_I]], align 32
// CHECK-NEXT:    [[TMP3:%.*]] = load <16 x half>, ptr [[__A_ADDR_I]], align 32
// CHECK-NEXT:    [[TMP4:%.*]] = load <16 x float>, ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = load i16, ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    [[TMP6:%.*]] = call <16 x float> @llvm.x86.avx512fp16.mask.vcvtph2psx.512(<16 x half> [[TMP3]], <16 x float> [[TMP4]], i16 [[TMP5]], i32 4)
// CHECK-NEXT:    ret <16 x float> [[TMP6]]
//
__m512 test_mm512_mask_cvtxph_ps(__m512 A, __mmask16 B, __m256h C) {
  return _mm512_mask_cvtxph_ps(A, B, C);
}

// CHECK-LABEL: define dso_local <16 x float> @test_mm512_maskz_cvtxph_ps(
// CHECK-SAME: i16 noundef zeroext [[A:%.*]], <16 x half> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <16 x float>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    store i16 [[A]], ptr [[A_ADDR]], align 2
// CHECK-NEXT:    store <16 x half> [[B]], ptr [[B_ADDR]], align 32
// CHECK-NEXT:    [[TMP0:%.*]] = load i16, ptr [[A_ADDR]], align 2
// CHECK-NEXT:    [[TMP1:%.*]] = load <16 x half>, ptr [[B_ADDR]], align 32
// CHECK-NEXT:    store i16 [[TMP0]], ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    store <16 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <16 x half>, ptr [[__A_ADDR_I]], align 32
// CHECK-NEXT:    store <16 x float> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <16 x float>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load i16, ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    [[TMP5:%.*]] = call <16 x float> @llvm.x86.avx512fp16.mask.vcvtph2psx.512(<16 x half> [[TMP2]], <16 x float> [[TMP3]], i16 [[TMP4]], i32 4)
// CHECK-NEXT:    ret <16 x float> [[TMP5]]
//
__m512 test_mm512_maskz_cvtxph_ps(__mmask16 A, __m256h B) {
  return _mm512_maskz_cvtxph_ps(A, B);
}

// CHECK-LABEL: define dso_local <16 x half> @test_mm512_cvtx_roundps_ph(
// CHECK-SAME: <16 x float> noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <16 x float>, align 64
// CHECK-NEXT:    store <16 x float> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x float>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = call <16 x half> @llvm.x86.avx512fp16.mask.vcvtps2phx.512(<16 x float> [[TMP0]], <16 x half> zeroinitializer, i16 -1, i32 11)
// CHECK-NEXT:    ret <16 x half> [[TMP1]]
//
__m256h test_mm512_cvtx_roundps_ph(__m512 A) {
  return _mm512_cvtx_roundps_ph(A, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <16 x half> @test_mm512_mask_cvtx_roundps_ph(
// CHECK-SAME: <16 x half> noundef [[A:%.*]], i16 noundef zeroext [[B:%.*]], <16 x float> noundef [[C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <16 x float>, align 64
// CHECK-NEXT:    store <16 x half> [[A]], ptr [[A_ADDR]], align 32
// CHECK-NEXT:    store i16 [[B]], ptr [[B_ADDR]], align 2
// CHECK-NEXT:    store <16 x float> [[C]], ptr [[C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x float>, ptr [[C_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <16 x half>, ptr [[A_ADDR]], align 32
// CHECK-NEXT:    [[TMP2:%.*]] = load i16, ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP3:%.*]] = call <16 x half> @llvm.x86.avx512fp16.mask.vcvtps2phx.512(<16 x float> [[TMP0]], <16 x half> [[TMP1]], i16 [[TMP2]], i32 11)
// CHECK-NEXT:    ret <16 x half> [[TMP3]]
//
__m256h test_mm512_mask_cvtx_roundps_ph(__m256h A, __mmask16 B, __m512 C) {
  return _mm512_mask_cvtx_roundps_ph(A, B, C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <16 x half> @test_mm512_maskz_cvtx_roundps_ph(
// CHECK-SAME: i16 noundef zeroext [[A:%.*]], <16 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <16 x float>, align 64
// CHECK-NEXT:    store i16 [[A]], ptr [[A_ADDR]], align 2
// CHECK-NEXT:    store <16 x float> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x float>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    store <16 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 32
// CHECK-NEXT:    [[TMP1:%.*]] = load <16 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 32
// CHECK-NEXT:    [[TMP2:%.*]] = load i16, ptr [[A_ADDR]], align 2
// CHECK-NEXT:    [[TMP3:%.*]] = call <16 x half> @llvm.x86.avx512fp16.mask.vcvtps2phx.512(<16 x float> [[TMP0]], <16 x half> [[TMP1]], i16 [[TMP2]], i32 11)
// CHECK-NEXT:    ret <16 x half> [[TMP3]]
//
__m256h test_mm512_maskz_cvtx_roundps_ph(__mmask16 A, __m512 B) {
  return _mm512_maskz_cvtx_roundps_ph(A, B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <16 x half> @test_mm512_cvtxps_ph(
// CHECK-SAME: <16 x float> noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <16 x float>, align 64
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <16 x float>, align 64
// CHECK-NEXT:    store <16 x float> [[A]], ptr [[A_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x float>, ptr [[A_ADDR]], align 64
// CHECK-NEXT:    store <16 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <16 x float>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <16 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <16 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 32
// CHECK-NEXT:    [[TMP3:%.*]] = call <16 x half> @llvm.x86.avx512fp16.mask.vcvtps2phx.512(<16 x float> [[TMP1]], <16 x half> [[TMP2]], i16 -1, i32 4)
// CHECK-NEXT:    ret <16 x half> [[TMP3]]
//
__m256h test_mm512_cvtxps_ph(__m512 A) {
  return _mm512_cvtxps_ph(A);
}

// CHECK-LABEL: define dso_local <16 x half> @test_mm512_mask_cvtxps_ph(
// CHECK-SAME: <16 x half> noundef [[A:%.*]], i16 noundef zeroext [[B:%.*]], <16 x float> noundef [[C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <16 x float>, align 64
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca <16 x float>, align 64
// CHECK-NEXT:    store <16 x half> [[A]], ptr [[A_ADDR]], align 32
// CHECK-NEXT:    store i16 [[B]], ptr [[B_ADDR]], align 2
// CHECK-NEXT:    store <16 x float> [[C]], ptr [[C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x half>, ptr [[A_ADDR]], align 32
// CHECK-NEXT:    [[TMP1:%.*]] = load i16, ptr [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP2:%.*]] = load <16 x float>, ptr [[C_ADDR]], align 64
// CHECK-NEXT:    store <16 x half> [[TMP0]], ptr [[__W_ADDR_I]], align 32
// CHECK-NEXT:    store i16 [[TMP1]], ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    store <16 x float> [[TMP2]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <16 x float>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load <16 x half>, ptr [[__W_ADDR_I]], align 32
// CHECK-NEXT:    [[TMP5:%.*]] = load i16, ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    [[TMP6:%.*]] = call <16 x half> @llvm.x86.avx512fp16.mask.vcvtps2phx.512(<16 x float> [[TMP3]], <16 x half> [[TMP4]], i16 [[TMP5]], i32 4)
// CHECK-NEXT:    ret <16 x half> [[TMP6]]
//
__m256h test_mm512_mask_cvtxps_ph(__m256h A, __mmask16 B, __m512 C) {
  return _mm512_mask_cvtxps_ph(A, B, C);
}

// CHECK-LABEL: define dso_local <16 x half> @test_mm512_maskz_cvtxps_ph(
// CHECK-SAME: i16 noundef zeroext [[A:%.*]], <16 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <16 x half>, align 32
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <16 x float>, align 64
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <16 x float>, align 64
// CHECK-NEXT:    store i16 [[A]], ptr [[A_ADDR]], align 2
// CHECK-NEXT:    store <16 x float> [[B]], ptr [[B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load i16, ptr [[A_ADDR]], align 2
// CHECK-NEXT:    [[TMP1:%.*]] = load <16 x float>, ptr [[B_ADDR]], align 64
// CHECK-NEXT:    store i16 [[TMP0]], ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    store <16 x float> [[TMP1]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <16 x float>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <16 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 32
// CHECK-NEXT:    [[TMP3:%.*]] = load <16 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 32
// CHECK-NEXT:    [[TMP4:%.*]] = load i16, ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    [[TMP5:%.*]] = call <16 x half> @llvm.x86.avx512fp16.mask.vcvtps2phx.512(<16 x float> [[TMP2]], <16 x half> [[TMP3]], i16 [[TMP4]], i32 4)
// CHECK-NEXT:    ret <16 x half> [[TMP5]]
//
__m256h test_mm512_maskz_cvtxps_ph(__mmask16 A, __m512 B) {
  return _mm512_maskz_cvtxps_ph(A, B);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_fmadd_round_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = call <32 x half> @llvm.x86.avx512fp16.vfmadd.ph.512(<32 x half> [[TMP0]], <32 x half> [[TMP1]], <32 x half> [[TMP2]], i32 11)
// CHECK-NEXT:    ret <32 x half> [[TMP3]]
//
__m512h test_mm512_fmadd_round_ph(__m512h __A, __m512h __B, __m512h __C) {
  return _mm512_fmadd_round_ph(__A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask_fmadd_round_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], i32 noundef [[__U:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = call <32 x half> @llvm.x86.avx512fp16.vfmadd.ph.512(<32 x half> [[TMP0]], <32 x half> [[TMP1]], <32 x half> [[TMP2]], i32 11)
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast i32 [[TMP3]] to <32 x i1>
// CHECK-NEXT:    [[TMP6:%.*]] = select <32 x i1> [[TMP5]], <32 x half> [[TMP4]], <32 x half> [[TMP0]]
// CHECK-NEXT:    ret <32 x half> [[TMP6]]
//
__m512h test_mm512_mask_fmadd_round_ph(__m512h __A, __mmask32 __U, __m512h __B, __m512h __C) {
  return _mm512_mask_fmadd_round_ph(__A, __U, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask3_fmadd_round_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]], i32 noundef [[__U:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = call <32 x half> @llvm.x86.avx512fp16.vfmadd.ph.512(<32 x half> [[TMP0]], <32 x half> [[TMP1]], <32 x half> [[TMP2]], i32 11)
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast i32 [[TMP3]] to <32 x i1>
// CHECK-NEXT:    [[TMP6:%.*]] = select <32 x i1> [[TMP5]], <32 x half> [[TMP4]], <32 x half> [[TMP2]]
// CHECK-NEXT:    ret <32 x half> [[TMP6]]
//
__m512h test_mm512_mask3_fmadd_round_ph(__m512h __A, __m512h __B, __m512h __C, __mmask32 __U) {
  return _mm512_mask3_fmadd_round_ph(__A, __B, __C, __U, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_maskz_fmadd_round_ph(
// CHECK-SAME: i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = call <32 x half> @llvm.x86.avx512fp16.vfmadd.ph.512(<32 x half> [[TMP0]], <32 x half> [[TMP1]], <32 x half> [[TMP2]], i32 11)
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast i32 [[TMP3]] to <32 x i1>
// CHECK-NEXT:    [[TMP6:%.*]] = select <32 x i1> [[TMP5]], <32 x half> [[TMP4]], <32 x half> zeroinitializer
// CHECK-NEXT:    ret <32 x half> [[TMP6]]
//
__m512h test_mm512_maskz_fmadd_round_ph(__mmask32 __U, __m512h __A, __m512h __B, __m512h __C) {
  return _mm512_maskz_fmadd_round_ph(__U, __A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_fmsub_round_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[FNEG:%.*]] = fneg <32 x half> [[TMP2]]
// CHECK-NEXT:    [[TMP3:%.*]] = call <32 x half> @llvm.x86.avx512fp16.vfmadd.ph.512(<32 x half> [[TMP0]], <32 x half> [[TMP1]], <32 x half> [[FNEG]], i32 11)
// CHECK-NEXT:    ret <32 x half> [[TMP3]]
//
__m512h test_mm512_fmsub_round_ph(__m512h __A, __m512h __B, __m512h __C) {
  return _mm512_fmsub_round_ph(__A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask_fmsub_round_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], i32 noundef [[__U:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[FNEG:%.*]] = fneg <32 x half> [[TMP2]]
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = call <32 x half> @llvm.x86.avx512fp16.vfmadd.ph.512(<32 x half> [[TMP0]], <32 x half> [[TMP1]], <32 x half> [[FNEG]], i32 11)
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast i32 [[TMP3]] to <32 x i1>
// CHECK-NEXT:    [[TMP6:%.*]] = select <32 x i1> [[TMP5]], <32 x half> [[TMP4]], <32 x half> [[TMP0]]
// CHECK-NEXT:    ret <32 x half> [[TMP6]]
//
__m512h test_mm512_mask_fmsub_round_ph(__m512h __A, __mmask32 __U, __m512h __B, __m512h __C) {
  return _mm512_mask_fmsub_round_ph(__A, __U, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_maskz_fmsub_round_ph(
// CHECK-SAME: i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[FNEG:%.*]] = fneg <32 x half> [[TMP2]]
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = call <32 x half> @llvm.x86.avx512fp16.vfmadd.ph.512(<32 x half> [[TMP0]], <32 x half> [[TMP1]], <32 x half> [[FNEG]], i32 11)
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast i32 [[TMP3]] to <32 x i1>
// CHECK-NEXT:    [[TMP6:%.*]] = select <32 x i1> [[TMP5]], <32 x half> [[TMP4]], <32 x half> zeroinitializer
// CHECK-NEXT:    ret <32 x half> [[TMP6]]
//
__m512h test_mm512_maskz_fmsub_round_ph(__mmask32 __U, __m512h __A, __m512h __B, __m512h __C) {
  return _mm512_maskz_fmsub_round_ph(__U, __A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_fnmadd_round_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[FNEG:%.*]] = fneg <32 x half> [[TMP1]]
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = call <32 x half> @llvm.x86.avx512fp16.vfmadd.ph.512(<32 x half> [[TMP0]], <32 x half> [[FNEG]], <32 x half> [[TMP2]], i32 11)
// CHECK-NEXT:    ret <32 x half> [[TMP3]]
//
__m512h test_mm512_fnmadd_round_ph(__m512h __A, __m512h __B, __m512h __C) {
  return _mm512_fnmadd_round_ph(__A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask3_fnmadd_round_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]], i32 noundef [[__U:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[FNEG:%.*]] = fneg <32 x half> [[TMP0]]
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = call <32 x half> @llvm.x86.avx512fp16.vfmadd.ph.512(<32 x half> [[FNEG]], <32 x half> [[TMP1]], <32 x half> [[TMP2]], i32 11)
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast i32 [[TMP3]] to <32 x i1>
// CHECK-NEXT:    [[TMP6:%.*]] = select <32 x i1> [[TMP5]], <32 x half> [[TMP4]], <32 x half> [[TMP2]]
// CHECK-NEXT:    ret <32 x half> [[TMP6]]
//
__m512h test_mm512_mask3_fnmadd_round_ph(__m512h __A, __m512h __B, __m512h __C, __mmask32 __U) {
  return _mm512_mask3_fnmadd_round_ph(__A, __B, __C, __U, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_maskz_fnmadd_round_ph(
// CHECK-SAME: i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[FNEG:%.*]] = fneg <32 x half> [[TMP0]]
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = call <32 x half> @llvm.x86.avx512fp16.vfmadd.ph.512(<32 x half> [[FNEG]], <32 x half> [[TMP1]], <32 x half> [[TMP2]], i32 11)
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast i32 [[TMP3]] to <32 x i1>
// CHECK-NEXT:    [[TMP6:%.*]] = select <32 x i1> [[TMP5]], <32 x half> [[TMP4]], <32 x half> zeroinitializer
// CHECK-NEXT:    ret <32 x half> [[TMP6]]
//
__m512h test_mm512_maskz_fnmadd_round_ph(__mmask32 __U, __m512h __A, __m512h __B, __m512h __C) {
  return _mm512_maskz_fnmadd_round_ph(__U, __A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_fnmsub_round_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[FNEG:%.*]] = fneg <32 x half> [[TMP1]]
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[FNEG1:%.*]] = fneg <32 x half> [[TMP2]]
// CHECK-NEXT:    [[TMP3:%.*]] = call <32 x half> @llvm.x86.avx512fp16.vfmadd.ph.512(<32 x half> [[TMP0]], <32 x half> [[FNEG]], <32 x half> [[FNEG1]], i32 11)
// CHECK-NEXT:    ret <32 x half> [[TMP3]]
//
__m512h test_mm512_fnmsub_round_ph(__m512h __A, __m512h __B, __m512h __C) {
  return _mm512_fnmsub_round_ph(__A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_maskz_fnmsub_round_ph(
// CHECK-SAME: i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[FNEG:%.*]] = fneg <32 x half> [[TMP0]]
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[FNEG1:%.*]] = fneg <32 x half> [[TMP2]]
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = call <32 x half> @llvm.x86.avx512fp16.vfmadd.ph.512(<32 x half> [[FNEG]], <32 x half> [[TMP1]], <32 x half> [[FNEG1]], i32 11)
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast i32 [[TMP3]] to <32 x i1>
// CHECK-NEXT:    [[TMP6:%.*]] = select <32 x i1> [[TMP5]], <32 x half> [[TMP4]], <32 x half> zeroinitializer
// CHECK-NEXT:    ret <32 x half> [[TMP6]]
//
__m512h test_mm512_maskz_fnmsub_round_ph(__mmask32 __U, __m512h __A, __m512h __B, __m512h __C) {
  return _mm512_maskz_fnmsub_round_ph(__U, __A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_fmadd_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP1]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = load <32 x half>, ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP6:%.*]] = call <32 x half> @llvm.fma.v32f16(<32 x half> [[TMP3]], <32 x half> [[TMP4]], <32 x half> [[TMP5]])
// CHECK-NEXT:    ret <32 x half> [[TMP6]]
//
__m512h test_mm512_fmadd_ph(__m512h __A, __m512h __B, __m512h __C) {
  return _mm512_fmadd_ph(__A, __B, __C);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask_fmadd_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], i32 noundef [[__U:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP3]], ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP6:%.*]] = load <32 x half>, ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP7:%.*]] = load i32, ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP8:%.*]] = call <32 x half> @llvm.fma.v32f16(<32 x half> [[TMP4]], <32 x half> [[TMP5]], <32 x half> [[TMP6]])
// CHECK-NEXT:    [[TMP9:%.*]] = bitcast i32 [[TMP7]] to <32 x i1>
// CHECK-NEXT:    [[TMP10:%.*]] = select <32 x i1> [[TMP9]], <32 x half> [[TMP8]], <32 x half> [[TMP4]]
// CHECK-NEXT:    ret <32 x half> [[TMP10]]
//
__m512h test_mm512_mask_fmadd_ph(__m512h __A, __mmask32 __U, __m512h __B, __m512h __C) {
  return _mm512_mask_fmadd_ph(__A, __U, __B, __C);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask3_fmadd_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]], i32 noundef [[__U:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP1]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    store i32 [[TMP3]], ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP6:%.*]] = load <32 x half>, ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP7:%.*]] = load i32, ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP8:%.*]] = call <32 x half> @llvm.fma.v32f16(<32 x half> [[TMP4]], <32 x half> [[TMP5]], <32 x half> [[TMP6]])
// CHECK-NEXT:    [[TMP9:%.*]] = bitcast i32 [[TMP7]] to <32 x i1>
// CHECK-NEXT:    [[TMP10:%.*]] = select <32 x i1> [[TMP9]], <32 x half> [[TMP8]], <32 x half> [[TMP6]]
// CHECK-NEXT:    ret <32 x half> [[TMP10]]
//
__m512h test_mm512_mask3_fmadd_ph(__m512h __A, __m512h __B, __m512h __C, __mmask32 __U) {
  return _mm512_mask3_fmadd_ph(__A, __B, __C, __U);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_maskz_fmadd_ph(
// CHECK-SAME: i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    store i32 [[TMP0]], ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    store <32 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP3]], ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP6:%.*]] = load <32 x half>, ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP7:%.*]] = load i32, ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP8:%.*]] = call <32 x half> @llvm.fma.v32f16(<32 x half> [[TMP4]], <32 x half> [[TMP5]], <32 x half> [[TMP6]])
// CHECK-NEXT:    [[TMP9:%.*]] = bitcast i32 [[TMP7]] to <32 x i1>
// CHECK-NEXT:    [[TMP10:%.*]] = select <32 x i1> [[TMP9]], <32 x half> [[TMP8]], <32 x half> zeroinitializer
// CHECK-NEXT:    ret <32 x half> [[TMP10]]
//
__m512h test_mm512_maskz_fmadd_ph(__mmask32 __U, __m512h __A, __m512h __B, __m512h __C) {
  return _mm512_maskz_fmadd_ph(__U, __A, __B, __C);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_fmsub_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP1]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = load <32 x half>, ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    [[FNEG_I:%.*]] = fneg <32 x half> [[TMP5]]
// CHECK-NEXT:    [[TMP6:%.*]] = call <32 x half> @llvm.fma.v32f16(<32 x half> [[TMP3]], <32 x half> [[TMP4]], <32 x half> [[FNEG_I]])
// CHECK-NEXT:    ret <32 x half> [[TMP6]]
//
__m512h test_mm512_fmsub_ph(__m512h __A, __m512h __B, __m512h __C) {
  return _mm512_fmsub_ph(__A, __B, __C);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask_fmsub_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], i32 noundef [[__U:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP3]], ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP6:%.*]] = load <32 x half>, ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    [[FNEG_I:%.*]] = fneg <32 x half> [[TMP6]]
// CHECK-NEXT:    [[TMP7:%.*]] = load i32, ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP8:%.*]] = call <32 x half> @llvm.fma.v32f16(<32 x half> [[TMP4]], <32 x half> [[TMP5]], <32 x half> [[FNEG_I]])
// CHECK-NEXT:    [[TMP9:%.*]] = bitcast i32 [[TMP7]] to <32 x i1>
// CHECK-NEXT:    [[TMP10:%.*]] = select <32 x i1> [[TMP9]], <32 x half> [[TMP8]], <32 x half> [[TMP4]]
// CHECK-NEXT:    ret <32 x half> [[TMP10]]
//
__m512h test_mm512_mask_fmsub_ph(__m512h __A, __mmask32 __U, __m512h __B, __m512h __C) {
  return _mm512_mask_fmsub_ph(__A, __U, __B, __C);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_maskz_fmsub_ph(
// CHECK-SAME: i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    store i32 [[TMP0]], ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    store <32 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP3]], ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP6:%.*]] = load <32 x half>, ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    [[FNEG_I:%.*]] = fneg <32 x half> [[TMP6]]
// CHECK-NEXT:    [[TMP7:%.*]] = load i32, ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP8:%.*]] = call <32 x half> @llvm.fma.v32f16(<32 x half> [[TMP4]], <32 x half> [[TMP5]], <32 x half> [[FNEG_I]])
// CHECK-NEXT:    [[TMP9:%.*]] = bitcast i32 [[TMP7]] to <32 x i1>
// CHECK-NEXT:    [[TMP10:%.*]] = select <32 x i1> [[TMP9]], <32 x half> [[TMP8]], <32 x half> zeroinitializer
// CHECK-NEXT:    ret <32 x half> [[TMP10]]
//
__m512h test_mm512_maskz_fmsub_ph(__mmask32 __U, __m512h __A, __m512h __B, __m512h __C) {
  return _mm512_maskz_fmsub_ph(__U, __A, __B, __C);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_fnmadd_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP1]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[FNEG_I:%.*]] = fneg <32 x half> [[TMP4]]
// CHECK-NEXT:    [[TMP5:%.*]] = load <32 x half>, ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP6:%.*]] = call <32 x half> @llvm.fma.v32f16(<32 x half> [[TMP3]], <32 x half> [[FNEG_I]], <32 x half> [[TMP5]])
// CHECK-NEXT:    ret <32 x half> [[TMP6]]
//
__m512h test_mm512_fnmadd_ph(__m512h __A, __m512h __B, __m512h __C) {
  return _mm512_fnmadd_ph(__A, __B, __C);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask3_fnmadd_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]], i32 noundef [[__U:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP1]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    store i32 [[TMP3]], ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[FNEG_I:%.*]] = fneg <32 x half> [[TMP4]]
// CHECK-NEXT:    [[TMP5:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP6:%.*]] = load <32 x half>, ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP7:%.*]] = load i32, ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP8:%.*]] = call <32 x half> @llvm.fma.v32f16(<32 x half> [[FNEG_I]], <32 x half> [[TMP5]], <32 x half> [[TMP6]])
// CHECK-NEXT:    [[TMP9:%.*]] = bitcast i32 [[TMP7]] to <32 x i1>
// CHECK-NEXT:    [[TMP10:%.*]] = select <32 x i1> [[TMP9]], <32 x half> [[TMP8]], <32 x half> [[TMP6]]
// CHECK-NEXT:    ret <32 x half> [[TMP10]]
//
__m512h test_mm512_mask3_fnmadd_ph(__m512h __A, __m512h __B, __m512h __C, __mmask32 __U) {
  return _mm512_mask3_fnmadd_ph(__A, __B, __C, __U);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_maskz_fnmadd_ph(
// CHECK-SAME: i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    store i32 [[TMP0]], ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    store <32 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP3]], ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[FNEG_I:%.*]] = fneg <32 x half> [[TMP4]]
// CHECK-NEXT:    [[TMP5:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP6:%.*]] = load <32 x half>, ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP7:%.*]] = load i32, ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP8:%.*]] = call <32 x half> @llvm.fma.v32f16(<32 x half> [[FNEG_I]], <32 x half> [[TMP5]], <32 x half> [[TMP6]])
// CHECK-NEXT:    [[TMP9:%.*]] = bitcast i32 [[TMP7]] to <32 x i1>
// CHECK-NEXT:    [[TMP10:%.*]] = select <32 x i1> [[TMP9]], <32 x half> [[TMP8]], <32 x half> zeroinitializer
// CHECK-NEXT:    ret <32 x half> [[TMP10]]
//
__m512h test_mm512_maskz_fnmadd_ph(__mmask32 __U, __m512h __A, __m512h __B, __m512h __C) {
  return _mm512_maskz_fnmadd_ph(__U, __A, __B, __C);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_fnmsub_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP1]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[FNEG_I:%.*]] = fneg <32 x half> [[TMP4]]
// CHECK-NEXT:    [[TMP5:%.*]] = load <32 x half>, ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    [[FNEG1_I:%.*]] = fneg <32 x half> [[TMP5]]
// CHECK-NEXT:    [[TMP6:%.*]] = call <32 x half> @llvm.fma.v32f16(<32 x half> [[TMP3]], <32 x half> [[FNEG_I]], <32 x half> [[FNEG1_I]])
// CHECK-NEXT:    ret <32 x half> [[TMP6]]
//
__m512h test_mm512_fnmsub_ph(__m512h __A, __m512h __B, __m512h __C) {
  return _mm512_fnmsub_ph(__A, __B, __C);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_maskz_fnmsub_ph(
// CHECK-SAME: i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    store i32 [[TMP0]], ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    store <32 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP3]], ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[FNEG_I:%.*]] = fneg <32 x half> [[TMP4]]
// CHECK-NEXT:    [[TMP5:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP6:%.*]] = load <32 x half>, ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    [[FNEG1_I:%.*]] = fneg <32 x half> [[TMP6]]
// CHECK-NEXT:    [[TMP7:%.*]] = load i32, ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP8:%.*]] = call <32 x half> @llvm.fma.v32f16(<32 x half> [[FNEG_I]], <32 x half> [[TMP5]], <32 x half> [[FNEG1_I]])
// CHECK-NEXT:    [[TMP9:%.*]] = bitcast i32 [[TMP7]] to <32 x i1>
// CHECK-NEXT:    [[TMP10:%.*]] = select <32 x i1> [[TMP9]], <32 x half> [[TMP8]], <32 x half> zeroinitializer
// CHECK-NEXT:    ret <32 x half> [[TMP10]]
//
__m512h test_mm512_maskz_fnmsub_ph(__mmask32 __U, __m512h __A, __m512h __B, __m512h __C) {
  return _mm512_maskz_fnmsub_ph(__U, __A, __B, __C);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_fmaddsub_round_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = call <32 x half> @llvm.x86.avx512fp16.vfmaddsub.ph.512(<32 x half> [[TMP0]], <32 x half> [[TMP1]], <32 x half> [[TMP2]], i32 11)
// CHECK-NEXT:    ret <32 x half> [[TMP3]]
//
__m512h test_mm512_fmaddsub_round_ph(__m512h __A, __m512h __B, __m512h __C) {
  return _mm512_fmaddsub_round_ph(__A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask_fmaddsub_round_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], i32 noundef [[__U:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = call <32 x half> @llvm.x86.avx512fp16.vfmaddsub.ph.512(<32 x half> [[TMP0]], <32 x half> [[TMP1]], <32 x half> [[TMP2]], i32 11)
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast i32 [[TMP3]] to <32 x i1>
// CHECK-NEXT:    [[TMP6:%.*]] = select <32 x i1> [[TMP5]], <32 x half> [[TMP4]], <32 x half> [[TMP0]]
// CHECK-NEXT:    ret <32 x half> [[TMP6]]
//
__m512h test_mm512_mask_fmaddsub_round_ph(__m512h __A, __mmask32 __U, __m512h __B, __m512h __C) {
  return _mm512_mask_fmaddsub_round_ph(__A, __U, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask3_fmaddsub_round_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]], i32 noundef [[__U:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = call <32 x half> @llvm.x86.avx512fp16.vfmaddsub.ph.512(<32 x half> [[TMP0]], <32 x half> [[TMP1]], <32 x half> [[TMP2]], i32 11)
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast i32 [[TMP3]] to <32 x i1>
// CHECK-NEXT:    [[TMP6:%.*]] = select <32 x i1> [[TMP5]], <32 x half> [[TMP4]], <32 x half> [[TMP2]]
// CHECK-NEXT:    ret <32 x half> [[TMP6]]
//
__m512h test_mm512_mask3_fmaddsub_round_ph(__m512h __A, __m512h __B, __m512h __C, __mmask32 __U) {
  return _mm512_mask3_fmaddsub_round_ph(__A, __B, __C, __U, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_maskz_fmaddsub_round_ph(
// CHECK-SAME: i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = call <32 x half> @llvm.x86.avx512fp16.vfmaddsub.ph.512(<32 x half> [[TMP0]], <32 x half> [[TMP1]], <32 x half> [[TMP2]], i32 11)
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast i32 [[TMP3]] to <32 x i1>
// CHECK-NEXT:    [[TMP6:%.*]] = select <32 x i1> [[TMP5]], <32 x half> [[TMP4]], <32 x half> zeroinitializer
// CHECK-NEXT:    ret <32 x half> [[TMP6]]
//
__m512h test_mm512_maskz_fmaddsub_round_ph(__mmask32 __U, __m512h __A, __m512h __B, __m512h __C) {
  return _mm512_maskz_fmaddsub_round_ph(__U, __A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_fmsubadd_round_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[FNEG:%.*]] = fneg <32 x half> [[TMP2]]
// CHECK-NEXT:    [[TMP3:%.*]] = call <32 x half> @llvm.x86.avx512fp16.vfmaddsub.ph.512(<32 x half> [[TMP0]], <32 x half> [[TMP1]], <32 x half> [[FNEG]], i32 11)
// CHECK-NEXT:    ret <32 x half> [[TMP3]]
//
__m512h test_mm512_fmsubadd_round_ph(__m512h __A, __m512h __B, __m512h __C) {
  return _mm512_fmsubadd_round_ph(__A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask_fmsubadd_round_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], i32 noundef [[__U:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[FNEG:%.*]] = fneg <32 x half> [[TMP2]]
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = call <32 x half> @llvm.x86.avx512fp16.vfmaddsub.ph.512(<32 x half> [[TMP0]], <32 x half> [[TMP1]], <32 x half> [[FNEG]], i32 11)
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast i32 [[TMP3]] to <32 x i1>
// CHECK-NEXT:    [[TMP6:%.*]] = select <32 x i1> [[TMP5]], <32 x half> [[TMP4]], <32 x half> [[TMP0]]
// CHECK-NEXT:    ret <32 x half> [[TMP6]]
//
__m512h test_mm512_mask_fmsubadd_round_ph(__m512h __A, __mmask32 __U, __m512h __B, __m512h __C) {
  return _mm512_mask_fmsubadd_round_ph(__A, __U, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_maskz_fmsubadd_round_ph(
// CHECK-SAME: i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[FNEG:%.*]] = fneg <32 x half> [[TMP2]]
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = call <32 x half> @llvm.x86.avx512fp16.vfmaddsub.ph.512(<32 x half> [[TMP0]], <32 x half> [[TMP1]], <32 x half> [[FNEG]], i32 11)
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast i32 [[TMP3]] to <32 x i1>
// CHECK-NEXT:    [[TMP6:%.*]] = select <32 x i1> [[TMP5]], <32 x half> [[TMP4]], <32 x half> zeroinitializer
// CHECK-NEXT:    ret <32 x half> [[TMP6]]
//
__m512h test_mm512_maskz_fmsubadd_round_ph(__mmask32 __U, __m512h __A, __m512h __B, __m512h __C) {
  return _mm512_maskz_fmsubadd_round_ph(__U, __A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_fmaddsub_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP1]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = load <32 x half>, ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP6:%.*]] = call <32 x half> @llvm.x86.avx512fp16.vfmaddsub.ph.512(<32 x half> [[TMP3]], <32 x half> [[TMP4]], <32 x half> [[TMP5]], i32 4)
// CHECK-NEXT:    ret <32 x half> [[TMP6]]
//
__m512h test_mm512_fmaddsub_ph(__m512h __A, __m512h __B, __m512h __C) {
  return _mm512_fmaddsub_ph(__A, __B, __C);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask_fmaddsub_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], i32 noundef [[__U:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP3]], ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP6:%.*]] = load <32 x half>, ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP7:%.*]] = load i32, ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP8:%.*]] = call <32 x half> @llvm.x86.avx512fp16.vfmaddsub.ph.512(<32 x half> [[TMP4]], <32 x half> [[TMP5]], <32 x half> [[TMP6]], i32 4)
// CHECK-NEXT:    [[TMP9:%.*]] = bitcast i32 [[TMP7]] to <32 x i1>
// CHECK-NEXT:    [[TMP10:%.*]] = select <32 x i1> [[TMP9]], <32 x half> [[TMP8]], <32 x half> [[TMP4]]
// CHECK-NEXT:    ret <32 x half> [[TMP10]]
//
__m512h test_mm512_mask_fmaddsub_ph(__m512h __A, __mmask32 __U, __m512h __B, __m512h __C) {
  return _mm512_mask_fmaddsub_ph(__A, __U, __B, __C);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask3_fmaddsub_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]], i32 noundef [[__U:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP1]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    store i32 [[TMP3]], ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP6:%.*]] = load <32 x half>, ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP7:%.*]] = load i32, ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP8:%.*]] = call <32 x half> @llvm.x86.avx512fp16.vfmaddsub.ph.512(<32 x half> [[TMP4]], <32 x half> [[TMP5]], <32 x half> [[TMP6]], i32 4)
// CHECK-NEXT:    [[TMP9:%.*]] = bitcast i32 [[TMP7]] to <32 x i1>
// CHECK-NEXT:    [[TMP10:%.*]] = select <32 x i1> [[TMP9]], <32 x half> [[TMP8]], <32 x half> [[TMP6]]
// CHECK-NEXT:    ret <32 x half> [[TMP10]]
//
__m512h test_mm512_mask3_fmaddsub_ph(__m512h __A, __m512h __B, __m512h __C, __mmask32 __U) {
  return _mm512_mask3_fmaddsub_ph(__A, __B, __C, __U);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_maskz_fmaddsub_ph(
// CHECK-SAME: i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    store i32 [[TMP0]], ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    store <32 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP3]], ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP6:%.*]] = load <32 x half>, ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP7:%.*]] = load i32, ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP8:%.*]] = call <32 x half> @llvm.x86.avx512fp16.vfmaddsub.ph.512(<32 x half> [[TMP4]], <32 x half> [[TMP5]], <32 x half> [[TMP6]], i32 4)
// CHECK-NEXT:    [[TMP9:%.*]] = bitcast i32 [[TMP7]] to <32 x i1>
// CHECK-NEXT:    [[TMP10:%.*]] = select <32 x i1> [[TMP9]], <32 x half> [[TMP8]], <32 x half> zeroinitializer
// CHECK-NEXT:    ret <32 x half> [[TMP10]]
//
__m512h test_mm512_maskz_fmaddsub_ph(__mmask32 __U, __m512h __A, __m512h __B, __m512h __C) {
  return _mm512_maskz_fmaddsub_ph(__U, __A, __B, __C);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_fmsubadd_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP1]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = load <32 x half>, ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    [[FNEG_I:%.*]] = fneg <32 x half> [[TMP5]]
// CHECK-NEXT:    [[TMP6:%.*]] = call <32 x half> @llvm.x86.avx512fp16.vfmaddsub.ph.512(<32 x half> [[TMP3]], <32 x half> [[TMP4]], <32 x half> [[FNEG_I]], i32 4)
// CHECK-NEXT:    ret <32 x half> [[TMP6]]
//
__m512h test_mm512_fmsubadd_ph(__m512h __A, __m512h __B, __m512h __C) {
  return _mm512_fmsubadd_ph(__A, __B, __C);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask_fmsubadd_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], i32 noundef [[__U:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP3]], ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP6:%.*]] = load <32 x half>, ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    [[FNEG_I:%.*]] = fneg <32 x half> [[TMP6]]
// CHECK-NEXT:    [[TMP7:%.*]] = load i32, ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP8:%.*]] = call <32 x half> @llvm.x86.avx512fp16.vfmaddsub.ph.512(<32 x half> [[TMP4]], <32 x half> [[TMP5]], <32 x half> [[FNEG_I]], i32 4)
// CHECK-NEXT:    [[TMP9:%.*]] = bitcast i32 [[TMP7]] to <32 x i1>
// CHECK-NEXT:    [[TMP10:%.*]] = select <32 x i1> [[TMP9]], <32 x half> [[TMP8]], <32 x half> [[TMP4]]
// CHECK-NEXT:    ret <32 x half> [[TMP10]]
//
__m512h test_mm512_mask_fmsubadd_ph(__m512h __A, __mmask32 __U, __m512h __B, __m512h __C) {
  return _mm512_mask_fmsubadd_ph(__A, __U, __B, __C);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_maskz_fmsubadd_ph(
// CHECK-SAME: i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    store i32 [[TMP0]], ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    store <32 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP3]], ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP6:%.*]] = load <32 x half>, ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    [[FNEG_I:%.*]] = fneg <32 x half> [[TMP6]]
// CHECK-NEXT:    [[TMP7:%.*]] = load i32, ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP8:%.*]] = call <32 x half> @llvm.x86.avx512fp16.vfmaddsub.ph.512(<32 x half> [[TMP4]], <32 x half> [[TMP5]], <32 x half> [[FNEG_I]], i32 4)
// CHECK-NEXT:    [[TMP9:%.*]] = bitcast i32 [[TMP7]] to <32 x i1>
// CHECK-NEXT:    [[TMP10:%.*]] = select <32 x i1> [[TMP9]], <32 x half> [[TMP8]], <32 x half> zeroinitializer
// CHECK-NEXT:    ret <32 x half> [[TMP10]]
//
__m512h test_mm512_maskz_fmsubadd_ph(__mmask32 __U, __m512h __A, __m512h __B, __m512h __C) {
  return _mm512_maskz_fmsubadd_ph(__U, __A, __B, __C);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask3_fmsub_round_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]], i32 noundef [[__U:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = fneg <32 x half> [[TMP2]]
// CHECK-NEXT:    [[TMP5:%.*]] = call <32 x half> @llvm.x86.avx512fp16.vfmadd.ph.512(<32 x half> [[TMP0]], <32 x half> [[TMP1]], <32 x half> [[TMP4]], i32 11)
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast i32 [[TMP3]] to <32 x i1>
// CHECK-NEXT:    [[TMP7:%.*]] = select <32 x i1> [[TMP6]], <32 x half> [[TMP5]], <32 x half> [[TMP2]]
// CHECK-NEXT:    ret <32 x half> [[TMP7]]
//
__m512h test_mm512_mask3_fmsub_round_ph(__m512h __A, __m512h __B, __m512h __C, __mmask32 __U) {
  return _mm512_mask3_fmsub_round_ph(__A, __B, __C, __U, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask3_fmsub_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]], i32 noundef [[__U:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP1]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    store i32 [[TMP3]], ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP6:%.*]] = load <32 x half>, ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP7:%.*]] = load i32, ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP8:%.*]] = fneg <32 x half> [[TMP6]]
// CHECK-NEXT:    [[TMP9:%.*]] = call <32 x half> @llvm.fma.v32f16(<32 x half> [[TMP4]], <32 x half> [[TMP5]], <32 x half> [[TMP8]])
// CHECK-NEXT:    [[TMP10:%.*]] = bitcast i32 [[TMP7]] to <32 x i1>
// CHECK-NEXT:    [[TMP11:%.*]] = select <32 x i1> [[TMP10]], <32 x half> [[TMP9]], <32 x half> [[TMP6]]
// CHECK-NEXT:    ret <32 x half> [[TMP11]]
//
__m512h test_mm512_mask3_fmsub_ph(__m512h __A, __m512h __B, __m512h __C, __mmask32 __U) {
  return _mm512_mask3_fmsub_ph(__A, __B, __C, __U);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask3_fmsubadd_round_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]], i32 noundef [[__U:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = fneg <32 x half> [[TMP2]]
// CHECK-NEXT:    [[TMP5:%.*]] = call <32 x half> @llvm.x86.avx512fp16.vfmaddsub.ph.512(<32 x half> [[TMP0]], <32 x half> [[TMP1]], <32 x half> [[TMP4]], i32 11)
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast i32 [[TMP3]] to <32 x i1>
// CHECK-NEXT:    [[TMP7:%.*]] = select <32 x i1> [[TMP6]], <32 x half> [[TMP5]], <32 x half> [[TMP2]]
// CHECK-NEXT:    ret <32 x half> [[TMP7]]
//
__m512h test_mm512_mask3_fmsubadd_round_ph(__m512h __A, __m512h __B, __m512h __C, __mmask32 __U) {
  return _mm512_mask3_fmsubadd_round_ph(__A, __B, __C, __U, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask3_fmsubadd_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]], i32 noundef [[__U:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP1]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    store i32 [[TMP3]], ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP6:%.*]] = load <32 x half>, ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP7:%.*]] = load i32, ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP8:%.*]] = fneg <32 x half> [[TMP6]]
// CHECK-NEXT:    [[TMP9:%.*]] = call <32 x half> @llvm.x86.avx512fp16.vfmaddsub.ph.512(<32 x half> [[TMP4]], <32 x half> [[TMP5]], <32 x half> [[TMP8]], i32 4)
// CHECK-NEXT:    [[TMP10:%.*]] = bitcast i32 [[TMP7]] to <32 x i1>
// CHECK-NEXT:    [[TMP11:%.*]] = select <32 x i1> [[TMP10]], <32 x half> [[TMP9]], <32 x half> [[TMP6]]
// CHECK-NEXT:    ret <32 x half> [[TMP11]]
//
__m512h test_mm512_mask3_fmsubadd_ph(__m512h __A, __m512h __B, __m512h __C, __mmask32 __U) {
  return _mm512_mask3_fmsubadd_ph(__A, __B, __C, __U);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask_fnmadd_round_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], i32 noundef [[__U:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[FNEG:%.*]] = fneg <32 x half> [[TMP1]]
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = call <32 x half> @llvm.x86.avx512fp16.vfmadd.ph.512(<32 x half> [[TMP0]], <32 x half> [[FNEG]], <32 x half> [[TMP2]], i32 11)
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast i32 [[TMP3]] to <32 x i1>
// CHECK-NEXT:    [[TMP6:%.*]] = select <32 x i1> [[TMP5]], <32 x half> [[TMP4]], <32 x half> [[TMP0]]
// CHECK-NEXT:    ret <32 x half> [[TMP6]]
//
__m512h test_mm512_mask_fnmadd_round_ph(__m512h __A, __mmask32 __U, __m512h __B, __m512h __C) {
  return _mm512_mask_fnmadd_round_ph(__A, __U, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask_fnmadd_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], i32 noundef [[__U:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP3]], ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[FNEG_I:%.*]] = fneg <32 x half> [[TMP5]]
// CHECK-NEXT:    [[TMP6:%.*]] = load <32 x half>, ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP7:%.*]] = load i32, ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP8:%.*]] = call <32 x half> @llvm.fma.v32f16(<32 x half> [[TMP4]], <32 x half> [[FNEG_I]], <32 x half> [[TMP6]])
// CHECK-NEXT:    [[TMP9:%.*]] = bitcast i32 [[TMP7]] to <32 x i1>
// CHECK-NEXT:    [[TMP10:%.*]] = select <32 x i1> [[TMP9]], <32 x half> [[TMP8]], <32 x half> [[TMP4]]
// CHECK-NEXT:    ret <32 x half> [[TMP10]]
//
__m512h test_mm512_mask_fnmadd_ph(__m512h __A, __mmask32 __U, __m512h __B, __m512h __C) {
  return _mm512_mask_fnmadd_ph(__A, __U, __B, __C);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask_fnmsub_round_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], i32 noundef [[__U:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[FNEG:%.*]] = fneg <32 x half> [[TMP1]]
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[FNEG1:%.*]] = fneg <32 x half> [[TMP2]]
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = call <32 x half> @llvm.x86.avx512fp16.vfmadd.ph.512(<32 x half> [[TMP0]], <32 x half> [[FNEG]], <32 x half> [[FNEG1]], i32 11)
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast i32 [[TMP3]] to <32 x i1>
// CHECK-NEXT:    [[TMP6:%.*]] = select <32 x i1> [[TMP5]], <32 x half> [[TMP4]], <32 x half> [[TMP0]]
// CHECK-NEXT:    ret <32 x half> [[TMP6]]
//
__m512h test_mm512_mask_fnmsub_round_ph(__m512h __A, __mmask32 __U, __m512h __B, __m512h __C) {
  return _mm512_mask_fnmsub_round_ph(__A, __U, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask3_fnmsub_round_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]], i32 noundef [[__U:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[FNEG:%.*]] = fneg <32 x half> [[TMP0]]
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = fneg <32 x half> [[TMP2]]
// CHECK-NEXT:    [[TMP5:%.*]] = call <32 x half> @llvm.x86.avx512fp16.vfmadd.ph.512(<32 x half> [[FNEG]], <32 x half> [[TMP1]], <32 x half> [[TMP4]], i32 11)
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast i32 [[TMP3]] to <32 x i1>
// CHECK-NEXT:    [[TMP7:%.*]] = select <32 x i1> [[TMP6]], <32 x half> [[TMP5]], <32 x half> [[TMP2]]
// CHECK-NEXT:    ret <32 x half> [[TMP7]]
//
__m512h test_mm512_mask3_fnmsub_round_ph(__m512h __A, __m512h __B, __m512h __C, __mmask32 __U) {
  return _mm512_mask3_fnmsub_round_ph(__A, __B, __C, __U, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask_fnmsub_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], i32 noundef [[__U:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP3]], ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[FNEG_I:%.*]] = fneg <32 x half> [[TMP5]]
// CHECK-NEXT:    [[TMP6:%.*]] = load <32 x half>, ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    [[FNEG1_I:%.*]] = fneg <32 x half> [[TMP6]]
// CHECK-NEXT:    [[TMP7:%.*]] = load i32, ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP8:%.*]] = call <32 x half> @llvm.fma.v32f16(<32 x half> [[TMP4]], <32 x half> [[FNEG_I]], <32 x half> [[FNEG1_I]])
// CHECK-NEXT:    [[TMP9:%.*]] = bitcast i32 [[TMP7]] to <32 x i1>
// CHECK-NEXT:    [[TMP10:%.*]] = select <32 x i1> [[TMP9]], <32 x half> [[TMP8]], <32 x half> [[TMP4]]
// CHECK-NEXT:    ret <32 x half> [[TMP10]]
//
__m512h test_mm512_mask_fnmsub_ph(__m512h __A, __mmask32 __U, __m512h __B, __m512h __C) {
  return _mm512_mask_fnmsub_ph(__A, __U, __B, __C);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask3_fnmsub_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]], i32 noundef [[__U:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP1]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    store i32 [[TMP3]], ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[FNEG_I:%.*]] = fneg <32 x half> [[TMP4]]
// CHECK-NEXT:    [[TMP5:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP6:%.*]] = load <32 x half>, ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP7:%.*]] = load i32, ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP8:%.*]] = fneg <32 x half> [[TMP6]]
// CHECK-NEXT:    [[TMP9:%.*]] = call <32 x half> @llvm.fma.v32f16(<32 x half> [[FNEG_I]], <32 x half> [[TMP5]], <32 x half> [[TMP8]])
// CHECK-NEXT:    [[TMP10:%.*]] = bitcast i32 [[TMP7]] to <32 x i1>
// CHECK-NEXT:    [[TMP11:%.*]] = select <32 x i1> [[TMP10]], <32 x half> [[TMP9]], <32 x half> [[TMP6]]
// CHECK-NEXT:    ret <32 x half> [[TMP11]]
//
__m512h test_mm512_mask3_fnmsub_ph(__m512h __A, __m512h __B, __m512h __C, __mmask32 __U) {
  return _mm512_mask3_fnmsub_ph(__A, __B, __C, __U);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_fmadd_sh(
// CHECK-SAME: <8 x half> noundef [[__W:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__W]], ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = extractelement <8 x half> [[TMP3]], i64 0
// CHECK-NEXT:    [[TMP7:%.*]] = extractelement <8 x half> [[TMP4]], i64 0
// CHECK-NEXT:    [[TMP8:%.*]] = extractelement <8 x half> [[TMP5]], i64 0
// CHECK-NEXT:    [[TMP9:%.*]] = call half @llvm.fma.f16(half [[TMP6]], half [[TMP7]], half [[TMP8]])
// CHECK-NEXT:    [[TMP10:%.*]] = insertelement <8 x half> [[TMP3]], half [[TMP9]], i64 0
// CHECK-NEXT:    ret <8 x half> [[TMP10]]
//
__m128h test_mm_fmadd_sh(__m128h __W, __m128h __A, __m128h __B) {
  return _mm_fmadd_sh(__W, __A, __B);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask_fmadd_sh(
// CHECK-SAME: <8 x half> noundef [[__W:%.*]], i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__W]], ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    store i8 [[TMP1]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP3]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP7:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP8:%.*]] = extractelement <8 x half> [[TMP4]], i64 0
// CHECK-NEXT:    [[TMP9:%.*]] = extractelement <8 x half> [[TMP5]], i64 0
// CHECK-NEXT:    [[TMP10:%.*]] = extractelement <8 x half> [[TMP6]], i64 0
// CHECK-NEXT:    [[TMP11:%.*]] = call half @llvm.fma.f16(half [[TMP8]], half [[TMP9]], half [[TMP10]])
// CHECK-NEXT:    [[TMP12:%.*]] = bitcast i8 [[TMP7]] to <8 x i1>
// CHECK-NEXT:    [[TMP13:%.*]] = extractelement <8 x i1> [[TMP12]], i64 0
// CHECK-NEXT:    [[TMP14:%.*]] = select i1 [[TMP13]], half [[TMP11]], half [[TMP8]]
// CHECK-NEXT:    [[TMP15:%.*]] = insertelement <8 x half> [[TMP4]], half [[TMP14]], i64 0
// CHECK-NEXT:    ret <8 x half> [[TMP15]]
//
__m128h test_mm_mask_fmadd_sh(__m128h __W, __mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_mask_fmadd_sh(__W, __U, __A, __B);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_fmadd_round_sh(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]], <8 x half> noundef [[__C:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__C]], ptr [[__C_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__C_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = extractelement <8 x half> [[TMP0]], i64 0
// CHECK-NEXT:    [[TMP4:%.*]] = extractelement <8 x half> [[TMP1]], i64 0
// CHECK-NEXT:    [[TMP5:%.*]] = extractelement <8 x half> [[TMP2]], i64 0
// CHECK-NEXT:    [[TMP6:%.*]] = call half @llvm.x86.avx512fp16.vfmadd.f16(half [[TMP3]], half [[TMP4]], half [[TMP5]], i32 11)
// CHECK-NEXT:    [[TMP7:%.*]] = insertelement <8 x half> [[TMP0]], half [[TMP6]], i64 0
// CHECK-NEXT:    ret <8 x half> [[TMP7]]
//
__m128h test_mm_fmadd_round_sh(__m128h __A, __m128h __B, __m128h __C) {
  return _mm_fmadd_round_sh(__A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask_fmadd_round_sh(
// CHECK-SAME: <8 x half> noundef [[__W:%.*]], i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__W]], ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP4:%.*]] = extractelement <8 x half> [[TMP0]], i64 0
// CHECK-NEXT:    [[TMP5:%.*]] = extractelement <8 x half> [[TMP1]], i64 0
// CHECK-NEXT:    [[TMP6:%.*]] = extractelement <8 x half> [[TMP2]], i64 0
// CHECK-NEXT:    [[TMP7:%.*]] = call half @llvm.x86.avx512fp16.vfmadd.f16(half [[TMP4]], half [[TMP5]], half [[TMP6]], i32 11)
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast i8 [[TMP3]] to <8 x i1>
// CHECK-NEXT:    [[TMP9:%.*]] = extractelement <8 x i1> [[TMP8]], i64 0
// CHECK-NEXT:    [[TMP10:%.*]] = select i1 [[TMP9]], half [[TMP7]], half [[TMP4]]
// CHECK-NEXT:    [[TMP11:%.*]] = insertelement <8 x half> [[TMP0]], half [[TMP10]], i64 0
// CHECK-NEXT:    ret <8 x half> [[TMP11]]
//
__m128h test_mm_mask_fmadd_round_sh(__m128h __W, __mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_mask_fmadd_round_sh(__W, __U, __A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_maskz_fmadd_sh(
// CHECK-SAME: i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]], <8 x half> noundef [[__C:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__C_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__C]], ptr [[__C_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__C_ADDR]], align 16
// CHECK-NEXT:    store i8 [[TMP0]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP3]], ptr [[__C_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = load <8 x half>, ptr [[__C_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP7:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP8:%.*]] = extractelement <8 x half> [[TMP4]], i64 0
// CHECK-NEXT:    [[TMP9:%.*]] = extractelement <8 x half> [[TMP5]], i64 0
// CHECK-NEXT:    [[TMP10:%.*]] = extractelement <8 x half> [[TMP6]], i64 0
// CHECK-NEXT:    [[TMP11:%.*]] = call half @llvm.fma.f16(half [[TMP8]], half [[TMP9]], half [[TMP10]])
// CHECK-NEXT:    [[TMP12:%.*]] = bitcast i8 [[TMP7]] to <8 x i1>
// CHECK-NEXT:    [[TMP13:%.*]] = extractelement <8 x i1> [[TMP12]], i64 0
// CHECK-NEXT:    [[TMP14:%.*]] = select i1 [[TMP13]], half [[TMP11]], half 0xH0000
// CHECK-NEXT:    [[TMP15:%.*]] = insertelement <8 x half> [[TMP4]], half [[TMP14]], i64 0
// CHECK-NEXT:    ret <8 x half> [[TMP15]]
//
__m128h test_mm_maskz_fmadd_sh(__mmask8 __U, __m128h __A, __m128h __B, __m128h __C) {
  return _mm_maskz_fmadd_sh(__U, __A, __B, __C);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_maskz_fmadd_round_sh(
// CHECK-SAME: i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]], <8 x half> noundef [[__C:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__C]], ptr [[__C_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__C_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP4:%.*]] = extractelement <8 x half> [[TMP0]], i64 0
// CHECK-NEXT:    [[TMP5:%.*]] = extractelement <8 x half> [[TMP1]], i64 0
// CHECK-NEXT:    [[TMP6:%.*]] = extractelement <8 x half> [[TMP2]], i64 0
// CHECK-NEXT:    [[TMP7:%.*]] = call half @llvm.x86.avx512fp16.vfmadd.f16(half [[TMP4]], half [[TMP5]], half [[TMP6]], i32 11)
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast i8 [[TMP3]] to <8 x i1>
// CHECK-NEXT:    [[TMP9:%.*]] = extractelement <8 x i1> [[TMP8]], i64 0
// CHECK-NEXT:    [[TMP10:%.*]] = select i1 [[TMP9]], half [[TMP7]], half 0xH0000
// CHECK-NEXT:    [[TMP11:%.*]] = insertelement <8 x half> [[TMP0]], half [[TMP10]], i64 0
// CHECK-NEXT:    ret <8 x half> [[TMP11]]
//
__m128h test_mm_maskz_fmadd_round_sh(__mmask8 __U, __m128h __A, __m128h __B, __m128h __C) {
  return _mm_maskz_fmadd_round_sh(__U, __A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask3_fmadd_sh(
// CHECK-SAME: <8 x half> noundef [[__W:%.*]], <8 x half> noundef [[__X:%.*]], <8 x half> noundef [[__Y:%.*]], i8 noundef zeroext [[__U:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__X_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__Y_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__X_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__Y_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    store <8 x half> [[__W]], ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__X]], ptr [[__X_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__Y]], ptr [[__Y_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__X_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__Y_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__X_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__Y_ADDR_I]], align 16
// CHECK-NEXT:    store i8 [[TMP3]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = load <8 x half>, ptr [[__X_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = load <8 x half>, ptr [[__Y_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP7:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP8:%.*]] = extractelement <8 x half> [[TMP4]], i64 0
// CHECK-NEXT:    [[TMP9:%.*]] = extractelement <8 x half> [[TMP5]], i64 0
// CHECK-NEXT:    [[TMP10:%.*]] = extractelement <8 x half> [[TMP6]], i64 0
// CHECK-NEXT:    [[TMP11:%.*]] = call half @llvm.fma.f16(half [[TMP8]], half [[TMP9]], half [[TMP10]])
// CHECK-NEXT:    [[TMP12:%.*]] = bitcast i8 [[TMP7]] to <8 x i1>
// CHECK-NEXT:    [[TMP13:%.*]] = extractelement <8 x i1> [[TMP12]], i64 0
// CHECK-NEXT:    [[TMP14:%.*]] = select i1 [[TMP13]], half [[TMP11]], half [[TMP10]]
// CHECK-NEXT:    [[TMP15:%.*]] = insertelement <8 x half> [[TMP6]], half [[TMP14]], i64 0
// CHECK-NEXT:    ret <8 x half> [[TMP15]]
//
__m128h test_mm_mask3_fmadd_sh(__m128h __W, __m128h __X, __m128h __Y, __mmask8 __U) {
  return _mm_mask3_fmadd_sh(__W, __X, __Y, __U);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask3_fmadd_round_sh(
// CHECK-SAME: <8 x half> noundef [[__W:%.*]], <8 x half> noundef [[__X:%.*]], <8 x half> noundef [[__Y:%.*]], i8 noundef zeroext [[__U:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__X_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__Y_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    store <8 x half> [[__W]], ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__X]], ptr [[__X_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__Y]], ptr [[__Y_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__X_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__Y_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP4:%.*]] = extractelement <8 x half> [[TMP0]], i64 0
// CHECK-NEXT:    [[TMP5:%.*]] = extractelement <8 x half> [[TMP1]], i64 0
// CHECK-NEXT:    [[TMP6:%.*]] = extractelement <8 x half> [[TMP2]], i64 0
// CHECK-NEXT:    [[TMP7:%.*]] = call half @llvm.x86.avx512fp16.vfmadd.f16(half [[TMP4]], half [[TMP5]], half [[TMP6]], i32 11)
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast i8 [[TMP3]] to <8 x i1>
// CHECK-NEXT:    [[TMP9:%.*]] = extractelement <8 x i1> [[TMP8]], i64 0
// CHECK-NEXT:    [[TMP10:%.*]] = select i1 [[TMP9]], half [[TMP7]], half [[TMP6]]
// CHECK-NEXT:    [[TMP11:%.*]] = insertelement <8 x half> [[TMP2]], half [[TMP10]], i64 0
// CHECK-NEXT:    ret <8 x half> [[TMP11]]
//
__m128h test_mm_mask3_fmadd_round_sh(__m128h __W, __m128h __X, __m128h __Y, __mmask8 __U) {
  return _mm_mask3_fmadd_round_sh(__W, __X, __Y, __U, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_fmsub_sh(
// CHECK-SAME: <8 x half> noundef [[__W:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__W]], ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[FNEG_I:%.*]] = fneg <8 x half> [[TMP5]]
// CHECK-NEXT:    [[TMP6:%.*]] = extractelement <8 x half> [[TMP3]], i64 0
// CHECK-NEXT:    [[TMP7:%.*]] = extractelement <8 x half> [[TMP4]], i64 0
// CHECK-NEXT:    [[TMP8:%.*]] = extractelement <8 x half> [[FNEG_I]], i64 0
// CHECK-NEXT:    [[TMP9:%.*]] = call half @llvm.fma.f16(half [[TMP6]], half [[TMP7]], half [[TMP8]])
// CHECK-NEXT:    [[TMP10:%.*]] = insertelement <8 x half> [[TMP3]], half [[TMP9]], i64 0
// CHECK-NEXT:    ret <8 x half> [[TMP10]]
//
__m128h test_mm_fmsub_sh(__m128h __W, __m128h __A, __m128h __B) {
  return _mm_fmsub_sh(__W, __A, __B);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask_fmsub_sh(
// CHECK-SAME: <8 x half> noundef [[__W:%.*]], i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__W]], ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    store i8 [[TMP1]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP3]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[FNEG_I:%.*]] = fneg <8 x half> [[TMP6]]
// CHECK-NEXT:    [[TMP7:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP8:%.*]] = extractelement <8 x half> [[TMP4]], i64 0
// CHECK-NEXT:    [[TMP9:%.*]] = extractelement <8 x half> [[TMP5]], i64 0
// CHECK-NEXT:    [[TMP10:%.*]] = extractelement <8 x half> [[FNEG_I]], i64 0
// CHECK-NEXT:    [[TMP11:%.*]] = call half @llvm.fma.f16(half [[TMP8]], half [[TMP9]], half [[TMP10]])
// CHECK-NEXT:    [[TMP12:%.*]] = bitcast i8 [[TMP7]] to <8 x i1>
// CHECK-NEXT:    [[TMP13:%.*]] = extractelement <8 x i1> [[TMP12]], i64 0
// CHECK-NEXT:    [[TMP14:%.*]] = select i1 [[TMP13]], half [[TMP11]], half [[TMP8]]
// CHECK-NEXT:    [[TMP15:%.*]] = insertelement <8 x half> [[TMP4]], half [[TMP14]], i64 0
// CHECK-NEXT:    ret <8 x half> [[TMP15]]
//
__m128h test_mm_mask_fmsub_sh(__m128h __W, __mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_mask_fmsub_sh(__W, __U, __A, __B);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_fmsub_round_sh(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]], <8 x half> noundef [[__C:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__C]], ptr [[__C_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__C_ADDR]], align 16
// CHECK-NEXT:    [[FNEG:%.*]] = fneg <8 x half> [[TMP2]]
// CHECK-NEXT:    [[TMP3:%.*]] = extractelement <8 x half> [[TMP0]], i64 0
// CHECK-NEXT:    [[TMP4:%.*]] = extractelement <8 x half> [[TMP1]], i64 0
// CHECK-NEXT:    [[TMP5:%.*]] = extractelement <8 x half> [[FNEG]], i64 0
// CHECK-NEXT:    [[TMP6:%.*]] = call half @llvm.x86.avx512fp16.vfmadd.f16(half [[TMP3]], half [[TMP4]], half [[TMP5]], i32 11)
// CHECK-NEXT:    [[TMP7:%.*]] = insertelement <8 x half> [[TMP0]], half [[TMP6]], i64 0
// CHECK-NEXT:    ret <8 x half> [[TMP7]]
//
__m128h test_mm_fmsub_round_sh(__m128h __A, __m128h __B, __m128h __C) {
  return _mm_fmsub_round_sh(__A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask_fmsub_round_sh(
// CHECK-SAME: <8 x half> noundef [[__W:%.*]], i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__W]], ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[FNEG:%.*]] = fneg <8 x half> [[TMP2]]
// CHECK-NEXT:    [[TMP3:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP4:%.*]] = extractelement <8 x half> [[TMP0]], i64 0
// CHECK-NEXT:    [[TMP5:%.*]] = extractelement <8 x half> [[TMP1]], i64 0
// CHECK-NEXT:    [[TMP6:%.*]] = extractelement <8 x half> [[FNEG]], i64 0
// CHECK-NEXT:    [[TMP7:%.*]] = call half @llvm.x86.avx512fp16.vfmadd.f16(half [[TMP4]], half [[TMP5]], half [[TMP6]], i32 11)
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast i8 [[TMP3]] to <8 x i1>
// CHECK-NEXT:    [[TMP9:%.*]] = extractelement <8 x i1> [[TMP8]], i64 0
// CHECK-NEXT:    [[TMP10:%.*]] = select i1 [[TMP9]], half [[TMP7]], half [[TMP4]]
// CHECK-NEXT:    [[TMP11:%.*]] = insertelement <8 x half> [[TMP0]], half [[TMP10]], i64 0
// CHECK-NEXT:    ret <8 x half> [[TMP11]]
//
__m128h test_mm_mask_fmsub_round_sh(__m128h __W, __mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_mask_fmsub_round_sh(__W, __U, __A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_maskz_fmsub_sh(
// CHECK-SAME: i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]], <8 x half> noundef [[__C:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__C_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__C]], ptr [[__C_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__C_ADDR]], align 16
// CHECK-NEXT:    store i8 [[TMP0]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP3]], ptr [[__C_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = load <8 x half>, ptr [[__C_ADDR_I]], align 16
// CHECK-NEXT:    [[FNEG_I:%.*]] = fneg <8 x half> [[TMP6]]
// CHECK-NEXT:    [[TMP7:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP8:%.*]] = extractelement <8 x half> [[TMP4]], i64 0
// CHECK-NEXT:    [[TMP9:%.*]] = extractelement <8 x half> [[TMP5]], i64 0
// CHECK-NEXT:    [[TMP10:%.*]] = extractelement <8 x half> [[FNEG_I]], i64 0
// CHECK-NEXT:    [[TMP11:%.*]] = call half @llvm.fma.f16(half [[TMP8]], half [[TMP9]], half [[TMP10]])
// CHECK-NEXT:    [[TMP12:%.*]] = bitcast i8 [[TMP7]] to <8 x i1>
// CHECK-NEXT:    [[TMP13:%.*]] = extractelement <8 x i1> [[TMP12]], i64 0
// CHECK-NEXT:    [[TMP14:%.*]] = select i1 [[TMP13]], half [[TMP11]], half 0xH0000
// CHECK-NEXT:    [[TMP15:%.*]] = insertelement <8 x half> [[TMP4]], half [[TMP14]], i64 0
// CHECK-NEXT:    ret <8 x half> [[TMP15]]
//
__m128h test_mm_maskz_fmsub_sh(__mmask8 __U, __m128h __A, __m128h __B, __m128h __C) {
  return _mm_maskz_fmsub_sh(__U, __A, __B, __C);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_maskz_fmsub_round_sh(
// CHECK-SAME: i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]], <8 x half> noundef [[__C:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__C]], ptr [[__C_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__C_ADDR]], align 16
// CHECK-NEXT:    [[FNEG:%.*]] = fneg <8 x half> [[TMP2]]
// CHECK-NEXT:    [[TMP3:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP4:%.*]] = extractelement <8 x half> [[TMP0]], i64 0
// CHECK-NEXT:    [[TMP5:%.*]] = extractelement <8 x half> [[TMP1]], i64 0
// CHECK-NEXT:    [[TMP6:%.*]] = extractelement <8 x half> [[FNEG]], i64 0
// CHECK-NEXT:    [[TMP7:%.*]] = call half @llvm.x86.avx512fp16.vfmadd.f16(half [[TMP4]], half [[TMP5]], half [[TMP6]], i32 11)
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast i8 [[TMP3]] to <8 x i1>
// CHECK-NEXT:    [[TMP9:%.*]] = extractelement <8 x i1> [[TMP8]], i64 0
// CHECK-NEXT:    [[TMP10:%.*]] = select i1 [[TMP9]], half [[TMP7]], half 0xH0000
// CHECK-NEXT:    [[TMP11:%.*]] = insertelement <8 x half> [[TMP0]], half [[TMP10]], i64 0
// CHECK-NEXT:    ret <8 x half> [[TMP11]]
//
__m128h test_mm_maskz_fmsub_round_sh(__mmask8 __U, __m128h __A, __m128h __B, __m128h __C) {
  return _mm_maskz_fmsub_round_sh(__U, __A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask3_fmsub_sh(
// CHECK-SAME: <8 x half> noundef [[__W:%.*]], <8 x half> noundef [[__X:%.*]], <8 x half> noundef [[__Y:%.*]], i8 noundef zeroext [[__U:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__X_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__Y_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__X_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__Y_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    store <8 x half> [[__W]], ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__X]], ptr [[__X_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__Y]], ptr [[__Y_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__X_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__Y_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__X_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__Y_ADDR_I]], align 16
// CHECK-NEXT:    store i8 [[TMP3]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = load <8 x half>, ptr [[__X_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = load <8 x half>, ptr [[__Y_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP7:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP8:%.*]] = fneg <8 x half> [[TMP6]]
// CHECK-NEXT:    [[TMP9:%.*]] = extractelement <8 x half> [[TMP4]], i64 0
// CHECK-NEXT:    [[TMP10:%.*]] = extractelement <8 x half> [[TMP5]], i64 0
// CHECK-NEXT:    [[TMP11:%.*]] = extractelement <8 x half> [[TMP8]], i64 0
// CHECK-NEXT:    [[TMP12:%.*]] = call half @llvm.fma.f16(half [[TMP9]], half [[TMP10]], half [[TMP11]])
// CHECK-NEXT:    [[TMP13:%.*]] = extractelement <8 x half> [[TMP6]], i64 0
// CHECK-NEXT:    [[TMP14:%.*]] = bitcast i8 [[TMP7]] to <8 x i1>
// CHECK-NEXT:    [[TMP15:%.*]] = extractelement <8 x i1> [[TMP14]], i64 0
// CHECK-NEXT:    [[TMP16:%.*]] = select i1 [[TMP15]], half [[TMP12]], half [[TMP13]]
// CHECK-NEXT:    [[TMP17:%.*]] = insertelement <8 x half> [[TMP6]], half [[TMP16]], i64 0
// CHECK-NEXT:    ret <8 x half> [[TMP17]]
//
__m128h test_mm_mask3_fmsub_sh(__m128h __W, __m128h __X, __m128h __Y, __mmask8 __U) {
  return _mm_mask3_fmsub_sh(__W, __X, __Y, __U);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask3_fmsub_round_sh(
// CHECK-SAME: <8 x half> noundef [[__W:%.*]], <8 x half> noundef [[__X:%.*]], <8 x half> noundef [[__Y:%.*]], i8 noundef zeroext [[__U:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__X_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__Y_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    store <8 x half> [[__W]], ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__X]], ptr [[__X_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__Y]], ptr [[__Y_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__X_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__Y_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP4:%.*]] = fneg <8 x half> [[TMP2]]
// CHECK-NEXT:    [[TMP5:%.*]] = extractelement <8 x half> [[TMP0]], i64 0
// CHECK-NEXT:    [[TMP6:%.*]] = extractelement <8 x half> [[TMP1]], i64 0
// CHECK-NEXT:    [[TMP7:%.*]] = extractelement <8 x half> [[TMP4]], i64 0
// CHECK-NEXT:    [[TMP8:%.*]] = call half @llvm.x86.avx512fp16.vfmadd.f16(half [[TMP5]], half [[TMP6]], half [[TMP7]], i32 11)
// CHECK-NEXT:    [[TMP9:%.*]] = extractelement <8 x half> [[TMP2]], i64 0
// CHECK-NEXT:    [[TMP10:%.*]] = bitcast i8 [[TMP3]] to <8 x i1>
// CHECK-NEXT:    [[TMP11:%.*]] = extractelement <8 x i1> [[TMP10]], i64 0
// CHECK-NEXT:    [[TMP12:%.*]] = select i1 [[TMP11]], half [[TMP8]], half [[TMP9]]
// CHECK-NEXT:    [[TMP13:%.*]] = insertelement <8 x half> [[TMP2]], half [[TMP12]], i64 0
// CHECK-NEXT:    ret <8 x half> [[TMP13]]
//
__m128h test_mm_mask3_fmsub_round_sh(__m128h __W, __m128h __X, __m128h __Y, __mmask8 __U) {
  return _mm_mask3_fmsub_round_sh(__W, __X, __Y, __U, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_fnmadd_sh(
// CHECK-SAME: <8 x half> noundef [[__W:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__W]], ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[FNEG_I:%.*]] = fneg <8 x half> [[TMP4]]
// CHECK-NEXT:    [[TMP5:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = extractelement <8 x half> [[TMP3]], i64 0
// CHECK-NEXT:    [[TMP7:%.*]] = extractelement <8 x half> [[FNEG_I]], i64 0
// CHECK-NEXT:    [[TMP8:%.*]] = extractelement <8 x half> [[TMP5]], i64 0
// CHECK-NEXT:    [[TMP9:%.*]] = call half @llvm.fma.f16(half [[TMP6]], half [[TMP7]], half [[TMP8]])
// CHECK-NEXT:    [[TMP10:%.*]] = insertelement <8 x half> [[TMP3]], half [[TMP9]], i64 0
// CHECK-NEXT:    ret <8 x half> [[TMP10]]
//
__m128h test_mm_fnmadd_sh(__m128h __W, __m128h __A, __m128h __B) {
  return _mm_fnmadd_sh(__W, __A, __B);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask_fnmadd_sh(
// CHECK-SAME: <8 x half> noundef [[__W:%.*]], i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__W]], ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    store i8 [[TMP1]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP3]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[FNEG_I:%.*]] = fneg <8 x half> [[TMP5]]
// CHECK-NEXT:    [[TMP6:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP7:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP8:%.*]] = extractelement <8 x half> [[TMP4]], i64 0
// CHECK-NEXT:    [[TMP9:%.*]] = extractelement <8 x half> [[FNEG_I]], i64 0
// CHECK-NEXT:    [[TMP10:%.*]] = extractelement <8 x half> [[TMP6]], i64 0
// CHECK-NEXT:    [[TMP11:%.*]] = call half @llvm.fma.f16(half [[TMP8]], half [[TMP9]], half [[TMP10]])
// CHECK-NEXT:    [[TMP12:%.*]] = bitcast i8 [[TMP7]] to <8 x i1>
// CHECK-NEXT:    [[TMP13:%.*]] = extractelement <8 x i1> [[TMP12]], i64 0
// CHECK-NEXT:    [[TMP14:%.*]] = select i1 [[TMP13]], half [[TMP11]], half [[TMP8]]
// CHECK-NEXT:    [[TMP15:%.*]] = insertelement <8 x half> [[TMP4]], half [[TMP14]], i64 0
// CHECK-NEXT:    ret <8 x half> [[TMP15]]
//
__m128h test_mm_mask_fnmadd_sh(__m128h __W, __mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_mask_fnmadd_sh(__W, __U, __A, __B);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_fnmadd_round_sh(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]], <8 x half> noundef [[__C:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__C]], ptr [[__C_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[FNEG:%.*]] = fneg <8 x half> [[TMP1]]
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__C_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = extractelement <8 x half> [[TMP0]], i64 0
// CHECK-NEXT:    [[TMP4:%.*]] = extractelement <8 x half> [[FNEG]], i64 0
// CHECK-NEXT:    [[TMP5:%.*]] = extractelement <8 x half> [[TMP2]], i64 0
// CHECK-NEXT:    [[TMP6:%.*]] = call half @llvm.x86.avx512fp16.vfmadd.f16(half [[TMP3]], half [[TMP4]], half [[TMP5]], i32 11)
// CHECK-NEXT:    [[TMP7:%.*]] = insertelement <8 x half> [[TMP0]], half [[TMP6]], i64 0
// CHECK-NEXT:    ret <8 x half> [[TMP7]]
//
__m128h test_mm_fnmadd_round_sh(__m128h __A, __m128h __B, __m128h __C) {
  return _mm_fnmadd_round_sh(__A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask_fnmadd_round_sh(
// CHECK-SAME: <8 x half> noundef [[__W:%.*]], i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__W]], ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[FNEG:%.*]] = fneg <8 x half> [[TMP1]]
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP4:%.*]] = extractelement <8 x half> [[TMP0]], i64 0
// CHECK-NEXT:    [[TMP5:%.*]] = extractelement <8 x half> [[FNEG]], i64 0
// CHECK-NEXT:    [[TMP6:%.*]] = extractelement <8 x half> [[TMP2]], i64 0
// CHECK-NEXT:    [[TMP7:%.*]] = call half @llvm.x86.avx512fp16.vfmadd.f16(half [[TMP4]], half [[TMP5]], half [[TMP6]], i32 11)
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast i8 [[TMP3]] to <8 x i1>
// CHECK-NEXT:    [[TMP9:%.*]] = extractelement <8 x i1> [[TMP8]], i64 0
// CHECK-NEXT:    [[TMP10:%.*]] = select i1 [[TMP9]], half [[TMP7]], half [[TMP4]]
// CHECK-NEXT:    [[TMP11:%.*]] = insertelement <8 x half> [[TMP0]], half [[TMP10]], i64 0
// CHECK-NEXT:    ret <8 x half> [[TMP11]]
//
__m128h test_mm_mask_fnmadd_round_sh(__m128h __W, __mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_mask_fnmadd_round_sh(__W, __U, __A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_maskz_fnmadd_sh(
// CHECK-SAME: i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]], <8 x half> noundef [[__C:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__C_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__C]], ptr [[__C_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__C_ADDR]], align 16
// CHECK-NEXT:    store i8 [[TMP0]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP3]], ptr [[__C_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[FNEG_I:%.*]] = fneg <8 x half> [[TMP5]]
// CHECK-NEXT:    [[TMP6:%.*]] = load <8 x half>, ptr [[__C_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP7:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP8:%.*]] = extractelement <8 x half> [[TMP4]], i64 0
// CHECK-NEXT:    [[TMP9:%.*]] = extractelement <8 x half> [[FNEG_I]], i64 0
// CHECK-NEXT:    [[TMP10:%.*]] = extractelement <8 x half> [[TMP6]], i64 0
// CHECK-NEXT:    [[TMP11:%.*]] = call half @llvm.fma.f16(half [[TMP8]], half [[TMP9]], half [[TMP10]])
// CHECK-NEXT:    [[TMP12:%.*]] = bitcast i8 [[TMP7]] to <8 x i1>
// CHECK-NEXT:    [[TMP13:%.*]] = extractelement <8 x i1> [[TMP12]], i64 0
// CHECK-NEXT:    [[TMP14:%.*]] = select i1 [[TMP13]], half [[TMP11]], half 0xH0000
// CHECK-NEXT:    [[TMP15:%.*]] = insertelement <8 x half> [[TMP4]], half [[TMP14]], i64 0
// CHECK-NEXT:    ret <8 x half> [[TMP15]]
//
__m128h test_mm_maskz_fnmadd_sh(__mmask8 __U, __m128h __A, __m128h __B, __m128h __C) {
  return _mm_maskz_fnmadd_sh(__U, __A, __B, __C);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_maskz_fnmadd_round_sh(
// CHECK-SAME: i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]], <8 x half> noundef [[__C:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__C]], ptr [[__C_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[FNEG:%.*]] = fneg <8 x half> [[TMP1]]
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__C_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP4:%.*]] = extractelement <8 x half> [[TMP0]], i64 0
// CHECK-NEXT:    [[TMP5:%.*]] = extractelement <8 x half> [[FNEG]], i64 0
// CHECK-NEXT:    [[TMP6:%.*]] = extractelement <8 x half> [[TMP2]], i64 0
// CHECK-NEXT:    [[TMP7:%.*]] = call half @llvm.x86.avx512fp16.vfmadd.f16(half [[TMP4]], half [[TMP5]], half [[TMP6]], i32 11)
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast i8 [[TMP3]] to <8 x i1>
// CHECK-NEXT:    [[TMP9:%.*]] = extractelement <8 x i1> [[TMP8]], i64 0
// CHECK-NEXT:    [[TMP10:%.*]] = select i1 [[TMP9]], half [[TMP7]], half 0xH0000
// CHECK-NEXT:    [[TMP11:%.*]] = insertelement <8 x half> [[TMP0]], half [[TMP10]], i64 0
// CHECK-NEXT:    ret <8 x half> [[TMP11]]
//
__m128h test_mm_maskz_fnmadd_round_sh(__mmask8 __U, __m128h __A, __m128h __B, __m128h __C) {
  return _mm_maskz_fnmadd_round_sh(__U, __A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask3_fnmadd_sh(
// CHECK-SAME: <8 x half> noundef [[__W:%.*]], <8 x half> noundef [[__X:%.*]], <8 x half> noundef [[__Y:%.*]], i8 noundef zeroext [[__U:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__X_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__Y_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__X_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__Y_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    store <8 x half> [[__W]], ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__X]], ptr [[__X_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__Y]], ptr [[__Y_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__X_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__Y_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__X_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__Y_ADDR_I]], align 16
// CHECK-NEXT:    store i8 [[TMP3]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = load <8 x half>, ptr [[__X_ADDR_I]], align 16
// CHECK-NEXT:    [[FNEG_I:%.*]] = fneg <8 x half> [[TMP5]]
// CHECK-NEXT:    [[TMP6:%.*]] = load <8 x half>, ptr [[__Y_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP7:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP8:%.*]] = extractelement <8 x half> [[TMP4]], i64 0
// CHECK-NEXT:    [[TMP9:%.*]] = extractelement <8 x half> [[FNEG_I]], i64 0
// CHECK-NEXT:    [[TMP10:%.*]] = extractelement <8 x half> [[TMP6]], i64 0
// CHECK-NEXT:    [[TMP11:%.*]] = call half @llvm.fma.f16(half [[TMP8]], half [[TMP9]], half [[TMP10]])
// CHECK-NEXT:    [[TMP12:%.*]] = bitcast i8 [[TMP7]] to <8 x i1>
// CHECK-NEXT:    [[TMP13:%.*]] = extractelement <8 x i1> [[TMP12]], i64 0
// CHECK-NEXT:    [[TMP14:%.*]] = select i1 [[TMP13]], half [[TMP11]], half [[TMP10]]
// CHECK-NEXT:    [[TMP15:%.*]] = insertelement <8 x half> [[TMP6]], half [[TMP14]], i64 0
// CHECK-NEXT:    ret <8 x half> [[TMP15]]
//
__m128h test_mm_mask3_fnmadd_sh(__m128h __W, __m128h __X, __m128h __Y, __mmask8 __U) {
  return _mm_mask3_fnmadd_sh(__W, __X, __Y, __U);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask3_fnmadd_round_sh(
// CHECK-SAME: <8 x half> noundef [[__W:%.*]], <8 x half> noundef [[__X:%.*]], <8 x half> noundef [[__Y:%.*]], i8 noundef zeroext [[__U:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__X_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__Y_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    store <8 x half> [[__W]], ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__X]], ptr [[__X_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__Y]], ptr [[__Y_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__X_ADDR]], align 16
// CHECK-NEXT:    [[FNEG:%.*]] = fneg <8 x half> [[TMP1]]
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__Y_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP4:%.*]] = extractelement <8 x half> [[TMP0]], i64 0
// CHECK-NEXT:    [[TMP5:%.*]] = extractelement <8 x half> [[FNEG]], i64 0
// CHECK-NEXT:    [[TMP6:%.*]] = extractelement <8 x half> [[TMP2]], i64 0
// CHECK-NEXT:    [[TMP7:%.*]] = call half @llvm.x86.avx512fp16.vfmadd.f16(half [[TMP4]], half [[TMP5]], half [[TMP6]], i32 11)
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast i8 [[TMP3]] to <8 x i1>
// CHECK-NEXT:    [[TMP9:%.*]] = extractelement <8 x i1> [[TMP8]], i64 0
// CHECK-NEXT:    [[TMP10:%.*]] = select i1 [[TMP9]], half [[TMP7]], half [[TMP6]]
// CHECK-NEXT:    [[TMP11:%.*]] = insertelement <8 x half> [[TMP2]], half [[TMP10]], i64 0
// CHECK-NEXT:    ret <8 x half> [[TMP11]]
//
__m128h test_mm_mask3_fnmadd_round_sh(__m128h __W, __m128h __X, __m128h __Y, __mmask8 __U) {
  return _mm_mask3_fnmadd_round_sh(__W, __X, __Y, __U, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_fnmsub_sh(
// CHECK-SAME: <8 x half> noundef [[__W:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__W]], ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[FNEG_I:%.*]] = fneg <8 x half> [[TMP4]]
// CHECK-NEXT:    [[TMP5:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[FNEG1_I:%.*]] = fneg <8 x half> [[TMP5]]
// CHECK-NEXT:    [[TMP6:%.*]] = extractelement <8 x half> [[TMP3]], i64 0
// CHECK-NEXT:    [[TMP7:%.*]] = extractelement <8 x half> [[FNEG_I]], i64 0
// CHECK-NEXT:    [[TMP8:%.*]] = extractelement <8 x half> [[FNEG1_I]], i64 0
// CHECK-NEXT:    [[TMP9:%.*]] = call half @llvm.fma.f16(half [[TMP6]], half [[TMP7]], half [[TMP8]])
// CHECK-NEXT:    [[TMP10:%.*]] = insertelement <8 x half> [[TMP3]], half [[TMP9]], i64 0
// CHECK-NEXT:    ret <8 x half> [[TMP10]]
//
__m128h test_mm_fnmsub_sh(__m128h __W, __m128h __A, __m128h __B) {
  return _mm_fnmsub_sh(__W, __A, __B);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask_fnmsub_sh(
// CHECK-SAME: <8 x half> noundef [[__W:%.*]], i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__W]], ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    store i8 [[TMP1]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP3]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[FNEG_I:%.*]] = fneg <8 x half> [[TMP5]]
// CHECK-NEXT:    [[TMP6:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[FNEG1_I:%.*]] = fneg <8 x half> [[TMP6]]
// CHECK-NEXT:    [[TMP7:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP8:%.*]] = extractelement <8 x half> [[TMP4]], i64 0
// CHECK-NEXT:    [[TMP9:%.*]] = extractelement <8 x half> [[FNEG_I]], i64 0
// CHECK-NEXT:    [[TMP10:%.*]] = extractelement <8 x half> [[FNEG1_I]], i64 0
// CHECK-NEXT:    [[TMP11:%.*]] = call half @llvm.fma.f16(half [[TMP8]], half [[TMP9]], half [[TMP10]])
// CHECK-NEXT:    [[TMP12:%.*]] = bitcast i8 [[TMP7]] to <8 x i1>
// CHECK-NEXT:    [[TMP13:%.*]] = extractelement <8 x i1> [[TMP12]], i64 0
// CHECK-NEXT:    [[TMP14:%.*]] = select i1 [[TMP13]], half [[TMP11]], half [[TMP8]]
// CHECK-NEXT:    [[TMP15:%.*]] = insertelement <8 x half> [[TMP4]], half [[TMP14]], i64 0
// CHECK-NEXT:    ret <8 x half> [[TMP15]]
//
__m128h test_mm_mask_fnmsub_sh(__m128h __W, __mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_mask_fnmsub_sh(__W, __U, __A, __B);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_fnmsub_round_sh(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]], <8 x half> noundef [[__C:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__C]], ptr [[__C_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[FNEG:%.*]] = fneg <8 x half> [[TMP1]]
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__C_ADDR]], align 16
// CHECK-NEXT:    [[FNEG1:%.*]] = fneg <8 x half> [[TMP2]]
// CHECK-NEXT:    [[TMP3:%.*]] = extractelement <8 x half> [[TMP0]], i64 0
// CHECK-NEXT:    [[TMP4:%.*]] = extractelement <8 x half> [[FNEG]], i64 0
// CHECK-NEXT:    [[TMP5:%.*]] = extractelement <8 x half> [[FNEG1]], i64 0
// CHECK-NEXT:    [[TMP6:%.*]] = call half @llvm.x86.avx512fp16.vfmadd.f16(half [[TMP3]], half [[TMP4]], half [[TMP5]], i32 11)
// CHECK-NEXT:    [[TMP7:%.*]] = insertelement <8 x half> [[TMP0]], half [[TMP6]], i64 0
// CHECK-NEXT:    ret <8 x half> [[TMP7]]
//
__m128h test_mm_fnmsub_round_sh(__m128h __A, __m128h __B, __m128h __C) {
  return _mm_fnmsub_round_sh(__A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask_fnmsub_round_sh(
// CHECK-SAME: <8 x half> noundef [[__W:%.*]], i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__W]], ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[FNEG:%.*]] = fneg <8 x half> [[TMP1]]
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[FNEG1:%.*]] = fneg <8 x half> [[TMP2]]
// CHECK-NEXT:    [[TMP3:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP4:%.*]] = extractelement <8 x half> [[TMP0]], i64 0
// CHECK-NEXT:    [[TMP5:%.*]] = extractelement <8 x half> [[FNEG]], i64 0
// CHECK-NEXT:    [[TMP6:%.*]] = extractelement <8 x half> [[FNEG1]], i64 0
// CHECK-NEXT:    [[TMP7:%.*]] = call half @llvm.x86.avx512fp16.vfmadd.f16(half [[TMP4]], half [[TMP5]], half [[TMP6]], i32 11)
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast i8 [[TMP3]] to <8 x i1>
// CHECK-NEXT:    [[TMP9:%.*]] = extractelement <8 x i1> [[TMP8]], i64 0
// CHECK-NEXT:    [[TMP10:%.*]] = select i1 [[TMP9]], half [[TMP7]], half [[TMP4]]
// CHECK-NEXT:    [[TMP11:%.*]] = insertelement <8 x half> [[TMP0]], half [[TMP10]], i64 0
// CHECK-NEXT:    ret <8 x half> [[TMP11]]
//
__m128h test_mm_mask_fnmsub_round_sh(__m128h __W, __mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_mask_fnmsub_round_sh(__W, __U, __A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_maskz_fnmsub_sh(
// CHECK-SAME: i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]], <8 x half> noundef [[__C:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__C_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__C]], ptr [[__C_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__C_ADDR]], align 16
// CHECK-NEXT:    store i8 [[TMP0]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP3]], ptr [[__C_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[FNEG_I:%.*]] = fneg <8 x half> [[TMP5]]
// CHECK-NEXT:    [[TMP6:%.*]] = load <8 x half>, ptr [[__C_ADDR_I]], align 16
// CHECK-NEXT:    [[FNEG1_I:%.*]] = fneg <8 x half> [[TMP6]]
// CHECK-NEXT:    [[TMP7:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP8:%.*]] = extractelement <8 x half> [[TMP4]], i64 0
// CHECK-NEXT:    [[TMP9:%.*]] = extractelement <8 x half> [[FNEG_I]], i64 0
// CHECK-NEXT:    [[TMP10:%.*]] = extractelement <8 x half> [[FNEG1_I]], i64 0
// CHECK-NEXT:    [[TMP11:%.*]] = call half @llvm.fma.f16(half [[TMP8]], half [[TMP9]], half [[TMP10]])
// CHECK-NEXT:    [[TMP12:%.*]] = bitcast i8 [[TMP7]] to <8 x i1>
// CHECK-NEXT:    [[TMP13:%.*]] = extractelement <8 x i1> [[TMP12]], i64 0
// CHECK-NEXT:    [[TMP14:%.*]] = select i1 [[TMP13]], half [[TMP11]], half 0xH0000
// CHECK-NEXT:    [[TMP15:%.*]] = insertelement <8 x half> [[TMP4]], half [[TMP14]], i64 0
// CHECK-NEXT:    ret <8 x half> [[TMP15]]
//
__m128h test_mm_maskz_fnmsub_sh(__mmask8 __U, __m128h __A, __m128h __B, __m128h __C) {
  return _mm_maskz_fnmsub_sh(__U, __A, __B, __C);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_maskz_fnmsub_round_sh(
// CHECK-SAME: i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]], <8 x half> noundef [[__C:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__C]], ptr [[__C_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[FNEG:%.*]] = fneg <8 x half> [[TMP1]]
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__C_ADDR]], align 16
// CHECK-NEXT:    [[FNEG1:%.*]] = fneg <8 x half> [[TMP2]]
// CHECK-NEXT:    [[TMP3:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP4:%.*]] = extractelement <8 x half> [[TMP0]], i64 0
// CHECK-NEXT:    [[TMP5:%.*]] = extractelement <8 x half> [[FNEG]], i64 0
// CHECK-NEXT:    [[TMP6:%.*]] = extractelement <8 x half> [[FNEG1]], i64 0
// CHECK-NEXT:    [[TMP7:%.*]] = call half @llvm.x86.avx512fp16.vfmadd.f16(half [[TMP4]], half [[TMP5]], half [[TMP6]], i32 11)
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast i8 [[TMP3]] to <8 x i1>
// CHECK-NEXT:    [[TMP9:%.*]] = extractelement <8 x i1> [[TMP8]], i64 0
// CHECK-NEXT:    [[TMP10:%.*]] = select i1 [[TMP9]], half [[TMP7]], half 0xH0000
// CHECK-NEXT:    [[TMP11:%.*]] = insertelement <8 x half> [[TMP0]], half [[TMP10]], i64 0
// CHECK-NEXT:    ret <8 x half> [[TMP11]]
//
__m128h test_mm_maskz_fnmsub_round_sh(__mmask8 __U, __m128h __A, __m128h __B, __m128h __C) {
  return _mm_maskz_fnmsub_round_sh(__U, __A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask3_fnmsub_sh(
// CHECK-SAME: <8 x half> noundef [[__W:%.*]], <8 x half> noundef [[__X:%.*]], <8 x half> noundef [[__Y:%.*]], i8 noundef zeroext [[__U:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__X_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__Y_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__X_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__Y_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    store <8 x half> [[__W]], ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__X]], ptr [[__X_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__Y]], ptr [[__Y_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__X_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__Y_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__X_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__Y_ADDR_I]], align 16
// CHECK-NEXT:    store i8 [[TMP3]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = load <8 x half>, ptr [[__X_ADDR_I]], align 16
// CHECK-NEXT:    [[FNEG_I:%.*]] = fneg <8 x half> [[TMP5]]
// CHECK-NEXT:    [[TMP6:%.*]] = load <8 x half>, ptr [[__Y_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP7:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP8:%.*]] = fneg <8 x half> [[TMP6]]
// CHECK-NEXT:    [[TMP9:%.*]] = extractelement <8 x half> [[TMP4]], i64 0
// CHECK-NEXT:    [[TMP10:%.*]] = extractelement <8 x half> [[FNEG_I]], i64 0
// CHECK-NEXT:    [[TMP11:%.*]] = extractelement <8 x half> [[TMP8]], i64 0
// CHECK-NEXT:    [[TMP12:%.*]] = call half @llvm.fma.f16(half [[TMP9]], half [[TMP10]], half [[TMP11]])
// CHECK-NEXT:    [[TMP13:%.*]] = extractelement <8 x half> [[TMP6]], i64 0
// CHECK-NEXT:    [[TMP14:%.*]] = bitcast i8 [[TMP7]] to <8 x i1>
// CHECK-NEXT:    [[TMP15:%.*]] = extractelement <8 x i1> [[TMP14]], i64 0
// CHECK-NEXT:    [[TMP16:%.*]] = select i1 [[TMP15]], half [[TMP12]], half [[TMP13]]
// CHECK-NEXT:    [[TMP17:%.*]] = insertelement <8 x half> [[TMP6]], half [[TMP16]], i64 0
// CHECK-NEXT:    ret <8 x half> [[TMP17]]
//
__m128h test_mm_mask3_fnmsub_sh(__m128h __W, __m128h __X, __m128h __Y, __mmask8 __U) {
  return _mm_mask3_fnmsub_sh(__W, __X, __Y, __U);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask3_fnmsub_round_sh(
// CHECK-SAME: <8 x half> noundef [[__W:%.*]], <8 x half> noundef [[__X:%.*]], <8 x half> noundef [[__Y:%.*]], i8 noundef zeroext [[__U:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__X_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__Y_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    store <8 x half> [[__W]], ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__X]], ptr [[__X_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__Y]], ptr [[__Y_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__X_ADDR]], align 16
// CHECK-NEXT:    [[FNEG:%.*]] = fneg <8 x half> [[TMP1]]
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__Y_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP4:%.*]] = fneg <8 x half> [[TMP2]]
// CHECK-NEXT:    [[TMP5:%.*]] = extractelement <8 x half> [[TMP0]], i64 0
// CHECK-NEXT:    [[TMP6:%.*]] = extractelement <8 x half> [[FNEG]], i64 0
// CHECK-NEXT:    [[TMP7:%.*]] = extractelement <8 x half> [[TMP4]], i64 0
// CHECK-NEXT:    [[TMP8:%.*]] = call half @llvm.x86.avx512fp16.vfmadd.f16(half [[TMP5]], half [[TMP6]], half [[TMP7]], i32 11)
// CHECK-NEXT:    [[TMP9:%.*]] = extractelement <8 x half> [[TMP2]], i64 0
// CHECK-NEXT:    [[TMP10:%.*]] = bitcast i8 [[TMP3]] to <8 x i1>
// CHECK-NEXT:    [[TMP11:%.*]] = extractelement <8 x i1> [[TMP10]], i64 0
// CHECK-NEXT:    [[TMP12:%.*]] = select i1 [[TMP11]], half [[TMP8]], half [[TMP9]]
// CHECK-NEXT:    [[TMP13:%.*]] = insertelement <8 x half> [[TMP2]], half [[TMP12]], i64 0
// CHECK-NEXT:    ret <8 x half> [[TMP13]]
//
__m128h test_mm_mask3_fnmsub_round_sh(__m128h __W, __m128h __X, __m128h __Y, __mmask8 __U) {
  return _mm_mask3_fnmsub_round_sh(__W, __X, __Y, __U, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_fcmadd_sch(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]], <8 x half> noundef [[__C:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__C_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__C]], ptr [[__C_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__C_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__C_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <8 x half> [[TMP3]] to <4 x float>
// CHECK-NEXT:    [[TMP5:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <8 x half> [[TMP5]] to <4 x float>
// CHECK-NEXT:    [[TMP7:%.*]] = load <8 x half>, ptr [[__C_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast <8 x half> [[TMP7]] to <4 x float>
// CHECK-NEXT:    [[TMP9:%.*]] = call <4 x float> @llvm.x86.avx512fp16.mask.vfcmadd.csh(<4 x float> [[TMP4]], <4 x float> [[TMP6]], <4 x float> [[TMP8]], i8 -1, i32 4)
// CHECK-NEXT:    [[TMP10:%.*]] = bitcast <4 x float> [[TMP9]] to <8 x half>
// CHECK-NEXT:    ret <8 x half> [[TMP10]]
//
__m128h test_mm_fcmadd_sch(__m128h __A, __m128h __B, __m128h __C) {
  return _mm_fcmadd_sch(__A, __B, __C);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask_fcmadd_sch(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__B:%.*]], <8 x half> noundef [[__C:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__C_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__C]], ptr [[__C_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__C_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store i8 [[TMP1]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP3]], ptr [[__C_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <8 x half> [[TMP4]] to <4 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast <8 x half> [[TMP6]] to <4 x float>
// CHECK-NEXT:    [[TMP8:%.*]] = load <8 x half>, ptr [[__C_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP9:%.*]] = bitcast <8 x half> [[TMP8]] to <4 x float>
// CHECK-NEXT:    [[TMP10:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP11:%.*]] = call <4 x float> @llvm.x86.avx512fp16.mask.vfcmadd.csh(<4 x float> [[TMP5]], <4 x float> [[TMP7]], <4 x float> [[TMP9]], i8 [[TMP10]], i32 4)
// CHECK-NEXT:    [[TMP12:%.*]] = and i8 [[TMP10]], 1
// CHECK-NEXT:    [[TMP13:%.*]] = bitcast i8 [[TMP12]] to <8 x i1>
// CHECK-NEXT:    [[EXTRACT_I:%.*]] = shufflevector <8 x i1> [[TMP13]], <8 x i1> [[TMP13]], <4 x i32> <i32 0, i32 1, i32 2, i32 3>
// CHECK-NEXT:    [[TMP14:%.*]] = select <4 x i1> [[EXTRACT_I]], <4 x float> [[TMP11]], <4 x float> [[TMP5]]
// CHECK-NEXT:    [[TMP15:%.*]] = bitcast <4 x float> [[TMP14]] to <8 x half>
// CHECK-NEXT:    ret <8 x half> [[TMP15]]
//
__m128h test_mm_mask_fcmadd_sch(__m128h __A, __mmask8 __U, __m128h __B, __m128h __C) {
  return _mm_mask_fcmadd_sch(__A, __U, __B, __C);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_maskz_fcmadd_sch(
// CHECK-SAME: i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]], <8 x half> noundef [[__C:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__C_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__C]], ptr [[__C_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__C_ADDR]], align 16
// CHECK-NEXT:    store i8 [[TMP0]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP3]], ptr [[__C_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <8 x half> [[TMP4]] to <4 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast <8 x half> [[TMP6]] to <4 x float>
// CHECK-NEXT:    [[TMP8:%.*]] = load <8 x half>, ptr [[__C_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP9:%.*]] = bitcast <8 x half> [[TMP8]] to <4 x float>
// CHECK-NEXT:    [[TMP10:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP11:%.*]] = call <4 x float> @llvm.x86.avx512fp16.maskz.vfcmadd.csh(<4 x float> [[TMP5]], <4 x float> [[TMP7]], <4 x float> [[TMP9]], i8 [[TMP10]], i32 4)
// CHECK-NEXT:    [[TMP12:%.*]] = bitcast <4 x float> [[TMP11]] to <8 x half>
// CHECK-NEXT:    ret <8 x half> [[TMP12]]
//
__m128h test_mm_maskz_fcmadd_sch(__mmask8 __U, __m128h __A, __m128h __B, __m128h __C) {
  return _mm_maskz_fcmadd_sch(__U, __A, __B, __C);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask3_fcmadd_sch(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]], <8 x half> noundef [[__C:%.*]], i8 noundef zeroext [[__U:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__C_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__C]], ptr [[__C_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__C_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__C_ADDR_I]], align 16
// CHECK-NEXT:    store i8 [[TMP3]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <8 x half> [[TMP4]] to <4 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast <8 x half> [[TMP6]] to <4 x float>
// CHECK-NEXT:    [[TMP8:%.*]] = load <8 x half>, ptr [[__C_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP9:%.*]] = bitcast <8 x half> [[TMP8]] to <4 x float>
// CHECK-NEXT:    [[TMP10:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP11:%.*]] = call <4 x float> @llvm.x86.avx512fp16.mask.vfcmadd.csh(<4 x float> [[TMP5]], <4 x float> [[TMP7]], <4 x float> [[TMP9]], i8 [[TMP10]], i32 4)
// CHECK-NEXT:    [[TMP12:%.*]] = shufflevector <4 x float> [[TMP11]], <4 x float> [[TMP9]], <4 x i32> <i32 0, i32 5, i32 6, i32 7>
// CHECK-NEXT:    [[TMP13:%.*]] = bitcast <4 x float> [[TMP12]] to <8 x half>
// CHECK-NEXT:    ret <8 x half> [[TMP13]]
//
__m128h test_mm_mask3_fcmadd_sch(__m128h __A, __m128h __B, __m128h __C, __mmask8 __U) {
  return _mm_mask3_fcmadd_sch(__A, __B, __C, __U);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_fcmadd_round_sch(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]], <8 x half> noundef [[__C:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__C]], ptr [[__C_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <8 x half> [[TMP0]] to <4 x float>
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <8 x half> [[TMP2]] to <4 x float>
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__C_ADDR]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <8 x half> [[TMP4]] to <4 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = call <4 x float> @llvm.x86.avx512fp16.mask.vfcmadd.csh(<4 x float> [[TMP1]], <4 x float> [[TMP3]], <4 x float> [[TMP5]], i8 -1, i32 11)
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast <4 x float> [[TMP6]] to <8 x half>
// CHECK-NEXT:    ret <8 x half> [[TMP7]]
//
__m128h test_mm_fcmadd_round_sch(__m128h __A, __m128h __B, __m128h __C) {
  return _mm_fcmadd_round_sch(__A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask_fcmadd_round_sch(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__B:%.*]], <8 x half> noundef [[__C:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__C]], ptr [[__C_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <8 x half> [[TMP0]] to <4 x float>
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <8 x half> [[TMP2]] to <4 x float>
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__C_ADDR]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <8 x half> [[TMP4]] to <4 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP7:%.*]] = call <4 x float> @llvm.x86.avx512fp16.mask.vfcmadd.csh(<4 x float> [[TMP1]], <4 x float> [[TMP3]], <4 x float> [[TMP5]], i8 [[TMP6]], i32 11)
// CHECK-NEXT:    [[TMP8:%.*]] = and i8 [[TMP6]], 1
// CHECK-NEXT:    [[TMP9:%.*]] = bitcast i8 [[TMP8]] to <8 x i1>
// CHECK-NEXT:    [[EXTRACT:%.*]] = shufflevector <8 x i1> [[TMP9]], <8 x i1> [[TMP9]], <4 x i32> <i32 0, i32 1, i32 2, i32 3>
// CHECK-NEXT:    [[TMP10:%.*]] = select <4 x i1> [[EXTRACT]], <4 x float> [[TMP7]], <4 x float> [[TMP1]]
// CHECK-NEXT:    [[TMP11:%.*]] = bitcast <4 x float> [[TMP10]] to <8 x half>
// CHECK-NEXT:    ret <8 x half> [[TMP11]]
//
__m128h test_mm_mask_fcmadd_round_sch(__m128h __A, __mmask8 __U, __m128h __B, __m128h __C) {
  return _mm_mask_fcmadd_round_sch(__A, __U, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_maskz_fcmadd_round_sch(
// CHECK-SAME: i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]], <8 x half> noundef [[__C:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__C]], ptr [[__C_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <8 x half> [[TMP0]] to <4 x float>
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <8 x half> [[TMP2]] to <4 x float>
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__C_ADDR]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <8 x half> [[TMP4]] to <4 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP7:%.*]] = call <4 x float> @llvm.x86.avx512fp16.maskz.vfcmadd.csh(<4 x float> [[TMP1]], <4 x float> [[TMP3]], <4 x float> [[TMP5]], i8 [[TMP6]], i32 11)
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast <4 x float> [[TMP7]] to <8 x half>
// CHECK-NEXT:    ret <8 x half> [[TMP8]]
//
__m128h test_mm_maskz_fcmadd_round_sch(__mmask8 __U, __m128h __A, __m128h __B, __m128h __C) {
  return _mm_maskz_fcmadd_round_sch(__U, __A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask3_fcmadd_round_sch(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]], <8 x half> noundef [[__C:%.*]], i8 noundef zeroext [[__U:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__C]], ptr [[__C_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <8 x half> [[TMP0]] to <4 x float>
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <8 x half> [[TMP2]] to <4 x float>
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__C_ADDR]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <8 x half> [[TMP4]] to <4 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP7:%.*]] = call <4 x float> @llvm.x86.avx512fp16.mask.vfcmadd.csh(<4 x float> [[TMP1]], <4 x float> [[TMP3]], <4 x float> [[TMP5]], i8 [[TMP6]], i32 11)
// CHECK-NEXT:    [[TMP8:%.*]] = shufflevector <4 x float> [[TMP7]], <4 x float> [[TMP5]], <4 x i32> <i32 0, i32 5, i32 6, i32 7>
// CHECK-NEXT:    [[TMP9:%.*]] = bitcast <4 x float> [[TMP8]] to <8 x half>
// CHECK-NEXT:    ret <8 x half> [[TMP9]]
//
__m128h test_mm_mask3_fcmadd_round_sch(__m128h __A, __m128h __B, __m128h __C, __mmask8 __U) {
  return _mm_mask3_fcmadd_round_sch(__A, __B, __C, __U, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_fmadd_sch(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]], <8 x half> noundef [[__C:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__C_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__C]], ptr [[__C_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__C_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__C_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <8 x half> [[TMP3]] to <4 x float>
// CHECK-NEXT:    [[TMP5:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <8 x half> [[TMP5]] to <4 x float>
// CHECK-NEXT:    [[TMP7:%.*]] = load <8 x half>, ptr [[__C_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast <8 x half> [[TMP7]] to <4 x float>
// CHECK-NEXT:    [[TMP9:%.*]] = call <4 x float> @llvm.x86.avx512fp16.mask.vfmadd.csh(<4 x float> [[TMP4]], <4 x float> [[TMP6]], <4 x float> [[TMP8]], i8 -1, i32 4)
// CHECK-NEXT:    [[TMP10:%.*]] = bitcast <4 x float> [[TMP9]] to <8 x half>
// CHECK-NEXT:    ret <8 x half> [[TMP10]]
//
__m128h test_mm_fmadd_sch(__m128h __A, __m128h __B, __m128h __C) {
  return _mm_fmadd_sch(__A, __B, __C);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask_fmadd_sch(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__B:%.*]], <8 x half> noundef [[__C:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__C_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__C]], ptr [[__C_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__C_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store i8 [[TMP1]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP3]], ptr [[__C_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <8 x half> [[TMP4]] to <4 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast <8 x half> [[TMP6]] to <4 x float>
// CHECK-NEXT:    [[TMP8:%.*]] = load <8 x half>, ptr [[__C_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP9:%.*]] = bitcast <8 x half> [[TMP8]] to <4 x float>
// CHECK-NEXT:    [[TMP10:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP11:%.*]] = call <4 x float> @llvm.x86.avx512fp16.mask.vfmadd.csh(<4 x float> [[TMP5]], <4 x float> [[TMP7]], <4 x float> [[TMP9]], i8 [[TMP10]], i32 4)
// CHECK-NEXT:    [[TMP12:%.*]] = and i8 [[TMP10]], 1
// CHECK-NEXT:    [[TMP13:%.*]] = bitcast i8 [[TMP12]] to <8 x i1>
// CHECK-NEXT:    [[EXTRACT_I:%.*]] = shufflevector <8 x i1> [[TMP13]], <8 x i1> [[TMP13]], <4 x i32> <i32 0, i32 1, i32 2, i32 3>
// CHECK-NEXT:    [[TMP14:%.*]] = select <4 x i1> [[EXTRACT_I]], <4 x float> [[TMP11]], <4 x float> [[TMP5]]
// CHECK-NEXT:    [[TMP15:%.*]] = bitcast <4 x float> [[TMP14]] to <8 x half>
// CHECK-NEXT:    ret <8 x half> [[TMP15]]
//
__m128h test_mm_mask_fmadd_sch(__m128h __A, __mmask8 __U, __m128h __B, __m128h __C) {
  return _mm_mask_fmadd_sch(__A, __U, __B, __C);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_maskz_fmadd_sch(
// CHECK-SAME: i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]], <8 x half> noundef [[__C:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__C_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__C]], ptr [[__C_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__C_ADDR]], align 16
// CHECK-NEXT:    store i8 [[TMP0]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP3]], ptr [[__C_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <8 x half> [[TMP4]] to <4 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast <8 x half> [[TMP6]] to <4 x float>
// CHECK-NEXT:    [[TMP8:%.*]] = load <8 x half>, ptr [[__C_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP9:%.*]] = bitcast <8 x half> [[TMP8]] to <4 x float>
// CHECK-NEXT:    [[TMP10:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP11:%.*]] = call <4 x float> @llvm.x86.avx512fp16.maskz.vfmadd.csh(<4 x float> [[TMP5]], <4 x float> [[TMP7]], <4 x float> [[TMP9]], i8 [[TMP10]], i32 4)
// CHECK-NEXT:    [[TMP12:%.*]] = bitcast <4 x float> [[TMP11]] to <8 x half>
// CHECK-NEXT:    ret <8 x half> [[TMP12]]
//
__m128h test_mm_maskz_fmadd_sch(__mmask8 __U, __m128h __A, __m128h __B, __m128h __C) {
  return _mm_maskz_fmadd_sch(__U, __A, __B, __C);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask3_fmadd_sch(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]], <8 x half> noundef [[__C:%.*]], i8 noundef zeroext [[__U:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__C_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__C]], ptr [[__C_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__C_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__C_ADDR_I]], align 16
// CHECK-NEXT:    store i8 [[TMP3]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <8 x half> [[TMP4]] to <4 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast <8 x half> [[TMP6]] to <4 x float>
// CHECK-NEXT:    [[TMP8:%.*]] = load <8 x half>, ptr [[__C_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP9:%.*]] = bitcast <8 x half> [[TMP8]] to <4 x float>
// CHECK-NEXT:    [[TMP10:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP11:%.*]] = call <4 x float> @llvm.x86.avx512fp16.mask.vfmadd.csh(<4 x float> [[TMP5]], <4 x float> [[TMP7]], <4 x float> [[TMP9]], i8 [[TMP10]], i32 4)
// CHECK-NEXT:    [[TMP12:%.*]] = shufflevector <4 x float> [[TMP11]], <4 x float> [[TMP9]], <4 x i32> <i32 0, i32 5, i32 6, i32 7>
// CHECK-NEXT:    [[TMP13:%.*]] = bitcast <4 x float> [[TMP12]] to <8 x half>
// CHECK-NEXT:    ret <8 x half> [[TMP13]]
//
__m128h test_mm_mask3_fmadd_sch(__m128h __A, __m128h __B, __m128h __C, __mmask8 __U) {
  return _mm_mask3_fmadd_sch(__A, __B, __C, __U);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_fmadd_round_sch(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]], <8 x half> noundef [[__C:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__C]], ptr [[__C_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <8 x half> [[TMP0]] to <4 x float>
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <8 x half> [[TMP2]] to <4 x float>
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__C_ADDR]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <8 x half> [[TMP4]] to <4 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = call <4 x float> @llvm.x86.avx512fp16.mask.vfmadd.csh(<4 x float> [[TMP1]], <4 x float> [[TMP3]], <4 x float> [[TMP5]], i8 -1, i32 11)
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast <4 x float> [[TMP6]] to <8 x half>
// CHECK-NEXT:    ret <8 x half> [[TMP7]]
//
__m128h test_mm_fmadd_round_sch(__m128h __A, __m128h __B, __m128h __C) {
  return _mm_fmadd_round_sch(__A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask_fmadd_round_sch(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__B:%.*]], <8 x half> noundef [[__C:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__C]], ptr [[__C_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <8 x half> [[TMP0]] to <4 x float>
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <8 x half> [[TMP2]] to <4 x float>
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__C_ADDR]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <8 x half> [[TMP4]] to <4 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP7:%.*]] = call <4 x float> @llvm.x86.avx512fp16.mask.vfmadd.csh(<4 x float> [[TMP1]], <4 x float> [[TMP3]], <4 x float> [[TMP5]], i8 [[TMP6]], i32 11)
// CHECK-NEXT:    [[TMP8:%.*]] = and i8 [[TMP6]], 1
// CHECK-NEXT:    [[TMP9:%.*]] = bitcast i8 [[TMP8]] to <8 x i1>
// CHECK-NEXT:    [[EXTRACT:%.*]] = shufflevector <8 x i1> [[TMP9]], <8 x i1> [[TMP9]], <4 x i32> <i32 0, i32 1, i32 2, i32 3>
// CHECK-NEXT:    [[TMP10:%.*]] = select <4 x i1> [[EXTRACT]], <4 x float> [[TMP7]], <4 x float> [[TMP1]]
// CHECK-NEXT:    [[TMP11:%.*]] = bitcast <4 x float> [[TMP10]] to <8 x half>
// CHECK-NEXT:    ret <8 x half> [[TMP11]]
//
__m128h test_mm_mask_fmadd_round_sch(__m128h __A, __mmask8 __U, __m128h __B, __m128h __C) {
  return _mm_mask_fmadd_round_sch(__A, __U, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_maskz_fmadd_round_sch(
// CHECK-SAME: i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]], <8 x half> noundef [[__C:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__C]], ptr [[__C_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <8 x half> [[TMP0]] to <4 x float>
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <8 x half> [[TMP2]] to <4 x float>
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__C_ADDR]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <8 x half> [[TMP4]] to <4 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP7:%.*]] = call <4 x float> @llvm.x86.avx512fp16.maskz.vfmadd.csh(<4 x float> [[TMP1]], <4 x float> [[TMP3]], <4 x float> [[TMP5]], i8 [[TMP6]], i32 11)
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast <4 x float> [[TMP7]] to <8 x half>
// CHECK-NEXT:    ret <8 x half> [[TMP8]]
//
__m128h test_mm_maskz_fmadd_round_sch(__mmask8 __U, __m128h __A, __m128h __B, __m128h __C) {
  return _mm_maskz_fmadd_round_sch(__U, __A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask3_fmadd_round_sch(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]], <8 x half> noundef [[__C:%.*]], i8 noundef zeroext [[__U:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__C]], ptr [[__C_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <8 x half> [[TMP0]] to <4 x float>
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <8 x half> [[TMP2]] to <4 x float>
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__C_ADDR]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <8 x half> [[TMP4]] to <4 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP7:%.*]] = call <4 x float> @llvm.x86.avx512fp16.mask.vfmadd.csh(<4 x float> [[TMP1]], <4 x float> [[TMP3]], <4 x float> [[TMP5]], i8 [[TMP6]], i32 11)
// CHECK-NEXT:    [[TMP8:%.*]] = shufflevector <4 x float> [[TMP7]], <4 x float> [[TMP5]], <4 x i32> <i32 0, i32 5, i32 6, i32 7>
// CHECK-NEXT:    [[TMP9:%.*]] = bitcast <4 x float> [[TMP8]] to <8 x half>
// CHECK-NEXT:    ret <8 x half> [[TMP9]]
//
__m128h test_mm_mask3_fmadd_round_sch(__m128h __A, __m128h __B, __m128h __C, __mmask8 __U) {
  return _mm_mask3_fmadd_round_sch(__A, __B, __C, __U, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_fcmul_sch(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <8 x half> [[TMP2]] to <4 x float>
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <8 x half> [[TMP4]] to <4 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = call <4 x float> @llvm.x86.avx512fp16.mask.vfcmul.csh(<4 x float> [[TMP3]], <4 x float> [[TMP5]], <4 x float> zeroinitializer, i8 -1, i32 4)
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast <4 x float> [[TMP6]] to <8 x half>
// CHECK-NEXT:    ret <8 x half> [[TMP7]]
//
__m128h test_mm_fcmul_sch(__m128h __A, __m128h __B) {
  return _mm_fcmul_sch(__A, __B);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask_fcmul_sch(
// CHECK-SAME: <8 x half> noundef [[__W:%.*]], i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__W]], ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    store i8 [[TMP1]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP3]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <8 x half> [[TMP4]] to <4 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast <8 x half> [[TMP6]] to <4 x float>
// CHECK-NEXT:    [[TMP8:%.*]] = load <8 x half>, ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP9:%.*]] = bitcast <8 x half> [[TMP8]] to <4 x float>
// CHECK-NEXT:    [[TMP10:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP11:%.*]] = call <4 x float> @llvm.x86.avx512fp16.mask.vfcmul.csh(<4 x float> [[TMP5]], <4 x float> [[TMP7]], <4 x float> [[TMP9]], i8 [[TMP10]], i32 4)
// CHECK-NEXT:    [[TMP12:%.*]] = bitcast <4 x float> [[TMP11]] to <8 x half>
// CHECK-NEXT:    ret <8 x half> [[TMP12]]
//
__m128h test_mm_mask_fcmul_sch(__m128h __W, __mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_mask_fcmul_sch(__W, __U, __A, __B);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_maskz_fcmul_sch(
// CHECK-SAME: i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store i8 [[TMP0]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <8 x half> [[TMP3]] to <4 x float>
// CHECK-NEXT:    [[TMP5:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <8 x half> [[TMP5]] to <4 x float>
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP7:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast <8 x half> [[TMP7]] to <4 x float>
// CHECK-NEXT:    [[TMP9:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP10:%.*]] = call <4 x float> @llvm.x86.avx512fp16.mask.vfcmul.csh(<4 x float> [[TMP4]], <4 x float> [[TMP6]], <4 x float> [[TMP8]], i8 [[TMP9]], i32 4)
// CHECK-NEXT:    [[TMP11:%.*]] = bitcast <4 x float> [[TMP10]] to <8 x half>
// CHECK-NEXT:    ret <8 x half> [[TMP11]]
//
__m128h test_mm_maskz_fcmul_sch(__mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_maskz_fcmul_sch(__U, __A, __B);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_fcmul_round_sch(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <8 x half> [[TMP0]] to <4 x float>
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <8 x half> [[TMP2]] to <4 x float>
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <8 x half> zeroinitializer to <4 x float>
// CHECK-NEXT:    [[TMP5:%.*]] = call <4 x float> @llvm.x86.avx512fp16.mask.vfcmul.csh(<4 x float> [[TMP1]], <4 x float> [[TMP3]], <4 x float> [[TMP4]], i8 -1, i32 11)
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <4 x float> [[TMP5]] to <8 x half>
// CHECK-NEXT:    ret <8 x half> [[TMP6]]
//
__m128h test_mm_fcmul_round_sch(__m128h __A, __m128h __B) {
  return _mm_fcmul_round_sch(__A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask_fcmul_round_sch(
// CHECK-SAME: <8 x half> noundef [[__W:%.*]], i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__W]], ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <8 x half> [[TMP0]] to <4 x float>
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <8 x half> [[TMP2]] to <4 x float>
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <8 x half> [[TMP4]] to <4 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP7:%.*]] = call <4 x float> @llvm.x86.avx512fp16.mask.vfcmul.csh(<4 x float> [[TMP1]], <4 x float> [[TMP3]], <4 x float> [[TMP5]], i8 [[TMP6]], i32 11)
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast <4 x float> [[TMP7]] to <8 x half>
// CHECK-NEXT:    ret <8 x half> [[TMP8]]
//
__m128h test_mm_mask_fcmul_round_sch(__m128h __W, __mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_mask_fcmul_round_sch(__W, __U, __A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_maskz_fcmul_round_sch(
// CHECK-SAME: i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <8 x half> [[TMP0]] to <4 x float>
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <8 x half> [[TMP2]] to <4 x float>
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <8 x half> [[TMP4]] to <4 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP7:%.*]] = call <4 x float> @llvm.x86.avx512fp16.mask.vfcmul.csh(<4 x float> [[TMP1]], <4 x float> [[TMP3]], <4 x float> [[TMP5]], i8 [[TMP6]], i32 11)
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast <4 x float> [[TMP7]] to <8 x half>
// CHECK-NEXT:    ret <8 x half> [[TMP8]]
//
__m128h test_mm_maskz_fcmul_round_sch(__mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_maskz_fcmul_round_sch(__U, __A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_fcmul_pch(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP1]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x half> [[TMP2]] to <16 x float>
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <32 x half> [[TMP4]] to <16 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = call <16 x float> @llvm.x86.avx512fp16.mask.vfcmul.cph.512(<16 x float> [[TMP3]], <16 x float> [[TMP5]], <16 x float> zeroinitializer, i16 -1, i32 4)
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast <16 x float> [[TMP6]] to <32 x half>
// CHECK-NEXT:    ret <32 x half> [[TMP7]]
//
__m512h test_mm512_fcmul_pch(__m512h __A, __m512h __B) {
  return _mm512_fcmul_pch(__A, __B);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask_fcmul_pch(
// CHECK-SAME: <32 x half> noundef [[__W:%.*]], i16 noundef zeroext [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__W]], ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    store i16 [[__U]], ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load i16, ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    store i16 [[TMP1]], ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP3]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <32 x half> [[TMP4]] to <16 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast <32 x half> [[TMP6]] to <16 x float>
// CHECK-NEXT:    [[TMP8:%.*]] = load <32 x half>, ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP9:%.*]] = bitcast <32 x half> [[TMP8]] to <16 x float>
// CHECK-NEXT:    [[TMP10:%.*]] = load i16, ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    [[TMP11:%.*]] = call <16 x float> @llvm.x86.avx512fp16.mask.vfcmul.cph.512(<16 x float> [[TMP5]], <16 x float> [[TMP7]], <16 x float> [[TMP9]], i16 [[TMP10]], i32 4)
// CHECK-NEXT:    [[TMP12:%.*]] = bitcast <16 x float> [[TMP11]] to <32 x half>
// CHECK-NEXT:    ret <32 x half> [[TMP12]]
//
__m512h test_mm512_mask_fcmul_pch(__m512h __W, __mmask16 __U, __m512h __A, __m512h __B) {
  return _mm512_mask_fcmul_pch(__W, __U, __A, __B);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_maskz_fcmul_pch(
// CHECK-SAME: i16 noundef zeroext [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i16 [[__U]], ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load i16, ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store i16 [[TMP0]], ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    store <32 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <32 x half> [[TMP3]] to <16 x float>
// CHECK-NEXT:    [[TMP5:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <32 x half> [[TMP5]] to <16 x float>
// CHECK-NEXT:    store <32 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP7:%.*]] = load <32 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast <32 x half> [[TMP7]] to <16 x float>
// CHECK-NEXT:    [[TMP9:%.*]] = load i16, ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    [[TMP10:%.*]] = call <16 x float> @llvm.x86.avx512fp16.mask.vfcmul.cph.512(<16 x float> [[TMP4]], <16 x float> [[TMP6]], <16 x float> [[TMP8]], i16 [[TMP9]], i32 4)
// CHECK-NEXT:    [[TMP11:%.*]] = bitcast <16 x float> [[TMP10]] to <32 x half>
// CHECK-NEXT:    ret <32 x half> [[TMP11]]
//
__m512h test_mm512_maskz_fcmul_pch(__mmask16 __U, __m512h __A, __m512h __B) {
  return _mm512_maskz_fcmul_pch(__U, __A, __B);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_fcmul_round_pch(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <32 x half> [[TMP0]] to <16 x float>
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x half> [[TMP2]] to <16 x float>
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <32 x half> zeroinitializer to <16 x float>
// CHECK-NEXT:    [[TMP5:%.*]] = call <16 x float> @llvm.x86.avx512fp16.mask.vfcmul.cph.512(<16 x float> [[TMP1]], <16 x float> [[TMP3]], <16 x float> [[TMP4]], i16 -1, i32 11)
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <16 x float> [[TMP5]] to <32 x half>
// CHECK-NEXT:    ret <32 x half> [[TMP6]]
//
__m512h test_mm512_fcmul_round_pch(__m512h __A, __m512h __B) {
  return _mm512_fcmul_round_pch(__A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask_fcmul_round_pch(
// CHECK-SAME: <32 x half> noundef [[__W:%.*]], i16 noundef zeroext [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__W]], ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    store i16 [[__U]], ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <32 x half> [[TMP0]] to <16 x float>
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x half> [[TMP2]] to <16 x float>
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <32 x half> [[TMP4]] to <16 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = load i16, ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    [[TMP7:%.*]] = call <16 x float> @llvm.x86.avx512fp16.mask.vfcmul.cph.512(<16 x float> [[TMP1]], <16 x float> [[TMP3]], <16 x float> [[TMP5]], i16 [[TMP6]], i32 11)
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast <16 x float> [[TMP7]] to <32 x half>
// CHECK-NEXT:    ret <32 x half> [[TMP8]]
//
__m512h test_mm512_mask_fcmul_round_pch(__m512h __W, __mmask16 __U, __m512h __A, __m512h __B) {
  return _mm512_mask_fcmul_round_pch(__W, __U, __A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_maskz_fcmul_round_pch(
// CHECK-SAME: i16 noundef zeroext [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i16 [[__U]], ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <32 x half> [[TMP0]] to <16 x float>
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x half> [[TMP2]] to <16 x float>
// CHECK-NEXT:    store <32 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <32 x half> [[TMP4]] to <16 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = load i16, ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    [[TMP7:%.*]] = call <16 x float> @llvm.x86.avx512fp16.mask.vfcmul.cph.512(<16 x float> [[TMP1]], <16 x float> [[TMP3]], <16 x float> [[TMP5]], i16 [[TMP6]], i32 11)
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast <16 x float> [[TMP7]] to <32 x half>
// CHECK-NEXT:    ret <32 x half> [[TMP8]]
//
__m512h test_mm512_maskz_fcmul_round_pch(__mmask16 __U, __m512h __A, __m512h __B) {
  return _mm512_maskz_fcmul_round_pch(__U, __A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_fcmadd_pch(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP1]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <32 x half> [[TMP3]] to <16 x float>
// CHECK-NEXT:    [[TMP5:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <32 x half> [[TMP5]] to <16 x float>
// CHECK-NEXT:    [[TMP7:%.*]] = load <32 x half>, ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast <32 x half> [[TMP7]] to <16 x float>
// CHECK-NEXT:    [[TMP9:%.*]] = call <16 x float> @llvm.x86.avx512fp16.mask.vfcmadd.cph.512(<16 x float> [[TMP4]], <16 x float> [[TMP6]], <16 x float> [[TMP8]], i16 -1, i32 4)
// CHECK-NEXT:    [[TMP10:%.*]] = bitcast <16 x float> [[TMP9]] to <32 x half>
// CHECK-NEXT:    ret <32 x half> [[TMP10]]
//
__m512h test_mm512_fcmadd_pch(__m512h __A, __m512h __B, __m512h __C) {
  return _mm512_fcmadd_pch(__A, __B, __C);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask_fcmadd_pch(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], i16 noundef zeroext [[__U:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store i16 [[__U]], ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load i16, ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store i16 [[TMP1]], ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP3]], ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <32 x half> [[TMP4]] to <16 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast <32 x half> [[TMP6]] to <16 x float>
// CHECK-NEXT:    [[TMP8:%.*]] = load <32 x half>, ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP9:%.*]] = bitcast <32 x half> [[TMP8]] to <16 x float>
// CHECK-NEXT:    [[TMP10:%.*]] = load i16, ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    [[TMP11:%.*]] = call <16 x float> @llvm.x86.avx512fp16.mask.vfcmadd.cph.512(<16 x float> [[TMP5]], <16 x float> [[TMP7]], <16 x float> [[TMP9]], i16 [[TMP10]], i32 4)
// CHECK-NEXT:    [[TMP12:%.*]] = bitcast i16 [[TMP10]] to <16 x i1>
// CHECK-NEXT:    [[TMP13:%.*]] = select <16 x i1> [[TMP12]], <16 x float> [[TMP11]], <16 x float> [[TMP5]]
// CHECK-NEXT:    [[TMP14:%.*]] = bitcast <16 x float> [[TMP13]] to <32 x half>
// CHECK-NEXT:    ret <32 x half> [[TMP14]]
//
__m512h test_mm512_mask_fcmadd_pch(__m512h __A, __mmask16 __U, __m512h __B, __m512h __C) {
  return _mm512_mask_fcmadd_pch(__A, __U, __B, __C);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask3_fcmadd_pch(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]], i16 noundef zeroext [[__U:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    store i16 [[__U]], ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load i16, ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP1]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    store i16 [[TMP3]], ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <32 x half> [[TMP4]] to <16 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast <32 x half> [[TMP6]] to <16 x float>
// CHECK-NEXT:    [[TMP8:%.*]] = load <32 x half>, ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP9:%.*]] = bitcast <32 x half> [[TMP8]] to <16 x float>
// CHECK-NEXT:    [[TMP10:%.*]] = load i16, ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    [[TMP11:%.*]] = call <16 x float> @llvm.x86.avx512fp16.mask.vfcmadd.cph.512(<16 x float> [[TMP5]], <16 x float> [[TMP7]], <16 x float> [[TMP9]], i16 [[TMP10]], i32 4)
// CHECK-NEXT:    [[TMP12:%.*]] = bitcast <16 x float> [[TMP11]] to <32 x half>
// CHECK-NEXT:    ret <32 x half> [[TMP12]]
//
__m512h test_mm512_mask3_fcmadd_pch(__m512h __A, __m512h __B, __m512h __C, __mmask16 __U) {
  return _mm512_mask3_fcmadd_pch(__A, __B, __C, __U);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_maskz_fcmadd_pch(
// CHECK-SAME: i16 noundef zeroext [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i16 [[__U]], ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load i16, ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    store i16 [[TMP0]], ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    store <32 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP3]], ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <32 x half> [[TMP4]] to <16 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast <32 x half> [[TMP6]] to <16 x float>
// CHECK-NEXT:    [[TMP8:%.*]] = load <32 x half>, ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP9:%.*]] = bitcast <32 x half> [[TMP8]] to <16 x float>
// CHECK-NEXT:    [[TMP10:%.*]] = load i16, ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    [[TMP11:%.*]] = call <16 x float> @llvm.x86.avx512fp16.maskz.vfcmadd.cph.512(<16 x float> [[TMP5]], <16 x float> [[TMP7]], <16 x float> [[TMP9]], i16 [[TMP10]], i32 4)
// CHECK-NEXT:    [[TMP12:%.*]] = bitcast <16 x float> [[TMP11]] to <32 x half>
// CHECK-NEXT:    ret <32 x half> [[TMP12]]
//
__m512h test_mm512_maskz_fcmadd_pch(__mmask16 __U, __m512h __A, __m512h __B, __m512h __C) {
  return _mm512_maskz_fcmadd_pch(__U, __A, __B, __C);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_fcmadd_round_pch(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <32 x half> [[TMP0]] to <16 x float>
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x half> [[TMP2]] to <16 x float>
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <32 x half> [[TMP4]] to <16 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = call <16 x float> @llvm.x86.avx512fp16.mask.vfcmadd.cph.512(<16 x float> [[TMP1]], <16 x float> [[TMP3]], <16 x float> [[TMP5]], i16 -1, i32 11)
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast <16 x float> [[TMP6]] to <32 x half>
// CHECK-NEXT:    ret <32 x half> [[TMP7]]
//
__m512h test_mm512_fcmadd_round_pch(__m512h __A, __m512h __B, __m512h __C) {
  return _mm512_fcmadd_round_pch(__A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask_fcmadd_round_pch(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], i16 noundef zeroext [[__U:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store i16 [[__U]], ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <32 x half> [[TMP0]] to <16 x float>
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x half> [[TMP2]] to <16 x float>
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <32 x half> [[TMP4]] to <16 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = load i16, ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    [[TMP7:%.*]] = call <16 x float> @llvm.x86.avx512fp16.mask.vfcmadd.cph.512(<16 x float> [[TMP1]], <16 x float> [[TMP3]], <16 x float> [[TMP5]], i16 [[TMP6]], i32 11)
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast i16 [[TMP6]] to <16 x i1>
// CHECK-NEXT:    [[TMP9:%.*]] = select <16 x i1> [[TMP8]], <16 x float> [[TMP7]], <16 x float> [[TMP1]]
// CHECK-NEXT:    [[TMP10:%.*]] = bitcast <16 x float> [[TMP9]] to <32 x half>
// CHECK-NEXT:    ret <32 x half> [[TMP10]]
//
__m512h test_mm512_mask_fcmadd_round_pch(__m512h __A, __mmask16 __U, __m512h __B, __m512h __C) {
  return _mm512_mask_fcmadd_round_pch(__A, __U, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask3_fcmadd_round_pch(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]], i16 noundef zeroext [[__U:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    store i16 [[__U]], ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <32 x half> [[TMP0]] to <16 x float>
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x half> [[TMP2]] to <16 x float>
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <32 x half> [[TMP4]] to <16 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = load i16, ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    [[TMP7:%.*]] = call <16 x float> @llvm.x86.avx512fp16.mask.vfcmadd.cph.512(<16 x float> [[TMP1]], <16 x float> [[TMP3]], <16 x float> [[TMP5]], i16 [[TMP6]], i32 11)
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast <16 x float> [[TMP7]] to <32 x half>
// CHECK-NEXT:    ret <32 x half> [[TMP8]]
//
__m512h test_mm512_mask3_fcmadd_round_pch(__m512h __A, __m512h __B, __m512h __C, __mmask16 __U) {
  return _mm512_mask3_fcmadd_round_pch(__A, __B, __C, __U, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_maskz_fcmadd_round_pch(
// CHECK-SAME: i16 noundef zeroext [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i16 [[__U]], ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <32 x half> [[TMP0]] to <16 x float>
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x half> [[TMP2]] to <16 x float>
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <32 x half> [[TMP4]] to <16 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = load i16, ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    [[TMP7:%.*]] = call <16 x float> @llvm.x86.avx512fp16.maskz.vfcmadd.cph.512(<16 x float> [[TMP1]], <16 x float> [[TMP3]], <16 x float> [[TMP5]], i16 [[TMP6]], i32 11)
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast <16 x float> [[TMP7]] to <32 x half>
// CHECK-NEXT:    ret <32 x half> [[TMP8]]
//
__m512h test_mm512_maskz_fcmadd_round_pch(__mmask16 __U, __m512h __A, __m512h __B, __m512h __C) {
  return _mm512_maskz_fcmadd_round_pch(__U, __A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_fmul_pch(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP1]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x half> [[TMP2]] to <16 x float>
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <32 x half> [[TMP4]] to <16 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = call <16 x float> @llvm.x86.avx512fp16.mask.vfmul.cph.512(<16 x float> [[TMP3]], <16 x float> [[TMP5]], <16 x float> zeroinitializer, i16 -1, i32 4)
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast <16 x float> [[TMP6]] to <32 x half>
// CHECK-NEXT:    ret <32 x half> [[TMP7]]
//
__m512h test_mm512_fmul_pch(__m512h __A, __m512h __B) {
  return _mm512_fmul_pch(__A, __B);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask_fmul_pch(
// CHECK-SAME: <32 x half> noundef [[__W:%.*]], i16 noundef zeroext [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__W]], ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    store i16 [[__U]], ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load i16, ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    store i16 [[TMP1]], ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP3]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <32 x half> [[TMP4]] to <16 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast <32 x half> [[TMP6]] to <16 x float>
// CHECK-NEXT:    [[TMP8:%.*]] = load <32 x half>, ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP9:%.*]] = bitcast <32 x half> [[TMP8]] to <16 x float>
// CHECK-NEXT:    [[TMP10:%.*]] = load i16, ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    [[TMP11:%.*]] = call <16 x float> @llvm.x86.avx512fp16.mask.vfmul.cph.512(<16 x float> [[TMP5]], <16 x float> [[TMP7]], <16 x float> [[TMP9]], i16 [[TMP10]], i32 4)
// CHECK-NEXT:    [[TMP12:%.*]] = bitcast <16 x float> [[TMP11]] to <32 x half>
// CHECK-NEXT:    ret <32 x half> [[TMP12]]
//
__m512h test_mm512_mask_fmul_pch(__m512h __W, __mmask16 __U, __m512h __A, __m512h __B) {
  return _mm512_mask_fmul_pch(__W, __U, __A, __B);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_maskz_fmul_pch(
// CHECK-SAME: i16 noundef zeroext [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i16 [[__U]], ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load i16, ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store i16 [[TMP0]], ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    store <32 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <32 x half> [[TMP3]] to <16 x float>
// CHECK-NEXT:    [[TMP5:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <32 x half> [[TMP5]] to <16 x float>
// CHECK-NEXT:    store <32 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP7:%.*]] = load <32 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast <32 x half> [[TMP7]] to <16 x float>
// CHECK-NEXT:    [[TMP9:%.*]] = load i16, ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    [[TMP10:%.*]] = call <16 x float> @llvm.x86.avx512fp16.mask.vfmul.cph.512(<16 x float> [[TMP4]], <16 x float> [[TMP6]], <16 x float> [[TMP8]], i16 [[TMP9]], i32 4)
// CHECK-NEXT:    [[TMP11:%.*]] = bitcast <16 x float> [[TMP10]] to <32 x half>
// CHECK-NEXT:    ret <32 x half> [[TMP11]]
//
__m512h test_mm512_maskz_fmul_pch(__mmask16 __U, __m512h __A, __m512h __B) {
  return _mm512_maskz_fmul_pch(__U, __A, __B);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_fmul_round_pch(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <32 x half> [[TMP0]] to <16 x float>
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x half> [[TMP2]] to <16 x float>
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <32 x half> zeroinitializer to <16 x float>
// CHECK-NEXT:    [[TMP5:%.*]] = call <16 x float> @llvm.x86.avx512fp16.mask.vfmul.cph.512(<16 x float> [[TMP1]], <16 x float> [[TMP3]], <16 x float> [[TMP4]], i16 -1, i32 11)
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <16 x float> [[TMP5]] to <32 x half>
// CHECK-NEXT:    ret <32 x half> [[TMP6]]
//
__m512h test_mm512_fmul_round_pch(__m512h __A, __m512h __B) {
  return _mm512_fmul_round_pch(__A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask_fmul_round_pch(
// CHECK-SAME: <32 x half> noundef [[__W:%.*]], i16 noundef zeroext [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__W]], ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    store i16 [[__U]], ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <32 x half> [[TMP0]] to <16 x float>
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x half> [[TMP2]] to <16 x float>
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <32 x half> [[TMP4]] to <16 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = load i16, ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    [[TMP7:%.*]] = call <16 x float> @llvm.x86.avx512fp16.mask.vfmul.cph.512(<16 x float> [[TMP1]], <16 x float> [[TMP3]], <16 x float> [[TMP5]], i16 [[TMP6]], i32 11)
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast <16 x float> [[TMP7]] to <32 x half>
// CHECK-NEXT:    ret <32 x half> [[TMP8]]
//
__m512h test_mm512_mask_fmul_round_pch(__m512h __W, __mmask16 __U, __m512h __A, __m512h __B) {
  return _mm512_mask_fmul_round_pch(__W, __U, __A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_maskz_fmul_round_pch(
// CHECK-SAME: i16 noundef zeroext [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i16 [[__U]], ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <32 x half> [[TMP0]] to <16 x float>
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x half> [[TMP2]] to <16 x float>
// CHECK-NEXT:    store <32 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <32 x half> [[TMP4]] to <16 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = load i16, ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    [[TMP7:%.*]] = call <16 x float> @llvm.x86.avx512fp16.mask.vfmul.cph.512(<16 x float> [[TMP1]], <16 x float> [[TMP3]], <16 x float> [[TMP5]], i16 [[TMP6]], i32 11)
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast <16 x float> [[TMP7]] to <32 x half>
// CHECK-NEXT:    ret <32 x half> [[TMP8]]
//
__m512h test_mm512_maskz_fmul_round_pch(__mmask16 __U, __m512h __A, __m512h __B) {
  return _mm512_maskz_fmul_round_pch(__U, __A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_fmadd_pch(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP1]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <32 x half> [[TMP3]] to <16 x float>
// CHECK-NEXT:    [[TMP5:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <32 x half> [[TMP5]] to <16 x float>
// CHECK-NEXT:    [[TMP7:%.*]] = load <32 x half>, ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast <32 x half> [[TMP7]] to <16 x float>
// CHECK-NEXT:    [[TMP9:%.*]] = call <16 x float> @llvm.x86.avx512fp16.mask.vfmadd.cph.512(<16 x float> [[TMP4]], <16 x float> [[TMP6]], <16 x float> [[TMP8]], i16 -1, i32 4)
// CHECK-NEXT:    [[TMP10:%.*]] = bitcast <16 x float> [[TMP9]] to <32 x half>
// CHECK-NEXT:    ret <32 x half> [[TMP10]]
//
__m512h test_mm512_fmadd_pch(__m512h __A, __m512h __B, __m512h __C) {
  return _mm512_fmadd_pch(__A, __B, __C);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask_fmadd_pch(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], i16 noundef zeroext [[__U:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store i16 [[__U]], ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load i16, ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store i16 [[TMP1]], ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP3]], ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <32 x half> [[TMP4]] to <16 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast <32 x half> [[TMP6]] to <16 x float>
// CHECK-NEXT:    [[TMP8:%.*]] = load <32 x half>, ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP9:%.*]] = bitcast <32 x half> [[TMP8]] to <16 x float>
// CHECK-NEXT:    [[TMP10:%.*]] = load i16, ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    [[TMP11:%.*]] = call <16 x float> @llvm.x86.avx512fp16.mask.vfmadd.cph.512(<16 x float> [[TMP5]], <16 x float> [[TMP7]], <16 x float> [[TMP9]], i16 [[TMP10]], i32 4)
// CHECK-NEXT:    [[TMP12:%.*]] = bitcast i16 [[TMP10]] to <16 x i1>
// CHECK-NEXT:    [[TMP13:%.*]] = select <16 x i1> [[TMP12]], <16 x float> [[TMP11]], <16 x float> [[TMP5]]
// CHECK-NEXT:    [[TMP14:%.*]] = bitcast <16 x float> [[TMP13]] to <32 x half>
// CHECK-NEXT:    ret <32 x half> [[TMP14]]
//
__m512h test_mm512_mask_fmadd_pch(__m512h __A, __mmask16 __U, __m512h __B, __m512h __C) {
  return _mm512_mask_fmadd_pch(__A, __U, __B, __C);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask3_fmadd_pch(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]], i16 noundef zeroext [[__U:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    store i16 [[__U]], ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load i16, ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP1]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    store i16 [[TMP3]], ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <32 x half> [[TMP4]] to <16 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast <32 x half> [[TMP6]] to <16 x float>
// CHECK-NEXT:    [[TMP8:%.*]] = load <32 x half>, ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP9:%.*]] = bitcast <32 x half> [[TMP8]] to <16 x float>
// CHECK-NEXT:    [[TMP10:%.*]] = load i16, ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    [[TMP11:%.*]] = call <16 x float> @llvm.x86.avx512fp16.mask.vfmadd.cph.512(<16 x float> [[TMP5]], <16 x float> [[TMP7]], <16 x float> [[TMP9]], i16 [[TMP10]], i32 4)
// CHECK-NEXT:    [[TMP12:%.*]] = bitcast <16 x float> [[TMP11]] to <32 x half>
// CHECK-NEXT:    ret <32 x half> [[TMP12]]
//
__m512h test_mm512_mask3_fmadd_pch(__m512h __A, __m512h __B, __m512h __C, __mmask16 __U) {
  return _mm512_mask3_fmadd_pch(__A, __B, __C, __U);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_maskz_fmadd_pch(
// CHECK-SAME: i16 noundef zeroext [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i16 [[__U]], ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load i16, ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    store i16 [[TMP0]], ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    store <32 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP3]], ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <32 x half> [[TMP4]] to <16 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast <32 x half> [[TMP6]] to <16 x float>
// CHECK-NEXT:    [[TMP8:%.*]] = load <32 x half>, ptr [[__C_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP9:%.*]] = bitcast <32 x half> [[TMP8]] to <16 x float>
// CHECK-NEXT:    [[TMP10:%.*]] = load i16, ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    [[TMP11:%.*]] = call <16 x float> @llvm.x86.avx512fp16.maskz.vfmadd.cph.512(<16 x float> [[TMP5]], <16 x float> [[TMP7]], <16 x float> [[TMP9]], i16 [[TMP10]], i32 4)
// CHECK-NEXT:    [[TMP12:%.*]] = bitcast <16 x float> [[TMP11]] to <32 x half>
// CHECK-NEXT:    ret <32 x half> [[TMP12]]
//
__m512h test_mm512_maskz_fmadd_pch(__mmask16 __U, __m512h __A, __m512h __B, __m512h __C) {
  return _mm512_maskz_fmadd_pch(__U, __A, __B, __C);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_fmadd_round_pch(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <32 x half> [[TMP0]] to <16 x float>
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x half> [[TMP2]] to <16 x float>
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <32 x half> [[TMP4]] to <16 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = call <16 x float> @llvm.x86.avx512fp16.mask.vfmadd.cph.512(<16 x float> [[TMP1]], <16 x float> [[TMP3]], <16 x float> [[TMP5]], i16 -1, i32 11)
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast <16 x float> [[TMP6]] to <32 x half>
// CHECK-NEXT:    ret <32 x half> [[TMP7]]
//
__m512h test_mm512_fmadd_round_pch(__m512h __A, __m512h __B, __m512h __C) {
  return _mm512_fmadd_round_pch(__A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask_fmadd_round_pch(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], i16 noundef zeroext [[__U:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store i16 [[__U]], ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <32 x half> [[TMP0]] to <16 x float>
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x half> [[TMP2]] to <16 x float>
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <32 x half> [[TMP4]] to <16 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = load i16, ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    [[TMP7:%.*]] = call <16 x float> @llvm.x86.avx512fp16.mask.vfmadd.cph.512(<16 x float> [[TMP1]], <16 x float> [[TMP3]], <16 x float> [[TMP5]], i16 [[TMP6]], i32 11)
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast i16 [[TMP6]] to <16 x i1>
// CHECK-NEXT:    [[TMP9:%.*]] = select <16 x i1> [[TMP8]], <16 x float> [[TMP7]], <16 x float> [[TMP1]]
// CHECK-NEXT:    [[TMP10:%.*]] = bitcast <16 x float> [[TMP9]] to <32 x half>
// CHECK-NEXT:    ret <32 x half> [[TMP10]]
//
__m512h test_mm512_mask_fmadd_round_pch(__m512h __A, __mmask16 __U, __m512h __B, __m512h __C) {
  return _mm512_mask_fmadd_round_pch(__A, __U, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask3_fmadd_round_pch(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]], i16 noundef zeroext [[__U:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    store i16 [[__U]], ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <32 x half> [[TMP0]] to <16 x float>
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x half> [[TMP2]] to <16 x float>
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <32 x half> [[TMP4]] to <16 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = load i16, ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    [[TMP7:%.*]] = call <16 x float> @llvm.x86.avx512fp16.mask.vfmadd.cph.512(<16 x float> [[TMP1]], <16 x float> [[TMP3]], <16 x float> [[TMP5]], i16 [[TMP6]], i32 11)
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast <16 x float> [[TMP7]] to <32 x half>
// CHECK-NEXT:    ret <32 x half> [[TMP8]]
//
__m512h test_mm512_mask3_fmadd_round_pch(__m512h __A, __m512h __B, __m512h __C, __mmask16 __U) {
  return _mm512_mask3_fmadd_round_pch(__A, __B, __C, __U, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_maskz_fmadd_round_pch(
// CHECK-SAME: i16 noundef zeroext [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]], <32 x half> noundef [[__C:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__C_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i16 [[__U]], ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__C]], ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <32 x half> [[TMP0]] to <16 x float>
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x half> [[TMP2]] to <16 x float>
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__C_ADDR]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <32 x half> [[TMP4]] to <16 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = load i16, ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    [[TMP7:%.*]] = call <16 x float> @llvm.x86.avx512fp16.maskz.vfmadd.cph.512(<16 x float> [[TMP1]], <16 x float> [[TMP3]], <16 x float> [[TMP5]], i16 [[TMP6]], i32 11)
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast <16 x float> [[TMP7]] to <32 x half>
// CHECK-NEXT:    ret <32 x half> [[TMP8]]
//
__m512h test_mm512_maskz_fmadd_round_pch(__mmask16 __U, __m512h __A, __m512h __B, __m512h __C) {
  return _mm512_maskz_fmadd_round_pch(__U, __A, __B, __C, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_fmul_sch(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <8 x half> [[TMP2]] to <4 x float>
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <8 x half> [[TMP4]] to <4 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = call <4 x float> @llvm.x86.avx512fp16.mask.vfmul.csh(<4 x float> [[TMP3]], <4 x float> [[TMP5]], <4 x float> zeroinitializer, i8 -1, i32 4)
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast <4 x float> [[TMP6]] to <8 x half>
// CHECK-NEXT:    ret <8 x half> [[TMP7]]
//
__m128h test_mm_fmul_sch(__m128h __A, __m128h __B) {
  return _mm_fmul_sch(__A, __B);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask_fmul_sch(
// CHECK-SAME: <8 x half> noundef [[__W:%.*]], i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__W]], ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    store i8 [[TMP1]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP3]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <8 x half> [[TMP4]] to <4 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast <8 x half> [[TMP6]] to <4 x float>
// CHECK-NEXT:    [[TMP8:%.*]] = load <8 x half>, ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP9:%.*]] = bitcast <8 x half> [[TMP8]] to <4 x float>
// CHECK-NEXT:    [[TMP10:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP11:%.*]] = call <4 x float> @llvm.x86.avx512fp16.mask.vfmul.csh(<4 x float> [[TMP5]], <4 x float> [[TMP7]], <4 x float> [[TMP9]], i8 [[TMP10]], i32 4)
// CHECK-NEXT:    [[TMP12:%.*]] = bitcast <4 x float> [[TMP11]] to <8 x half>
// CHECK-NEXT:    ret <8 x half> [[TMP12]]
//
__m128h test_mm_mask_fmul_sch(__m128h __W, __mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_mask_fmul_sch(__W, __U, __A, __B);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_maskz_fmul_sch(
// CHECK-SAME: i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store i8 [[TMP0]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <8 x half> [[TMP3]] to <4 x float>
// CHECK-NEXT:    [[TMP5:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <8 x half> [[TMP5]] to <4 x float>
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP7:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast <8 x half> [[TMP7]] to <4 x float>
// CHECK-NEXT:    [[TMP9:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP10:%.*]] = call <4 x float> @llvm.x86.avx512fp16.mask.vfmul.csh(<4 x float> [[TMP4]], <4 x float> [[TMP6]], <4 x float> [[TMP8]], i8 [[TMP9]], i32 4)
// CHECK-NEXT:    [[TMP11:%.*]] = bitcast <4 x float> [[TMP10]] to <8 x half>
// CHECK-NEXT:    ret <8 x half> [[TMP11]]
//
__m128h test_mm_maskz_fmul_sch(__mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_maskz_fmul_sch(__U, __A, __B);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_fmul_round_sch(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <8 x half> [[TMP0]] to <4 x float>
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <8 x half> [[TMP2]] to <4 x float>
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <8 x half> zeroinitializer to <4 x float>
// CHECK-NEXT:    [[TMP5:%.*]] = call <4 x float> @llvm.x86.avx512fp16.mask.vfmul.csh(<4 x float> [[TMP1]], <4 x float> [[TMP3]], <4 x float> [[TMP4]], i8 -1, i32 11)
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <4 x float> [[TMP5]] to <8 x half>
// CHECK-NEXT:    ret <8 x half> [[TMP6]]
//
__m128h test_mm_fmul_round_sch(__m128h __A, __m128h __B) {
  return _mm_fmul_round_sch(__A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask_fmul_round_sch(
// CHECK-SAME: <8 x half> noundef [[__W:%.*]], i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__W]], ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <8 x half> [[TMP0]] to <4 x float>
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <8 x half> [[TMP2]] to <4 x float>
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <8 x half> [[TMP4]] to <4 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP7:%.*]] = call <4 x float> @llvm.x86.avx512fp16.mask.vfmul.csh(<4 x float> [[TMP1]], <4 x float> [[TMP3]], <4 x float> [[TMP5]], i8 [[TMP6]], i32 11)
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast <4 x float> [[TMP7]] to <8 x half>
// CHECK-NEXT:    ret <8 x half> [[TMP8]]
//
__m128h test_mm_mask_fmul_round_sch(__m128h __W, __mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_mask_fmul_round_sch(__W, __U, __A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_maskz_fmul_round_sch(
// CHECK-SAME: i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <8 x half> [[TMP0]] to <4 x float>
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <8 x half> [[TMP2]] to <4 x float>
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <8 x half> [[TMP4]] to <4 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP7:%.*]] = call <4 x float> @llvm.x86.avx512fp16.mask.vfmul.csh(<4 x float> [[TMP1]], <4 x float> [[TMP3]], <4 x float> [[TMP5]], i8 [[TMP6]], i32 11)
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast <4 x float> [[TMP7]] to <8 x half>
// CHECK-NEXT:    ret <8 x half> [[TMP8]]
//
__m128h test_mm_maskz_fmul_round_sch(__mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_maskz_fmul_round_sch(__U, __A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local half @test_mm512_reduce_add_ph(
// CHECK-SAME: <32 x half> noundef [[__W:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__W]], ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = call reassoc half @llvm.vector.reduce.fadd.v32f16(half 0xH8000, <32 x half> [[TMP1]])
// CHECK-NEXT:    ret half [[TMP2]]
//
_Float16 test_mm512_reduce_add_ph(__m512h __W) {
  return _mm512_reduce_add_ph(__W);
}

// CHECK-LABEL: define dso_local half @test_mm512_reduce_mul_ph(
// CHECK-SAME: <32 x half> noundef [[__W:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__W]], ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = call reassoc half @llvm.vector.reduce.fmul.v32f16(half 0xH3C00, <32 x half> [[TMP1]])
// CHECK-NEXT:    ret half [[TMP2]]
//
_Float16 test_mm512_reduce_mul_ph(__m512h __W) {
  return _mm512_reduce_mul_ph(__W);
}

// CHECK-LABEL: define dso_local half @test_mm512_reduce_max_ph(
// CHECK-SAME: <32 x half> noundef [[__W:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__V_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__W]], ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__V_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__V_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = call nnan half @llvm.vector.reduce.fmax.v32f16(<32 x half> [[TMP1]])
// CHECK-NEXT:    ret half [[TMP2]]
//
_Float16 test_mm512_reduce_max_ph(__m512h __W) {
  return _mm512_reduce_max_ph(__W);
}

// CHECK-LABEL: define dso_local half @test_mm512_reduce_min_ph(
// CHECK-SAME: <32 x half> noundef [[__W:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__V_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__W]], ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__V_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__V_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = call nnan half @llvm.vector.reduce.fmin.v32f16(<32 x half> [[TMP1]])
// CHECK-NEXT:    ret half [[TMP2]]
//
_Float16 test_mm512_reduce_min_ph(__m512h __W) {
  return _mm512_reduce_min_ph(__W);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask_blend_ph(
// CHECK-SAME: i32 noundef [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__W:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i32 [[__U]], ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__W]], ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[__U_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    store i32 [[TMP0]], ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    store <32 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[__U_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast i32 [[TMP3]] to <32 x i1>
// CHECK-NEXT:    [[TMP7:%.*]] = select <32 x i1> [[TMP6]], <32 x half> [[TMP4]], <32 x half> [[TMP5]]
// CHECK-NEXT:    ret <32 x half> [[TMP7]]
//
__m512h test_mm512_mask_blend_ph(__mmask32 __U, __m512h __A, __m512h __W) {
  return _mm512_mask_blend_ph(__U, __A, __W);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_permutex2var_ph(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], <8 x i64> noundef [[__I:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__I_ADDR_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__I_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <8 x i64> [[__I]], ptr [[__I_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i64>, ptr [[__I_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <8 x i64> [[TMP1]], ptr [[__I_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <32 x half> [[TMP3]] to <32 x i16>
// CHECK-NEXT:    [[TMP5:%.*]] = load <8 x i64>, ptr [[__I_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <8 x i64> [[TMP5]] to <32 x i16>
// CHECK-NEXT:    [[TMP7:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast <32 x half> [[TMP7]] to <32 x i16>
// CHECK-NEXT:    [[TMP9:%.*]] = call <32 x i16> @llvm.x86.avx512.vpermi2var.hi.512(<32 x i16> [[TMP4]], <32 x i16> [[TMP6]], <32 x i16> [[TMP8]])
// CHECK-NEXT:    [[TMP10:%.*]] = bitcast <32 x i16> [[TMP9]] to <32 x half>
// CHECK-NEXT:    ret <32 x half> [[TMP10]]
//
__m512h test_mm512_permutex2var_ph(__m512h __A, __m512i __I, __m512h __B) {
  return _mm512_permutex2var_ph(__A, __I, __B);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_permutexvar_epi16(
// CHECK-SAME: <8 x i64> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x i64>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <8 x i64> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i64>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <8 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP1]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x half> [[TMP2]] to <32 x i16>
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x i64>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <8 x i64> [[TMP4]] to <32 x i16>
// CHECK-NEXT:    [[TMP6:%.*]] = call <32 x i16> @llvm.x86.avx512.permvar.hi.512(<32 x i16> [[TMP3]], <32 x i16> [[TMP5]])
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast <32 x i16> [[TMP6]] to <32 x half>
// CHECK-NEXT:    ret <32 x half> [[TMP7]]
//
__m512h test_mm512_permutexvar_epi16(__m512i __A, __m512h __B) {
  return _mm512_permutexvar_ph(__A, __B);
}

// tests below are for alias intrinsics.
// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mul_pch(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP1]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x half> [[TMP2]] to <16 x float>
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <32 x half> [[TMP4]] to <16 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = call <16 x float> @llvm.x86.avx512fp16.mask.vfmul.cph.512(<16 x float> [[TMP3]], <16 x float> [[TMP5]], <16 x float> zeroinitializer, i16 -1, i32 4)
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast <16 x float> [[TMP6]] to <32 x half>
// CHECK-NEXT:    ret <32 x half> [[TMP7]]
//
__m512h test_mm512_mul_pch(__m512h __A, __m512h __B) {
  return _mm512_mul_pch(__A, __B);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask_mul_pch(
// CHECK-SAME: <32 x half> noundef [[__W:%.*]], i16 noundef zeroext [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__W]], ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    store i16 [[__U]], ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load i16, ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    store i16 [[TMP1]], ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP3]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <32 x half> [[TMP4]] to <16 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast <32 x half> [[TMP6]] to <16 x float>
// CHECK-NEXT:    [[TMP8:%.*]] = load <32 x half>, ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP9:%.*]] = bitcast <32 x half> [[TMP8]] to <16 x float>
// CHECK-NEXT:    [[TMP10:%.*]] = load i16, ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    [[TMP11:%.*]] = call <16 x float> @llvm.x86.avx512fp16.mask.vfmul.cph.512(<16 x float> [[TMP5]], <16 x float> [[TMP7]], <16 x float> [[TMP9]], i16 [[TMP10]], i32 4)
// CHECK-NEXT:    [[TMP12:%.*]] = bitcast <16 x float> [[TMP11]] to <32 x half>
// CHECK-NEXT:    ret <32 x half> [[TMP12]]
//
__m512h test_mm512_mask_mul_pch(__m512h __W, __mmask16 __U, __m512h __A, __m512h __B) {
  return _mm512_mask_mul_pch(__W, __U, __A, __B);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_maskz_mul_pch(
// CHECK-SAME: i16 noundef zeroext [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i16 [[__U]], ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load i16, ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store i16 [[TMP0]], ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    store <32 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <32 x half> [[TMP3]] to <16 x float>
// CHECK-NEXT:    [[TMP5:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <32 x half> [[TMP5]] to <16 x float>
// CHECK-NEXT:    store <32 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP7:%.*]] = load <32 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast <32 x half> [[TMP7]] to <16 x float>
// CHECK-NEXT:    [[TMP9:%.*]] = load i16, ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    [[TMP10:%.*]] = call <16 x float> @llvm.x86.avx512fp16.mask.vfmul.cph.512(<16 x float> [[TMP4]], <16 x float> [[TMP6]], <16 x float> [[TMP8]], i16 [[TMP9]], i32 4)
// CHECK-NEXT:    [[TMP11:%.*]] = bitcast <16 x float> [[TMP10]] to <32 x half>
// CHECK-NEXT:    ret <32 x half> [[TMP11]]
//
__m512h test_mm512_maskz_mul_pch(__mmask16 __U, __m512h __A, __m512h __B) {
  return _mm512_maskz_mul_pch(__U, __A, __B);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_cmul_pch(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP1]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x half> [[TMP2]] to <16 x float>
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <32 x half> [[TMP4]] to <16 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = call <16 x float> @llvm.x86.avx512fp16.mask.vfcmul.cph.512(<16 x float> [[TMP3]], <16 x float> [[TMP5]], <16 x float> zeroinitializer, i16 -1, i32 4)
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast <16 x float> [[TMP6]] to <32 x half>
// CHECK-NEXT:    ret <32 x half> [[TMP7]]
//
__m512h test_mm512_cmul_pch(__m512h __A, __m512h __B) {
  return _mm512_cmul_pch(__A, __B);
}
// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask_cmul_pch(
// CHECK-SAME: <32 x half> noundef [[__W:%.*]], i16 noundef zeroext [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__W]], ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    store i16 [[__U]], ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = load i16, ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP0]], ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    store i16 [[TMP1]], ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP3]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <32 x half> [[TMP4]] to <16 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast <32 x half> [[TMP6]] to <16 x float>
// CHECK-NEXT:    [[TMP8:%.*]] = load <32 x half>, ptr [[__W_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP9:%.*]] = bitcast <32 x half> [[TMP8]] to <16 x float>
// CHECK-NEXT:    [[TMP10:%.*]] = load i16, ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    [[TMP11:%.*]] = call <16 x float> @llvm.x86.avx512fp16.mask.vfcmul.cph.512(<16 x float> [[TMP5]], <16 x float> [[TMP7]], <16 x float> [[TMP9]], i16 [[TMP10]], i32 4)
// CHECK-NEXT:    [[TMP12:%.*]] = bitcast <16 x float> [[TMP11]] to <32 x half>
// CHECK-NEXT:    ret <32 x half> [[TMP12]]
//
__m512h test_mm512_mask_cmul_pch(__m512h __W, __mmask16 __U, __m512h __A, __m512h __B) {
  return _mm512_mask_cmul_pch(__W, __U, __A, __B);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_maskz_cmul_pch(
// CHECK-SAME: i16 noundef zeroext [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i16 [[__U]], ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load i16, ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    store i16 [[TMP0]], ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    store <32 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    store <32 x half> [[TMP2]], ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x half>, ptr [[__A_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <32 x half> [[TMP3]] to <16 x float>
// CHECK-NEXT:    [[TMP5:%.*]] = load <32 x half>, ptr [[__B_ADDR_I]], align 64
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <32 x half> [[TMP5]] to <16 x float>
// CHECK-NEXT:    store <32 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP7:%.*]] = load <32 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 64
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast <32 x half> [[TMP7]] to <16 x float>
// CHECK-NEXT:    [[TMP9:%.*]] = load i16, ptr [[__U_ADDR_I]], align 2
// CHECK-NEXT:    [[TMP10:%.*]] = call <16 x float> @llvm.x86.avx512fp16.mask.vfcmul.cph.512(<16 x float> [[TMP4]], <16 x float> [[TMP6]], <16 x float> [[TMP8]], i16 [[TMP9]], i32 4)
// CHECK-NEXT:    [[TMP11:%.*]] = bitcast <16 x float> [[TMP10]] to <32 x half>
// CHECK-NEXT:    ret <32 x half> [[TMP11]]
//
__m512h test_mm512_maskz_cmul_pch(__mmask16 __U, __m512h __A, __m512h __B) {
  return _mm512_maskz_cmul_pch(__U, __A, __B);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_mul_sch(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <8 x half> [[TMP2]] to <4 x float>
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <8 x half> [[TMP4]] to <4 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = call <4 x float> @llvm.x86.avx512fp16.mask.vfmul.csh(<4 x float> [[TMP3]], <4 x float> [[TMP5]], <4 x float> zeroinitializer, i8 -1, i32 4)
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast <4 x float> [[TMP6]] to <8 x half>
// CHECK-NEXT:    ret <8 x half> [[TMP7]]
//
__m128h test_mm_mul_sch(__m128h __A, __m128h __B) {
  return _mm_mul_sch(__A, __B);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask_mul_sch(
// CHECK-SAME: <8 x half> noundef [[__W:%.*]], i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__W]], ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    store i8 [[TMP1]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP3]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <8 x half> [[TMP4]] to <4 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast <8 x half> [[TMP6]] to <4 x float>
// CHECK-NEXT:    [[TMP8:%.*]] = load <8 x half>, ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP9:%.*]] = bitcast <8 x half> [[TMP8]] to <4 x float>
// CHECK-NEXT:    [[TMP10:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP11:%.*]] = call <4 x float> @llvm.x86.avx512fp16.mask.vfmul.csh(<4 x float> [[TMP5]], <4 x float> [[TMP7]], <4 x float> [[TMP9]], i8 [[TMP10]], i32 4)
// CHECK-NEXT:    [[TMP12:%.*]] = bitcast <4 x float> [[TMP11]] to <8 x half>
// CHECK-NEXT:    ret <8 x half> [[TMP12]]
//
__m128h test_mm_mask_mul_sch(__m128h __W, __mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_mask_mul_sch(__W, __U, __A, __B);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_maskz_mul_sch(
// CHECK-SAME: i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store i8 [[TMP0]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <8 x half> [[TMP3]] to <4 x float>
// CHECK-NEXT:    [[TMP5:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <8 x half> [[TMP5]] to <4 x float>
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP7:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast <8 x half> [[TMP7]] to <4 x float>
// CHECK-NEXT:    [[TMP9:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP10:%.*]] = call <4 x float> @llvm.x86.avx512fp16.mask.vfmul.csh(<4 x float> [[TMP4]], <4 x float> [[TMP6]], <4 x float> [[TMP8]], i8 [[TMP9]], i32 4)
// CHECK-NEXT:    [[TMP11:%.*]] = bitcast <4 x float> [[TMP10]] to <8 x half>
// CHECK-NEXT:    ret <8 x half> [[TMP11]]
//
__m128h test_mm_maskz_mul_sch(__mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_maskz_mul_sch(__U, __A, __B);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_mul_round_sch(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <8 x half> [[TMP0]] to <4 x float>
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <8 x half> [[TMP2]] to <4 x float>
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <8 x half> zeroinitializer to <4 x float>
// CHECK-NEXT:    [[TMP5:%.*]] = call <4 x float> @llvm.x86.avx512fp16.mask.vfmul.csh(<4 x float> [[TMP1]], <4 x float> [[TMP3]], <4 x float> [[TMP4]], i8 -1, i32 11)
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <4 x float> [[TMP5]] to <8 x half>
// CHECK-NEXT:    ret <8 x half> [[TMP6]]
//
__m128h test_mm_mul_round_sch(__m128h __A, __m128h __B) {
  return _mm_mul_round_sch(__A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask_mul_round_sch(
// CHECK-SAME: <8 x half> noundef [[__W:%.*]], i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__W]], ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <8 x half> [[TMP0]] to <4 x float>
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <8 x half> [[TMP2]] to <4 x float>
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <8 x half> [[TMP4]] to <4 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP7:%.*]] = call <4 x float> @llvm.x86.avx512fp16.mask.vfmul.csh(<4 x float> [[TMP1]], <4 x float> [[TMP3]], <4 x float> [[TMP5]], i8 [[TMP6]], i32 11)
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast <4 x float> [[TMP7]] to <8 x half>
// CHECK-NEXT:    ret <8 x half> [[TMP8]]
//
__m128h test_mm_mask_mul_round_sch(__m128h __W, __mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_mask_mul_round_sch(__W, __U, __A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_maskz_mul_round_sch(
// CHECK-SAME: i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <8 x half> [[TMP0]] to <4 x float>
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <8 x half> [[TMP2]] to <4 x float>
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <8 x half> [[TMP4]] to <4 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP7:%.*]] = call <4 x float> @llvm.x86.avx512fp16.mask.vfmul.csh(<4 x float> [[TMP1]], <4 x float> [[TMP3]], <4 x float> [[TMP5]], i8 [[TMP6]], i32 11)
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast <4 x float> [[TMP7]] to <8 x half>
// CHECK-NEXT:    ret <8 x half> [[TMP8]]
//
__m128h test_mm_maskz_mul_round_sch(__mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_maskz_mul_round_sch(__U, __A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mul_round_pch(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <32 x half> [[TMP0]] to <16 x float>
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x half> [[TMP2]] to <16 x float>
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <32 x half> zeroinitializer to <16 x float>
// CHECK-NEXT:    [[TMP5:%.*]] = call <16 x float> @llvm.x86.avx512fp16.mask.vfmul.cph.512(<16 x float> [[TMP1]], <16 x float> [[TMP3]], <16 x float> [[TMP4]], i16 -1, i32 11)
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <16 x float> [[TMP5]] to <32 x half>
// CHECK-NEXT:    ret <32 x half> [[TMP6]]
//
__m512h test_mm512_mul_round_pch(__m512h __A, __m512h __B) {
  return _mm512_mul_round_pch(__A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask_mul_round_pch(
// CHECK-SAME: <32 x half> noundef [[__W:%.*]], i16 noundef zeroext [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__W]], ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    store i16 [[__U]], ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <32 x half> [[TMP0]] to <16 x float>
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x half> [[TMP2]] to <16 x float>
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <32 x half> [[TMP4]] to <16 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = load i16, ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    [[TMP7:%.*]] = call <16 x float> @llvm.x86.avx512fp16.mask.vfmul.cph.512(<16 x float> [[TMP1]], <16 x float> [[TMP3]], <16 x float> [[TMP5]], i16 [[TMP6]], i32 11)
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast <16 x float> [[TMP7]] to <32 x half>
// CHECK-NEXT:    ret <32 x half> [[TMP8]]
//
__m512h test_mm512_mask_mul_round_pch(__m512h __W, __mmask16 __U, __m512h __A, __m512h __B) {
  return _mm512_mask_mul_round_pch(__W, __U, __A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_maskz_mul_round_pch(
// CHECK-SAME: i16 noundef zeroext [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i16 [[__U]], ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <32 x half> [[TMP0]] to <16 x float>
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x half> [[TMP2]] to <16 x float>
// CHECK-NEXT:    store <32 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <32 x half> [[TMP4]] to <16 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = load i16, ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    [[TMP7:%.*]] = call <16 x float> @llvm.x86.avx512fp16.mask.vfmul.cph.512(<16 x float> [[TMP1]], <16 x float> [[TMP3]], <16 x float> [[TMP5]], i16 [[TMP6]], i32 11)
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast <16 x float> [[TMP7]] to <32 x half>
// CHECK-NEXT:    ret <32 x half> [[TMP8]]
//
__m512h test_mm512_maskz_mul_round_pch(__mmask16 __U, __m512h __A, __m512h __B) {
  return _mm512_maskz_mul_round_pch(__U, __A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_cmul_round_pch(
// CHECK-SAME: <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <32 x half> [[TMP0]] to <16 x float>
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x half> [[TMP2]] to <16 x float>
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <32 x half> zeroinitializer to <16 x float>
// CHECK-NEXT:    [[TMP5:%.*]] = call <16 x float> @llvm.x86.avx512fp16.mask.vfcmul.cph.512(<16 x float> [[TMP1]], <16 x float> [[TMP3]], <16 x float> [[TMP4]], i16 -1, i32 11)
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <16 x float> [[TMP5]] to <32 x half>
// CHECK-NEXT:    ret <32 x half> [[TMP6]]
//
__m512h test_mm512_cmul_round_pch(__m512h __A, __m512h __B) {
  return _mm512_cmul_round_pch(__A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_mask_cmul_round_pch(
// CHECK-SAME: <32 x half> noundef [[__W:%.*]], i16 noundef zeroext [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store <32 x half> [[__W]], ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    store i16 [[__U]], ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <32 x half> [[TMP0]] to <16 x float>
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x half> [[TMP2]] to <16 x float>
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[__W_ADDR]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <32 x half> [[TMP4]] to <16 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = load i16, ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    [[TMP7:%.*]] = call <16 x float> @llvm.x86.avx512fp16.mask.vfcmul.cph.512(<16 x float> [[TMP1]], <16 x float> [[TMP3]], <16 x float> [[TMP5]], i16 [[TMP6]], i32 11)
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast <16 x float> [[TMP7]] to <32 x half>
// CHECK-NEXT:    ret <32 x half> [[TMP8]]
//
__m512h test_mm512_mask_cmul_round_pch(__m512h __W, __mmask16 __U, __m512h __A, __m512h __B) {
  return _mm512_mask_cmul_round_pch(__W, __U, __A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <32 x half> @test_mm512_maskz_cmul_round_pch(
// CHECK-SAME: i16 noundef zeroext [[__U:%.*]], <32 x half> noundef [[__A:%.*]], <32 x half> noundef [[__B:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i16, align 2
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <32 x half>, align 64
// CHECK-NEXT:    store i16 [[__U]], ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    store <32 x half> [[__A]], ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    store <32 x half> [[__B]], ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x half>, ptr [[__A_ADDR]], align 64
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <32 x half> [[TMP0]] to <16 x float>
// CHECK-NEXT:    [[TMP2:%.*]] = load <32 x half>, ptr [[__B_ADDR]], align 64
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x half> [[TMP2]] to <16 x float>
// CHECK-NEXT:    store <32 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP4:%.*]] = load <32 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 64
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <32 x half> [[TMP4]] to <16 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = load i16, ptr [[__U_ADDR]], align 2
// CHECK-NEXT:    [[TMP7:%.*]] = call <16 x float> @llvm.x86.avx512fp16.mask.vfcmul.cph.512(<16 x float> [[TMP1]], <16 x float> [[TMP3]], <16 x float> [[TMP5]], i16 [[TMP6]], i32 11)
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast <16 x float> [[TMP7]] to <32 x half>
// CHECK-NEXT:    ret <32 x half> [[TMP8]]
//
__m512h test_mm512_maskz_cmul_round_pch(__mmask16 __U, __m512h __A, __m512h __B) {
  return _mm512_maskz_cmul_round_pch(__U, __A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_cmul_sch(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <8 x half> [[TMP2]] to <4 x float>
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <8 x half> [[TMP4]] to <4 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = call <4 x float> @llvm.x86.avx512fp16.mask.vfcmul.csh(<4 x float> [[TMP3]], <4 x float> [[TMP5]], <4 x float> zeroinitializer, i8 -1, i32 4)
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast <4 x float> [[TMP6]] to <8 x half>
// CHECK-NEXT:    ret <8 x half> [[TMP7]]
//
__m128h test_mm_cmul_sch(__m128h __A, __m128h __B) {
  return _mm_cmul_sch(__A, __B);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask_cmul_sch(
// CHECK-SAME: <8 x half> noundef [[__W:%.*]], i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__W]], ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP0]], ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    store i8 [[TMP1]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP3]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <8 x half> [[TMP4]] to <4 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast <8 x half> [[TMP6]] to <4 x float>
// CHECK-NEXT:    [[TMP8:%.*]] = load <8 x half>, ptr [[__W_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP9:%.*]] = bitcast <8 x half> [[TMP8]] to <4 x float>
// CHECK-NEXT:    [[TMP10:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP11:%.*]] = call <4 x float> @llvm.x86.avx512fp16.mask.vfcmul.csh(<4 x float> [[TMP5]], <4 x float> [[TMP7]], <4 x float> [[TMP9]], i8 [[TMP10]], i32 4)
// CHECK-NEXT:    [[TMP12:%.*]] = bitcast <4 x float> [[TMP11]] to <8 x half>
// CHECK-NEXT:    ret <8 x half> [[TMP12]]
//
__m128h test_mm_mask_cmul_sch(__m128h __W, __mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_mask_cmul_sch(__W, __U, __A, __B);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_maskz_cmul_sch(
// CHECK-SAME: i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR_I:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP1:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    store i8 [[TMP0]], ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    store <8 x half> [[TMP1]], ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = load <8 x half>, ptr [[__A_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <8 x half> [[TMP3]] to <4 x float>
// CHECK-NEXT:    [[TMP5:%.*]] = load <8 x half>, ptr [[__B_ADDR_I]], align 16
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <8 x half> [[TMP5]] to <4 x float>
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP7:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 16
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast <8 x half> [[TMP7]] to <4 x float>
// CHECK-NEXT:    [[TMP9:%.*]] = load i8, ptr [[__U_ADDR_I]], align 1
// CHECK-NEXT:    [[TMP10:%.*]] = call <4 x float> @llvm.x86.avx512fp16.mask.vfcmul.csh(<4 x float> [[TMP4]], <4 x float> [[TMP6]], <4 x float> [[TMP8]], i8 [[TMP9]], i32 4)
// CHECK-NEXT:    [[TMP11:%.*]] = bitcast <4 x float> [[TMP10]] to <8 x half>
// CHECK-NEXT:    ret <8 x half> [[TMP11]]
//
__m128h test_mm_maskz_cmul_sch(__mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_maskz_cmul_sch(__U, __A, __B);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_cmul_round_sch(
// CHECK-SAME: <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <8 x half> [[TMP0]] to <4 x float>
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <8 x half> [[TMP2]] to <4 x float>
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <8 x half> zeroinitializer to <4 x float>
// CHECK-NEXT:    [[TMP5:%.*]] = call <4 x float> @llvm.x86.avx512fp16.mask.vfcmul.csh(<4 x float> [[TMP1]], <4 x float> [[TMP3]], <4 x float> [[TMP4]], i8 -1, i32 11)
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <4 x float> [[TMP5]] to <8 x half>
// CHECK-NEXT:    ret <8 x half> [[TMP6]]
//
__m128h test_mm_cmul_round_sch(__m128h __A, __m128h __B) {
  return _mm_cmul_round_sch(__A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_mask_cmul_round_sch(
// CHECK-SAME: <8 x half> noundef [[__W:%.*]], i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[__W_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store <8 x half> [[__W]], ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <8 x half> [[TMP0]] to <4 x float>
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <8 x half> [[TMP2]] to <4 x float>
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[__W_ADDR]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <8 x half> [[TMP4]] to <4 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP7:%.*]] = call <4 x float> @llvm.x86.avx512fp16.mask.vfcmul.csh(<4 x float> [[TMP1]], <4 x float> [[TMP3]], <4 x float> [[TMP5]], i8 [[TMP6]], i32 11)
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast <4 x float> [[TMP7]] to <8 x half>
// CHECK-NEXT:    ret <8 x half> [[TMP8]]
//
__m128h test_mm_mask_cmul_round_sch(__m128h __W, __mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_mask_cmul_round_sch(__W, __U, __A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}

// CHECK-LABEL: define dso_local <8 x half> @test_mm_maskz_cmul_round_sch(
// CHECK-SAME: i8 noundef zeroext [[__U:%.*]], <8 x half> noundef [[__A:%.*]], <8 x half> noundef [[__B:%.*]]) #[[ATTR1]] {
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__U_ADDR:%.*]] = alloca i8, align 1
// CHECK-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    [[__B_ADDR:%.*]] = alloca <8 x half>, align 16
// CHECK-NEXT:    store i8 [[__U]], ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    store <8 x half> [[__A]], ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    store <8 x half> [[__B]], ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[__A_ADDR]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <8 x half> [[TMP0]] to <4 x float>
// CHECK-NEXT:    [[TMP2:%.*]] = load <8 x half>, ptr [[__B_ADDR]], align 16
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <8 x half> [[TMP2]] to <4 x float>
// CHECK-NEXT:    store <8 x half> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP4:%.*]] = load <8 x half>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <8 x half> [[TMP4]] to <4 x float>
// CHECK-NEXT:    [[TMP6:%.*]] = load i8, ptr [[__U_ADDR]], align 1
// CHECK-NEXT:    [[TMP7:%.*]] = call <4 x float> @llvm.x86.avx512fp16.mask.vfcmul.csh(<4 x float> [[TMP1]], <4 x float> [[TMP3]], <4 x float> [[TMP5]], i8 [[TMP6]], i32 11)
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast <4 x float> [[TMP7]] to <8 x half>
// CHECK-NEXT:    ret <8 x half> [[TMP8]]
//
__m128h test_mm_maskz_cmul_round_sch(__mmask8 __U, __m128h __A, __m128h __B) {
  return _mm_maskz_cmul_round_sch(__U, __A, __B, _MM_FROUND_TO_ZERO | _MM_FROUND_NO_EXC);
}
