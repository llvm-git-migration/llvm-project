// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py UTC_ARGS: --version 4
// RUN: %clang_cc1 -flax-vector-conversions=none -ffreestanding %s -triple=x86_64-apple-darwin -target-feature +avx -emit-llvm -o - -Wall -Werror | FileCheck %s --check-prefixes=CHECK,X64
// RUN: %clang_cc1 -flax-vector-conversions=none -ffreestanding %s -triple=x86_64-apple-darwin -target-feature +avx -fno-signed-char -emit-llvm -o - -Wall -Werror | FileCheck %s --check-prefixes=CHECK,X64
// RUN: %clang_cc1 -flax-vector-conversions=none -ffreestanding %s -triple=i386-apple-darwin -target-feature +avx -emit-llvm -o - -Wall -Werror | FileCheck %s --check-prefixes=CHECK,X86
// RUN: %clang_cc1 -flax-vector-conversions=none -ffreestanding %s -triple=i386-apple-darwin -target-feature +avx -fno-signed-char -emit-llvm -o - -Wall -Werror | FileCheck %s --check-prefixes=CHECK,X86
// RUN: %clang_cc1 -flax-vector-conversions=none -fms-extensions -fms-compatibility -ffreestanding %s -triple=x86_64-windows-msvc -target-feature +avx -emit-llvm -o - -Wall -Werror | FileCheck %s --check-prefixes=CHECK,X64


#include <immintrin.h>

// NOTE: This should match the tests in llvm/test/CodeGen/X86/sse-intrinsics-fast-isel.ll

//
// X86-LABEL: define void @test_mm256_add_pd(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[A:%.*]], <4 x double> noundef [[B:%.*]]) #[[ATTR0:[0-9]+]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[B_ADDR]], align 32
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META3:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META3]]
// X86-NEXT:    store <4 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 32, !noalias [[META3]]
// X86-NEXT:    store <4 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 32, !noalias [[META3]]
// X86-NEXT:    [[TMP2:%.*]] = load <4 x double>, ptr [[__A_ADDR_I]], align 32, !noalias [[META3]]
// X86-NEXT:    [[TMP3:%.*]] = load <4 x double>, ptr [[__B_ADDR_I]], align 32, !noalias [[META3]]
// X86-NEXT:    [[ADD_I:%.*]] = fadd <4 x double> [[TMP2]], [[TMP3]]
// X86-NEXT:    store <4 x double> [[ADD_I]], ptr [[TMP]], align 32, !alias.scope [[META3]]
// X86-NEXT:    [[TMP4:%.*]] = load <4 x double>, ptr [[TMP]], align 32, !alias.scope [[META3]]
// X86-NEXT:    store <4 x double> [[TMP4]], ptr [[TMP]], align 32, !alias.scope [[META3]]
// X86-NEXT:    [[TMP5:%.*]] = load <4 x double>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x double> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP6:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP6]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_add_pd(__m256d A, __m256d B) {
  return _mm256_add_pd(A, B);
}

//
// X86-LABEL: define void @test_mm256_add_ps(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]], <8 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[TMP:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[B_ADDR]], align 32
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META6:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META6]]
// X86-NEXT:    store <8 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 32, !noalias [[META6]]
// X86-NEXT:    store <8 x float> [[TMP1]], ptr [[__B_ADDR_I]], align 32, !noalias [[META6]]
// X86-NEXT:    [[TMP2:%.*]] = load <8 x float>, ptr [[__A_ADDR_I]], align 32, !noalias [[META6]]
// X86-NEXT:    [[TMP3:%.*]] = load <8 x float>, ptr [[__B_ADDR_I]], align 32, !noalias [[META6]]
// X86-NEXT:    [[ADD_I:%.*]] = fadd <8 x float> [[TMP2]], [[TMP3]]
// X86-NEXT:    store <8 x float> [[ADD_I]], ptr [[TMP]], align 32, !alias.scope [[META6]]
// X86-NEXT:    [[TMP4:%.*]] = load <8 x float>, ptr [[TMP]], align 32, !alias.scope [[META6]]
// X86-NEXT:    store <8 x float> [[TMP4]], ptr [[TMP]], align 32, !alias.scope [[META6]]
// X86-NEXT:    [[TMP5:%.*]] = load <8 x float>, ptr [[TMP]], align 32
// X86-NEXT:    store <8 x float> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP6:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP6]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_add_ps(__m256 A, __m256 B) {
  return _mm256_add_ps(A, B);
}

//
// X86-LABEL: define void @test_mm256_addsub_pd(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[A:%.*]], <4 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[B_ADDR]], align 32
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META9:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META9]]
// X86-NEXT:    store <4 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 32, !noalias [[META9]]
// X86-NEXT:    store <4 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 32, !noalias [[META9]]
// X86-NEXT:    [[TMP2:%.*]] = load <4 x double>, ptr [[__A_ADDR_I]], align 32, !noalias [[META9]]
// X86-NEXT:    [[TMP3:%.*]] = load <4 x double>, ptr [[__B_ADDR_I]], align 32, !noalias [[META9]]
// X86-NEXT:    [[TMP4:%.*]] = call <4 x double> @llvm.x86.avx.addsub.pd.256(<4 x double> [[TMP2]], <4 x double> [[TMP3]])
// X86-NEXT:    store <4 x double> [[TMP4]], ptr [[TMP]], align 32, !alias.scope [[META9]]
// X86-NEXT:    [[TMP5:%.*]] = load <4 x double>, ptr [[TMP]], align 32, !alias.scope [[META9]]
// X86-NEXT:    store <4 x double> [[TMP5]], ptr [[TMP]], align 32, !alias.scope [[META9]]
// X86-NEXT:    [[TMP6:%.*]] = load <4 x double>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x double> [[TMP6]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP7:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP7]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_addsub_pd(__m256d A, __m256d B) {
  return _mm256_addsub_pd(A, B);
}

//
// X86-LABEL: define void @test_mm256_addsub_ps(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]], <8 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[TMP:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[B_ADDR]], align 32
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META12:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META12]]
// X86-NEXT:    store <8 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 32, !noalias [[META12]]
// X86-NEXT:    store <8 x float> [[TMP1]], ptr [[__B_ADDR_I]], align 32, !noalias [[META12]]
// X86-NEXT:    [[TMP2:%.*]] = load <8 x float>, ptr [[__A_ADDR_I]], align 32, !noalias [[META12]]
// X86-NEXT:    [[TMP3:%.*]] = load <8 x float>, ptr [[__B_ADDR_I]], align 32, !noalias [[META12]]
// X86-NEXT:    [[TMP4:%.*]] = call <8 x float> @llvm.x86.avx.addsub.ps.256(<8 x float> [[TMP2]], <8 x float> [[TMP3]])
// X86-NEXT:    store <8 x float> [[TMP4]], ptr [[TMP]], align 32, !alias.scope [[META12]]
// X86-NEXT:    [[TMP5:%.*]] = load <8 x float>, ptr [[TMP]], align 32, !alias.scope [[META12]]
// X86-NEXT:    store <8 x float> [[TMP5]], ptr [[TMP]], align 32, !alias.scope [[META12]]
// X86-NEXT:    [[TMP6:%.*]] = load <8 x float>, ptr [[TMP]], align 32
// X86-NEXT:    store <8 x float> [[TMP6]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP7:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP7]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_addsub_ps(__m256 A, __m256 B) {
  return _mm256_addsub_ps(A, B);
}

//
// X86-LABEL: define void @test_mm256_and_pd(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[A:%.*]], <4 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[B_ADDR]], align 32
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META15:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META15]]
// X86-NEXT:    store <4 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 32, !noalias [[META15]]
// X86-NEXT:    store <4 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 32, !noalias [[META15]]
// X86-NEXT:    [[TMP2:%.*]] = load <4 x double>, ptr [[__A_ADDR_I]], align 32, !noalias [[META15]]
// X86-NEXT:    [[TMP3:%.*]] = bitcast <4 x double> [[TMP2]] to <4 x i64>
// X86-NEXT:    [[TMP4:%.*]] = load <4 x double>, ptr [[__B_ADDR_I]], align 32, !noalias [[META15]]
// X86-NEXT:    [[TMP5:%.*]] = bitcast <4 x double> [[TMP4]] to <4 x i64>
// X86-NEXT:    [[AND_I:%.*]] = and <4 x i64> [[TMP3]], [[TMP5]]
// X86-NEXT:    [[TMP6:%.*]] = bitcast <4 x i64> [[AND_I]] to <4 x double>
// X86-NEXT:    store <4 x double> [[TMP6]], ptr [[TMP]], align 32, !alias.scope [[META15]]
// X86-NEXT:    [[TMP7:%.*]] = load <4 x double>, ptr [[TMP]], align 32, !alias.scope [[META15]]
// X86-NEXT:    store <4 x double> [[TMP7]], ptr [[TMP]], align 32, !alias.scope [[META15]]
// X86-NEXT:    [[TMP8:%.*]] = load <4 x double>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x double> [[TMP8]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP9:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP9]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_and_pd(__m256d A, __m256d B) {
  return _mm256_and_pd(A, B);
}

//
// X86-LABEL: define void @test_mm256_and_ps(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]], <8 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[TMP:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[B_ADDR]], align 32
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META18:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META18]]
// X86-NEXT:    store <8 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 32, !noalias [[META18]]
// X86-NEXT:    store <8 x float> [[TMP1]], ptr [[__B_ADDR_I]], align 32, !noalias [[META18]]
// X86-NEXT:    [[TMP2:%.*]] = load <8 x float>, ptr [[__A_ADDR_I]], align 32, !noalias [[META18]]
// X86-NEXT:    [[TMP3:%.*]] = bitcast <8 x float> [[TMP2]] to <8 x i32>
// X86-NEXT:    [[TMP4:%.*]] = load <8 x float>, ptr [[__B_ADDR_I]], align 32, !noalias [[META18]]
// X86-NEXT:    [[TMP5:%.*]] = bitcast <8 x float> [[TMP4]] to <8 x i32>
// X86-NEXT:    [[AND_I:%.*]] = and <8 x i32> [[TMP3]], [[TMP5]]
// X86-NEXT:    [[TMP6:%.*]] = bitcast <8 x i32> [[AND_I]] to <8 x float>
// X86-NEXT:    store <8 x float> [[TMP6]], ptr [[TMP]], align 32, !alias.scope [[META18]]
// X86-NEXT:    [[TMP7:%.*]] = load <8 x float>, ptr [[TMP]], align 32, !alias.scope [[META18]]
// X86-NEXT:    store <8 x float> [[TMP7]], ptr [[TMP]], align 32, !alias.scope [[META18]]
// X86-NEXT:    [[TMP8:%.*]] = load <8 x float>, ptr [[TMP]], align 32
// X86-NEXT:    store <8 x float> [[TMP8]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP9:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP9]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_and_ps(__m256 A, __m256 B) {
  return _mm256_and_ps(A, B);
}

//
// X86-LABEL: define void @test_mm256_andnot_pd(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[A:%.*]], <4 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[B_ADDR]], align 32
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META21:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META21]]
// X86-NEXT:    store <4 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 32, !noalias [[META21]]
// X86-NEXT:    store <4 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 32, !noalias [[META21]]
// X86-NEXT:    [[TMP2:%.*]] = load <4 x double>, ptr [[__A_ADDR_I]], align 32, !noalias [[META21]]
// X86-NEXT:    [[TMP3:%.*]] = bitcast <4 x double> [[TMP2]] to <4 x i64>
// X86-NEXT:    [[NOT_I:%.*]] = xor <4 x i64> [[TMP3]], <i64 -1, i64 -1, i64 -1, i64 -1>
// X86-NEXT:    [[TMP4:%.*]] = load <4 x double>, ptr [[__B_ADDR_I]], align 32, !noalias [[META21]]
// X86-NEXT:    [[TMP5:%.*]] = bitcast <4 x double> [[TMP4]] to <4 x i64>
// X86-NEXT:    [[AND_I:%.*]] = and <4 x i64> [[NOT_I]], [[TMP5]]
// X86-NEXT:    [[TMP6:%.*]] = bitcast <4 x i64> [[AND_I]] to <4 x double>
// X86-NEXT:    store <4 x double> [[TMP6]], ptr [[TMP]], align 32, !alias.scope [[META21]]
// X86-NEXT:    [[TMP7:%.*]] = load <4 x double>, ptr [[TMP]], align 32, !alias.scope [[META21]]
// X86-NEXT:    store <4 x double> [[TMP7]], ptr [[TMP]], align 32, !alias.scope [[META21]]
// X86-NEXT:    [[TMP8:%.*]] = load <4 x double>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x double> [[TMP8]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP9:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP9]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_andnot_pd(__m256d A, __m256d B) {
  return _mm256_andnot_pd(A, B);
}

//
// X86-LABEL: define void @test_mm256_andnot_ps(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]], <8 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[TMP:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[B_ADDR]], align 32
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META24:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META24]]
// X86-NEXT:    store <8 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 32, !noalias [[META24]]
// X86-NEXT:    store <8 x float> [[TMP1]], ptr [[__B_ADDR_I]], align 32, !noalias [[META24]]
// X86-NEXT:    [[TMP2:%.*]] = load <8 x float>, ptr [[__A_ADDR_I]], align 32, !noalias [[META24]]
// X86-NEXT:    [[TMP3:%.*]] = bitcast <8 x float> [[TMP2]] to <8 x i32>
// X86-NEXT:    [[NOT_I:%.*]] = xor <8 x i32> [[TMP3]], <i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1>
// X86-NEXT:    [[TMP4:%.*]] = load <8 x float>, ptr [[__B_ADDR_I]], align 32, !noalias [[META24]]
// X86-NEXT:    [[TMP5:%.*]] = bitcast <8 x float> [[TMP4]] to <8 x i32>
// X86-NEXT:    [[AND_I:%.*]] = and <8 x i32> [[NOT_I]], [[TMP5]]
// X86-NEXT:    [[TMP6:%.*]] = bitcast <8 x i32> [[AND_I]] to <8 x float>
// X86-NEXT:    store <8 x float> [[TMP6]], ptr [[TMP]], align 32, !alias.scope [[META24]]
// X86-NEXT:    [[TMP7:%.*]] = load <8 x float>, ptr [[TMP]], align 32, !alias.scope [[META24]]
// X86-NEXT:    store <8 x float> [[TMP7]], ptr [[TMP]], align 32, !alias.scope [[META24]]
// X86-NEXT:    [[TMP8:%.*]] = load <8 x float>, ptr [[TMP]], align 32
// X86-NEXT:    store <8 x float> [[TMP8]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP9:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP9]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_andnot_ps(__m256 A, __m256 B) {
  return _mm256_andnot_ps(A, B);
}

//
// X86-LABEL: define void @test_mm256_blend_pd(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[A:%.*]], <4 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[BLEND:%.*]] = shufflevector <4 x double> [[TMP0]], <4 x double> [[TMP1]], <4 x i32> <i32 4, i32 1, i32 6, i32 3>
// X86-NEXT:    store <4 x double> [[BLEND]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP2:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP2]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_blend_pd(__m256d A, __m256d B) {
  return _mm256_blend_pd(A, B, 0x05);
}

//
// X86-LABEL: define void @test_mm256_blend_ps(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]], <8 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[BLEND:%.*]] = shufflevector <8 x float> [[TMP0]], <8 x float> [[TMP1]], <8 x i32> <i32 8, i32 1, i32 10, i32 3, i32 12, i32 13, i32 6, i32 7>
// X86-NEXT:    store <8 x float> [[BLEND]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP2:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP2]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_blend_ps(__m256 A, __m256 B) {
  return _mm256_blend_ps(A, B, 0x35);
}

//
// X86-LABEL: define void @test_mm256_blendv_pd(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[V1:%.*]], <4 x double> noundef [[V2:%.*]], <4 x double> noundef [[V3:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[__C_ADDR_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[V1_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[V2_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[V3_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[V1]], ptr [[V1_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[V2]], ptr [[V2_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[V3]], ptr [[V3_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[V1_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[V2_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = load <4 x double>, ptr [[V3_ADDR]], align 32
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META27:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META27]]
// X86-NEXT:    store <4 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 32, !noalias [[META27]]
// X86-NEXT:    store <4 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 32, !noalias [[META27]]
// X86-NEXT:    store <4 x double> [[TMP2]], ptr [[__C_ADDR_I]], align 32, !noalias [[META27]]
// X86-NEXT:    [[TMP3:%.*]] = load <4 x double>, ptr [[__A_ADDR_I]], align 32, !noalias [[META27]]
// X86-NEXT:    [[TMP4:%.*]] = load <4 x double>, ptr [[__B_ADDR_I]], align 32, !noalias [[META27]]
// X86-NEXT:    [[TMP5:%.*]] = load <4 x double>, ptr [[__C_ADDR_I]], align 32, !noalias [[META27]]
// X86-NEXT:    [[TMP6:%.*]] = call <4 x double> @llvm.x86.avx.blendv.pd.256(<4 x double> [[TMP3]], <4 x double> [[TMP4]], <4 x double> [[TMP5]])
// X86-NEXT:    store <4 x double> [[TMP6]], ptr [[TMP]], align 32, !alias.scope [[META27]]
// X86-NEXT:    [[TMP7:%.*]] = load <4 x double>, ptr [[TMP]], align 32, !alias.scope [[META27]]
// X86-NEXT:    store <4 x double> [[TMP7]], ptr [[TMP]], align 32, !alias.scope [[META27]]
// X86-NEXT:    [[TMP8:%.*]] = load <4 x double>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x double> [[TMP8]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP9:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP9]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_blendv_pd(__m256d V1, __m256d V2, __m256d V3) {
  return _mm256_blendv_pd(V1, V2, V3);
}

//
// X86-LABEL: define void @test_mm256_blendv_ps(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[V1:%.*]], <8 x float> noundef [[V2:%.*]], <8 x float> noundef [[V3:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[__C_ADDR_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[V1_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[V2_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[V3_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[TMP:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[V1]], ptr [[V1_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[V2]], ptr [[V2_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[V3]], ptr [[V3_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[V1_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[V2_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = load <8 x float>, ptr [[V3_ADDR]], align 32
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META30:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META30]]
// X86-NEXT:    store <8 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 32, !noalias [[META30]]
// X86-NEXT:    store <8 x float> [[TMP1]], ptr [[__B_ADDR_I]], align 32, !noalias [[META30]]
// X86-NEXT:    store <8 x float> [[TMP2]], ptr [[__C_ADDR_I]], align 32, !noalias [[META30]]
// X86-NEXT:    [[TMP3:%.*]] = load <8 x float>, ptr [[__A_ADDR_I]], align 32, !noalias [[META30]]
// X86-NEXT:    [[TMP4:%.*]] = load <8 x float>, ptr [[__B_ADDR_I]], align 32, !noalias [[META30]]
// X86-NEXT:    [[TMP5:%.*]] = load <8 x float>, ptr [[__C_ADDR_I]], align 32, !noalias [[META30]]
// X86-NEXT:    [[TMP6:%.*]] = call <8 x float> @llvm.x86.avx.blendv.ps.256(<8 x float> [[TMP3]], <8 x float> [[TMP4]], <8 x float> [[TMP5]])
// X86-NEXT:    store <8 x float> [[TMP6]], ptr [[TMP]], align 32, !alias.scope [[META30]]
// X86-NEXT:    [[TMP7:%.*]] = load <8 x float>, ptr [[TMP]], align 32, !alias.scope [[META30]]
// X86-NEXT:    store <8 x float> [[TMP7]], ptr [[TMP]], align 32, !alias.scope [[META30]]
// X86-NEXT:    [[TMP8:%.*]] = load <8 x float>, ptr [[TMP]], align 32
// X86-NEXT:    store <8 x float> [[TMP8]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP9:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP9]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_blendv_ps(__m256 V1, __m256 V2, __m256 V3) {
  return _mm256_blendv_ps(V1, V2, V3);
}

//
// X86-LABEL: define void @test_mm256_broadcast_pd(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], ptr noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__DP_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__B_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[COERCE_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META33:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META33]]
// X86-NEXT:    store ptr [[TMP0]], ptr [[__A_ADDR_I]], align 4, !noalias [[META33]]
// X86-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[__A_ADDR_I]], align 4, !noalias [[META33]]
// X86-NEXT:    store ptr [[TMP1]], ptr [[__DP_ADDR_I]], align 4
// X86-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[__DP_ADDR_I]], align 4
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[TMP2]], align 1
// X86-NEXT:    store <2 x double> [[TMP3]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP4]], ptr [[COERCE_I]], align 16, !noalias [[META33]]
// X86-NEXT:    [[TMP5:%.*]] = load <2 x double>, ptr [[COERCE_I]], align 16, !noalias [[META33]]
// X86-NEXT:    store <2 x double> [[TMP5]], ptr [[__B_I]], align 16, !noalias [[META33]]
// X86-NEXT:    [[TMP6:%.*]] = load <2 x double>, ptr [[__B_I]], align 16, !noalias [[META33]]
// X86-NEXT:    [[TMP7:%.*]] = load <2 x double>, ptr [[__B_I]], align 16, !noalias [[META33]]
// X86-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <2 x double> [[TMP6]], <2 x double> [[TMP7]], <4 x i32> <i32 0, i32 1, i32 0, i32 1>
// X86-NEXT:    store <4 x double> [[SHUFFLE_I]], ptr [[TMP]], align 32, !alias.scope [[META33]]
// X86-NEXT:    [[TMP8:%.*]] = load <4 x double>, ptr [[TMP]], align 32, !alias.scope [[META33]]
// X86-NEXT:    store <4 x double> [[TMP8]], ptr [[TMP]], align 32, !alias.scope [[META33]]
// X86-NEXT:    [[TMP9:%.*]] = load <4 x double>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x double> [[TMP9]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP10:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP10]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_broadcast_pd(__m128d* A) {
  return _mm256_broadcast_pd(A);
}

//
// X86-LABEL: define void @test_mm256_broadcast_ps(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], ptr noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[__P_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__B_I:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[COERCE_I:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[TMP:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META36:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META36]]
// X86-NEXT:    store ptr [[TMP0]], ptr [[__A_ADDR_I]], align 4, !noalias [[META36]]
// X86-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[__A_ADDR_I]], align 4, !noalias [[META36]]
// X86-NEXT:    store ptr [[TMP1]], ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    [[TMP3:%.*]] = load <4 x float>, ptr [[TMP2]], align 1
// X86-NEXT:    store <4 x float> [[TMP3]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP4]], ptr [[COERCE_I]], align 16, !noalias [[META36]]
// X86-NEXT:    [[TMP5:%.*]] = load <4 x float>, ptr [[COERCE_I]], align 16, !noalias [[META36]]
// X86-NEXT:    store <4 x float> [[TMP5]], ptr [[__B_I]], align 16, !noalias [[META36]]
// X86-NEXT:    [[TMP6:%.*]] = load <4 x float>, ptr [[__B_I]], align 16, !noalias [[META36]]
// X86-NEXT:    [[TMP7:%.*]] = load <4 x float>, ptr [[__B_I]], align 16, !noalias [[META36]]
// X86-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <4 x float> [[TMP6]], <4 x float> [[TMP7]], <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 0, i32 1, i32 2, i32 3>
// X86-NEXT:    store <8 x float> [[SHUFFLE_I]], ptr [[TMP]], align 32, !alias.scope [[META36]]
// X86-NEXT:    [[TMP8:%.*]] = load <8 x float>, ptr [[TMP]], align 32, !alias.scope [[META36]]
// X86-NEXT:    store <8 x float> [[TMP8]], ptr [[TMP]], align 32, !alias.scope [[META36]]
// X86-NEXT:    [[TMP9:%.*]] = load <8 x float>, ptr [[TMP]], align 32
// X86-NEXT:    store <8 x float> [[TMP9]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP10:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP10]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_broadcast_ps(__m128* A) {
  return _mm256_broadcast_ps(A);
}

//
// X86-LABEL: define void @test_mm256_broadcast_sd(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], ptr noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__D_I:%.*]] = alloca double, align 8
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META39:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META39]]
// X86-NEXT:    store ptr [[TMP0]], ptr [[__A_ADDR_I]], align 4, !noalias [[META39]]
// X86-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[__A_ADDR_I]], align 4, !noalias [[META39]]
// X86-NEXT:    [[TMP2:%.*]] = load double, ptr [[TMP1]], align 1
// X86-NEXT:    store double [[TMP2]], ptr [[__D_I]], align 8, !noalias [[META39]]
// X86-NEXT:    [[TMP3:%.*]] = load double, ptr [[__D_I]], align 8, !noalias [[META39]]
// X86-NEXT:    [[VECINIT_I:%.*]] = insertelement <4 x double> poison, double [[TMP3]], i32 0
// X86-NEXT:    [[TMP4:%.*]] = load double, ptr [[__D_I]], align 8, !noalias [[META39]]
// X86-NEXT:    [[VECINIT2_I:%.*]] = insertelement <4 x double> [[VECINIT_I]], double [[TMP4]], i32 1
// X86-NEXT:    [[TMP5:%.*]] = load double, ptr [[__D_I]], align 8, !noalias [[META39]]
// X86-NEXT:    [[VECINIT3_I:%.*]] = insertelement <4 x double> [[VECINIT2_I]], double [[TMP5]], i32 2
// X86-NEXT:    [[TMP6:%.*]] = load double, ptr [[__D_I]], align 8, !noalias [[META39]]
// X86-NEXT:    [[VECINIT4_I:%.*]] = insertelement <4 x double> [[VECINIT3_I]], double [[TMP6]], i32 3
// X86-NEXT:    store <4 x double> [[VECINIT4_I]], ptr [[DOTCOMPOUNDLITERAL_I]], align 32, !noalias [[META39]]
// X86-NEXT:    [[TMP7:%.*]] = load <4 x double>, ptr [[DOTCOMPOUNDLITERAL_I]], align 32, !noalias [[META39]]
// X86-NEXT:    store <4 x double> [[TMP7]], ptr [[TMP]], align 32, !alias.scope [[META39]]
// X86-NEXT:    [[TMP8:%.*]] = load <4 x double>, ptr [[TMP]], align 32, !alias.scope [[META39]]
// X86-NEXT:    store <4 x double> [[TMP8]], ptr [[TMP]], align 32, !alias.scope [[META39]]
// X86-NEXT:    [[TMP9:%.*]] = load <4 x double>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x double> [[TMP9]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP10:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP10]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_broadcast_sd(double* A) {
  return _mm256_broadcast_sd(A);
}

//
// X86-LABEL: define <2 x i64> @test_mm_broadcast_ss(
// X86-SAME: ptr noundef [[A:%.*]]) #[[ATTR1:[0-9]+]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__F_I:%.*]] = alloca float, align 4
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[COERCE:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    store ptr [[TMP0]], ptr [[__A_ADDR_I]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[__A_ADDR_I]], align 4
// X86-NEXT:    [[TMP2:%.*]] = load float, ptr [[TMP1]], align 1
// X86-NEXT:    store float [[TMP2]], ptr [[__F_I]], align 4
// X86-NEXT:    [[TMP3:%.*]] = load float, ptr [[__F_I]], align 4
// X86-NEXT:    [[VECINIT_I:%.*]] = insertelement <4 x float> poison, float [[TMP3]], i32 0
// X86-NEXT:    [[TMP4:%.*]] = load float, ptr [[__F_I]], align 4
// X86-NEXT:    [[VECINIT2_I:%.*]] = insertelement <4 x float> [[VECINIT_I]], float [[TMP4]], i32 1
// X86-NEXT:    [[TMP5:%.*]] = load float, ptr [[__F_I]], align 4
// X86-NEXT:    [[VECINIT3_I:%.*]] = insertelement <4 x float> [[VECINIT2_I]], float [[TMP5]], i32 2
// X86-NEXT:    [[TMP6:%.*]] = load float, ptr [[__F_I]], align 4
// X86-NEXT:    [[VECINIT4_I:%.*]] = insertelement <4 x float> [[VECINIT3_I]], float [[TMP6]], i32 3
// X86-NEXT:    store <4 x float> [[VECINIT4_I]], ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    [[TMP7:%.*]] = load <4 x float>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    store <4 x float> [[TMP7]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP8:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP8]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP9:%.*]] = load <4 x float>, ptr [[COERCE]], align 16
// X86-NEXT:    store <4 x float> [[TMP9]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP10:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP10]]
//
__m128 test_mm_broadcast_ss(float* A) {
  return _mm_broadcast_ss(A);
}

//
// X86-LABEL: define void @test_mm256_broadcast_ss(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], ptr noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__F_I:%.*]] = alloca float, align 4
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[TMP:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META42:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META42]]
// X86-NEXT:    store ptr [[TMP0]], ptr [[__A_ADDR_I]], align 4, !noalias [[META42]]
// X86-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[__A_ADDR_I]], align 4, !noalias [[META42]]
// X86-NEXT:    [[TMP2:%.*]] = load float, ptr [[TMP1]], align 1
// X86-NEXT:    store float [[TMP2]], ptr [[__F_I]], align 4, !noalias [[META42]]
// X86-NEXT:    [[TMP3:%.*]] = load float, ptr [[__F_I]], align 4, !noalias [[META42]]
// X86-NEXT:    [[VECINIT_I:%.*]] = insertelement <8 x float> poison, float [[TMP3]], i32 0
// X86-NEXT:    [[TMP4:%.*]] = load float, ptr [[__F_I]], align 4, !noalias [[META42]]
// X86-NEXT:    [[VECINIT2_I:%.*]] = insertelement <8 x float> [[VECINIT_I]], float [[TMP4]], i32 1
// X86-NEXT:    [[TMP5:%.*]] = load float, ptr [[__F_I]], align 4, !noalias [[META42]]
// X86-NEXT:    [[VECINIT3_I:%.*]] = insertelement <8 x float> [[VECINIT2_I]], float [[TMP5]], i32 2
// X86-NEXT:    [[TMP6:%.*]] = load float, ptr [[__F_I]], align 4, !noalias [[META42]]
// X86-NEXT:    [[VECINIT4_I:%.*]] = insertelement <8 x float> [[VECINIT3_I]], float [[TMP6]], i32 3
// X86-NEXT:    [[TMP7:%.*]] = load float, ptr [[__F_I]], align 4, !noalias [[META42]]
// X86-NEXT:    [[VECINIT5_I:%.*]] = insertelement <8 x float> [[VECINIT4_I]], float [[TMP7]], i32 4
// X86-NEXT:    [[TMP8:%.*]] = load float, ptr [[__F_I]], align 4, !noalias [[META42]]
// X86-NEXT:    [[VECINIT6_I:%.*]] = insertelement <8 x float> [[VECINIT5_I]], float [[TMP8]], i32 5
// X86-NEXT:    [[TMP9:%.*]] = load float, ptr [[__F_I]], align 4, !noalias [[META42]]
// X86-NEXT:    [[VECINIT7_I:%.*]] = insertelement <8 x float> [[VECINIT6_I]], float [[TMP9]], i32 6
// X86-NEXT:    [[TMP10:%.*]] = load float, ptr [[__F_I]], align 4, !noalias [[META42]]
// X86-NEXT:    [[VECINIT8_I:%.*]] = insertelement <8 x float> [[VECINIT7_I]], float [[TMP10]], i32 7
// X86-NEXT:    store <8 x float> [[VECINIT8_I]], ptr [[DOTCOMPOUNDLITERAL_I]], align 32, !noalias [[META42]]
// X86-NEXT:    [[TMP11:%.*]] = load <8 x float>, ptr [[DOTCOMPOUNDLITERAL_I]], align 32, !noalias [[META42]]
// X86-NEXT:    store <8 x float> [[TMP11]], ptr [[TMP]], align 32, !alias.scope [[META42]]
// X86-NEXT:    [[TMP12:%.*]] = load <8 x float>, ptr [[TMP]], align 32, !alias.scope [[META42]]
// X86-NEXT:    store <8 x float> [[TMP12]], ptr [[TMP]], align 32, !alias.scope [[META42]]
// X86-NEXT:    [[TMP13:%.*]] = load <8 x float>, ptr [[TMP]], align 32
// X86-NEXT:    store <8 x float> [[TMP13]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP14:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP14]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_broadcast_ss(float* A) {
  return _mm256_broadcast_ss(A);
}

//
// X86-LABEL: define void @test_mm256_castpd_ps(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[TMP:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META45:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META45]]
// X86-NEXT:    store <4 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 32, !noalias [[META45]]
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[__A_ADDR_I]], align 32, !noalias [[META45]]
// X86-NEXT:    [[TMP2:%.*]] = bitcast <4 x double> [[TMP1]] to <8 x float>
// X86-NEXT:    store <8 x float> [[TMP2]], ptr [[TMP]], align 32, !alias.scope [[META45]]
// X86-NEXT:    [[TMP3:%.*]] = load <8 x float>, ptr [[TMP]], align 32, !alias.scope [[META45]]
// X86-NEXT:    store <8 x float> [[TMP3]], ptr [[TMP]], align 32, !alias.scope [[META45]]
// X86-NEXT:    [[TMP4:%.*]] = load <8 x float>, ptr [[TMP]], align 32
// X86-NEXT:    store <8 x float> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_castpd_ps(__m256d A) {
  return _mm256_castpd_ps(A);
}

//
// X86-LABEL: define void @test_mm256_castpd_si256(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x i64>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META48:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META48]]
// X86-NEXT:    store <4 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 32, !noalias [[META48]]
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[__A_ADDR_I]], align 32, !noalias [[META48]]
// X86-NEXT:    [[TMP2:%.*]] = bitcast <4 x double> [[TMP1]] to <4 x i64>
// X86-NEXT:    store <4 x i64> [[TMP2]], ptr [[TMP]], align 32, !alias.scope [[META48]]
// X86-NEXT:    [[TMP3:%.*]] = load <4 x i64>, ptr [[TMP]], align 32, !alias.scope [[META48]]
// X86-NEXT:    store <4 x i64> [[TMP3]], ptr [[TMP]], align 32, !alias.scope [[META48]]
// X86-NEXT:    [[TMP4:%.*]] = load <4 x i64>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x i64> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <4 x i64>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x i64> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256i test_mm256_castpd_si256(__m256d A) {
  return _mm256_castpd_si256(A);
}

//
// X86-LABEL: define void @test_mm256_castpd128_pd256(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <2 x double> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META51:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META51]]
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16, !noalias [[META51]]
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16, !noalias [[META51]]
// X86-NEXT:    [[TMP2:%.*]] = freeze <2 x double> poison
// X86-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <2 x double> [[TMP1]], <2 x double> [[TMP2]], <4 x i32> <i32 0, i32 1, i32 2, i32 3>
// X86-NEXT:    store <4 x double> [[SHUFFLE_I]], ptr [[TMP]], align 32, !alias.scope [[META51]]
// X86-NEXT:    [[TMP3:%.*]] = load <4 x double>, ptr [[TMP]], align 32, !alias.scope [[META51]]
// X86-NEXT:    store <4 x double> [[TMP3]], ptr [[TMP]], align 32, !alias.scope [[META51]]
// X86-NEXT:    [[TMP4:%.*]] = load <4 x double>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x double> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_castpd128_pd256(__m128d A) {
  return _mm256_castpd128_pd256(A);
}

//
// X86-LABEL: define <2 x i64> @test_mm256_castpd256_pd128(
// X86-SAME: <4 x double> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    [[TMP2:%.*]] = load <4 x double>, ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <4 x double> [[TMP1]], <4 x double> [[TMP2]], <2 x i32> <i32 0, i32 1>
// X86-NEXT:    store <2 x double> [[SHUFFLE_I]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP3]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP4:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128d test_mm256_castpd256_pd128(__m256d A) {
  return _mm256_castpd256_pd128(A);
}

//
// X86-LABEL: define void @test_mm256_castps_pd(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META54:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META54]]
// X86-NEXT:    store <8 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 32, !noalias [[META54]]
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[__A_ADDR_I]], align 32, !noalias [[META54]]
// X86-NEXT:    [[TMP2:%.*]] = bitcast <8 x float> [[TMP1]] to <4 x double>
// X86-NEXT:    store <4 x double> [[TMP2]], ptr [[TMP]], align 32, !alias.scope [[META54]]
// X86-NEXT:    [[TMP3:%.*]] = load <4 x double>, ptr [[TMP]], align 32, !alias.scope [[META54]]
// X86-NEXT:    store <4 x double> [[TMP3]], ptr [[TMP]], align 32, !alias.scope [[META54]]
// X86-NEXT:    [[TMP4:%.*]] = load <4 x double>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x double> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_castps_pd(__m256 A) {
  return _mm256_castps_pd(A);
}

//
// X86-LABEL: define void @test_mm256_castps_si256(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x i64>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META57:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META57]]
// X86-NEXT:    store <8 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 32, !noalias [[META57]]
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[__A_ADDR_I]], align 32, !noalias [[META57]]
// X86-NEXT:    [[TMP2:%.*]] = bitcast <8 x float> [[TMP1]] to <4 x i64>
// X86-NEXT:    store <4 x i64> [[TMP2]], ptr [[TMP]], align 32, !alias.scope [[META57]]
// X86-NEXT:    [[TMP3:%.*]] = load <4 x i64>, ptr [[TMP]], align 32, !alias.scope [[META57]]
// X86-NEXT:    store <4 x i64> [[TMP3]], ptr [[TMP]], align 32, !alias.scope [[META57]]
// X86-NEXT:    [[TMP4:%.*]] = load <4 x i64>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x i64> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <4 x i64>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x i64> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256i test_mm256_castps_si256(__m256 A) {
  return _mm256_castps_si256(A);
}

//
// X86-LABEL: define void @test_mm256_castps128_ps256(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <4 x float> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[TMP:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META60:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META60]]
// X86-NEXT:    store <4 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 16, !noalias [[META60]]
// X86-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[__A_ADDR_I]], align 16, !noalias [[META60]]
// X86-NEXT:    [[TMP2:%.*]] = freeze <4 x float> poison
// X86-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <4 x float> [[TMP1]], <4 x float> [[TMP2]], <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
// X86-NEXT:    store <8 x float> [[SHUFFLE_I]], ptr [[TMP]], align 32, !alias.scope [[META60]]
// X86-NEXT:    [[TMP3:%.*]] = load <8 x float>, ptr [[TMP]], align 32, !alias.scope [[META60]]
// X86-NEXT:    store <8 x float> [[TMP3]], ptr [[TMP]], align 32, !alias.scope [[META60]]
// X86-NEXT:    [[TMP4:%.*]] = load <8 x float>, ptr [[TMP]], align 32
// X86-NEXT:    store <8 x float> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_castps128_ps256(__m128 A) {
  return _mm256_castps128_ps256(A);
}

//
// X86-LABEL: define <2 x i64> @test_mm256_castps256_ps128(
// X86-SAME: <8 x float> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[RETVAL:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[COERCE:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    [[TMP2:%.*]] = load <8 x float>, ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <8 x float> [[TMP1]], <8 x float> [[TMP2]], <4 x i32> <i32 0, i32 1, i32 2, i32 3>
// X86-NEXT:    store <4 x float> [[SHUFFLE_I]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP3]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP4:%.*]] = load <4 x float>, ptr [[COERCE]], align 16
// X86-NEXT:    store <4 x float> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128 test_mm256_castps256_ps128(__m256 A) {
  return _mm256_castps256_ps128(A);
}

//
// X86-LABEL: define void @test_mm256_castsi128_si256(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x i64>) align 32 [[AGG_RESULT:%.*]], <2 x i64> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META63:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META63]]
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16, !noalias [[META63]]
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16, !noalias [[META63]]
// X86-NEXT:    [[TMP2:%.*]] = freeze <2 x i64> poison
// X86-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <2 x i64> [[TMP1]], <2 x i64> [[TMP2]], <4 x i32> <i32 0, i32 1, i32 2, i32 3>
// X86-NEXT:    store <4 x i64> [[SHUFFLE_I]], ptr [[TMP]], align 32, !alias.scope [[META63]]
// X86-NEXT:    [[TMP3:%.*]] = load <4 x i64>, ptr [[TMP]], align 32, !alias.scope [[META63]]
// X86-NEXT:    store <4 x i64> [[TMP3]], ptr [[TMP]], align 32, !alias.scope [[META63]]
// X86-NEXT:    [[TMP4:%.*]] = load <4 x i64>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x i64> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <4 x i64>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x i64> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256i test_mm256_castsi128_si256(__m128i A) {
  return _mm256_castsi128_si256(A);
}

//
// X86-LABEL: define void @test_mm256_castsi256_pd(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x i64> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x i64> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x i64>, ptr [[A_ADDR]], align 32
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META66:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META66]]
// X86-NEXT:    store <4 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 32, !noalias [[META66]]
// X86-NEXT:    [[TMP1:%.*]] = load <4 x i64>, ptr [[__A_ADDR_I]], align 32, !noalias [[META66]]
// X86-NEXT:    [[TMP2:%.*]] = bitcast <4 x i64> [[TMP1]] to <4 x double>
// X86-NEXT:    store <4 x double> [[TMP2]], ptr [[TMP]], align 32, !alias.scope [[META66]]
// X86-NEXT:    [[TMP3:%.*]] = load <4 x double>, ptr [[TMP]], align 32, !alias.scope [[META66]]
// X86-NEXT:    store <4 x double> [[TMP3]], ptr [[TMP]], align 32, !alias.scope [[META66]]
// X86-NEXT:    [[TMP4:%.*]] = load <4 x double>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x double> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_castsi256_pd(__m256i A) {
  return _mm256_castsi256_pd(A);
}

//
// X86-LABEL: define void @test_mm256_castsi256_ps(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <4 x i64> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    [[TMP:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x i64> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x i64>, ptr [[A_ADDR]], align 32
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META69:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META69]]
// X86-NEXT:    store <4 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 32, !noalias [[META69]]
// X86-NEXT:    [[TMP1:%.*]] = load <4 x i64>, ptr [[__A_ADDR_I]], align 32, !noalias [[META69]]
// X86-NEXT:    [[TMP2:%.*]] = bitcast <4 x i64> [[TMP1]] to <8 x float>
// X86-NEXT:    store <8 x float> [[TMP2]], ptr [[TMP]], align 32, !alias.scope [[META69]]
// X86-NEXT:    [[TMP3:%.*]] = load <8 x float>, ptr [[TMP]], align 32, !alias.scope [[META69]]
// X86-NEXT:    store <8 x float> [[TMP3]], ptr [[TMP]], align 32, !alias.scope [[META69]]
// X86-NEXT:    [[TMP4:%.*]] = load <8 x float>, ptr [[TMP]], align 32
// X86-NEXT:    store <8 x float> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_castsi256_ps(__m256i A) {
  return _mm256_castsi256_ps(A);
}

//
// X86-LABEL: define <2 x i64> @test_mm256_castsi256_si128(
// X86-SAME: <4 x i64> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    store <4 x i64> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x i64>, ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x i64>, ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    [[TMP2:%.*]] = load <4 x i64>, ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <4 x i64> [[TMP1]], <4 x i64> [[TMP2]], <2 x i32> <i32 0, i32 1>
// X86-NEXT:    ret <2 x i64> [[SHUFFLE_I]]
//
__m128i test_mm256_castsi256_si128(__m256i A) {
  return _mm256_castsi256_si128(A);
}

//
// X86-LABEL: define void @test_mm256_ceil_pd(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[X:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[X_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[X]], ptr [[X_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[X_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = call <4 x double> @llvm.x86.avx.round.pd.256(<4 x double> [[TMP0]], i32 2)
// X86-NEXT:    store <4 x double> [[TMP1]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP2:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP2]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_ceil_pd(__m256d x) {
  return _mm256_ceil_pd(x);
}

//
// X86-LABEL: define void @test_mm_ceil_ps(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[X:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[X_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[X]], ptr [[X_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[X_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = call <8 x float> @llvm.x86.avx.round.ps.256(<8 x float> [[TMP0]], i32 2)
// X86-NEXT:    store <8 x float> [[TMP1]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP2:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP2]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm_ceil_ps(__m256 x) {
  return _mm256_ceil_ps(x);
}

//
// X86-LABEL: define void @test_mm256_cmp_pd_eq_oq(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[A:%.*]], <4 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp oeq <4 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <4 x i1> [[TMP2]] to <4 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <4 x i64> [[TMP3]] to <4 x double>
// X86-NEXT:    store <4 x double> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_cmp_pd_eq_oq(__m256d a, __m256d b) {
  return _mm256_cmp_pd(a, b, _CMP_EQ_OQ);
}

//
// X86-LABEL: define void @test_mm256_cmp_pd_lt_os(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[A:%.*]], <4 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp olt <4 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <4 x i1> [[TMP2]] to <4 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <4 x i64> [[TMP3]] to <4 x double>
// X86-NEXT:    store <4 x double> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_cmp_pd_lt_os(__m256d a, __m256d b) {
  return _mm256_cmp_pd(a, b, _CMP_LT_OS);
}

//
// X86-LABEL: define void @test_mm256_cmp_pd_le_os(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[A:%.*]], <4 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp ole <4 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <4 x i1> [[TMP2]] to <4 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <4 x i64> [[TMP3]] to <4 x double>
// X86-NEXT:    store <4 x double> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_cmp_pd_le_os(__m256d a, __m256d b) {
  return _mm256_cmp_pd(a, b, _CMP_LE_OS);
}

//
// X86-LABEL: define void @test_mm256_cmp_pd_unord_q(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[A:%.*]], <4 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp uno <4 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <4 x i1> [[TMP2]] to <4 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <4 x i64> [[TMP3]] to <4 x double>
// X86-NEXT:    store <4 x double> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_cmp_pd_unord_q(__m256d a, __m256d b) {
  return _mm256_cmp_pd(a, b, _CMP_UNORD_Q);
}

//
// X86-LABEL: define void @test_mm256_cmp_pd_neq_uq(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[A:%.*]], <4 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp une <4 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <4 x i1> [[TMP2]] to <4 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <4 x i64> [[TMP3]] to <4 x double>
// X86-NEXT:    store <4 x double> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_cmp_pd_neq_uq(__m256d a, __m256d b) {
  return _mm256_cmp_pd(a, b, _CMP_NEQ_UQ);
}

//
// X86-LABEL: define void @test_mm256_cmp_pd_nlt_us(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[A:%.*]], <4 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp uge <4 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <4 x i1> [[TMP2]] to <4 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <4 x i64> [[TMP3]] to <4 x double>
// X86-NEXT:    store <4 x double> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_cmp_pd_nlt_us(__m256d a, __m256d b) {
  return _mm256_cmp_pd(a, b, _CMP_NLT_US);
}

//
// X86-LABEL: define void @test_mm256_cmp_pd_nle_us(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[A:%.*]], <4 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp ugt <4 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <4 x i1> [[TMP2]] to <4 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <4 x i64> [[TMP3]] to <4 x double>
// X86-NEXT:    store <4 x double> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_cmp_pd_nle_us(__m256d a, __m256d b) {
  return _mm256_cmp_pd(a, b, _CMP_NLE_US);
}

//
// X86-LABEL: define void @test_mm256_cmp_pd_ord_q(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[A:%.*]], <4 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp ord <4 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <4 x i1> [[TMP2]] to <4 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <4 x i64> [[TMP3]] to <4 x double>
// X86-NEXT:    store <4 x double> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_cmp_pd_ord_q(__m256d a, __m256d b) {
  return _mm256_cmp_pd(a, b, _CMP_ORD_Q);
}

//
// X86-LABEL: define void @test_mm256_cmp_pd_eq_uq(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[A:%.*]], <4 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp ueq <4 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <4 x i1> [[TMP2]] to <4 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <4 x i64> [[TMP3]] to <4 x double>
// X86-NEXT:    store <4 x double> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_cmp_pd_eq_uq(__m256d a, __m256d b) {
  return _mm256_cmp_pd(a, b, _CMP_EQ_UQ);
}

//
// X86-LABEL: define void @test_mm256_cmp_pd_nge_us(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[A:%.*]], <4 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp ult <4 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <4 x i1> [[TMP2]] to <4 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <4 x i64> [[TMP3]] to <4 x double>
// X86-NEXT:    store <4 x double> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_cmp_pd_nge_us(__m256d a, __m256d b) {
  return _mm256_cmp_pd(a, b, _CMP_NGE_US);
}

//
// X86-LABEL: define void @test_mm256_cmp_pd_ngt_us(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[A:%.*]], <4 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp ule <4 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <4 x i1> [[TMP2]] to <4 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <4 x i64> [[TMP3]] to <4 x double>
// X86-NEXT:    store <4 x double> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_cmp_pd_ngt_us(__m256d a, __m256d b) {
  return _mm256_cmp_pd(a, b, _CMP_NGT_US);
}

//
// X86-LABEL: define void @test_mm256_cmp_pd_false_oq(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[A:%.*]], <4 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp false <4 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <4 x i1> [[TMP2]] to <4 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <4 x i64> [[TMP3]] to <4 x double>
// X86-NEXT:    store <4 x double> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_cmp_pd_false_oq(__m256d a, __m256d b) {
  return _mm256_cmp_pd(a, b, _CMP_FALSE_OQ);
}

//
// X86-LABEL: define void @test_mm256_cmp_pd_neq_oq(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[A:%.*]], <4 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp one <4 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <4 x i1> [[TMP2]] to <4 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <4 x i64> [[TMP3]] to <4 x double>
// X86-NEXT:    store <4 x double> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_cmp_pd_neq_oq(__m256d a, __m256d b) {
  return _mm256_cmp_pd(a, b, _CMP_NEQ_OQ);
}

//
// X86-LABEL: define void @test_mm256_cmp_pd_ge_os(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[A:%.*]], <4 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp oge <4 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <4 x i1> [[TMP2]] to <4 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <4 x i64> [[TMP3]] to <4 x double>
// X86-NEXT:    store <4 x double> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_cmp_pd_ge_os(__m256d a, __m256d b) {
  return _mm256_cmp_pd(a, b, _CMP_GE_OS);
}

//
// X86-LABEL: define void @test_mm256_cmp_pd_gt_os(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[A:%.*]], <4 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp ogt <4 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <4 x i1> [[TMP2]] to <4 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <4 x i64> [[TMP3]] to <4 x double>
// X86-NEXT:    store <4 x double> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_cmp_pd_gt_os(__m256d a, __m256d b) {
  return _mm256_cmp_pd(a, b, _CMP_GT_OS);
}

//
// X86-LABEL: define void @test_mm256_cmp_pd_true_uq(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[A:%.*]], <4 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp true <4 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <4 x i1> [[TMP2]] to <4 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <4 x i64> [[TMP3]] to <4 x double>
// X86-NEXT:    store <4 x double> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_cmp_pd_true_uq(__m256d a, __m256d b) {
  return _mm256_cmp_pd(a, b, _CMP_TRUE_UQ);
}

//
// X86-LABEL: define void @test_mm256_cmp_pd_eq_os(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[A:%.*]], <4 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp oeq <4 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <4 x i1> [[TMP2]] to <4 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <4 x i64> [[TMP3]] to <4 x double>
// X86-NEXT:    store <4 x double> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_cmp_pd_eq_os(__m256d a, __m256d b) {
  return _mm256_cmp_pd(a, b, _CMP_EQ_OS);
}

//
// X86-LABEL: define void @test_mm256_cmp_pd_lt_oq(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[A:%.*]], <4 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp olt <4 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <4 x i1> [[TMP2]] to <4 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <4 x i64> [[TMP3]] to <4 x double>
// X86-NEXT:    store <4 x double> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_cmp_pd_lt_oq(__m256d a, __m256d b) {
  return _mm256_cmp_pd(a, b, _CMP_LT_OQ);
}

//
// X86-LABEL: define void @test_mm256_cmp_pd_le_oq(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[A:%.*]], <4 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp ole <4 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <4 x i1> [[TMP2]] to <4 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <4 x i64> [[TMP3]] to <4 x double>
// X86-NEXT:    store <4 x double> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_cmp_pd_le_oq(__m256d a, __m256d b) {
  return _mm256_cmp_pd(a, b, _CMP_LE_OQ);
}

//
// X86-LABEL: define void @test_mm256_cmp_pd_unord_s(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[A:%.*]], <4 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp uno <4 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <4 x i1> [[TMP2]] to <4 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <4 x i64> [[TMP3]] to <4 x double>
// X86-NEXT:    store <4 x double> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_cmp_pd_unord_s(__m256d a, __m256d b) {
  return _mm256_cmp_pd(a, b, _CMP_UNORD_S);
}

//
// X86-LABEL: define void @test_mm256_cmp_pd_neq_us(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[A:%.*]], <4 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp une <4 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <4 x i1> [[TMP2]] to <4 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <4 x i64> [[TMP3]] to <4 x double>
// X86-NEXT:    store <4 x double> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_cmp_pd_neq_us(__m256d a, __m256d b) {
  return _mm256_cmp_pd(a, b, _CMP_NEQ_US);
}

//
// X86-LABEL: define void @test_mm256_cmp_pd_nlt_uq(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[A:%.*]], <4 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp uge <4 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <4 x i1> [[TMP2]] to <4 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <4 x i64> [[TMP3]] to <4 x double>
// X86-NEXT:    store <4 x double> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_cmp_pd_nlt_uq(__m256d a, __m256d b) {
  return _mm256_cmp_pd(a, b, _CMP_NLT_UQ);
}

//
// X86-LABEL: define void @test_mm256_cmp_pd_nle_uq(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[A:%.*]], <4 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp ugt <4 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <4 x i1> [[TMP2]] to <4 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <4 x i64> [[TMP3]] to <4 x double>
// X86-NEXT:    store <4 x double> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_cmp_pd_nle_uq(__m256d a, __m256d b) {
  return _mm256_cmp_pd(a, b, _CMP_NLE_UQ);
}

//
// X86-LABEL: define void @test_mm256_cmp_pd_ord_s(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[A:%.*]], <4 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp ord <4 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <4 x i1> [[TMP2]] to <4 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <4 x i64> [[TMP3]] to <4 x double>
// X86-NEXT:    store <4 x double> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_cmp_pd_ord_s(__m256d a, __m256d b) {
  return _mm256_cmp_pd(a, b, _CMP_ORD_S);
}

//
// X86-LABEL: define void @test_mm256_cmp_pd_eq_us(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[A:%.*]], <4 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp ueq <4 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <4 x i1> [[TMP2]] to <4 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <4 x i64> [[TMP3]] to <4 x double>
// X86-NEXT:    store <4 x double> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_cmp_pd_eq_us(__m256d a, __m256d b) {
  return _mm256_cmp_pd(a, b, _CMP_EQ_US);
}

//
// X86-LABEL: define void @test_mm256_cmp_pd_nge_uq(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[A:%.*]], <4 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp ult <4 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <4 x i1> [[TMP2]] to <4 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <4 x i64> [[TMP3]] to <4 x double>
// X86-NEXT:    store <4 x double> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_cmp_pd_nge_uq(__m256d a, __m256d b) {
  return _mm256_cmp_pd(a, b, _CMP_NGE_UQ);
}

//
// X86-LABEL: define void @test_mm256_cmp_pd_ngt_uq(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[A:%.*]], <4 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp ule <4 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <4 x i1> [[TMP2]] to <4 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <4 x i64> [[TMP3]] to <4 x double>
// X86-NEXT:    store <4 x double> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_cmp_pd_ngt_uq(__m256d a, __m256d b) {
  return _mm256_cmp_pd(a, b, _CMP_NGT_UQ);
}

//
// X86-LABEL: define void @test_mm256_cmp_pd_false_os(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[A:%.*]], <4 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp false <4 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <4 x i1> [[TMP2]] to <4 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <4 x i64> [[TMP3]] to <4 x double>
// X86-NEXT:    store <4 x double> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_cmp_pd_false_os(__m256d a, __m256d b) {
  return _mm256_cmp_pd(a, b, _CMP_FALSE_OS);
}

//
// X86-LABEL: define void @test_mm256_cmp_pd_neq_os(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[A:%.*]], <4 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp one <4 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <4 x i1> [[TMP2]] to <4 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <4 x i64> [[TMP3]] to <4 x double>
// X86-NEXT:    store <4 x double> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_cmp_pd_neq_os(__m256d a, __m256d b) {
  return _mm256_cmp_pd(a, b, _CMP_NEQ_OS);
}

//
// X86-LABEL: define void @test_mm256_cmp_pd_ge_oq(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[A:%.*]], <4 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp oge <4 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <4 x i1> [[TMP2]] to <4 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <4 x i64> [[TMP3]] to <4 x double>
// X86-NEXT:    store <4 x double> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_cmp_pd_ge_oq(__m256d a, __m256d b) {
  return _mm256_cmp_pd(a, b, _CMP_GE_OQ);
}

//
// X86-LABEL: define void @test_mm256_cmp_pd_gt_oq(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[A:%.*]], <4 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp ogt <4 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <4 x i1> [[TMP2]] to <4 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <4 x i64> [[TMP3]] to <4 x double>
// X86-NEXT:    store <4 x double> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_cmp_pd_gt_oq(__m256d a, __m256d b) {
  return _mm256_cmp_pd(a, b, _CMP_GT_OQ);
}

//
// X86-LABEL: define void @test_mm256_cmp_pd_true_us(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[A:%.*]], <4 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp true <4 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <4 x i1> [[TMP2]] to <4 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <4 x i64> [[TMP3]] to <4 x double>
// X86-NEXT:    store <4 x double> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_cmp_pd_true_us(__m256d a, __m256d b) {
  return _mm256_cmp_pd(a, b, _CMP_TRUE_US);
}

//
// X86-LABEL: define void @test_mm256_cmp_ps_eq_oq(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]], <8 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp oeq <8 x float> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <8 x i1> [[TMP2]] to <8 x i32>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <8 x i32> [[TMP3]] to <8 x float>
// X86-NEXT:    store <8 x float> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_cmp_ps_eq_oq(__m256 a, __m256 b) {
  return _mm256_cmp_ps(a, b, _CMP_EQ_OQ);
}

//
// X86-LABEL: define void @test_mm256_cmp_ps_lt_os(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]], <8 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp olt <8 x float> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <8 x i1> [[TMP2]] to <8 x i32>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <8 x i32> [[TMP3]] to <8 x float>
// X86-NEXT:    store <8 x float> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_cmp_ps_lt_os(__m256 a, __m256 b) {
  return _mm256_cmp_ps(a, b, _CMP_LT_OS);
}

//
// X86-LABEL: define void @test_mm256_cmp_ps_le_os(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]], <8 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp ole <8 x float> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <8 x i1> [[TMP2]] to <8 x i32>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <8 x i32> [[TMP3]] to <8 x float>
// X86-NEXT:    store <8 x float> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_cmp_ps_le_os(__m256 a, __m256 b) {
  return _mm256_cmp_ps(a, b, _CMP_LE_OS);
}

//
// X86-LABEL: define void @test_mm256_cmp_ps_unord_q(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]], <8 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp uno <8 x float> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <8 x i1> [[TMP2]] to <8 x i32>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <8 x i32> [[TMP3]] to <8 x float>
// X86-NEXT:    store <8 x float> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_cmp_ps_unord_q(__m256 a, __m256 b) {
  return _mm256_cmp_ps(a, b, _CMP_UNORD_Q);
}

//
// X86-LABEL: define void @test_mm256_cmp_ps_neq_uq(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]], <8 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp une <8 x float> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <8 x i1> [[TMP2]] to <8 x i32>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <8 x i32> [[TMP3]] to <8 x float>
// X86-NEXT:    store <8 x float> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_cmp_ps_neq_uq(__m256 a, __m256 b) {
  return _mm256_cmp_ps(a, b, _CMP_NEQ_UQ);
}

//
// X86-LABEL: define void @test_mm256_cmp_ps_nlt_us(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]], <8 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp uge <8 x float> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <8 x i1> [[TMP2]] to <8 x i32>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <8 x i32> [[TMP3]] to <8 x float>
// X86-NEXT:    store <8 x float> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_cmp_ps_nlt_us(__m256 a, __m256 b) {
  return _mm256_cmp_ps(a, b, _CMP_NLT_US);
}

//
// X86-LABEL: define void @test_mm256_cmp_ps_nle_us(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]], <8 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp ugt <8 x float> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <8 x i1> [[TMP2]] to <8 x i32>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <8 x i32> [[TMP3]] to <8 x float>
// X86-NEXT:    store <8 x float> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_cmp_ps_nle_us(__m256 a, __m256 b) {
  return _mm256_cmp_ps(a, b, _CMP_NLE_US);
}

//
// X86-LABEL: define void @test_mm256_cmp_ps_ord_q(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]], <8 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp ord <8 x float> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <8 x i1> [[TMP2]] to <8 x i32>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <8 x i32> [[TMP3]] to <8 x float>
// X86-NEXT:    store <8 x float> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_cmp_ps_ord_q(__m256 a, __m256 b) {
  return _mm256_cmp_ps(a, b, _CMP_ORD_Q);
}

//
// X86-LABEL: define void @test_mm256_cmp_ps_eq_uq(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]], <8 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp ueq <8 x float> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <8 x i1> [[TMP2]] to <8 x i32>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <8 x i32> [[TMP3]] to <8 x float>
// X86-NEXT:    store <8 x float> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_cmp_ps_eq_uq(__m256 a, __m256 b) {
  return _mm256_cmp_ps(a, b, _CMP_EQ_UQ);
}

//
// X86-LABEL: define void @test_mm256_cmp_ps_nge_us(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]], <8 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp ult <8 x float> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <8 x i1> [[TMP2]] to <8 x i32>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <8 x i32> [[TMP3]] to <8 x float>
// X86-NEXT:    store <8 x float> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_cmp_ps_nge_us(__m256 a, __m256 b) {
  return _mm256_cmp_ps(a, b, _CMP_NGE_US);
}

//
// X86-LABEL: define void @test_mm256_cmp_ps_ngt_us(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]], <8 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp ule <8 x float> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <8 x i1> [[TMP2]] to <8 x i32>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <8 x i32> [[TMP3]] to <8 x float>
// X86-NEXT:    store <8 x float> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_cmp_ps_ngt_us(__m256 a, __m256 b) {
  return _mm256_cmp_ps(a, b, _CMP_NGT_US);
}

//
// X86-LABEL: define void @test_mm256_cmp_ps_false_oq(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]], <8 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp false <8 x float> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <8 x i1> [[TMP2]] to <8 x i32>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <8 x i32> [[TMP3]] to <8 x float>
// X86-NEXT:    store <8 x float> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_cmp_ps_false_oq(__m256 a, __m256 b) {
  return _mm256_cmp_ps(a, b, _CMP_FALSE_OQ);
}

//
// X86-LABEL: define void @test_mm256_cmp_ps_neq_oq(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]], <8 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp one <8 x float> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <8 x i1> [[TMP2]] to <8 x i32>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <8 x i32> [[TMP3]] to <8 x float>
// X86-NEXT:    store <8 x float> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_cmp_ps_neq_oq(__m256 a, __m256 b) {
  return _mm256_cmp_ps(a, b, _CMP_NEQ_OQ);
}

//
// X86-LABEL: define void @test_mm256_cmp_ps_ge_os(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]], <8 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp oge <8 x float> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <8 x i1> [[TMP2]] to <8 x i32>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <8 x i32> [[TMP3]] to <8 x float>
// X86-NEXT:    store <8 x float> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_cmp_ps_ge_os(__m256 a, __m256 b) {
  return _mm256_cmp_ps(a, b, _CMP_GE_OS);
}

//
// X86-LABEL: define void @test_mm256_cmp_ps_gt_os(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]], <8 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp ogt <8 x float> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <8 x i1> [[TMP2]] to <8 x i32>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <8 x i32> [[TMP3]] to <8 x float>
// X86-NEXT:    store <8 x float> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_cmp_ps_gt_os(__m256 a, __m256 b) {
  return _mm256_cmp_ps(a, b, _CMP_GT_OS);
}

//
// X86-LABEL: define void @test_mm256_cmp_ps_true_uq(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]], <8 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp true <8 x float> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <8 x i1> [[TMP2]] to <8 x i32>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <8 x i32> [[TMP3]] to <8 x float>
// X86-NEXT:    store <8 x float> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_cmp_ps_true_uq(__m256 a, __m256 b) {
  return _mm256_cmp_ps(a, b, _CMP_TRUE_UQ);
}

//
// X86-LABEL: define void @test_mm256_cmp_ps_eq_os(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]], <8 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp oeq <8 x float> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <8 x i1> [[TMP2]] to <8 x i32>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <8 x i32> [[TMP3]] to <8 x float>
// X86-NEXT:    store <8 x float> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_cmp_ps_eq_os(__m256 a, __m256 b) {
  return _mm256_cmp_ps(a, b, _CMP_EQ_OS);
}

//
// X86-LABEL: define void @test_mm256_cmp_ps_lt_oq(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]], <8 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp olt <8 x float> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <8 x i1> [[TMP2]] to <8 x i32>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <8 x i32> [[TMP3]] to <8 x float>
// X86-NEXT:    store <8 x float> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_cmp_ps_lt_oq(__m256 a, __m256 b) {
  return _mm256_cmp_ps(a, b, _CMP_LT_OQ);
}

//
// X86-LABEL: define void @test_mm256_cmp_ps_le_oq(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]], <8 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp ole <8 x float> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <8 x i1> [[TMP2]] to <8 x i32>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <8 x i32> [[TMP3]] to <8 x float>
// X86-NEXT:    store <8 x float> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_cmp_ps_le_oq(__m256 a, __m256 b) {
  return _mm256_cmp_ps(a, b, _CMP_LE_OQ);
}

//
// X86-LABEL: define void @test_mm256_cmp_ps_unord_s(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]], <8 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp uno <8 x float> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <8 x i1> [[TMP2]] to <8 x i32>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <8 x i32> [[TMP3]] to <8 x float>
// X86-NEXT:    store <8 x float> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_cmp_ps_unord_s(__m256 a, __m256 b) {
  return _mm256_cmp_ps(a, b, _CMP_UNORD_S);
}

//
// X86-LABEL: define void @test_mm256_cmp_ps_neq_us(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]], <8 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp une <8 x float> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <8 x i1> [[TMP2]] to <8 x i32>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <8 x i32> [[TMP3]] to <8 x float>
// X86-NEXT:    store <8 x float> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_cmp_ps_neq_us(__m256 a, __m256 b) {
  return _mm256_cmp_ps(a, b, _CMP_NEQ_US);
}

//
// X86-LABEL: define void @test_mm256_cmp_ps_nlt_uq(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]], <8 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp uge <8 x float> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <8 x i1> [[TMP2]] to <8 x i32>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <8 x i32> [[TMP3]] to <8 x float>
// X86-NEXT:    store <8 x float> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_cmp_ps_nlt_uq(__m256 a, __m256 b) {
  return _mm256_cmp_ps(a, b, _CMP_NLT_UQ);
}

//
// X86-LABEL: define void @test_mm256_cmp_ps_nle_uq(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]], <8 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp ugt <8 x float> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <8 x i1> [[TMP2]] to <8 x i32>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <8 x i32> [[TMP3]] to <8 x float>
// X86-NEXT:    store <8 x float> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_cmp_ps_nle_uq(__m256 a, __m256 b) {
  return _mm256_cmp_ps(a, b, _CMP_NLE_UQ);
}

//
// X86-LABEL: define void @test_mm256_cmp_ps_ord_s(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]], <8 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp ord <8 x float> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <8 x i1> [[TMP2]] to <8 x i32>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <8 x i32> [[TMP3]] to <8 x float>
// X86-NEXT:    store <8 x float> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_cmp_ps_ord_s(__m256 a, __m256 b) {
  return _mm256_cmp_ps(a, b, _CMP_ORD_S);
}

//
// X86-LABEL: define void @test_mm256_cmp_ps_eq_us(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]], <8 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp ueq <8 x float> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <8 x i1> [[TMP2]] to <8 x i32>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <8 x i32> [[TMP3]] to <8 x float>
// X86-NEXT:    store <8 x float> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_cmp_ps_eq_us(__m256 a, __m256 b) {
  return _mm256_cmp_ps(a, b, _CMP_EQ_US);
}

//
// X86-LABEL: define void @test_mm256_cmp_ps_nge_uq(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]], <8 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp ult <8 x float> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <8 x i1> [[TMP2]] to <8 x i32>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <8 x i32> [[TMP3]] to <8 x float>
// X86-NEXT:    store <8 x float> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_cmp_ps_nge_uq(__m256 a, __m256 b) {
  return _mm256_cmp_ps(a, b, _CMP_NGE_UQ);
}

//
// X86-LABEL: define void @test_mm256_cmp_ps_ngt_uq(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]], <8 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp ule <8 x float> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <8 x i1> [[TMP2]] to <8 x i32>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <8 x i32> [[TMP3]] to <8 x float>
// X86-NEXT:    store <8 x float> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_cmp_ps_ngt_uq(__m256 a, __m256 b) {
  return _mm256_cmp_ps(a, b, _CMP_NGT_UQ);
}

//
// X86-LABEL: define void @test_mm256_cmp_ps_false_os(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]], <8 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp false <8 x float> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <8 x i1> [[TMP2]] to <8 x i32>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <8 x i32> [[TMP3]] to <8 x float>
// X86-NEXT:    store <8 x float> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_cmp_ps_false_os(__m256 a, __m256 b) {
  return _mm256_cmp_ps(a, b, _CMP_FALSE_OS);
}

//
// X86-LABEL: define void @test_mm256_cmp_ps_neq_os(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]], <8 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp one <8 x float> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <8 x i1> [[TMP2]] to <8 x i32>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <8 x i32> [[TMP3]] to <8 x float>
// X86-NEXT:    store <8 x float> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_cmp_ps_neq_os(__m256 a, __m256 b) {
  return _mm256_cmp_ps(a, b, _CMP_NEQ_OS);
}

//
// X86-LABEL: define void @test_mm256_cmp_ps_ge_oq(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]], <8 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp oge <8 x float> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <8 x i1> [[TMP2]] to <8 x i32>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <8 x i32> [[TMP3]] to <8 x float>
// X86-NEXT:    store <8 x float> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_cmp_ps_ge_oq(__m256 a, __m256 b) {
  return _mm256_cmp_ps(a, b, _CMP_GE_OQ);
}

//
// X86-LABEL: define void @test_mm256_cmp_ps_gt_oq(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]], <8 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp ogt <8 x float> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <8 x i1> [[TMP2]] to <8 x i32>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <8 x i32> [[TMP3]] to <8 x float>
// X86-NEXT:    store <8 x float> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_cmp_ps_gt_oq(__m256 a, __m256 b) {
  return _mm256_cmp_ps(a, b, _CMP_GT_OQ);
}

//
// X86-LABEL: define void @test_mm256_cmp_ps_true_us(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]], <8 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = fcmp true <8 x float> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <8 x i1> [[TMP2]] to <8 x i32>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <8 x i32> [[TMP3]] to <8 x float>
// X86-NEXT:    store <8 x float> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_cmp_ps_true_us(__m256 a, __m256 b) {
  return _mm256_cmp_ps(a, b, _CMP_TRUE_US);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmp_pd_eq_uq(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = fcmp ueq <2 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <2 x i1> [[TMP2]] to <2 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <2 x i64> [[TMP3]] to <2 x double>
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128d test_mm_cmp_pd_eq_uq(__m128d a, __m128d b) {
  return _mm_cmp_pd(a, b, _CMP_EQ_UQ);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmp_pd_nge_us(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = fcmp ult <2 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <2 x i1> [[TMP2]] to <2 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <2 x i64> [[TMP3]] to <2 x double>
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128d test_mm_cmp_pd_nge_us(__m128d a, __m128d b) {
  return _mm_cmp_pd(a, b, _CMP_NGE_US);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmp_pd_ngt_us(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = fcmp ule <2 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <2 x i1> [[TMP2]] to <2 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <2 x i64> [[TMP3]] to <2 x double>
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128d test_mm_cmp_pd_ngt_us(__m128d a, __m128d b) {
  return _mm_cmp_pd(a, b, _CMP_NGT_US);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmp_pd_false_oq(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = fcmp false <2 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <2 x i1> [[TMP2]] to <2 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <2 x i64> [[TMP3]] to <2 x double>
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128d test_mm_cmp_pd_false_oq(__m128d a, __m128d b) {
  return _mm_cmp_pd(a, b, _CMP_FALSE_OQ);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmp_pd_neq_oq(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = fcmp one <2 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <2 x i1> [[TMP2]] to <2 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <2 x i64> [[TMP3]] to <2 x double>
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128d test_mm_cmp_pd_neq_oq(__m128d a, __m128d b) {
  return _mm_cmp_pd(a, b, _CMP_NEQ_OQ);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmp_pd_ge_os(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = fcmp oge <2 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <2 x i1> [[TMP2]] to <2 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <2 x i64> [[TMP3]] to <2 x double>
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128d test_mm_cmp_pd_ge_os(__m128d a, __m128d b) {
  return _mm_cmp_pd(a, b, _CMP_GE_OS);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmp_pd_gt_os(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = fcmp ogt <2 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <2 x i1> [[TMP2]] to <2 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <2 x i64> [[TMP3]] to <2 x double>
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128d test_mm_cmp_pd_gt_os(__m128d a, __m128d b) {
  return _mm_cmp_pd(a, b, _CMP_GT_OS);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmp_pd_true_uq(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = fcmp true <2 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <2 x i1> [[TMP2]] to <2 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <2 x i64> [[TMP3]] to <2 x double>
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128d test_mm_cmp_pd_true_uq(__m128d a, __m128d b) {
  return _mm_cmp_pd(a, b, _CMP_TRUE_UQ);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmp_pd_eq_os(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = fcmp oeq <2 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <2 x i1> [[TMP2]] to <2 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <2 x i64> [[TMP3]] to <2 x double>
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128d test_mm_cmp_pd_eq_os(__m128d a, __m128d b) {
  return _mm_cmp_pd(a, b, _CMP_EQ_OS);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmp_pd_lt_oq(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = fcmp olt <2 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <2 x i1> [[TMP2]] to <2 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <2 x i64> [[TMP3]] to <2 x double>
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128d test_mm_cmp_pd_lt_oq(__m128d a, __m128d b) {
  return _mm_cmp_pd(a, b, _CMP_LT_OQ);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmp_pd_le_oq(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = fcmp ole <2 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <2 x i1> [[TMP2]] to <2 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <2 x i64> [[TMP3]] to <2 x double>
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128d test_mm_cmp_pd_le_oq(__m128d a, __m128d b) {
  return _mm_cmp_pd(a, b, _CMP_LE_OQ);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmp_pd_unord_s(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = fcmp uno <2 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <2 x i1> [[TMP2]] to <2 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <2 x i64> [[TMP3]] to <2 x double>
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128d test_mm_cmp_pd_unord_s(__m128d a, __m128d b) {
  return _mm_cmp_pd(a, b, _CMP_UNORD_S);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmp_pd_neq_us(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = fcmp une <2 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <2 x i1> [[TMP2]] to <2 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <2 x i64> [[TMP3]] to <2 x double>
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128d test_mm_cmp_pd_neq_us(__m128d a, __m128d b) {
  return _mm_cmp_pd(a, b, _CMP_NEQ_US);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmp_pd_nlt_uq(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = fcmp uge <2 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <2 x i1> [[TMP2]] to <2 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <2 x i64> [[TMP3]] to <2 x double>
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128d test_mm_cmp_pd_nlt_uq(__m128d a, __m128d b) {
  return _mm_cmp_pd(a, b, _CMP_NLT_UQ);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmp_pd_nle_uq(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = fcmp ugt <2 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <2 x i1> [[TMP2]] to <2 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <2 x i64> [[TMP3]] to <2 x double>
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128d test_mm_cmp_pd_nle_uq(__m128d a, __m128d b) {
  return _mm_cmp_pd(a, b, _CMP_NLE_UQ);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmp_pd_ord_s(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = fcmp ord <2 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <2 x i1> [[TMP2]] to <2 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <2 x i64> [[TMP3]] to <2 x double>
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128d test_mm_cmp_pd_ord_s(__m128d a, __m128d b) {
  return _mm_cmp_pd(a, b, _CMP_ORD_S);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmp_pd_eq_us(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = fcmp ueq <2 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <2 x i1> [[TMP2]] to <2 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <2 x i64> [[TMP3]] to <2 x double>
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128d test_mm_cmp_pd_eq_us(__m128d a, __m128d b) {
  return _mm_cmp_pd(a, b, _CMP_EQ_US);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmp_pd_nge_uq(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = fcmp ult <2 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <2 x i1> [[TMP2]] to <2 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <2 x i64> [[TMP3]] to <2 x double>
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128d test_mm_cmp_pd_nge_uq(__m128d a, __m128d b) {
  return _mm_cmp_pd(a, b, _CMP_NGE_UQ);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmp_pd_ngt_uq(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = fcmp ule <2 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <2 x i1> [[TMP2]] to <2 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <2 x i64> [[TMP3]] to <2 x double>
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128d test_mm_cmp_pd_ngt_uq(__m128d a, __m128d b) {
  return _mm_cmp_pd(a, b, _CMP_NGT_UQ);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmp_pd_false_os(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = fcmp false <2 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <2 x i1> [[TMP2]] to <2 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <2 x i64> [[TMP3]] to <2 x double>
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128d test_mm_cmp_pd_false_os(__m128d a, __m128d b) {
  return _mm_cmp_pd(a, b, _CMP_FALSE_OS);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmp_pd_neq_os(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = fcmp one <2 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <2 x i1> [[TMP2]] to <2 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <2 x i64> [[TMP3]] to <2 x double>
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128d test_mm_cmp_pd_neq_os(__m128d a, __m128d b) {
  return _mm_cmp_pd(a, b, _CMP_NEQ_OS);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmp_pd_ge_oq(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = fcmp oge <2 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <2 x i1> [[TMP2]] to <2 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <2 x i64> [[TMP3]] to <2 x double>
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128d test_mm_cmp_pd_ge_oq(__m128d a, __m128d b) {
  return _mm_cmp_pd(a, b, _CMP_GE_OQ);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmp_pd_gt_oq(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = fcmp ogt <2 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <2 x i1> [[TMP2]] to <2 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <2 x i64> [[TMP3]] to <2 x double>
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128d test_mm_cmp_pd_gt_oq(__m128d a, __m128d b) {
  return _mm_cmp_pd(a, b, _CMP_GT_OQ);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmp_pd_true_us(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = fcmp true <2 x double> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <2 x i1> [[TMP2]] to <2 x i64>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <2 x i64> [[TMP3]] to <2 x double>
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128d test_mm_cmp_pd_true_us(__m128d a, __m128d b) {
  return _mm_cmp_pd(a, b, _CMP_TRUE_US);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmp_ps_eq_uq(
// X86-SAME: <4 x float> noundef [[A:%.*]], <4 x float> noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <4 x float> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = fcmp ueq <4 x float> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <4 x i1> [[TMP2]] to <4 x i32>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <4 x i32> [[TMP3]] to <4 x float>
// X86-NEXT:    store <4 x float> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128 test_mm_cmp_ps_eq_uq(__m128 a, __m128 b) {
  return _mm_cmp_ps(a, b, _CMP_EQ_UQ);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmp_ps_nge_us(
// X86-SAME: <4 x float> noundef [[A:%.*]], <4 x float> noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <4 x float> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = fcmp ult <4 x float> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <4 x i1> [[TMP2]] to <4 x i32>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <4 x i32> [[TMP3]] to <4 x float>
// X86-NEXT:    store <4 x float> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128 test_mm_cmp_ps_nge_us(__m128 a, __m128 b) {
  return _mm_cmp_ps(a, b, _CMP_NGE_US);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmp_ps_ngt_us(
// X86-SAME: <4 x float> noundef [[A:%.*]], <4 x float> noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <4 x float> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = fcmp ule <4 x float> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <4 x i1> [[TMP2]] to <4 x i32>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <4 x i32> [[TMP3]] to <4 x float>
// X86-NEXT:    store <4 x float> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128 test_mm_cmp_ps_ngt_us(__m128 a, __m128 b) {
  return _mm_cmp_ps(a, b, _CMP_NGT_US);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmp_ps_false_oq(
// X86-SAME: <4 x float> noundef [[A:%.*]], <4 x float> noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <4 x float> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = fcmp false <4 x float> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <4 x i1> [[TMP2]] to <4 x i32>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <4 x i32> [[TMP3]] to <4 x float>
// X86-NEXT:    store <4 x float> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128 test_mm_cmp_ps_false_oq(__m128 a, __m128 b) {
  return _mm_cmp_ps(a, b, _CMP_FALSE_OQ);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmp_ps_neq_oq(
// X86-SAME: <4 x float> noundef [[A:%.*]], <4 x float> noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <4 x float> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = fcmp one <4 x float> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <4 x i1> [[TMP2]] to <4 x i32>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <4 x i32> [[TMP3]] to <4 x float>
// X86-NEXT:    store <4 x float> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128 test_mm_cmp_ps_neq_oq(__m128 a, __m128 b) {
  return _mm_cmp_ps(a, b, _CMP_NEQ_OQ);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmp_ps_ge_os(
// X86-SAME: <4 x float> noundef [[A:%.*]], <4 x float> noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <4 x float> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = fcmp oge <4 x float> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <4 x i1> [[TMP2]] to <4 x i32>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <4 x i32> [[TMP3]] to <4 x float>
// X86-NEXT:    store <4 x float> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128 test_mm_cmp_ps_ge_os(__m128 a, __m128 b) {
  return _mm_cmp_ps(a, b, _CMP_GE_OS);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmp_ps_gt_os(
// X86-SAME: <4 x float> noundef [[A:%.*]], <4 x float> noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <4 x float> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = fcmp ogt <4 x float> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <4 x i1> [[TMP2]] to <4 x i32>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <4 x i32> [[TMP3]] to <4 x float>
// X86-NEXT:    store <4 x float> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128 test_mm_cmp_ps_gt_os(__m128 a, __m128 b) {
  return _mm_cmp_ps(a, b, _CMP_GT_OS);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmp_ps_true_uq(
// X86-SAME: <4 x float> noundef [[A:%.*]], <4 x float> noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <4 x float> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = fcmp true <4 x float> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <4 x i1> [[TMP2]] to <4 x i32>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <4 x i32> [[TMP3]] to <4 x float>
// X86-NEXT:    store <4 x float> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128 test_mm_cmp_ps_true_uq(__m128 a, __m128 b) {
  return _mm_cmp_ps(a, b, _CMP_TRUE_UQ);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmp_ps_eq_os(
// X86-SAME: <4 x float> noundef [[A:%.*]], <4 x float> noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <4 x float> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = fcmp oeq <4 x float> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <4 x i1> [[TMP2]] to <4 x i32>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <4 x i32> [[TMP3]] to <4 x float>
// X86-NEXT:    store <4 x float> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128 test_mm_cmp_ps_eq_os(__m128 a, __m128 b) {
  return _mm_cmp_ps(a, b, _CMP_EQ_OS);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmp_ps_lt_oq(
// X86-SAME: <4 x float> noundef [[A:%.*]], <4 x float> noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <4 x float> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = fcmp olt <4 x float> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <4 x i1> [[TMP2]] to <4 x i32>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <4 x i32> [[TMP3]] to <4 x float>
// X86-NEXT:    store <4 x float> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128 test_mm_cmp_ps_lt_oq(__m128 a, __m128 b) {
  return _mm_cmp_ps(a, b, _CMP_LT_OQ);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmp_ps_le_oq(
// X86-SAME: <4 x float> noundef [[A:%.*]], <4 x float> noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <4 x float> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = fcmp ole <4 x float> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <4 x i1> [[TMP2]] to <4 x i32>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <4 x i32> [[TMP3]] to <4 x float>
// X86-NEXT:    store <4 x float> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128 test_mm_cmp_ps_le_oq(__m128 a, __m128 b) {
  return _mm_cmp_ps(a, b, _CMP_LE_OQ);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmp_ps_unord_s(
// X86-SAME: <4 x float> noundef [[A:%.*]], <4 x float> noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <4 x float> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = fcmp uno <4 x float> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <4 x i1> [[TMP2]] to <4 x i32>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <4 x i32> [[TMP3]] to <4 x float>
// X86-NEXT:    store <4 x float> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128 test_mm_cmp_ps_unord_s(__m128 a, __m128 b) {
  return _mm_cmp_ps(a, b, _CMP_UNORD_S);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmp_ps_neq_us(
// X86-SAME: <4 x float> noundef [[A:%.*]], <4 x float> noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <4 x float> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = fcmp une <4 x float> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <4 x i1> [[TMP2]] to <4 x i32>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <4 x i32> [[TMP3]] to <4 x float>
// X86-NEXT:    store <4 x float> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128 test_mm_cmp_ps_neq_us(__m128 a, __m128 b) {
  return _mm_cmp_ps(a, b, _CMP_NEQ_US);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmp_ps_nlt_uq(
// X86-SAME: <4 x float> noundef [[A:%.*]], <4 x float> noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <4 x float> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = fcmp uge <4 x float> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <4 x i1> [[TMP2]] to <4 x i32>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <4 x i32> [[TMP3]] to <4 x float>
// X86-NEXT:    store <4 x float> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128 test_mm_cmp_ps_nlt_uq(__m128 a, __m128 b) {
  return _mm_cmp_ps(a, b, _CMP_NLT_UQ);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmp_ps_nle_uq(
// X86-SAME: <4 x float> noundef [[A:%.*]], <4 x float> noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <4 x float> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = fcmp ugt <4 x float> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <4 x i1> [[TMP2]] to <4 x i32>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <4 x i32> [[TMP3]] to <4 x float>
// X86-NEXT:    store <4 x float> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128 test_mm_cmp_ps_nle_uq(__m128 a, __m128 b) {
  return _mm_cmp_ps(a, b, _CMP_NLE_UQ);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmp_ps_ord_s(
// X86-SAME: <4 x float> noundef [[A:%.*]], <4 x float> noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <4 x float> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = fcmp ord <4 x float> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <4 x i1> [[TMP2]] to <4 x i32>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <4 x i32> [[TMP3]] to <4 x float>
// X86-NEXT:    store <4 x float> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128 test_mm_cmp_ps_ord_s(__m128 a, __m128 b) {
  return _mm_cmp_ps(a, b, _CMP_ORD_S);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmp_ps_eq_us(
// X86-SAME: <4 x float> noundef [[A:%.*]], <4 x float> noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <4 x float> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = fcmp ueq <4 x float> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <4 x i1> [[TMP2]] to <4 x i32>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <4 x i32> [[TMP3]] to <4 x float>
// X86-NEXT:    store <4 x float> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128 test_mm_cmp_ps_eq_us(__m128 a, __m128 b) {
  return _mm_cmp_ps(a, b, _CMP_EQ_US);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmp_ps_nge_uq(
// X86-SAME: <4 x float> noundef [[A:%.*]], <4 x float> noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <4 x float> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = fcmp ult <4 x float> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <4 x i1> [[TMP2]] to <4 x i32>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <4 x i32> [[TMP3]] to <4 x float>
// X86-NEXT:    store <4 x float> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128 test_mm_cmp_ps_nge_uq(__m128 a, __m128 b) {
  return _mm_cmp_ps(a, b, _CMP_NGE_UQ);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmp_ps_ngt_uq(
// X86-SAME: <4 x float> noundef [[A:%.*]], <4 x float> noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <4 x float> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = fcmp ule <4 x float> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <4 x i1> [[TMP2]] to <4 x i32>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <4 x i32> [[TMP3]] to <4 x float>
// X86-NEXT:    store <4 x float> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128 test_mm_cmp_ps_ngt_uq(__m128 a, __m128 b) {
  return _mm_cmp_ps(a, b, _CMP_NGT_UQ);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmp_ps_false_os(
// X86-SAME: <4 x float> noundef [[A:%.*]], <4 x float> noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <4 x float> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = fcmp false <4 x float> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <4 x i1> [[TMP2]] to <4 x i32>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <4 x i32> [[TMP3]] to <4 x float>
// X86-NEXT:    store <4 x float> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128 test_mm_cmp_ps_false_os(__m128 a, __m128 b) {
  return _mm_cmp_ps(a, b, _CMP_FALSE_OS);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmp_ps_neq_os(
// X86-SAME: <4 x float> noundef [[A:%.*]], <4 x float> noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <4 x float> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = fcmp one <4 x float> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <4 x i1> [[TMP2]] to <4 x i32>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <4 x i32> [[TMP3]] to <4 x float>
// X86-NEXT:    store <4 x float> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128 test_mm_cmp_ps_neq_os(__m128 a, __m128 b) {
  return _mm_cmp_ps(a, b, _CMP_NEQ_OS);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmp_ps_ge_oq(
// X86-SAME: <4 x float> noundef [[A:%.*]], <4 x float> noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <4 x float> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = fcmp oge <4 x float> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <4 x i1> [[TMP2]] to <4 x i32>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <4 x i32> [[TMP3]] to <4 x float>
// X86-NEXT:    store <4 x float> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128 test_mm_cmp_ps_ge_oq(__m128 a, __m128 b) {
  return _mm_cmp_ps(a, b, _CMP_GE_OQ);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmp_ps_gt_oq(
// X86-SAME: <4 x float> noundef [[A:%.*]], <4 x float> noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <4 x float> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = fcmp ogt <4 x float> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <4 x i1> [[TMP2]] to <4 x i32>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <4 x i32> [[TMP3]] to <4 x float>
// X86-NEXT:    store <4 x float> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128 test_mm_cmp_ps_gt_oq(__m128 a, __m128 b) {
  return _mm_cmp_ps(a, b, _CMP_GT_OQ);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmp_ps_true_us(
// X86-SAME: <4 x float> noundef [[A:%.*]], <4 x float> noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <4 x float> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = fcmp true <4 x float> [[TMP0]], [[TMP1]]
// X86-NEXT:    [[TMP3:%.*]] = sext <4 x i1> [[TMP2]] to <4 x i32>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <4 x i32> [[TMP3]] to <4 x float>
// X86-NEXT:    store <4 x float> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128 test_mm_cmp_ps_true_us(__m128 a, __m128 b) {
  return _mm_cmp_ps(a, b, _CMP_TRUE_US);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmp_sd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = call <2 x double> @llvm.x86.sse2.cmp.sd(<2 x double> [[TMP0]], <2 x double> [[TMP1]], i8 13)
// X86-NEXT:    store <2 x double> [[TMP2]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP3]]
//
__m128d test_mm_cmp_sd(__m128d A, __m128d B) {
  return _mm_cmp_sd(A, B, _CMP_GE_OS);
}

//
// X86-LABEL: define <2 x i64> @test_mm_cmp_ss(
// X86-SAME: <4 x float> noundef [[A:%.*]], <4 x float> noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <4 x float> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = call <4 x float> @llvm.x86.sse.cmp.ss(<4 x float> [[TMP0]], <4 x float> [[TMP1]], i8 13)
// X86-NEXT:    store <4 x float> [[TMP2]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP3]]
//
__m128 test_mm_cmp_ss(__m128 A, __m128 B) {
  return _mm_cmp_ss(A, B, _CMP_GE_OS);
}

//
// X86-LABEL: define void @test_mm256_cvtepi32_pd(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <2 x i64> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META72:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META72]]
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16, !noalias [[META72]]
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16, !noalias [[META72]]
// X86-NEXT:    [[TMP2:%.*]] = bitcast <2 x i64> [[TMP1]] to <4 x i32>
// X86-NEXT:    [[CONV_I:%.*]] = sitofp <4 x i32> [[TMP2]] to <4 x double>
// X86-NEXT:    store <4 x double> [[CONV_I]], ptr [[TMP]], align 32, !alias.scope [[META72]]
// X86-NEXT:    [[TMP3:%.*]] = load <4 x double>, ptr [[TMP]], align 32, !alias.scope [[META72]]
// X86-NEXT:    store <4 x double> [[TMP3]], ptr [[TMP]], align 32, !alias.scope [[META72]]
// X86-NEXT:    [[TMP4:%.*]] = load <4 x double>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x double> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_cvtepi32_pd(__m128i A) {
  return _mm256_cvtepi32_pd(A);
}

//
// X86-LABEL: define void @test_mm256_cvtepi32_ps(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <4 x i64> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    [[TMP:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x i64> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x i64>, ptr [[A_ADDR]], align 32
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META75:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META75]]
// X86-NEXT:    store <4 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 32, !noalias [[META75]]
// X86-NEXT:    [[TMP1:%.*]] = load <4 x i64>, ptr [[__A_ADDR_I]], align 32, !noalias [[META75]]
// X86-NEXT:    [[TMP2:%.*]] = bitcast <4 x i64> [[TMP1]] to <8 x i32>
// X86-NEXT:    [[CONV_I:%.*]] = sitofp <8 x i32> [[TMP2]] to <8 x float>
// X86-NEXT:    store <8 x float> [[CONV_I]], ptr [[TMP]], align 32, !alias.scope [[META75]]
// X86-NEXT:    [[TMP3:%.*]] = load <8 x float>, ptr [[TMP]], align 32, !alias.scope [[META75]]
// X86-NEXT:    store <8 x float> [[TMP3]], ptr [[TMP]], align 32, !alias.scope [[META75]]
// X86-NEXT:    [[TMP4:%.*]] = load <8 x float>, ptr [[TMP]], align 32
// X86-NEXT:    store <8 x float> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_cvtepi32_ps(__m256i A) {
  return _mm256_cvtepi32_ps(A);
}

//
// X86-LABEL: define <2 x i64> @test_mm256_cvtpd_epi32(
// X86-SAME: <4 x double> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    [[TMP2:%.*]] = call <4 x i32> @llvm.x86.avx.cvt.pd2dq.256(<4 x double> [[TMP1]])
// X86-NEXT:    [[TMP3:%.*]] = bitcast <4 x i32> [[TMP2]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP3]]
//
__m128i test_mm256_cvtpd_epi32(__m256d A) {
  return _mm256_cvtpd_epi32(A);
}

//
// X86-LABEL: define <2 x i64> @test_mm256_cvtpd_ps(
// X86-SAME: <4 x double> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[RETVAL:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[COERCE:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    [[TMP2:%.*]] = call <4 x float> @llvm.x86.avx.cvt.pd2.ps.256(<4 x double> [[TMP1]])
// X86-NEXT:    store <4 x float> [[TMP2]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP3]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP4:%.*]] = load <4 x float>, ptr [[COERCE]], align 16
// X86-NEXT:    store <4 x float> [[TMP4]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP5]]
//
__m128 test_mm256_cvtpd_ps(__m256d A) {
  return _mm256_cvtpd_ps(A);
}

//
// X86-LABEL: define void @test_mm256_cvtps_epi32(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x i64>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META78:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META78]]
// X86-NEXT:    store <8 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 32, !noalias [[META78]]
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[__A_ADDR_I]], align 32, !noalias [[META78]]
// X86-NEXT:    [[TMP2:%.*]] = call <8 x i32> @llvm.x86.avx.cvt.ps2dq.256(<8 x float> [[TMP1]])
// X86-NEXT:    [[TMP3:%.*]] = bitcast <8 x i32> [[TMP2]] to <4 x i64>
// X86-NEXT:    store <4 x i64> [[TMP3]], ptr [[TMP]], align 32, !alias.scope [[META78]]
// X86-NEXT:    [[TMP4:%.*]] = load <4 x i64>, ptr [[TMP]], align 32, !alias.scope [[META78]]
// X86-NEXT:    store <4 x i64> [[TMP4]], ptr [[TMP]], align 32, !alias.scope [[META78]]
// X86-NEXT:    [[TMP5:%.*]] = load <4 x i64>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x i64> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP6:%.*]] = load <4 x i64>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x i64> [[TMP6]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256i test_mm256_cvtps_epi32(__m256 A) {
  return _mm256_cvtps_epi32(A);
}

//
// X86-LABEL: define void @test_mm256_cvtps_pd(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x float> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META81:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META81]]
// X86-NEXT:    store <4 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 16, !noalias [[META81]]
// X86-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[__A_ADDR_I]], align 16, !noalias [[META81]]
// X86-NEXT:    [[CONV_I:%.*]] = fpext <4 x float> [[TMP1]] to <4 x double>
// X86-NEXT:    store <4 x double> [[CONV_I]], ptr [[TMP]], align 32, !alias.scope [[META81]]
// X86-NEXT:    [[TMP2:%.*]] = load <4 x double>, ptr [[TMP]], align 32, !alias.scope [[META81]]
// X86-NEXT:    store <4 x double> [[TMP2]], ptr [[TMP]], align 32, !alias.scope [[META81]]
// X86-NEXT:    [[TMP3:%.*]] = load <4 x double>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x double> [[TMP3]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP4:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_cvtps_pd(__m128 A) {
  return _mm256_cvtps_pd(A);
}

//
// X86-LABEL: define <2 x i64> @test_mm256_cvttpd_epi32(
// X86-SAME: <4 x double> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    [[TMP2:%.*]] = call <4 x i32> @llvm.x86.avx.cvtt.pd2dq.256(<4 x double> [[TMP1]])
// X86-NEXT:    [[TMP3:%.*]] = bitcast <4 x i32> [[TMP2]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP3]]
//
__m128i test_mm256_cvttpd_epi32(__m256d A) {
  return _mm256_cvttpd_epi32(A);
}

//
// X86-LABEL: define void @test_mm256_cvttps_epi32(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x i64>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META84:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META84]]
// X86-NEXT:    store <8 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 32, !noalias [[META84]]
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[__A_ADDR_I]], align 32, !noalias [[META84]]
// X86-NEXT:    [[TMP2:%.*]] = call <8 x i32> @llvm.x86.avx.cvtt.ps2dq.256(<8 x float> [[TMP1]])
// X86-NEXT:    [[TMP3:%.*]] = bitcast <8 x i32> [[TMP2]] to <4 x i64>
// X86-NEXT:    store <4 x i64> [[TMP3]], ptr [[TMP]], align 32, !alias.scope [[META84]]
// X86-NEXT:    [[TMP4:%.*]] = load <4 x i64>, ptr [[TMP]], align 32, !alias.scope [[META84]]
// X86-NEXT:    store <4 x i64> [[TMP4]], ptr [[TMP]], align 32, !alias.scope [[META84]]
// X86-NEXT:    [[TMP5:%.*]] = load <4 x i64>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x i64> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP6:%.*]] = load <4 x i64>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x i64> [[TMP6]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256i test_mm256_cvttps_epi32(__m256 A) {
  return _mm256_cvttps_epi32(A);
}

//
// X86-LABEL: define void @test_mm256_div_pd(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[A:%.*]], <4 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[B_ADDR]], align 32
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META87:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META87]]
// X86-NEXT:    store <4 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 32, !noalias [[META87]]
// X86-NEXT:    store <4 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 32, !noalias [[META87]]
// X86-NEXT:    [[TMP2:%.*]] = load <4 x double>, ptr [[__A_ADDR_I]], align 32, !noalias [[META87]]
// X86-NEXT:    [[TMP3:%.*]] = load <4 x double>, ptr [[__B_ADDR_I]], align 32, !noalias [[META87]]
// X86-NEXT:    [[DIV_I:%.*]] = fdiv <4 x double> [[TMP2]], [[TMP3]]
// X86-NEXT:    store <4 x double> [[DIV_I]], ptr [[TMP]], align 32, !alias.scope [[META87]]
// X86-NEXT:    [[TMP4:%.*]] = load <4 x double>, ptr [[TMP]], align 32, !alias.scope [[META87]]
// X86-NEXT:    store <4 x double> [[TMP4]], ptr [[TMP]], align 32, !alias.scope [[META87]]
// X86-NEXT:    [[TMP5:%.*]] = load <4 x double>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x double> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP6:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP6]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_div_pd(__m256d A, __m256d B) {
  return _mm256_div_pd(A, B);
}

//
// X86-LABEL: define void @test_mm256_div_ps(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]], <8 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[TMP:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[B_ADDR]], align 32
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META90:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META90]]
// X86-NEXT:    store <8 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 32, !noalias [[META90]]
// X86-NEXT:    store <8 x float> [[TMP1]], ptr [[__B_ADDR_I]], align 32, !noalias [[META90]]
// X86-NEXT:    [[TMP2:%.*]] = load <8 x float>, ptr [[__A_ADDR_I]], align 32, !noalias [[META90]]
// X86-NEXT:    [[TMP3:%.*]] = load <8 x float>, ptr [[__B_ADDR_I]], align 32, !noalias [[META90]]
// X86-NEXT:    [[DIV_I:%.*]] = fdiv <8 x float> [[TMP2]], [[TMP3]]
// X86-NEXT:    store <8 x float> [[DIV_I]], ptr [[TMP]], align 32, !alias.scope [[META90]]
// X86-NEXT:    [[TMP4:%.*]] = load <8 x float>, ptr [[TMP]], align 32, !alias.scope [[META90]]
// X86-NEXT:    store <8 x float> [[TMP4]], ptr [[TMP]], align 32, !alias.scope [[META90]]
// X86-NEXT:    [[TMP5:%.*]] = load <8 x float>, ptr [[TMP]], align 32
// X86-NEXT:    store <8 x float> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP6:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP6]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_div_ps(__m256 A, __m256 B) {
  return _mm256_div_ps(A, B);
}

//
// X86-LABEL: define void @test_mm256_dp_ps(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]], <8 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = call <8 x float> @llvm.x86.avx.dp.ps.256(<8 x float> [[TMP0]], <8 x float> [[TMP1]], i8 7)
// X86-NEXT:    store <8 x float> [[TMP2]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP3:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP3]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_dp_ps(__m256 A, __m256 B) {
  return _mm256_dp_ps(A, B, 7);
}

//
// X86-LABEL: define i32 @test_mm256_extract_epi8(
// X86-SAME: <4 x i64> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    store <4 x i64> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x i64>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = bitcast <4 x i64> [[TMP0]] to <32 x i8>
// X86-NEXT:    [[TMP2:%.*]] = extractelement <32 x i8> [[TMP1]], i64 31
// X86-NEXT:    [[CONV:%.*]] = zext i8 [[TMP2]] to i32
// X86-NEXT:    ret i32 [[CONV]]
//
int test_mm256_extract_epi8(__m256i A) {
  return _mm256_extract_epi8(A, 31);
}

//
// X86-LABEL: define i32 @test_mm256_extract_epi16(
// X86-SAME: <4 x i64> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    store <4 x i64> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x i64>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = bitcast <4 x i64> [[TMP0]] to <16 x i16>
// X86-NEXT:    [[TMP2:%.*]] = extractelement <16 x i16> [[TMP1]], i64 15
// X86-NEXT:    [[CONV:%.*]] = zext i16 [[TMP2]] to i32
// X86-NEXT:    ret i32 [[CONV]]
//
int test_mm256_extract_epi16(__m256i A) {
  return _mm256_extract_epi16(A, 15);
}

//
// X86-LABEL: define i32 @test_mm256_extract_epi32(
// X86-SAME: <4 x i64> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    store <4 x i64> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x i64>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = bitcast <4 x i64> [[TMP0]] to <8 x i32>
// X86-NEXT:    [[TMP2:%.*]] = extractelement <8 x i32> [[TMP1]], i64 7
// X86-NEXT:    ret i32 [[TMP2]]
//
int test_mm256_extract_epi32(__m256i A) {
  return _mm256_extract_epi32(A, 7);
}

#if __x86_64__
//
long long test_mm256_extract_epi64(__m256i A) {
  return _mm256_extract_epi64(A, 3);
}
#endif

//
// X86-LABEL: define <2 x i64> @test_mm256_extractf128_pd(
// X86-SAME: <4 x double> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[EXTRACT:%.*]] = shufflevector <4 x double> [[TMP0]], <4 x double> poison, <2 x i32> <i32 2, i32 3>
// X86-NEXT:    store <2 x double> [[EXTRACT]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP1]]
//
__m128d test_mm256_extractf128_pd(__m256d A) {
  return _mm256_extractf128_pd(A, 1);
}

//
// X86-LABEL: define <2 x i64> @test_mm256_extractf128_ps(
// X86-SAME: <8 x float> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[EXTRACT:%.*]] = shufflevector <8 x float> [[TMP0]], <8 x float> poison, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
// X86-NEXT:    store <4 x float> [[EXTRACT]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP1]]
//
__m128 test_mm256_extractf128_ps(__m256 A) {
  return _mm256_extractf128_ps(A, 1);
}

//
// X86-LABEL: define <2 x i64> @test_mm256_extractf128_si256(
// X86-SAME: <4 x i64> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    store <4 x i64> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x i64>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = bitcast <4 x i64> [[TMP0]] to <8 x i32>
// X86-NEXT:    [[EXTRACT:%.*]] = shufflevector <8 x i32> [[TMP1]], <8 x i32> poison, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
// X86-NEXT:    [[TMP2:%.*]] = bitcast <4 x i32> [[EXTRACT]] to <2 x i64>
// X86-NEXT:    ret <2 x i64> [[TMP2]]
//
__m128i test_mm256_extractf128_si256(__m256i A) {
  return _mm256_extractf128_si256(A, 1);
}

//
// X86-LABEL: define void @test_mm256_floor_pd(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[X:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[X_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[X]], ptr [[X_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[X_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = call <4 x double> @llvm.x86.avx.round.pd.256(<4 x double> [[TMP0]], i32 1)
// X86-NEXT:    store <4 x double> [[TMP1]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP2:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP2]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_floor_pd(__m256d x) {
  return _mm256_floor_pd(x);
}

//
// X86-LABEL: define void @test_mm_floor_ps(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[X:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[X_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[X]], ptr [[X_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[X_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = call <8 x float> @llvm.x86.avx.round.ps.256(<8 x float> [[TMP0]], i32 1)
// X86-NEXT:    store <8 x float> [[TMP1]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP2:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP2]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm_floor_ps(__m256 x) {
  return _mm256_floor_ps(x);
}

//
// X86-LABEL: define void @test_mm256_hadd_pd(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[A:%.*]], <4 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[B_ADDR]], align 32
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META93:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META93]]
// X86-NEXT:    store <4 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 32, !noalias [[META93]]
// X86-NEXT:    store <4 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 32, !noalias [[META93]]
// X86-NEXT:    [[TMP2:%.*]] = load <4 x double>, ptr [[__A_ADDR_I]], align 32, !noalias [[META93]]
// X86-NEXT:    [[TMP3:%.*]] = load <4 x double>, ptr [[__B_ADDR_I]], align 32, !noalias [[META93]]
// X86-NEXT:    [[TMP4:%.*]] = call <4 x double> @llvm.x86.avx.hadd.pd.256(<4 x double> [[TMP2]], <4 x double> [[TMP3]])
// X86-NEXT:    store <4 x double> [[TMP4]], ptr [[TMP]], align 32, !alias.scope [[META93]]
// X86-NEXT:    [[TMP5:%.*]] = load <4 x double>, ptr [[TMP]], align 32, !alias.scope [[META93]]
// X86-NEXT:    store <4 x double> [[TMP5]], ptr [[TMP]], align 32, !alias.scope [[META93]]
// X86-NEXT:    [[TMP6:%.*]] = load <4 x double>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x double> [[TMP6]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP7:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP7]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_hadd_pd(__m256d A, __m256d B) {
  return _mm256_hadd_pd(A, B);
}

//
// X86-LABEL: define void @test_mm256_hadd_ps(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]], <8 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[TMP:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[B_ADDR]], align 32
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META96:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META96]]
// X86-NEXT:    store <8 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 32, !noalias [[META96]]
// X86-NEXT:    store <8 x float> [[TMP1]], ptr [[__B_ADDR_I]], align 32, !noalias [[META96]]
// X86-NEXT:    [[TMP2:%.*]] = load <8 x float>, ptr [[__A_ADDR_I]], align 32, !noalias [[META96]]
// X86-NEXT:    [[TMP3:%.*]] = load <8 x float>, ptr [[__B_ADDR_I]], align 32, !noalias [[META96]]
// X86-NEXT:    [[TMP4:%.*]] = call <8 x float> @llvm.x86.avx.hadd.ps.256(<8 x float> [[TMP2]], <8 x float> [[TMP3]])
// X86-NEXT:    store <8 x float> [[TMP4]], ptr [[TMP]], align 32, !alias.scope [[META96]]
// X86-NEXT:    [[TMP5:%.*]] = load <8 x float>, ptr [[TMP]], align 32, !alias.scope [[META96]]
// X86-NEXT:    store <8 x float> [[TMP5]], ptr [[TMP]], align 32, !alias.scope [[META96]]
// X86-NEXT:    [[TMP6:%.*]] = load <8 x float>, ptr [[TMP]], align 32
// X86-NEXT:    store <8 x float> [[TMP6]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP7:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP7]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_hadd_ps(__m256 A, __m256 B) {
  return _mm256_hadd_ps(A, B);
}

//
// X86-LABEL: define void @test_mm256_hsub_pd(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[A:%.*]], <4 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[B_ADDR]], align 32
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META99:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META99]]
// X86-NEXT:    store <4 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 32, !noalias [[META99]]
// X86-NEXT:    store <4 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 32, !noalias [[META99]]
// X86-NEXT:    [[TMP2:%.*]] = load <4 x double>, ptr [[__A_ADDR_I]], align 32, !noalias [[META99]]
// X86-NEXT:    [[TMP3:%.*]] = load <4 x double>, ptr [[__B_ADDR_I]], align 32, !noalias [[META99]]
// X86-NEXT:    [[TMP4:%.*]] = call <4 x double> @llvm.x86.avx.hsub.pd.256(<4 x double> [[TMP2]], <4 x double> [[TMP3]])
// X86-NEXT:    store <4 x double> [[TMP4]], ptr [[TMP]], align 32, !alias.scope [[META99]]
// X86-NEXT:    [[TMP5:%.*]] = load <4 x double>, ptr [[TMP]], align 32, !alias.scope [[META99]]
// X86-NEXT:    store <4 x double> [[TMP5]], ptr [[TMP]], align 32, !alias.scope [[META99]]
// X86-NEXT:    [[TMP6:%.*]] = load <4 x double>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x double> [[TMP6]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP7:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP7]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_hsub_pd(__m256d A, __m256d B) {
  return _mm256_hsub_pd(A, B);
}

//
// X86-LABEL: define void @test_mm256_hsub_ps(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]], <8 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[TMP:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[B_ADDR]], align 32
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META102:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META102]]
// X86-NEXT:    store <8 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 32, !noalias [[META102]]
// X86-NEXT:    store <8 x float> [[TMP1]], ptr [[__B_ADDR_I]], align 32, !noalias [[META102]]
// X86-NEXT:    [[TMP2:%.*]] = load <8 x float>, ptr [[__A_ADDR_I]], align 32, !noalias [[META102]]
// X86-NEXT:    [[TMP3:%.*]] = load <8 x float>, ptr [[__B_ADDR_I]], align 32, !noalias [[META102]]
// X86-NEXT:    [[TMP4:%.*]] = call <8 x float> @llvm.x86.avx.hsub.ps.256(<8 x float> [[TMP2]], <8 x float> [[TMP3]])
// X86-NEXT:    store <8 x float> [[TMP4]], ptr [[TMP]], align 32, !alias.scope [[META102]]
// X86-NEXT:    [[TMP5:%.*]] = load <8 x float>, ptr [[TMP]], align 32, !alias.scope [[META102]]
// X86-NEXT:    store <8 x float> [[TMP5]], ptr [[TMP]], align 32, !alias.scope [[META102]]
// X86-NEXT:    [[TMP6:%.*]] = load <8 x float>, ptr [[TMP]], align 32
// X86-NEXT:    store <8 x float> [[TMP6]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP7:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP7]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_hsub_ps(__m256 A, __m256 B) {
  return _mm256_hsub_ps(A, B);
}

//
__m256i test_mm256_insert_epi8(__m256i x, char b) {
  return _mm256_insert_epi8(x, b, 14);
}

//
// X86-LABEL: define void @test_mm256_insert_epi16(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x i64>) align 32 [[AGG_RESULT:%.*]], <4 x i64> noundef [[X:%.*]], i32 noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[X_ADDR:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x i64> [[X]], ptr [[X_ADDR]], align 32
// X86-NEXT:    store i32 [[B]], ptr [[B_ADDR]], align 4
// X86-NEXT:    [[TMP0:%.*]] = load <4 x i64>, ptr [[X_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = bitcast <4 x i64> [[TMP0]] to <16 x i16>
// X86-NEXT:    [[TMP2:%.*]] = load i32, ptr [[B_ADDR]], align 4
// X86-NEXT:    [[CONV:%.*]] = trunc i32 [[TMP2]] to i16
// X86-NEXT:    [[TMP3:%.*]] = insertelement <16 x i16> [[TMP1]], i16 [[CONV]], i64 4
// X86-NEXT:    [[TMP4:%.*]] = bitcast <16 x i16> [[TMP3]] to <4 x i64>
// X86-NEXT:    store <4 x i64> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <4 x i64>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x i64> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256i test_mm256_insert_epi16(__m256i x, int b) {
  return _mm256_insert_epi16(x, b, 4);
}

//
// X86-LABEL: define void @test_mm256_insert_epi32(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x i64>) align 32 [[AGG_RESULT:%.*]], <4 x i64> noundef [[X:%.*]], i32 noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[X_ADDR:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x i64> [[X]], ptr [[X_ADDR]], align 32
// X86-NEXT:    store i32 [[B]], ptr [[B_ADDR]], align 4
// X86-NEXT:    [[TMP0:%.*]] = load <4 x i64>, ptr [[X_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = bitcast <4 x i64> [[TMP0]] to <8 x i32>
// X86-NEXT:    [[TMP2:%.*]] = load i32, ptr [[B_ADDR]], align 4
// X86-NEXT:    [[TMP3:%.*]] = insertelement <8 x i32> [[TMP1]], i32 [[TMP2]], i64 5
// X86-NEXT:    [[TMP4:%.*]] = bitcast <8 x i32> [[TMP3]] to <4 x i64>
// X86-NEXT:    store <4 x i64> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <4 x i64>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x i64> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256i test_mm256_insert_epi32(__m256i x, int b) {
  return _mm256_insert_epi32(x, b, 5);
}

#if __x86_64__
//
__m256i test_mm256_insert_epi64(__m256i x, long long b) {
  return _mm256_insert_epi64(x, b, 2);
}
#endif

//
// X86-LABEL: define void @test_mm256_insertf128_pd(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[WIDEN:%.*]] = shufflevector <2 x double> [[TMP1]], <2 x double> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 3>
// X86-NEXT:    [[INSERT:%.*]] = shufflevector <4 x double> [[TMP0]], <4 x double> [[WIDEN]], <4 x i32> <i32 4, i32 5, i32 2, i32 3>
// X86-NEXT:    store <4 x double> [[INSERT]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP2:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP2]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_insertf128_pd(__m256d A, __m128d B) {
  return _mm256_insertf128_pd(A, B, 0);
}

//
// X86-LABEL: define void @test_mm256_insertf128_ps(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]], <4 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x float> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[WIDEN:%.*]] = shufflevector <4 x float> [[TMP1]], <4 x float> poison, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
// X86-NEXT:    [[INSERT:%.*]] = shufflevector <8 x float> [[TMP0]], <8 x float> [[WIDEN]], <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
// X86-NEXT:    store <8 x float> [[INSERT]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP2:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP2]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_insertf128_ps(__m256 A, __m128 B) {
  return _mm256_insertf128_ps(A, B, 1);
}

//
// X86-LABEL: define void @test_mm256_insertf128_si256(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x i64>) align 32 [[AGG_RESULT:%.*]], <4 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x i64> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <4 x i64>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = bitcast <4 x i64> [[TMP0]] to <8 x i32>
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP3:%.*]] = bitcast <2 x i64> [[TMP2]] to <4 x i32>
// X86-NEXT:    [[WIDEN:%.*]] = shufflevector <4 x i32> [[TMP3]], <4 x i32> poison, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
// X86-NEXT:    [[INSERT:%.*]] = shufflevector <8 x i32> [[TMP1]], <8 x i32> [[WIDEN]], <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 4, i32 5, i32 6, i32 7>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <8 x i32> [[INSERT]] to <4 x i64>
// X86-NEXT:    store <4 x i64> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <4 x i64>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x i64> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256i test_mm256_insertf128_si256(__m256i A, __m128i B) {
  return _mm256_insertf128_si256(A, B, 0);
}

//
// X86-LABEL: define void @test_mm256_lddqu_si256(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x i64>) align 32 [[AGG_RESULT:%.*]], ptr noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__P_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META105:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META105]]
// X86-NEXT:    store ptr [[TMP0]], ptr [[__P_ADDR_I]], align 4, !noalias [[META105]]
// X86-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[__P_ADDR_I]], align 4, !noalias [[META105]]
// X86-NEXT:    [[TMP2:%.*]] = call <32 x i8> @llvm.x86.avx.ldu.dq.256(ptr [[TMP1]])
// X86-NEXT:    [[TMP3:%.*]] = bitcast <32 x i8> [[TMP2]] to <4 x i64>
// X86-NEXT:    store <4 x i64> [[TMP3]], ptr [[TMP]], align 32, !alias.scope [[META105]]
// X86-NEXT:    [[TMP4:%.*]] = load <4 x i64>, ptr [[TMP]], align 32, !alias.scope [[META105]]
// X86-NEXT:    store <4 x i64> [[TMP4]], ptr [[TMP]], align 32, !alias.scope [[META105]]
// X86-NEXT:    [[TMP5:%.*]] = load <4 x i64>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x i64> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP6:%.*]] = load <4 x i64>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x i64> [[TMP6]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256i test_mm256_lddqu_si256(__m256i* A) {
  return _mm256_lddqu_si256(A);
}

//
// X86-LABEL: define void @test_mm256_load_pd(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], ptr noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__P_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META108:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META108]]
// X86-NEXT:    store ptr [[TMP0]], ptr [[__P_ADDR_I]], align 4, !noalias [[META108]]
// X86-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[__P_ADDR_I]], align 4, !noalias [[META108]]
// X86-NEXT:    [[TMP2:%.*]] = load <4 x double>, ptr [[TMP1]], align 32
// X86-NEXT:    store <4 x double> [[TMP2]], ptr [[TMP]], align 32, !alias.scope [[META108]]
// X86-NEXT:    [[TMP3:%.*]] = load <4 x double>, ptr [[TMP]], align 32, !alias.scope [[META108]]
// X86-NEXT:    store <4 x double> [[TMP3]], ptr [[TMP]], align 32, !alias.scope [[META108]]
// X86-NEXT:    [[TMP4:%.*]] = load <4 x double>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x double> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_load_pd(double* A) {
  return _mm256_load_pd(A);
}

//
// X86-LABEL: define void @test_mm256_load_ps(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], ptr noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__P_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[TMP:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META111:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META111]]
// X86-NEXT:    store ptr [[TMP0]], ptr [[__P_ADDR_I]], align 4, !noalias [[META111]]
// X86-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[__P_ADDR_I]], align 4, !noalias [[META111]]
// X86-NEXT:    [[TMP2:%.*]] = load <8 x float>, ptr [[TMP1]], align 32
// X86-NEXT:    store <8 x float> [[TMP2]], ptr [[TMP]], align 32, !alias.scope [[META111]]
// X86-NEXT:    [[TMP3:%.*]] = load <8 x float>, ptr [[TMP]], align 32, !alias.scope [[META111]]
// X86-NEXT:    store <8 x float> [[TMP3]], ptr [[TMP]], align 32, !alias.scope [[META111]]
// X86-NEXT:    [[TMP4:%.*]] = load <8 x float>, ptr [[TMP]], align 32
// X86-NEXT:    store <8 x float> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_load_ps(float* A) {
  return _mm256_load_ps(A);
}

//
// X86-LABEL: define void @test_mm256_load_si256(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x i64>) align 32 [[AGG_RESULT:%.*]], ptr noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__P_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META114:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META114]]
// X86-NEXT:    store ptr [[TMP0]], ptr [[__P_ADDR_I]], align 4, !noalias [[META114]]
// X86-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[__P_ADDR_I]], align 4, !noalias [[META114]]
// X86-NEXT:    [[TMP2:%.*]] = load <4 x i64>, ptr [[TMP1]], align 32
// X86-NEXT:    store <4 x i64> [[TMP2]], ptr [[TMP]], align 32, !alias.scope [[META114]]
// X86-NEXT:    [[TMP3:%.*]] = load <4 x i64>, ptr [[TMP]], align 32, !alias.scope [[META114]]
// X86-NEXT:    store <4 x i64> [[TMP3]], ptr [[TMP]], align 32, !alias.scope [[META114]]
// X86-NEXT:    [[TMP4:%.*]] = load <4 x i64>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x i64> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <4 x i64>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x i64> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256i test_mm256_load_si256(__m256i* A) {
  return _mm256_load_si256(A);
}

//
// X86-LABEL: define void @test_mm256_loadu_pd(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], ptr noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__P_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META117:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META117]]
// X86-NEXT:    store ptr [[TMP0]], ptr [[__P_ADDR_I]], align 4, !noalias [[META117]]
// X86-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[__P_ADDR_I]], align 4, !noalias [[META117]]
// X86-NEXT:    [[TMP2:%.*]] = load <4 x double>, ptr [[TMP1]], align 1
// X86-NEXT:    store <4 x double> [[TMP2]], ptr [[TMP]], align 32, !alias.scope [[META117]]
// X86-NEXT:    [[TMP3:%.*]] = load <4 x double>, ptr [[TMP]], align 32, !alias.scope [[META117]]
// X86-NEXT:    store <4 x double> [[TMP3]], ptr [[TMP]], align 32, !alias.scope [[META117]]
// X86-NEXT:    [[TMP4:%.*]] = load <4 x double>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x double> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_loadu_pd(double* A) {
  return _mm256_loadu_pd(A);
}

//
// X86-LABEL: define void @test_mm256_loadu_ps(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], ptr noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__P_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[TMP:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META120:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META120]]
// X86-NEXT:    store ptr [[TMP0]], ptr [[__P_ADDR_I]], align 4, !noalias [[META120]]
// X86-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[__P_ADDR_I]], align 4, !noalias [[META120]]
// X86-NEXT:    [[TMP2:%.*]] = load <8 x float>, ptr [[TMP1]], align 1
// X86-NEXT:    store <8 x float> [[TMP2]], ptr [[TMP]], align 32, !alias.scope [[META120]]
// X86-NEXT:    [[TMP3:%.*]] = load <8 x float>, ptr [[TMP]], align 32, !alias.scope [[META120]]
// X86-NEXT:    store <8 x float> [[TMP3]], ptr [[TMP]], align 32, !alias.scope [[META120]]
// X86-NEXT:    [[TMP4:%.*]] = load <8 x float>, ptr [[TMP]], align 32
// X86-NEXT:    store <8 x float> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_loadu_ps(float* A) {
  return _mm256_loadu_ps(A);
}

//
// X86-LABEL: define void @test_mm256_loadu_si256(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x i64>) align 32 [[AGG_RESULT:%.*]], ptr noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__P_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META123:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META123]]
// X86-NEXT:    store ptr [[TMP0]], ptr [[__P_ADDR_I]], align 4, !noalias [[META123]]
// X86-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[__P_ADDR_I]], align 4, !noalias [[META123]]
// X86-NEXT:    [[TMP2:%.*]] = load <4 x i64>, ptr [[TMP1]], align 1
// X86-NEXT:    store <4 x i64> [[TMP2]], ptr [[TMP]], align 32, !alias.scope [[META123]]
// X86-NEXT:    [[TMP3:%.*]] = load <4 x i64>, ptr [[TMP]], align 32, !alias.scope [[META123]]
// X86-NEXT:    store <4 x i64> [[TMP3]], ptr [[TMP]], align 32, !alias.scope [[META123]]
// X86-NEXT:    [[TMP4:%.*]] = load <4 x i64>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x i64> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <4 x i64>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x i64> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256i test_mm256_loadu_si256(__m256i* A) {
  return _mm256_loadu_si256(A);
}

//
// X86-LABEL: define void @test_mm256_loadu2_m128(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], ptr noundef [[A:%.*]], ptr noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I2:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[__P_ADDR_I3:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[__P_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[RESULT_PTR_I1:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__HI_ADDR_I:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[__LO_ADDR_I:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__ADDR_HI_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__ADDR_LO_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[COERCE_I:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[COERCE2_I:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[TMP_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[B_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[TMP:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    store ptr [[B]], ptr [[B_ADDR]], align 4
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[B_ADDR]], align 4
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META126:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META126]]
// X86-NEXT:    store ptr [[TMP0]], ptr [[__ADDR_HI_ADDR_I]], align 4, !noalias [[META126]]
// X86-NEXT:    store ptr [[TMP1]], ptr [[__ADDR_LO_ADDR_I]], align 4, !noalias [[META126]]
// X86-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[__ADDR_HI_ADDR_I]], align 4, !noalias [[META126]]
// X86-NEXT:    store ptr [[TMP2]], ptr [[__P_ADDR_I3]], align 4
// X86-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[__P_ADDR_I3]], align 4
// X86-NEXT:    [[TMP4:%.*]] = load <4 x float>, ptr [[TMP3]], align 1
// X86-NEXT:    store <4 x float> [[TMP4]], ptr [[RETVAL_I2]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL_I2]], align 16
// X86-NEXT:    store <2 x i64> [[TMP5]], ptr [[COERCE_I]], align 16, !noalias [[META126]]
// X86-NEXT:    [[TMP6:%.*]] = load <4 x float>, ptr [[COERCE_I]], align 16, !noalias [[META126]]
// X86-NEXT:    [[TMP7:%.*]] = load ptr, ptr [[__ADDR_LO_ADDR_I]], align 4, !noalias [[META126]]
// X86-NEXT:    store ptr [[TMP7]], ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    [[TMP8:%.*]] = load ptr, ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    [[TMP9:%.*]] = load <4 x float>, ptr [[TMP8]], align 1
// X86-NEXT:    store <4 x float> [[TMP9]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP10:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP10]], ptr [[COERCE2_I]], align 16, !noalias [[META126]]
// X86-NEXT:    [[TMP11:%.*]] = load <4 x float>, ptr [[COERCE2_I]], align 16, !noalias [[META126]]
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META129:![0-9]+]])
// X86-NEXT:    store ptr [[TMP_I]], ptr [[RESULT_PTR_I1]], align 4, !noalias [[META129]]
// X86-NEXT:    store <4 x float> [[TMP6]], ptr [[__HI_ADDR_I]], align 16, !noalias [[META129]]
// X86-NEXT:    store <4 x float> [[TMP11]], ptr [[__LO_ADDR_I]], align 16, !noalias [[META129]]
// X86-NEXT:    [[TMP12:%.*]] = load <4 x float>, ptr [[__LO_ADDR_I]], align 16, !noalias [[META129]]
// X86-NEXT:    [[TMP13:%.*]] = load <4 x float>, ptr [[__HI_ADDR_I]], align 16, !noalias [[META129]]
// X86-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <4 x float> [[TMP12]], <4 x float> [[TMP13]], <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
// X86-NEXT:    store <8 x float> [[SHUFFLE_I]], ptr [[TMP_I]], align 32, !alias.scope [[META129]]
// X86-NEXT:    [[TMP14:%.*]] = load <8 x float>, ptr [[TMP_I]], align 32, !alias.scope [[META129]]
// X86-NEXT:    store <8 x float> [[TMP14]], ptr [[TMP_I]], align 32, !alias.scope [[META129]]
// X86-NEXT:    [[TMP15:%.*]] = load <8 x float>, ptr [[TMP_I]], align 32, !noalias [[META126]]
// X86-NEXT:    store <8 x float> [[TMP15]], ptr [[TMP]], align 32, !alias.scope [[META126]]
// X86-NEXT:    [[TMP16:%.*]] = load <8 x float>, ptr [[TMP]], align 32, !alias.scope [[META126]]
// X86-NEXT:    store <8 x float> [[TMP16]], ptr [[TMP]], align 32, !alias.scope [[META126]]
// X86-NEXT:    [[TMP17:%.*]] = load <8 x float>, ptr [[TMP]], align 32
// X86-NEXT:    store <8 x float> [[TMP17]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP18:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP18]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_loadu2_m128(float* A, float* B) {
  return _mm256_loadu2_m128(A, B);
}

//
// X86-LABEL: define void @test_mm256_loadu2_m128d(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], ptr noundef [[A:%.*]], ptr noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I2:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__DP_ADDR_I3:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__DP_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[RESULT_PTR_I1:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__HI_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__LO_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__ADDR_HI_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__ADDR_LO_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[COERCE_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[COERCE2_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[TMP_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[B_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    store ptr [[B]], ptr [[B_ADDR]], align 4
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[B_ADDR]], align 4
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META132:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META132]]
// X86-NEXT:    store ptr [[TMP0]], ptr [[__ADDR_HI_ADDR_I]], align 4, !noalias [[META132]]
// X86-NEXT:    store ptr [[TMP1]], ptr [[__ADDR_LO_ADDR_I]], align 4, !noalias [[META132]]
// X86-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[__ADDR_HI_ADDR_I]], align 4, !noalias [[META132]]
// X86-NEXT:    store ptr [[TMP2]], ptr [[__DP_ADDR_I3]], align 4
// X86-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[__DP_ADDR_I3]], align 4
// X86-NEXT:    [[TMP4:%.*]] = load <2 x double>, ptr [[TMP3]], align 1
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL_I2]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL_I2]], align 16
// X86-NEXT:    store <2 x i64> [[TMP5]], ptr [[COERCE_I]], align 16, !noalias [[META132]]
// X86-NEXT:    [[TMP6:%.*]] = load <2 x double>, ptr [[COERCE_I]], align 16, !noalias [[META132]]
// X86-NEXT:    [[TMP7:%.*]] = load ptr, ptr [[__ADDR_LO_ADDR_I]], align 4, !noalias [[META132]]
// X86-NEXT:    store ptr [[TMP7]], ptr [[__DP_ADDR_I]], align 4
// X86-NEXT:    [[TMP8:%.*]] = load ptr, ptr [[__DP_ADDR_I]], align 4
// X86-NEXT:    [[TMP9:%.*]] = load <2 x double>, ptr [[TMP8]], align 1
// X86-NEXT:    store <2 x double> [[TMP9]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP10:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP10]], ptr [[COERCE2_I]], align 16, !noalias [[META132]]
// X86-NEXT:    [[TMP11:%.*]] = load <2 x double>, ptr [[COERCE2_I]], align 16, !noalias [[META132]]
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META135:![0-9]+]])
// X86-NEXT:    store ptr [[TMP_I]], ptr [[RESULT_PTR_I1]], align 4, !noalias [[META135]]
// X86-NEXT:    store <2 x double> [[TMP6]], ptr [[__HI_ADDR_I]], align 16, !noalias [[META135]]
// X86-NEXT:    store <2 x double> [[TMP11]], ptr [[__LO_ADDR_I]], align 16, !noalias [[META135]]
// X86-NEXT:    [[TMP12:%.*]] = load <2 x double>, ptr [[__LO_ADDR_I]], align 16, !noalias [[META135]]
// X86-NEXT:    [[TMP13:%.*]] = load <2 x double>, ptr [[__HI_ADDR_I]], align 16, !noalias [[META135]]
// X86-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <2 x double> [[TMP12]], <2 x double> [[TMP13]], <4 x i32> <i32 0, i32 1, i32 2, i32 3>
// X86-NEXT:    store <4 x double> [[SHUFFLE_I]], ptr [[TMP_I]], align 32, !alias.scope [[META135]]
// X86-NEXT:    [[TMP14:%.*]] = load <4 x double>, ptr [[TMP_I]], align 32, !alias.scope [[META135]]
// X86-NEXT:    store <4 x double> [[TMP14]], ptr [[TMP_I]], align 32, !alias.scope [[META135]]
// X86-NEXT:    [[TMP15:%.*]] = load <4 x double>, ptr [[TMP_I]], align 32, !noalias [[META132]]
// X86-NEXT:    store <4 x double> [[TMP15]], ptr [[TMP]], align 32, !alias.scope [[META132]]
// X86-NEXT:    [[TMP16:%.*]] = load <4 x double>, ptr [[TMP]], align 32, !alias.scope [[META132]]
// X86-NEXT:    store <4 x double> [[TMP16]], ptr [[TMP]], align 32, !alias.scope [[META132]]
// X86-NEXT:    [[TMP17:%.*]] = load <4 x double>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x double> [[TMP17]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP18:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP18]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_loadu2_m128d(double* A, double* B) {
  return _mm256_loadu2_m128d(A, B);
}

//
// X86-LABEL: define void @test_mm256_loadu2_m128i(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x i64>) align 32 [[AGG_RESULT:%.*]], ptr noundef [[A:%.*]], ptr noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__P_ADDR_I2:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__P_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[RESULT_PTR_I1:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__HI_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__LO_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__ADDR_HI_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__ADDR_LO_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[TMP_I:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[B_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    store ptr [[B]], ptr [[B_ADDR]], align 4
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[B_ADDR]], align 4
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META138:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META138]]
// X86-NEXT:    store ptr [[TMP0]], ptr [[__ADDR_HI_ADDR_I]], align 4, !noalias [[META138]]
// X86-NEXT:    store ptr [[TMP1]], ptr [[__ADDR_LO_ADDR_I]], align 4, !noalias [[META138]]
// X86-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[__ADDR_HI_ADDR_I]], align 4, !noalias [[META138]]
// X86-NEXT:    store ptr [[TMP2]], ptr [[__P_ADDR_I2]], align 4
// X86-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[__P_ADDR_I2]], align 4
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[TMP3]], align 1
// X86-NEXT:    [[TMP5:%.*]] = load ptr, ptr [[__ADDR_LO_ADDR_I]], align 4, !noalias [[META138]]
// X86-NEXT:    store ptr [[TMP5]], ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    [[TMP6:%.*]] = load ptr, ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    [[TMP7:%.*]] = load <2 x i64>, ptr [[TMP6]], align 1
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META141:![0-9]+]])
// X86-NEXT:    store ptr [[TMP_I]], ptr [[RESULT_PTR_I1]], align 4, !noalias [[META141]]
// X86-NEXT:    store <2 x i64> [[TMP4]], ptr [[__HI_ADDR_I]], align 16, !noalias [[META141]]
// X86-NEXT:    store <2 x i64> [[TMP7]], ptr [[__LO_ADDR_I]], align 16, !noalias [[META141]]
// X86-NEXT:    [[TMP8:%.*]] = load <2 x i64>, ptr [[__LO_ADDR_I]], align 16, !noalias [[META141]]
// X86-NEXT:    [[TMP9:%.*]] = load <2 x i64>, ptr [[__HI_ADDR_I]], align 16, !noalias [[META141]]
// X86-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <2 x i64> [[TMP8]], <2 x i64> [[TMP9]], <4 x i32> <i32 0, i32 1, i32 2, i32 3>
// X86-NEXT:    store <4 x i64> [[SHUFFLE_I]], ptr [[TMP_I]], align 32, !alias.scope [[META141]]
// X86-NEXT:    [[TMP10:%.*]] = load <4 x i64>, ptr [[TMP_I]], align 32, !alias.scope [[META141]]
// X86-NEXT:    store <4 x i64> [[TMP10]], ptr [[TMP_I]], align 32, !alias.scope [[META141]]
// X86-NEXT:    [[TMP11:%.*]] = load <4 x i64>, ptr [[TMP_I]], align 32, !noalias [[META138]]
// X86-NEXT:    store <4 x i64> [[TMP11]], ptr [[TMP]], align 32, !alias.scope [[META138]]
// X86-NEXT:    [[TMP12:%.*]] = load <4 x i64>, ptr [[TMP]], align 32, !alias.scope [[META138]]
// X86-NEXT:    store <4 x i64> [[TMP12]], ptr [[TMP]], align 32, !alias.scope [[META138]]
// X86-NEXT:    [[TMP13:%.*]] = load <4 x i64>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x i64> [[TMP13]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP14:%.*]] = load <4 x i64>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x i64> [[TMP14]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256i test_mm256_loadu2_m128i(__m128i* A, __m128i* B) {
  return _mm256_loadu2_m128i(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_maskload_pd(
// X86-SAME: ptr noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__P_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__M_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store ptr [[TMP0]], ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__M_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    [[TMP3:%.*]] = load <2 x i64>, ptr [[__M_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = call <2 x double> @llvm.x86.avx.maskload.pd(ptr [[TMP2]], <2 x i64> [[TMP3]])
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP5]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP6:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP6]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP7:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP7]]
//
__m128d test_mm_maskload_pd(double* A, __m128i B) {
  return _mm_maskload_pd(A, B);
}

//
// X86-LABEL: define void @test_mm256_maskload_pd(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], ptr noundef [[A:%.*]], <4 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__P_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__M_ADDR_I:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    store <4 x i64> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load <4 x i64>, ptr [[B_ADDR]], align 32
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META144:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META144]]
// X86-NEXT:    store ptr [[TMP0]], ptr [[__P_ADDR_I]], align 4, !noalias [[META144]]
// X86-NEXT:    store <4 x i64> [[TMP1]], ptr [[__M_ADDR_I]], align 32, !noalias [[META144]]
// X86-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[__P_ADDR_I]], align 4, !noalias [[META144]]
// X86-NEXT:    [[TMP3:%.*]] = load <4 x i64>, ptr [[__M_ADDR_I]], align 32, !noalias [[META144]]
// X86-NEXT:    [[TMP4:%.*]] = call <4 x double> @llvm.x86.avx.maskload.pd.256(ptr [[TMP2]], <4 x i64> [[TMP3]])
// X86-NEXT:    store <4 x double> [[TMP4]], ptr [[TMP]], align 32, !alias.scope [[META144]]
// X86-NEXT:    [[TMP5:%.*]] = load <4 x double>, ptr [[TMP]], align 32, !alias.scope [[META144]]
// X86-NEXT:    store <4 x double> [[TMP5]], ptr [[TMP]], align 32, !alias.scope [[META144]]
// X86-NEXT:    [[TMP6:%.*]] = load <4 x double>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x double> [[TMP6]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP7:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP7]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_maskload_pd(double* A, __m256i B) {
  return _mm256_maskload_pd(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_maskload_ps(
// X86-SAME: ptr noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[__P_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__M_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store ptr [[TMP0]], ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__M_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    [[TMP3:%.*]] = load <2 x i64>, ptr [[__M_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = bitcast <2 x i64> [[TMP3]] to <4 x i32>
// X86-NEXT:    [[TMP5:%.*]] = call <4 x float> @llvm.x86.avx.maskload.ps(ptr [[TMP2]], <4 x i32> [[TMP4]])
// X86-NEXT:    store <4 x float> [[TMP5]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP6:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP6]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP7:%.*]] = load <4 x float>, ptr [[COERCE]], align 16
// X86-NEXT:    store <4 x float> [[TMP7]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP8:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP8]]
//
__m128 test_mm_maskload_ps(float* A, __m128i B) {
  return _mm_maskload_ps(A, B);
}

//
// X86-LABEL: define void @test_mm256_maskload_ps(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], ptr noundef [[A:%.*]], <4 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__P_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__M_ADDR_I:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    [[TMP:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    store <4 x i64> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load <4 x i64>, ptr [[B_ADDR]], align 32
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META147:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META147]]
// X86-NEXT:    store ptr [[TMP0]], ptr [[__P_ADDR_I]], align 4, !noalias [[META147]]
// X86-NEXT:    store <4 x i64> [[TMP1]], ptr [[__M_ADDR_I]], align 32, !noalias [[META147]]
// X86-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[__P_ADDR_I]], align 4, !noalias [[META147]]
// X86-NEXT:    [[TMP3:%.*]] = load <4 x i64>, ptr [[__M_ADDR_I]], align 32, !noalias [[META147]]
// X86-NEXT:    [[TMP4:%.*]] = bitcast <4 x i64> [[TMP3]] to <8 x i32>
// X86-NEXT:    [[TMP5:%.*]] = call <8 x float> @llvm.x86.avx.maskload.ps.256(ptr [[TMP2]], <8 x i32> [[TMP4]])
// X86-NEXT:    store <8 x float> [[TMP5]], ptr [[TMP]], align 32, !alias.scope [[META147]]
// X86-NEXT:    [[TMP6:%.*]] = load <8 x float>, ptr [[TMP]], align 32, !alias.scope [[META147]]
// X86-NEXT:    store <8 x float> [[TMP6]], ptr [[TMP]], align 32, !alias.scope [[META147]]
// X86-NEXT:    [[TMP7:%.*]] = load <8 x float>, ptr [[TMP]], align 32
// X86-NEXT:    store <8 x float> [[TMP7]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP8:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP8]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_maskload_ps(float* A, __m256i B) {
  return _mm256_maskload_ps(A, B);
}

//
// X86-LABEL: define void @test_mm_maskstore_pd(
// X86-SAME: ptr noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]], <2 x double> noundef [[C:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__P_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__M_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[C_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[C]], ptr [[C_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[C_ADDR]], align 16
// X86-NEXT:    store ptr [[TMP0]], ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__M_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP2]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[__M_ADDR_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    call void @llvm.x86.avx.maskstore.pd(ptr [[TMP3]], <2 x i64> [[TMP4]], <2 x double> [[TMP5]])
// X86-NEXT:    ret void
//
void test_mm_maskstore_pd(double* A, __m128i B, __m128d C) {
  _mm_maskstore_pd(A, B, C);
}

//
// X86-LABEL: define void @test_mm256_maskstore_pd(
// X86-SAME: ptr noundef [[A:%.*]], <4 x i64> noundef [[B:%.*]], <4 x double> noundef [[C:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__P_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__M_ADDR_I:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    [[C_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    store <4 x i64> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[C]], ptr [[C_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load <4 x i64>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = load <4 x double>, ptr [[C_ADDR]], align 32
// X86-NEXT:    store ptr [[TMP0]], ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    store <4 x i64> [[TMP1]], ptr [[__M_ADDR_I]], align 32
// X86-NEXT:    store <4 x double> [[TMP2]], ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    [[TMP4:%.*]] = load <4 x i64>, ptr [[__M_ADDR_I]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <4 x double>, ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    call void @llvm.x86.avx.maskstore.pd.256(ptr [[TMP3]], <4 x i64> [[TMP4]], <4 x double> [[TMP5]])
// X86-NEXT:    ret void
//
void test_mm256_maskstore_pd(double* A, __m256i B, __m256d C) {
  _mm256_maskstore_pd(A, B, C);
}

//
// X86-LABEL: define void @test_mm_maskstore_ps(
// X86-SAME: ptr noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]], <4 x float> noundef [[C:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__P_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__M_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[C_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    store <4 x float> [[C]], ptr [[C_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <4 x float>, ptr [[C_ADDR]], align 16
// X86-NEXT:    store ptr [[TMP0]], ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__M_ADDR_I]], align 16
// X86-NEXT:    store <4 x float> [[TMP2]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[__M_ADDR_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = bitcast <2 x i64> [[TMP4]] to <4 x i32>
// X86-NEXT:    [[TMP6:%.*]] = load <4 x float>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    call void @llvm.x86.avx.maskstore.ps(ptr [[TMP3]], <4 x i32> [[TMP5]], <4 x float> [[TMP6]])
// X86-NEXT:    ret void
//
void test_mm_maskstore_ps(float* A, __m128i B, __m128 C) {
  _mm_maskstore_ps(A, B, C);
}

//
// X86-LABEL: define void @test_mm256_maskstore_ps(
// X86-SAME: ptr noundef [[A:%.*]], <4 x i64> noundef [[B:%.*]], <8 x float> noundef [[C:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__P_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__M_ADDR_I:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    [[C_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    store <4 x i64> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[C]], ptr [[C_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load <4 x i64>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP2:%.*]] = load <8 x float>, ptr [[C_ADDR]], align 32
// X86-NEXT:    store ptr [[TMP0]], ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    store <4 x i64> [[TMP1]], ptr [[__M_ADDR_I]], align 32
// X86-NEXT:    store <8 x float> [[TMP2]], ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    [[TMP4:%.*]] = load <4 x i64>, ptr [[__M_ADDR_I]], align 32
// X86-NEXT:    [[TMP5:%.*]] = bitcast <4 x i64> [[TMP4]] to <8 x i32>
// X86-NEXT:    [[TMP6:%.*]] = load <8 x float>, ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    call void @llvm.x86.avx.maskstore.ps.256(ptr [[TMP3]], <8 x i32> [[TMP5]], <8 x float> [[TMP6]])
// X86-NEXT:    ret void
//
void test_mm256_maskstore_ps(float* A, __m256i B, __m256 C) {
  _mm256_maskstore_ps(A, B, C);
}

//
// X86-LABEL: define void @test_mm256_max_pd(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[A:%.*]], <4 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[B_ADDR]], align 32
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META150:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META150]]
// X86-NEXT:    store <4 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 32, !noalias [[META150]]
// X86-NEXT:    store <4 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 32, !noalias [[META150]]
// X86-NEXT:    [[TMP2:%.*]] = load <4 x double>, ptr [[__A_ADDR_I]], align 32, !noalias [[META150]]
// X86-NEXT:    [[TMP3:%.*]] = load <4 x double>, ptr [[__B_ADDR_I]], align 32, !noalias [[META150]]
// X86-NEXT:    [[TMP4:%.*]] = call <4 x double> @llvm.x86.avx.max.pd.256(<4 x double> [[TMP2]], <4 x double> [[TMP3]])
// X86-NEXT:    store <4 x double> [[TMP4]], ptr [[TMP]], align 32, !alias.scope [[META150]]
// X86-NEXT:    [[TMP5:%.*]] = load <4 x double>, ptr [[TMP]], align 32, !alias.scope [[META150]]
// X86-NEXT:    store <4 x double> [[TMP5]], ptr [[TMP]], align 32, !alias.scope [[META150]]
// X86-NEXT:    [[TMP6:%.*]] = load <4 x double>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x double> [[TMP6]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP7:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP7]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_max_pd(__m256d A, __m256d B) {
  return _mm256_max_pd(A, B);
}

//
// X86-LABEL: define void @test_mm256_max_ps(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]], <8 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[TMP:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[B_ADDR]], align 32
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META153:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META153]]
// X86-NEXT:    store <8 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 32, !noalias [[META153]]
// X86-NEXT:    store <8 x float> [[TMP1]], ptr [[__B_ADDR_I]], align 32, !noalias [[META153]]
// X86-NEXT:    [[TMP2:%.*]] = load <8 x float>, ptr [[__A_ADDR_I]], align 32, !noalias [[META153]]
// X86-NEXT:    [[TMP3:%.*]] = load <8 x float>, ptr [[__B_ADDR_I]], align 32, !noalias [[META153]]
// X86-NEXT:    [[TMP4:%.*]] = call <8 x float> @llvm.x86.avx.max.ps.256(<8 x float> [[TMP2]], <8 x float> [[TMP3]])
// X86-NEXT:    store <8 x float> [[TMP4]], ptr [[TMP]], align 32, !alias.scope [[META153]]
// X86-NEXT:    [[TMP5:%.*]] = load <8 x float>, ptr [[TMP]], align 32, !alias.scope [[META153]]
// X86-NEXT:    store <8 x float> [[TMP5]], ptr [[TMP]], align 32, !alias.scope [[META153]]
// X86-NEXT:    [[TMP6:%.*]] = load <8 x float>, ptr [[TMP]], align 32
// X86-NEXT:    store <8 x float> [[TMP6]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP7:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP7]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_max_ps(__m256 A, __m256 B) {
  return _mm256_max_ps(A, B);
}

//
// X86-LABEL: define void @test_mm256_min_pd(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[A:%.*]], <4 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[B_ADDR]], align 32
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META156:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META156]]
// X86-NEXT:    store <4 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 32, !noalias [[META156]]
// X86-NEXT:    store <4 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 32, !noalias [[META156]]
// X86-NEXT:    [[TMP2:%.*]] = load <4 x double>, ptr [[__A_ADDR_I]], align 32, !noalias [[META156]]
// X86-NEXT:    [[TMP3:%.*]] = load <4 x double>, ptr [[__B_ADDR_I]], align 32, !noalias [[META156]]
// X86-NEXT:    [[TMP4:%.*]] = call <4 x double> @llvm.x86.avx.min.pd.256(<4 x double> [[TMP2]], <4 x double> [[TMP3]])
// X86-NEXT:    store <4 x double> [[TMP4]], ptr [[TMP]], align 32, !alias.scope [[META156]]
// X86-NEXT:    [[TMP5:%.*]] = load <4 x double>, ptr [[TMP]], align 32, !alias.scope [[META156]]
// X86-NEXT:    store <4 x double> [[TMP5]], ptr [[TMP]], align 32, !alias.scope [[META156]]
// X86-NEXT:    [[TMP6:%.*]] = load <4 x double>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x double> [[TMP6]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP7:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP7]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_min_pd(__m256d A, __m256d B) {
  return _mm256_min_pd(A, B);
}

//
// X86-LABEL: define void @test_mm256_min_ps(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]], <8 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[TMP:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[B_ADDR]], align 32
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META159:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META159]]
// X86-NEXT:    store <8 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 32, !noalias [[META159]]
// X86-NEXT:    store <8 x float> [[TMP1]], ptr [[__B_ADDR_I]], align 32, !noalias [[META159]]
// X86-NEXT:    [[TMP2:%.*]] = load <8 x float>, ptr [[__A_ADDR_I]], align 32, !noalias [[META159]]
// X86-NEXT:    [[TMP3:%.*]] = load <8 x float>, ptr [[__B_ADDR_I]], align 32, !noalias [[META159]]
// X86-NEXT:    [[TMP4:%.*]] = call <8 x float> @llvm.x86.avx.min.ps.256(<8 x float> [[TMP2]], <8 x float> [[TMP3]])
// X86-NEXT:    store <8 x float> [[TMP4]], ptr [[TMP]], align 32, !alias.scope [[META159]]
// X86-NEXT:    [[TMP5:%.*]] = load <8 x float>, ptr [[TMP]], align 32, !alias.scope [[META159]]
// X86-NEXT:    store <8 x float> [[TMP5]], ptr [[TMP]], align 32, !alias.scope [[META159]]
// X86-NEXT:    [[TMP6:%.*]] = load <8 x float>, ptr [[TMP]], align 32
// X86-NEXT:    store <8 x float> [[TMP6]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP7:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP7]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_min_ps(__m256 A, __m256 B) {
  return _mm256_min_ps(A, B);
}

//
// X86-LABEL: define void @test_mm256_movedup_pd(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META162:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META162]]
// X86-NEXT:    store <4 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 32, !noalias [[META162]]
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[__A_ADDR_I]], align 32, !noalias [[META162]]
// X86-NEXT:    [[TMP2:%.*]] = load <4 x double>, ptr [[__A_ADDR_I]], align 32, !noalias [[META162]]
// X86-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <4 x double> [[TMP1]], <4 x double> [[TMP2]], <4 x i32> <i32 0, i32 0, i32 2, i32 2>
// X86-NEXT:    store <4 x double> [[SHUFFLE_I]], ptr [[TMP]], align 32, !alias.scope [[META162]]
// X86-NEXT:    [[TMP3:%.*]] = load <4 x double>, ptr [[TMP]], align 32, !alias.scope [[META162]]
// X86-NEXT:    store <4 x double> [[TMP3]], ptr [[TMP]], align 32, !alias.scope [[META162]]
// X86-NEXT:    [[TMP4:%.*]] = load <4 x double>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x double> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_movedup_pd(__m256d A) {
  return _mm256_movedup_pd(A);
}

//
// X86-LABEL: define void @test_mm256_movehdup_ps(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[TMP:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META165:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META165]]
// X86-NEXT:    store <8 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 32, !noalias [[META165]]
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[__A_ADDR_I]], align 32, !noalias [[META165]]
// X86-NEXT:    [[TMP2:%.*]] = load <8 x float>, ptr [[__A_ADDR_I]], align 32, !noalias [[META165]]
// X86-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <8 x float> [[TMP1]], <8 x float> [[TMP2]], <8 x i32> <i32 1, i32 1, i32 3, i32 3, i32 5, i32 5, i32 7, i32 7>
// X86-NEXT:    store <8 x float> [[SHUFFLE_I]], ptr [[TMP]], align 32, !alias.scope [[META165]]
// X86-NEXT:    [[TMP3:%.*]] = load <8 x float>, ptr [[TMP]], align 32, !alias.scope [[META165]]
// X86-NEXT:    store <8 x float> [[TMP3]], ptr [[TMP]], align 32, !alias.scope [[META165]]
// X86-NEXT:    [[TMP4:%.*]] = load <8 x float>, ptr [[TMP]], align 32
// X86-NEXT:    store <8 x float> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_movehdup_ps(__m256 A) {
  return _mm256_movehdup_ps(A);
}

//
// X86-LABEL: define void @test_mm256_moveldup_ps(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[TMP:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META168:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META168]]
// X86-NEXT:    store <8 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 32, !noalias [[META168]]
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[__A_ADDR_I]], align 32, !noalias [[META168]]
// X86-NEXT:    [[TMP2:%.*]] = load <8 x float>, ptr [[__A_ADDR_I]], align 32, !noalias [[META168]]
// X86-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <8 x float> [[TMP1]], <8 x float> [[TMP2]], <8 x i32> <i32 0, i32 0, i32 2, i32 2, i32 4, i32 4, i32 6, i32 6>
// X86-NEXT:    store <8 x float> [[SHUFFLE_I]], ptr [[TMP]], align 32, !alias.scope [[META168]]
// X86-NEXT:    [[TMP3:%.*]] = load <8 x float>, ptr [[TMP]], align 32, !alias.scope [[META168]]
// X86-NEXT:    store <8 x float> [[TMP3]], ptr [[TMP]], align 32, !alias.scope [[META168]]
// X86-NEXT:    [[TMP4:%.*]] = load <8 x float>, ptr [[TMP]], align 32
// X86-NEXT:    store <8 x float> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_moveldup_ps(__m256 A) {
  return _mm256_moveldup_ps(A);
}

//
// X86-LABEL: define i32 @test_mm256_movemask_pd(
// X86-SAME: <4 x double> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    [[TMP2:%.*]] = call i32 @llvm.x86.avx.movmsk.pd.256(<4 x double> [[TMP1]])
// X86-NEXT:    ret i32 [[TMP2]]
//
int test_mm256_movemask_pd(__m256d A) {
  return _mm256_movemask_pd(A);
}

//
// X86-LABEL: define i32 @test_mm256_movemask_ps(
// X86-SAME: <8 x float> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    [[TMP2:%.*]] = call i32 @llvm.x86.avx.movmsk.ps.256(<8 x float> [[TMP1]])
// X86-NEXT:    ret i32 [[TMP2]]
//
int test_mm256_movemask_ps(__m256 A) {
  return _mm256_movemask_ps(A);
}

//
// X86-LABEL: define void @test_mm256_mul_pd(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[A:%.*]], <4 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[B_ADDR]], align 32
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META171:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META171]]
// X86-NEXT:    store <4 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 32, !noalias [[META171]]
// X86-NEXT:    store <4 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 32, !noalias [[META171]]
// X86-NEXT:    [[TMP2:%.*]] = load <4 x double>, ptr [[__A_ADDR_I]], align 32, !noalias [[META171]]
// X86-NEXT:    [[TMP3:%.*]] = load <4 x double>, ptr [[__B_ADDR_I]], align 32, !noalias [[META171]]
// X86-NEXT:    [[MUL_I:%.*]] = fmul <4 x double> [[TMP2]], [[TMP3]]
// X86-NEXT:    store <4 x double> [[MUL_I]], ptr [[TMP]], align 32, !alias.scope [[META171]]
// X86-NEXT:    [[TMP4:%.*]] = load <4 x double>, ptr [[TMP]], align 32, !alias.scope [[META171]]
// X86-NEXT:    store <4 x double> [[TMP4]], ptr [[TMP]], align 32, !alias.scope [[META171]]
// X86-NEXT:    [[TMP5:%.*]] = load <4 x double>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x double> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP6:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP6]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_mul_pd(__m256d A, __m256d B) {
  return _mm256_mul_pd(A, B);
}

//
// X86-LABEL: define void @test_mm256_mul_ps(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]], <8 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[TMP:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[B_ADDR]], align 32
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META174:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META174]]
// X86-NEXT:    store <8 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 32, !noalias [[META174]]
// X86-NEXT:    store <8 x float> [[TMP1]], ptr [[__B_ADDR_I]], align 32, !noalias [[META174]]
// X86-NEXT:    [[TMP2:%.*]] = load <8 x float>, ptr [[__A_ADDR_I]], align 32, !noalias [[META174]]
// X86-NEXT:    [[TMP3:%.*]] = load <8 x float>, ptr [[__B_ADDR_I]], align 32, !noalias [[META174]]
// X86-NEXT:    [[MUL_I:%.*]] = fmul <8 x float> [[TMP2]], [[TMP3]]
// X86-NEXT:    store <8 x float> [[MUL_I]], ptr [[TMP]], align 32, !alias.scope [[META174]]
// X86-NEXT:    [[TMP4:%.*]] = load <8 x float>, ptr [[TMP]], align 32, !alias.scope [[META174]]
// X86-NEXT:    store <8 x float> [[TMP4]], ptr [[TMP]], align 32, !alias.scope [[META174]]
// X86-NEXT:    [[TMP5:%.*]] = load <8 x float>, ptr [[TMP]], align 32
// X86-NEXT:    store <8 x float> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP6:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP6]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_mul_ps(__m256 A, __m256 B) {
  return _mm256_mul_ps(A, B);
}

//
// X86-LABEL: define void @test_mm256_or_pd(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[A:%.*]], <4 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[B_ADDR]], align 32
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META177:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META177]]
// X86-NEXT:    store <4 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 32, !noalias [[META177]]
// X86-NEXT:    store <4 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 32, !noalias [[META177]]
// X86-NEXT:    [[TMP2:%.*]] = load <4 x double>, ptr [[__A_ADDR_I]], align 32, !noalias [[META177]]
// X86-NEXT:    [[TMP3:%.*]] = bitcast <4 x double> [[TMP2]] to <4 x i64>
// X86-NEXT:    [[TMP4:%.*]] = load <4 x double>, ptr [[__B_ADDR_I]], align 32, !noalias [[META177]]
// X86-NEXT:    [[TMP5:%.*]] = bitcast <4 x double> [[TMP4]] to <4 x i64>
// X86-NEXT:    [[OR_I:%.*]] = or <4 x i64> [[TMP3]], [[TMP5]]
// X86-NEXT:    [[TMP6:%.*]] = bitcast <4 x i64> [[OR_I]] to <4 x double>
// X86-NEXT:    store <4 x double> [[TMP6]], ptr [[TMP]], align 32, !alias.scope [[META177]]
// X86-NEXT:    [[TMP7:%.*]] = load <4 x double>, ptr [[TMP]], align 32, !alias.scope [[META177]]
// X86-NEXT:    store <4 x double> [[TMP7]], ptr [[TMP]], align 32, !alias.scope [[META177]]
// X86-NEXT:    [[TMP8:%.*]] = load <4 x double>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x double> [[TMP8]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP9:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP9]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_or_pd(__m256d A, __m256d B) {
  return _mm256_or_pd(A, B);
}

//
// X86-LABEL: define void @test_mm256_or_ps(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]], <8 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[TMP:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[B_ADDR]], align 32
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META180:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META180]]
// X86-NEXT:    store <8 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 32, !noalias [[META180]]
// X86-NEXT:    store <8 x float> [[TMP1]], ptr [[__B_ADDR_I]], align 32, !noalias [[META180]]
// X86-NEXT:    [[TMP2:%.*]] = load <8 x float>, ptr [[__A_ADDR_I]], align 32, !noalias [[META180]]
// X86-NEXT:    [[TMP3:%.*]] = bitcast <8 x float> [[TMP2]] to <8 x i32>
// X86-NEXT:    [[TMP4:%.*]] = load <8 x float>, ptr [[__B_ADDR_I]], align 32, !noalias [[META180]]
// X86-NEXT:    [[TMP5:%.*]] = bitcast <8 x float> [[TMP4]] to <8 x i32>
// X86-NEXT:    [[OR_I:%.*]] = or <8 x i32> [[TMP3]], [[TMP5]]
// X86-NEXT:    [[TMP6:%.*]] = bitcast <8 x i32> [[OR_I]] to <8 x float>
// X86-NEXT:    store <8 x float> [[TMP6]], ptr [[TMP]], align 32, !alias.scope [[META180]]
// X86-NEXT:    [[TMP7:%.*]] = load <8 x float>, ptr [[TMP]], align 32, !alias.scope [[META180]]
// X86-NEXT:    store <8 x float> [[TMP7]], ptr [[TMP]], align 32, !alias.scope [[META180]]
// X86-NEXT:    [[TMP8:%.*]] = load <8 x float>, ptr [[TMP]], align 32
// X86-NEXT:    store <8 x float> [[TMP8]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP9:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP9]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_or_ps(__m256 A, __m256 B) {
  return _mm256_or_ps(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_permute_pd(
// X86-SAME: <2 x double> noundef [[A:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[PERMIL:%.*]] = shufflevector <2 x double> [[TMP0]], <2 x double> poison, <2 x i32> <i32 1, i32 0>
// X86-NEXT:    store <2 x double> [[PERMIL]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP1]]
//
__m128d test_mm_permute_pd(__m128d A) {
  return _mm_permute_pd(A, 1);
}

//
// X86-LABEL: define void @test_mm256_permute_pd(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[PERMIL:%.*]] = shufflevector <4 x double> [[TMP0]], <4 x double> poison, <4 x i32> <i32 1, i32 0, i32 3, i32 2>
// X86-NEXT:    store <4 x double> [[PERMIL]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP1]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_permute_pd(__m256d A) {
  return _mm256_permute_pd(A, 5);
}

//
// X86-LABEL: define <2 x i64> @test_mm_permute_ps(
// X86-SAME: <4 x float> noundef [[A:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[PERMIL:%.*]] = shufflevector <4 x float> [[TMP0]], <4 x float> poison, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
// X86-NEXT:    store <4 x float> [[PERMIL]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP1]]
//
__m128 test_mm_permute_ps(__m128 A) {
  return _mm_permute_ps(A, 0x1b);
}

// Test case for PR12401
//
// X86-LABEL: define <2 x i64> @test2_mm_permute_ps(
// X86-SAME: <4 x float> noundef [[A:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[PERMIL:%.*]] = shufflevector <4 x float> [[TMP0]], <4 x float> poison, <4 x i32> <i32 2, i32 1, i32 2, i32 3>
// X86-NEXT:    store <4 x float> [[PERMIL]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP1]]
//
__m128 test2_mm_permute_ps(__m128 a) {
  return _mm_permute_ps(a, 0xe6);
}

//
// X86-LABEL: define void @test_mm256_permute_ps(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[PERMIL:%.*]] = shufflevector <8 x float> [[TMP0]], <8 x float> poison, <8 x i32> <i32 3, i32 2, i32 1, i32 0, i32 7, i32 6, i32 5, i32 4>
// X86-NEXT:    store <8 x float> [[PERMIL]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP1]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_permute_ps(__m256 A) {
  return _mm256_permute_ps(A, 0x1b);
}

//
// X86-LABEL: define void @test_mm256_permute2f128_pd(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[A:%.*]], <4 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[VPERM:%.*]] = shufflevector <4 x double> [[TMP0]], <4 x double> [[TMP1]], <4 x i32> <i32 2, i32 3, i32 6, i32 7>
// X86-NEXT:    store <4 x double> [[VPERM]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP2:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP2]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_permute2f128_pd(__m256d A, __m256d B) {
  return _mm256_permute2f128_pd(A, B, 0x31);
}

//
// X86-LABEL: define void @test_mm256_permute2f128_ps(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]], <8 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[VPERM:%.*]] = shufflevector <8 x float> [[TMP1]], <8 x float> [[TMP0]], <8 x i32> <i32 4, i32 5, i32 6, i32 7, i32 12, i32 13, i32 14, i32 15>
// X86-NEXT:    store <8 x float> [[VPERM]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP2:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP2]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_permute2f128_ps(__m256 A, __m256 B) {
  return _mm256_permute2f128_ps(A, B, 0x13);
}

//
// X86-LABEL: define void @test_mm256_permute2f128_si256(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x i64>) align 32 [[AGG_RESULT:%.*]], <4 x i64> noundef [[A:%.*]], <4 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x i64> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x i64> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x i64>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = bitcast <4 x i64> [[TMP0]] to <8 x i32>
// X86-NEXT:    [[TMP2:%.*]] = load <4 x i64>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP3:%.*]] = bitcast <4 x i64> [[TMP2]] to <8 x i32>
// X86-NEXT:    [[VPERM:%.*]] = shufflevector <8 x i32> [[TMP1]], <8 x i32> [[TMP3]], <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 8, i32 9, i32 10, i32 11>
// X86-NEXT:    [[TMP4:%.*]] = bitcast <8 x i32> [[VPERM]] to <4 x i64>
// X86-NEXT:    store <4 x i64> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <4 x i64>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x i64> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256i test_mm256_permute2f128_si256(__m256i A, __m256i B) {
  return _mm256_permute2f128_si256(A, B, 0x20);
}

//
// X86-LABEL: define <2 x i64> @test_mm_permutevar_pd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__C_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__C_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x i64>, ptr [[__C_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = call <2 x double> @llvm.x86.avx.vpermilvar.pd(<2 x double> [[TMP2]], <2 x i64> [[TMP3]])
// X86-NEXT:    store <2 x double> [[TMP4]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP5]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP6:%.*]] = load <2 x double>, ptr [[COERCE]], align 16
// X86-NEXT:    store <2 x double> [[TMP6]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP7:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP7]]
//
__m128d test_mm_permutevar_pd(__m128d A, __m128i B) {
  return _mm_permutevar_pd(A, B);
}

//
// X86-LABEL: define void @test_mm256_permutevar_pd(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[A:%.*]], <4 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[__C_ADDR_I:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x i64> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x i64>, ptr [[B_ADDR]], align 32
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META183:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META183]]
// X86-NEXT:    store <4 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 32, !noalias [[META183]]
// X86-NEXT:    store <4 x i64> [[TMP1]], ptr [[__C_ADDR_I]], align 32, !noalias [[META183]]
// X86-NEXT:    [[TMP2:%.*]] = load <4 x double>, ptr [[__A_ADDR_I]], align 32, !noalias [[META183]]
// X86-NEXT:    [[TMP3:%.*]] = load <4 x i64>, ptr [[__C_ADDR_I]], align 32, !noalias [[META183]]
// X86-NEXT:    [[TMP4:%.*]] = call <4 x double> @llvm.x86.avx.vpermilvar.pd.256(<4 x double> [[TMP2]], <4 x i64> [[TMP3]])
// X86-NEXT:    store <4 x double> [[TMP4]], ptr [[TMP]], align 32, !alias.scope [[META183]]
// X86-NEXT:    [[TMP5:%.*]] = load <4 x double>, ptr [[TMP]], align 32, !alias.scope [[META183]]
// X86-NEXT:    store <4 x double> [[TMP5]], ptr [[TMP]], align 32, !alias.scope [[META183]]
// X86-NEXT:    [[TMP6:%.*]] = load <4 x double>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x double> [[TMP6]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP7:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP7]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_permutevar_pd(__m256d A, __m256i B) {
  return _mm256_permutevar_pd(A, B);
}

//
// X86-LABEL: define <2 x i64> @test_mm_permutevar_ps(
// X86-SAME: <4 x float> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[__C_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[RETVAL:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[COERCE:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <4 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__C_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <4 x float>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x i64>, ptr [[__C_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = bitcast <2 x i64> [[TMP3]] to <4 x i32>
// X86-NEXT:    [[TMP5:%.*]] = call <4 x float> @llvm.x86.avx.vpermilvar.ps(<4 x float> [[TMP2]], <4 x i32> [[TMP4]])
// X86-NEXT:    store <4 x float> [[TMP5]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP6:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP6]], ptr [[COERCE]], align 16
// X86-NEXT:    [[TMP7:%.*]] = load <4 x float>, ptr [[COERCE]], align 16
// X86-NEXT:    store <4 x float> [[TMP7]], ptr [[RETVAL]], align 16
// X86-NEXT:    [[TMP8:%.*]] = load <2 x i64>, ptr [[RETVAL]], align 16
// X86-NEXT:    ret <2 x i64> [[TMP8]]
//
__m128 test_mm_permutevar_ps(__m128 A, __m128i B) {
  return _mm_permutevar_ps(A, B);
}

//
// X86-LABEL: define void @test_mm256_permutevar_ps(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]], <4 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[__C_ADDR_I:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    [[TMP:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x i64> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x i64>, ptr [[B_ADDR]], align 32
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META186:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META186]]
// X86-NEXT:    store <8 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 32, !noalias [[META186]]
// X86-NEXT:    store <4 x i64> [[TMP1]], ptr [[__C_ADDR_I]], align 32, !noalias [[META186]]
// X86-NEXT:    [[TMP2:%.*]] = load <8 x float>, ptr [[__A_ADDR_I]], align 32, !noalias [[META186]]
// X86-NEXT:    [[TMP3:%.*]] = load <4 x i64>, ptr [[__C_ADDR_I]], align 32, !noalias [[META186]]
// X86-NEXT:    [[TMP4:%.*]] = bitcast <4 x i64> [[TMP3]] to <8 x i32>
// X86-NEXT:    [[TMP5:%.*]] = call <8 x float> @llvm.x86.avx.vpermilvar.ps.256(<8 x float> [[TMP2]], <8 x i32> [[TMP4]])
// X86-NEXT:    store <8 x float> [[TMP5]], ptr [[TMP]], align 32, !alias.scope [[META186]]
// X86-NEXT:    [[TMP6:%.*]] = load <8 x float>, ptr [[TMP]], align 32, !alias.scope [[META186]]
// X86-NEXT:    store <8 x float> [[TMP6]], ptr [[TMP]], align 32, !alias.scope [[META186]]
// X86-NEXT:    [[TMP7:%.*]] = load <8 x float>, ptr [[TMP]], align 32
// X86-NEXT:    store <8 x float> [[TMP7]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP8:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP8]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_permutevar_ps(__m256 A, __m256i B) {
  return _mm256_permutevar_ps(A, B);
}

//
// X86-LABEL: define void @test_mm256_rcp_ps(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[TMP:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META189:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META189]]
// X86-NEXT:    store <8 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 32, !noalias [[META189]]
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[__A_ADDR_I]], align 32, !noalias [[META189]]
// X86-NEXT:    [[TMP2:%.*]] = call <8 x float> @llvm.x86.avx.rcp.ps.256(<8 x float> [[TMP1]])
// X86-NEXT:    store <8 x float> [[TMP2]], ptr [[TMP]], align 32, !alias.scope [[META189]]
// X86-NEXT:    [[TMP3:%.*]] = load <8 x float>, ptr [[TMP]], align 32, !alias.scope [[META189]]
// X86-NEXT:    store <8 x float> [[TMP3]], ptr [[TMP]], align 32, !alias.scope [[META189]]
// X86-NEXT:    [[TMP4:%.*]] = load <8 x float>, ptr [[TMP]], align 32
// X86-NEXT:    store <8 x float> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_rcp_ps(__m256 A) {
  return _mm256_rcp_ps(A);
}

//
// X86-LABEL: define void @test_mm256_round_pd(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[X:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[X_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[X]], ptr [[X_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[X_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = call <4 x double> @llvm.x86.avx.round.pd.256(<4 x double> [[TMP0]], i32 4)
// X86-NEXT:    store <4 x double> [[TMP1]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP2:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP2]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_round_pd(__m256d x) {
  return _mm256_round_pd(x, 4);
}

//
// X86-LABEL: define void @test_mm256_round_ps(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[X:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[X_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[X]], ptr [[X_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[X_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = call <8 x float> @llvm.x86.avx.round.ps.256(<8 x float> [[TMP0]], i32 4)
// X86-NEXT:    store <8 x float> [[TMP1]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP2:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP2]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_round_ps(__m256 x) {
  return _mm256_round_ps(x, 4);
}

//
// X86-LABEL: define void @test_mm256_rsqrt_ps(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[TMP:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META192:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META192]]
// X86-NEXT:    store <8 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 32, !noalias [[META192]]
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[__A_ADDR_I]], align 32, !noalias [[META192]]
// X86-NEXT:    [[TMP2:%.*]] = call <8 x float> @llvm.x86.avx.rsqrt.ps.256(<8 x float> [[TMP1]])
// X86-NEXT:    store <8 x float> [[TMP2]], ptr [[TMP]], align 32, !alias.scope [[META192]]
// X86-NEXT:    [[TMP3:%.*]] = load <8 x float>, ptr [[TMP]], align 32, !alias.scope [[META192]]
// X86-NEXT:    store <8 x float> [[TMP3]], ptr [[TMP]], align 32, !alias.scope [[META192]]
// X86-NEXT:    [[TMP4:%.*]] = load <8 x float>, ptr [[TMP]], align 32
// X86-NEXT:    store <8 x float> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_rsqrt_ps(__m256 A) {
  return _mm256_rsqrt_ps(A);
}

//
__m256i test_mm256_set_epi8(char A0, char A1, char A2, char A3, char A4, char A5, char A6, char A7,
                            char A8, char A9, char A10, char A11, char A12, char A13, char A14, char A15,
                            char A16, char A17, char A18, char A19, char A20, char A21, char A22, char A23,
                            char A24, char A25, char A26, char A27, char A28, char A29, char A30, char A31) {
  return _mm256_set_epi8(A0, A1, A2, A3, A4, A5, A6, A7, A8, A9, A10, A11, A12, A13, A14, A15, A16, A17, A18, A19, A20, A21, A22, A23, A24, A25, A26, A27, A28, A29, A30, A31);
}

//
// X86-LABEL: define void @test_mm256_set_epi16(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x i64>) align 32 [[AGG_RESULT:%.*]], i16 noundef signext [[A0:%.*]], i16 noundef signext [[A1:%.*]], i16 noundef signext [[A2:%.*]], i16 noundef signext [[A3:%.*]], i16 noundef signext [[A4:%.*]], i16 noundef signext [[A5:%.*]], i16 noundef signext [[A6:%.*]], i16 noundef signext [[A7:%.*]], i16 noundef signext [[A8:%.*]], i16 noundef signext [[A9:%.*]], i16 noundef signext [[A10:%.*]], i16 noundef signext [[A11:%.*]], i16 noundef signext [[A12:%.*]], i16 noundef signext [[A13:%.*]], i16 noundef signext [[A14:%.*]], i16 noundef signext [[A15:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__W15_ADDR_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W14_ADDR_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W13_ADDR_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W12_ADDR_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W11_ADDR_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W10_ADDR_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W09_ADDR_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W08_ADDR_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W07_ADDR_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W06_ADDR_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W05_ADDR_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W04_ADDR_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W03_ADDR_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W02_ADDR_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W01_ADDR_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W00_ADDR_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <16 x i16>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A0_ADDR:%.*]] = alloca i16, align 2
// X86-NEXT:    [[A1_ADDR:%.*]] = alloca i16, align 2
// X86-NEXT:    [[A2_ADDR:%.*]] = alloca i16, align 2
// X86-NEXT:    [[A3_ADDR:%.*]] = alloca i16, align 2
// X86-NEXT:    [[A4_ADDR:%.*]] = alloca i16, align 2
// X86-NEXT:    [[A5_ADDR:%.*]] = alloca i16, align 2
// X86-NEXT:    [[A6_ADDR:%.*]] = alloca i16, align 2
// X86-NEXT:    [[A7_ADDR:%.*]] = alloca i16, align 2
// X86-NEXT:    [[A8_ADDR:%.*]] = alloca i16, align 2
// X86-NEXT:    [[A9_ADDR:%.*]] = alloca i16, align 2
// X86-NEXT:    [[A10_ADDR:%.*]] = alloca i16, align 2
// X86-NEXT:    [[A11_ADDR:%.*]] = alloca i16, align 2
// X86-NEXT:    [[A12_ADDR:%.*]] = alloca i16, align 2
// X86-NEXT:    [[A13_ADDR:%.*]] = alloca i16, align 2
// X86-NEXT:    [[A14_ADDR:%.*]] = alloca i16, align 2
// X86-NEXT:    [[A15_ADDR:%.*]] = alloca i16, align 2
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store i16 [[A0]], ptr [[A0_ADDR]], align 2
// X86-NEXT:    store i16 [[A1]], ptr [[A1_ADDR]], align 2
// X86-NEXT:    store i16 [[A2]], ptr [[A2_ADDR]], align 2
// X86-NEXT:    store i16 [[A3]], ptr [[A3_ADDR]], align 2
// X86-NEXT:    store i16 [[A4]], ptr [[A4_ADDR]], align 2
// X86-NEXT:    store i16 [[A5]], ptr [[A5_ADDR]], align 2
// X86-NEXT:    store i16 [[A6]], ptr [[A6_ADDR]], align 2
// X86-NEXT:    store i16 [[A7]], ptr [[A7_ADDR]], align 2
// X86-NEXT:    store i16 [[A8]], ptr [[A8_ADDR]], align 2
// X86-NEXT:    store i16 [[A9]], ptr [[A9_ADDR]], align 2
// X86-NEXT:    store i16 [[A10]], ptr [[A10_ADDR]], align 2
// X86-NEXT:    store i16 [[A11]], ptr [[A11_ADDR]], align 2
// X86-NEXT:    store i16 [[A12]], ptr [[A12_ADDR]], align 2
// X86-NEXT:    store i16 [[A13]], ptr [[A13_ADDR]], align 2
// X86-NEXT:    store i16 [[A14]], ptr [[A14_ADDR]], align 2
// X86-NEXT:    store i16 [[A15]], ptr [[A15_ADDR]], align 2
// X86-NEXT:    [[TMP0:%.*]] = load i16, ptr [[A0_ADDR]], align 2
// X86-NEXT:    [[TMP1:%.*]] = load i16, ptr [[A1_ADDR]], align 2
// X86-NEXT:    [[TMP2:%.*]] = load i16, ptr [[A2_ADDR]], align 2
// X86-NEXT:    [[TMP3:%.*]] = load i16, ptr [[A3_ADDR]], align 2
// X86-NEXT:    [[TMP4:%.*]] = load i16, ptr [[A4_ADDR]], align 2
// X86-NEXT:    [[TMP5:%.*]] = load i16, ptr [[A5_ADDR]], align 2
// X86-NEXT:    [[TMP6:%.*]] = load i16, ptr [[A6_ADDR]], align 2
// X86-NEXT:    [[TMP7:%.*]] = load i16, ptr [[A7_ADDR]], align 2
// X86-NEXT:    [[TMP8:%.*]] = load i16, ptr [[A8_ADDR]], align 2
// X86-NEXT:    [[TMP9:%.*]] = load i16, ptr [[A9_ADDR]], align 2
// X86-NEXT:    [[TMP10:%.*]] = load i16, ptr [[A10_ADDR]], align 2
// X86-NEXT:    [[TMP11:%.*]] = load i16, ptr [[A11_ADDR]], align 2
// X86-NEXT:    [[TMP12:%.*]] = load i16, ptr [[A12_ADDR]], align 2
// X86-NEXT:    [[TMP13:%.*]] = load i16, ptr [[A13_ADDR]], align 2
// X86-NEXT:    [[TMP14:%.*]] = load i16, ptr [[A14_ADDR]], align 2
// X86-NEXT:    [[TMP15:%.*]] = load i16, ptr [[A15_ADDR]], align 2
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META198:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META198]]
// X86-NEXT:    store i16 [[TMP0]], ptr [[__W15_ADDR_I]], align 2, !noalias [[META198]]
// X86-NEXT:    store i16 [[TMP1]], ptr [[__W14_ADDR_I]], align 2, !noalias [[META198]]
// X86-NEXT:    store i16 [[TMP2]], ptr [[__W13_ADDR_I]], align 2, !noalias [[META198]]
// X86-NEXT:    store i16 [[TMP3]], ptr [[__W12_ADDR_I]], align 2, !noalias [[META198]]
// X86-NEXT:    store i16 [[TMP4]], ptr [[__W11_ADDR_I]], align 2, !noalias [[META198]]
// X86-NEXT:    store i16 [[TMP5]], ptr [[__W10_ADDR_I]], align 2, !noalias [[META198]]
// X86-NEXT:    store i16 [[TMP6]], ptr [[__W09_ADDR_I]], align 2, !noalias [[META198]]
// X86-NEXT:    store i16 [[TMP7]], ptr [[__W08_ADDR_I]], align 2, !noalias [[META198]]
// X86-NEXT:    store i16 [[TMP8]], ptr [[__W07_ADDR_I]], align 2, !noalias [[META198]]
// X86-NEXT:    store i16 [[TMP9]], ptr [[__W06_ADDR_I]], align 2, !noalias [[META198]]
// X86-NEXT:    store i16 [[TMP10]], ptr [[__W05_ADDR_I]], align 2, !noalias [[META198]]
// X86-NEXT:    store i16 [[TMP11]], ptr [[__W04_ADDR_I]], align 2, !noalias [[META198]]
// X86-NEXT:    store i16 [[TMP12]], ptr [[__W03_ADDR_I]], align 2, !noalias [[META198]]
// X86-NEXT:    store i16 [[TMP13]], ptr [[__W02_ADDR_I]], align 2, !noalias [[META198]]
// X86-NEXT:    store i16 [[TMP14]], ptr [[__W01_ADDR_I]], align 2, !noalias [[META198]]
// X86-NEXT:    store i16 [[TMP15]], ptr [[__W00_ADDR_I]], align 2, !noalias [[META198]]
// X86-NEXT:    [[TMP16:%.*]] = load i16, ptr [[__W00_ADDR_I]], align 2, !noalias [[META198]]
// X86-NEXT:    [[VECINIT_I:%.*]] = insertelement <16 x i16> poison, i16 [[TMP16]], i32 0
// X86-NEXT:    [[TMP17:%.*]] = load i16, ptr [[__W01_ADDR_I]], align 2, !noalias [[META198]]
// X86-NEXT:    [[VECINIT1_I:%.*]] = insertelement <16 x i16> [[VECINIT_I]], i16 [[TMP17]], i32 1
// X86-NEXT:    [[TMP18:%.*]] = load i16, ptr [[__W02_ADDR_I]], align 2, !noalias [[META198]]
// X86-NEXT:    [[VECINIT2_I:%.*]] = insertelement <16 x i16> [[VECINIT1_I]], i16 [[TMP18]], i32 2
// X86-NEXT:    [[TMP19:%.*]] = load i16, ptr [[__W03_ADDR_I]], align 2, !noalias [[META198]]
// X86-NEXT:    [[VECINIT3_I:%.*]] = insertelement <16 x i16> [[VECINIT2_I]], i16 [[TMP19]], i32 3
// X86-NEXT:    [[TMP20:%.*]] = load i16, ptr [[__W04_ADDR_I]], align 2, !noalias [[META198]]
// X86-NEXT:    [[VECINIT4_I:%.*]] = insertelement <16 x i16> [[VECINIT3_I]], i16 [[TMP20]], i32 4
// X86-NEXT:    [[TMP21:%.*]] = load i16, ptr [[__W05_ADDR_I]], align 2, !noalias [[META198]]
// X86-NEXT:    [[VECINIT5_I:%.*]] = insertelement <16 x i16> [[VECINIT4_I]], i16 [[TMP21]], i32 5
// X86-NEXT:    [[TMP22:%.*]] = load i16, ptr [[__W06_ADDR_I]], align 2, !noalias [[META198]]
// X86-NEXT:    [[VECINIT6_I:%.*]] = insertelement <16 x i16> [[VECINIT5_I]], i16 [[TMP22]], i32 6
// X86-NEXT:    [[TMP23:%.*]] = load i16, ptr [[__W07_ADDR_I]], align 2, !noalias [[META198]]
// X86-NEXT:    [[VECINIT7_I:%.*]] = insertelement <16 x i16> [[VECINIT6_I]], i16 [[TMP23]], i32 7
// X86-NEXT:    [[TMP24:%.*]] = load i16, ptr [[__W08_ADDR_I]], align 2, !noalias [[META198]]
// X86-NEXT:    [[VECINIT8_I:%.*]] = insertelement <16 x i16> [[VECINIT7_I]], i16 [[TMP24]], i32 8
// X86-NEXT:    [[TMP25:%.*]] = load i16, ptr [[__W09_ADDR_I]], align 2, !noalias [[META198]]
// X86-NEXT:    [[VECINIT9_I:%.*]] = insertelement <16 x i16> [[VECINIT8_I]], i16 [[TMP25]], i32 9
// X86-NEXT:    [[TMP26:%.*]] = load i16, ptr [[__W10_ADDR_I]], align 2, !noalias [[META198]]
// X86-NEXT:    [[VECINIT10_I:%.*]] = insertelement <16 x i16> [[VECINIT9_I]], i16 [[TMP26]], i32 10
// X86-NEXT:    [[TMP27:%.*]] = load i16, ptr [[__W11_ADDR_I]], align 2, !noalias [[META198]]
// X86-NEXT:    [[VECINIT11_I:%.*]] = insertelement <16 x i16> [[VECINIT10_I]], i16 [[TMP27]], i32 11
// X86-NEXT:    [[TMP28:%.*]] = load i16, ptr [[__W12_ADDR_I]], align 2, !noalias [[META198]]
// X86-NEXT:    [[VECINIT12_I:%.*]] = insertelement <16 x i16> [[VECINIT11_I]], i16 [[TMP28]], i32 12
// X86-NEXT:    [[TMP29:%.*]] = load i16, ptr [[__W13_ADDR_I]], align 2, !noalias [[META198]]
// X86-NEXT:    [[VECINIT13_I:%.*]] = insertelement <16 x i16> [[VECINIT12_I]], i16 [[TMP29]], i32 13
// X86-NEXT:    [[TMP30:%.*]] = load i16, ptr [[__W14_ADDR_I]], align 2, !noalias [[META198]]
// X86-NEXT:    [[VECINIT14_I:%.*]] = insertelement <16 x i16> [[VECINIT13_I]], i16 [[TMP30]], i32 14
// X86-NEXT:    [[TMP31:%.*]] = load i16, ptr [[__W15_ADDR_I]], align 2, !noalias [[META198]]
// X86-NEXT:    [[VECINIT15_I:%.*]] = insertelement <16 x i16> [[VECINIT14_I]], i16 [[TMP31]], i32 15
// X86-NEXT:    store <16 x i16> [[VECINIT15_I]], ptr [[DOTCOMPOUNDLITERAL_I]], align 32, !noalias [[META198]]
// X86-NEXT:    [[TMP32:%.*]] = load <16 x i16>, ptr [[DOTCOMPOUNDLITERAL_I]], align 32, !noalias [[META198]]
// X86-NEXT:    [[TMP33:%.*]] = bitcast <16 x i16> [[TMP32]] to <4 x i64>
// X86-NEXT:    store <4 x i64> [[TMP33]], ptr [[TMP]], align 32, !alias.scope [[META198]]
// X86-NEXT:    [[TMP34:%.*]] = load <4 x i64>, ptr [[TMP]], align 32, !alias.scope [[META198]]
// X86-NEXT:    store <4 x i64> [[TMP34]], ptr [[TMP]], align 32, !alias.scope [[META198]]
// X86-NEXT:    [[TMP35:%.*]] = load <4 x i64>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x i64> [[TMP35]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP36:%.*]] = load <4 x i64>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x i64> [[TMP36]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256i test_mm256_set_epi16(short A0, short A1, short A2, short A3, short A4, short A5, short A6, short A7,
                             short A8, short A9, short A10, short A11, short A12, short A13, short A14, short A15) {
  return _mm256_set_epi16(A0, A1, A2, A3, A4, A5, A6, A7, A8, A9, A10, A11, A12, A13, A14, A15);
}

//
// X86-LABEL: define void @test_mm256_set_epi32(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x i64>) align 32 [[AGG_RESULT:%.*]], i32 noundef [[A0:%.*]], i32 noundef [[A1:%.*]], i32 noundef [[A2:%.*]], i32 noundef [[A3:%.*]], i32 noundef [[A4:%.*]], i32 noundef [[A5:%.*]], i32 noundef [[A6:%.*]], i32 noundef [[A7:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__I0_ADDR_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[__I1_ADDR_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[__I2_ADDR_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[__I3_ADDR_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[__I4_ADDR_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[__I5_ADDR_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[__I6_ADDR_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[__I7_ADDR_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x i32>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A0_ADDR:%.*]] = alloca i32, align 4
// X86-NEXT:    [[A1_ADDR:%.*]] = alloca i32, align 4
// X86-NEXT:    [[A2_ADDR:%.*]] = alloca i32, align 4
// X86-NEXT:    [[A3_ADDR:%.*]] = alloca i32, align 4
// X86-NEXT:    [[A4_ADDR:%.*]] = alloca i32, align 4
// X86-NEXT:    [[A5_ADDR:%.*]] = alloca i32, align 4
// X86-NEXT:    [[A6_ADDR:%.*]] = alloca i32, align 4
// X86-NEXT:    [[A7_ADDR:%.*]] = alloca i32, align 4
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store i32 [[A0]], ptr [[A0_ADDR]], align 4
// X86-NEXT:    store i32 [[A1]], ptr [[A1_ADDR]], align 4
// X86-NEXT:    store i32 [[A2]], ptr [[A2_ADDR]], align 4
// X86-NEXT:    store i32 [[A3]], ptr [[A3_ADDR]], align 4
// X86-NEXT:    store i32 [[A4]], ptr [[A4_ADDR]], align 4
// X86-NEXT:    store i32 [[A5]], ptr [[A5_ADDR]], align 4
// X86-NEXT:    store i32 [[A6]], ptr [[A6_ADDR]], align 4
// X86-NEXT:    store i32 [[A7]], ptr [[A7_ADDR]], align 4
// X86-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A0_ADDR]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load i32, ptr [[A1_ADDR]], align 4
// X86-NEXT:    [[TMP2:%.*]] = load i32, ptr [[A2_ADDR]], align 4
// X86-NEXT:    [[TMP3:%.*]] = load i32, ptr [[A3_ADDR]], align 4
// X86-NEXT:    [[TMP4:%.*]] = load i32, ptr [[A4_ADDR]], align 4
// X86-NEXT:    [[TMP5:%.*]] = load i32, ptr [[A5_ADDR]], align 4
// X86-NEXT:    [[TMP6:%.*]] = load i32, ptr [[A6_ADDR]], align 4
// X86-NEXT:    [[TMP7:%.*]] = load i32, ptr [[A7_ADDR]], align 4
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META201:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META201]]
// X86-NEXT:    store i32 [[TMP0]], ptr [[__I0_ADDR_I]], align 4, !noalias [[META201]]
// X86-NEXT:    store i32 [[TMP1]], ptr [[__I1_ADDR_I]], align 4, !noalias [[META201]]
// X86-NEXT:    store i32 [[TMP2]], ptr [[__I2_ADDR_I]], align 4, !noalias [[META201]]
// X86-NEXT:    store i32 [[TMP3]], ptr [[__I3_ADDR_I]], align 4, !noalias [[META201]]
// X86-NEXT:    store i32 [[TMP4]], ptr [[__I4_ADDR_I]], align 4, !noalias [[META201]]
// X86-NEXT:    store i32 [[TMP5]], ptr [[__I5_ADDR_I]], align 4, !noalias [[META201]]
// X86-NEXT:    store i32 [[TMP6]], ptr [[__I6_ADDR_I]], align 4, !noalias [[META201]]
// X86-NEXT:    store i32 [[TMP7]], ptr [[__I7_ADDR_I]], align 4, !noalias [[META201]]
// X86-NEXT:    [[TMP8:%.*]] = load i32, ptr [[__I7_ADDR_I]], align 4, !noalias [[META201]]
// X86-NEXT:    [[VECINIT_I:%.*]] = insertelement <8 x i32> poison, i32 [[TMP8]], i32 0
// X86-NEXT:    [[TMP9:%.*]] = load i32, ptr [[__I6_ADDR_I]], align 4, !noalias [[META201]]
// X86-NEXT:    [[VECINIT1_I:%.*]] = insertelement <8 x i32> [[VECINIT_I]], i32 [[TMP9]], i32 1
// X86-NEXT:    [[TMP10:%.*]] = load i32, ptr [[__I5_ADDR_I]], align 4, !noalias [[META201]]
// X86-NEXT:    [[VECINIT2_I:%.*]] = insertelement <8 x i32> [[VECINIT1_I]], i32 [[TMP10]], i32 2
// X86-NEXT:    [[TMP11:%.*]] = load i32, ptr [[__I4_ADDR_I]], align 4, !noalias [[META201]]
// X86-NEXT:    [[VECINIT3_I:%.*]] = insertelement <8 x i32> [[VECINIT2_I]], i32 [[TMP11]], i32 3
// X86-NEXT:    [[TMP12:%.*]] = load i32, ptr [[__I3_ADDR_I]], align 4, !noalias [[META201]]
// X86-NEXT:    [[VECINIT4_I:%.*]] = insertelement <8 x i32> [[VECINIT3_I]], i32 [[TMP12]], i32 4
// X86-NEXT:    [[TMP13:%.*]] = load i32, ptr [[__I2_ADDR_I]], align 4, !noalias [[META201]]
// X86-NEXT:    [[VECINIT5_I:%.*]] = insertelement <8 x i32> [[VECINIT4_I]], i32 [[TMP13]], i32 5
// X86-NEXT:    [[TMP14:%.*]] = load i32, ptr [[__I1_ADDR_I]], align 4, !noalias [[META201]]
// X86-NEXT:    [[VECINIT6_I:%.*]] = insertelement <8 x i32> [[VECINIT5_I]], i32 [[TMP14]], i32 6
// X86-NEXT:    [[TMP15:%.*]] = load i32, ptr [[__I0_ADDR_I]], align 4, !noalias [[META201]]
// X86-NEXT:    [[VECINIT7_I:%.*]] = insertelement <8 x i32> [[VECINIT6_I]], i32 [[TMP15]], i32 7
// X86-NEXT:    store <8 x i32> [[VECINIT7_I]], ptr [[DOTCOMPOUNDLITERAL_I]], align 32, !noalias [[META201]]
// X86-NEXT:    [[TMP16:%.*]] = load <8 x i32>, ptr [[DOTCOMPOUNDLITERAL_I]], align 32, !noalias [[META201]]
// X86-NEXT:    [[TMP17:%.*]] = bitcast <8 x i32> [[TMP16]] to <4 x i64>
// X86-NEXT:    store <4 x i64> [[TMP17]], ptr [[TMP]], align 32, !alias.scope [[META201]]
// X86-NEXT:    [[TMP18:%.*]] = load <4 x i64>, ptr [[TMP]], align 32, !alias.scope [[META201]]
// X86-NEXT:    store <4 x i64> [[TMP18]], ptr [[TMP]], align 32, !alias.scope [[META201]]
// X86-NEXT:    [[TMP19:%.*]] = load <4 x i64>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x i64> [[TMP19]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP20:%.*]] = load <4 x i64>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x i64> [[TMP20]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256i test_mm256_set_epi32(int A0, int A1, int A2, int A3, int A4, int A5, int A6, int A7) {
  return _mm256_set_epi32(A0, A1, A2, A3, A4, A5, A6, A7);
}

//
// X86-LABEL: define void @test_mm256_set_epi64x(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x i64>) align 32 [[AGG_RESULT:%.*]], i64 noundef [[A0:%.*]], i64 noundef [[A1:%.*]], i64 noundef [[A2:%.*]], i64 noundef [[A3:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca i64, align 8
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca i64, align 8
// X86-NEXT:    [[__C_ADDR_I:%.*]] = alloca i64, align 8
// X86-NEXT:    [[__D_ADDR_I:%.*]] = alloca i64, align 8
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A0_ADDR:%.*]] = alloca i64, align 8
// X86-NEXT:    [[A1_ADDR:%.*]] = alloca i64, align 8
// X86-NEXT:    [[A2_ADDR:%.*]] = alloca i64, align 8
// X86-NEXT:    [[A3_ADDR:%.*]] = alloca i64, align 8
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store i64 [[A0]], ptr [[A0_ADDR]], align 8
// X86-NEXT:    store i64 [[A1]], ptr [[A1_ADDR]], align 8
// X86-NEXT:    store i64 [[A2]], ptr [[A2_ADDR]], align 8
// X86-NEXT:    store i64 [[A3]], ptr [[A3_ADDR]], align 8
// X86-NEXT:    [[TMP0:%.*]] = load i64, ptr [[A0_ADDR]], align 8
// X86-NEXT:    [[TMP1:%.*]] = load i64, ptr [[A1_ADDR]], align 8
// X86-NEXT:    [[TMP2:%.*]] = load i64, ptr [[A2_ADDR]], align 8
// X86-NEXT:    [[TMP3:%.*]] = load i64, ptr [[A3_ADDR]], align 8
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META204:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META204]]
// X86-NEXT:    store i64 [[TMP0]], ptr [[__A_ADDR_I]], align 8, !noalias [[META204]]
// X86-NEXT:    store i64 [[TMP1]], ptr [[__B_ADDR_I]], align 8, !noalias [[META204]]
// X86-NEXT:    store i64 [[TMP2]], ptr [[__C_ADDR_I]], align 8, !noalias [[META204]]
// X86-NEXT:    store i64 [[TMP3]], ptr [[__D_ADDR_I]], align 8, !noalias [[META204]]
// X86-NEXT:    [[TMP4:%.*]] = load i64, ptr [[__D_ADDR_I]], align 8, !noalias [[META204]]
// X86-NEXT:    [[VECINIT_I:%.*]] = insertelement <4 x i64> poison, i64 [[TMP4]], i32 0
// X86-NEXT:    [[TMP5:%.*]] = load i64, ptr [[__C_ADDR_I]], align 8, !noalias [[META204]]
// X86-NEXT:    [[VECINIT1_I:%.*]] = insertelement <4 x i64> [[VECINIT_I]], i64 [[TMP5]], i32 1
// X86-NEXT:    [[TMP6:%.*]] = load i64, ptr [[__B_ADDR_I]], align 8, !noalias [[META204]]
// X86-NEXT:    [[VECINIT2_I:%.*]] = insertelement <4 x i64> [[VECINIT1_I]], i64 [[TMP6]], i32 2
// X86-NEXT:    [[TMP7:%.*]] = load i64, ptr [[__A_ADDR_I]], align 8, !noalias [[META204]]
// X86-NEXT:    [[VECINIT3_I:%.*]] = insertelement <4 x i64> [[VECINIT2_I]], i64 [[TMP7]], i32 3
// X86-NEXT:    store <4 x i64> [[VECINIT3_I]], ptr [[DOTCOMPOUNDLITERAL_I]], align 32, !noalias [[META204]]
// X86-NEXT:    [[TMP8:%.*]] = load <4 x i64>, ptr [[DOTCOMPOUNDLITERAL_I]], align 32, !noalias [[META204]]
// X86-NEXT:    store <4 x i64> [[TMP8]], ptr [[TMP]], align 32, !alias.scope [[META204]]
// X86-NEXT:    [[TMP9:%.*]] = load <4 x i64>, ptr [[TMP]], align 32, !alias.scope [[META204]]
// X86-NEXT:    store <4 x i64> [[TMP9]], ptr [[TMP]], align 32, !alias.scope [[META204]]
// X86-NEXT:    [[TMP10:%.*]] = load <4 x i64>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x i64> [[TMP10]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP11:%.*]] = load <4 x i64>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x i64> [[TMP11]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256i test_mm256_set_epi64x(long long A0, long long A1, long long A2, long long A3) {
  return _mm256_set_epi64x(A0, A1, A2, A3);
}

//
// X86-LABEL: define void @test_mm256_set_m128(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <4 x float> noundef [[A:%.*]], <4 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__HI_ADDR_I:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[__LO_ADDR_I:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[TMP:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <4 x float> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[B_ADDR]], align 16
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META207:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META207]]
// X86-NEXT:    store <4 x float> [[TMP0]], ptr [[__HI_ADDR_I]], align 16, !noalias [[META207]]
// X86-NEXT:    store <4 x float> [[TMP1]], ptr [[__LO_ADDR_I]], align 16, !noalias [[META207]]
// X86-NEXT:    [[TMP2:%.*]] = load <4 x float>, ptr [[__LO_ADDR_I]], align 16, !noalias [[META207]]
// X86-NEXT:    [[TMP3:%.*]] = load <4 x float>, ptr [[__HI_ADDR_I]], align 16, !noalias [[META207]]
// X86-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <4 x float> [[TMP2]], <4 x float> [[TMP3]], <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
// X86-NEXT:    store <8 x float> [[SHUFFLE_I]], ptr [[TMP]], align 32, !alias.scope [[META207]]
// X86-NEXT:    [[TMP4:%.*]] = load <8 x float>, ptr [[TMP]], align 32, !alias.scope [[META207]]
// X86-NEXT:    store <8 x float> [[TMP4]], ptr [[TMP]], align 32, !alias.scope [[META207]]
// X86-NEXT:    [[TMP5:%.*]] = load <8 x float>, ptr [[TMP]], align 32
// X86-NEXT:    store <8 x float> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP6:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP6]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_set_m128(__m128 A, __m128 B) {
  return _mm256_set_m128(A, B);
}

//
// X86-LABEL: define void @test_mm256_set_m128d(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__HI_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__LO_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META210:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META210]]
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__HI_ADDR_I]], align 16, !noalias [[META210]]
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__LO_ADDR_I]], align 16, !noalias [[META210]]
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__LO_ADDR_I]], align 16, !noalias [[META210]]
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__HI_ADDR_I]], align 16, !noalias [[META210]]
// X86-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <2 x double> [[TMP2]], <2 x double> [[TMP3]], <4 x i32> <i32 0, i32 1, i32 2, i32 3>
// X86-NEXT:    store <4 x double> [[SHUFFLE_I]], ptr [[TMP]], align 32, !alias.scope [[META210]]
// X86-NEXT:    [[TMP4:%.*]] = load <4 x double>, ptr [[TMP]], align 32, !alias.scope [[META210]]
// X86-NEXT:    store <4 x double> [[TMP4]], ptr [[TMP]], align 32, !alias.scope [[META210]]
// X86-NEXT:    [[TMP5:%.*]] = load <4 x double>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x double> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP6:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP6]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_set_m128d(__m128d A, __m128d B) {
  return _mm256_set_m128d(A, B);
}

//
// X86-LABEL: define void @test_mm256_set_m128i(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x i64>) align 32 [[AGG_RESULT:%.*]], <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__HI_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__LO_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META213:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META213]]
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__HI_ADDR_I]], align 16, !noalias [[META213]]
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__LO_ADDR_I]], align 16, !noalias [[META213]]
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__LO_ADDR_I]], align 16, !noalias [[META213]]
// X86-NEXT:    [[TMP3:%.*]] = load <2 x i64>, ptr [[__HI_ADDR_I]], align 16, !noalias [[META213]]
// X86-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <2 x i64> [[TMP2]], <2 x i64> [[TMP3]], <4 x i32> <i32 0, i32 1, i32 2, i32 3>
// X86-NEXT:    store <4 x i64> [[SHUFFLE_I]], ptr [[TMP]], align 32, !alias.scope [[META213]]
// X86-NEXT:    [[TMP4:%.*]] = load <4 x i64>, ptr [[TMP]], align 32, !alias.scope [[META213]]
// X86-NEXT:    store <4 x i64> [[TMP4]], ptr [[TMP]], align 32, !alias.scope [[META213]]
// X86-NEXT:    [[TMP5:%.*]] = load <4 x i64>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x i64> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP6:%.*]] = load <4 x i64>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x i64> [[TMP6]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256i test_mm256_set_m128i(__m128i A, __m128i B) {
  return _mm256_set_m128i(A, B);
}

//
// X86-LABEL: define void @test_mm256_set_pd(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], double noundef [[A0:%.*]], double noundef [[A1:%.*]], double noundef [[A2:%.*]], double noundef [[A3:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca double, align 8
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca double, align 8
// X86-NEXT:    [[__C_ADDR_I:%.*]] = alloca double, align 8
// X86-NEXT:    [[__D_ADDR_I:%.*]] = alloca double, align 8
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A0_ADDR:%.*]] = alloca double, align 8
// X86-NEXT:    [[A1_ADDR:%.*]] = alloca double, align 8
// X86-NEXT:    [[A2_ADDR:%.*]] = alloca double, align 8
// X86-NEXT:    [[A3_ADDR:%.*]] = alloca double, align 8
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store double [[A0]], ptr [[A0_ADDR]], align 8
// X86-NEXT:    store double [[A1]], ptr [[A1_ADDR]], align 8
// X86-NEXT:    store double [[A2]], ptr [[A2_ADDR]], align 8
// X86-NEXT:    store double [[A3]], ptr [[A3_ADDR]], align 8
// X86-NEXT:    [[TMP0:%.*]] = load double, ptr [[A0_ADDR]], align 8
// X86-NEXT:    [[TMP1:%.*]] = load double, ptr [[A1_ADDR]], align 8
// X86-NEXT:    [[TMP2:%.*]] = load double, ptr [[A2_ADDR]], align 8
// X86-NEXT:    [[TMP3:%.*]] = load double, ptr [[A3_ADDR]], align 8
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META216:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META216]]
// X86-NEXT:    store double [[TMP0]], ptr [[__A_ADDR_I]], align 8, !noalias [[META216]]
// X86-NEXT:    store double [[TMP1]], ptr [[__B_ADDR_I]], align 8, !noalias [[META216]]
// X86-NEXT:    store double [[TMP2]], ptr [[__C_ADDR_I]], align 8, !noalias [[META216]]
// X86-NEXT:    store double [[TMP3]], ptr [[__D_ADDR_I]], align 8, !noalias [[META216]]
// X86-NEXT:    [[TMP4:%.*]] = load double, ptr [[__D_ADDR_I]], align 8, !noalias [[META216]]
// X86-NEXT:    [[VECINIT_I:%.*]] = insertelement <4 x double> poison, double [[TMP4]], i32 0
// X86-NEXT:    [[TMP5:%.*]] = load double, ptr [[__C_ADDR_I]], align 8, !noalias [[META216]]
// X86-NEXT:    [[VECINIT1_I:%.*]] = insertelement <4 x double> [[VECINIT_I]], double [[TMP5]], i32 1
// X86-NEXT:    [[TMP6:%.*]] = load double, ptr [[__B_ADDR_I]], align 8, !noalias [[META216]]
// X86-NEXT:    [[VECINIT2_I:%.*]] = insertelement <4 x double> [[VECINIT1_I]], double [[TMP6]], i32 2
// X86-NEXT:    [[TMP7:%.*]] = load double, ptr [[__A_ADDR_I]], align 8, !noalias [[META216]]
// X86-NEXT:    [[VECINIT3_I:%.*]] = insertelement <4 x double> [[VECINIT2_I]], double [[TMP7]], i32 3
// X86-NEXT:    store <4 x double> [[VECINIT3_I]], ptr [[DOTCOMPOUNDLITERAL_I]], align 32, !noalias [[META216]]
// X86-NEXT:    [[TMP8:%.*]] = load <4 x double>, ptr [[DOTCOMPOUNDLITERAL_I]], align 32, !noalias [[META216]]
// X86-NEXT:    store <4 x double> [[TMP8]], ptr [[TMP]], align 32, !alias.scope [[META216]]
// X86-NEXT:    [[TMP9:%.*]] = load <4 x double>, ptr [[TMP]], align 32, !alias.scope [[META216]]
// X86-NEXT:    store <4 x double> [[TMP9]], ptr [[TMP]], align 32, !alias.scope [[META216]]
// X86-NEXT:    [[TMP10:%.*]] = load <4 x double>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x double> [[TMP10]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP11:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP11]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_set_pd(double A0, double A1, double A2, double A3) {
  return _mm256_set_pd(A0, A1, A2, A3);
}

//
// X86-LABEL: define void @test_mm256_set_ps(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], float noundef [[A0:%.*]], float noundef [[A1:%.*]], float noundef [[A2:%.*]], float noundef [[A3:%.*]], float noundef [[A4:%.*]], float noundef [[A5:%.*]], float noundef [[A6:%.*]], float noundef [[A7:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca float, align 4
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca float, align 4
// X86-NEXT:    [[__C_ADDR_I:%.*]] = alloca float, align 4
// X86-NEXT:    [[__D_ADDR_I:%.*]] = alloca float, align 4
// X86-NEXT:    [[__E_ADDR_I:%.*]] = alloca float, align 4
// X86-NEXT:    [[__F_ADDR_I:%.*]] = alloca float, align 4
// X86-NEXT:    [[__G_ADDR_I:%.*]] = alloca float, align 4
// X86-NEXT:    [[__H_ADDR_I:%.*]] = alloca float, align 4
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A0_ADDR:%.*]] = alloca float, align 4
// X86-NEXT:    [[A1_ADDR:%.*]] = alloca float, align 4
// X86-NEXT:    [[A2_ADDR:%.*]] = alloca float, align 4
// X86-NEXT:    [[A3_ADDR:%.*]] = alloca float, align 4
// X86-NEXT:    [[A4_ADDR:%.*]] = alloca float, align 4
// X86-NEXT:    [[A5_ADDR:%.*]] = alloca float, align 4
// X86-NEXT:    [[A6_ADDR:%.*]] = alloca float, align 4
// X86-NEXT:    [[A7_ADDR:%.*]] = alloca float, align 4
// X86-NEXT:    [[TMP:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store float [[A0]], ptr [[A0_ADDR]], align 4
// X86-NEXT:    store float [[A1]], ptr [[A1_ADDR]], align 4
// X86-NEXT:    store float [[A2]], ptr [[A2_ADDR]], align 4
// X86-NEXT:    store float [[A3]], ptr [[A3_ADDR]], align 4
// X86-NEXT:    store float [[A4]], ptr [[A4_ADDR]], align 4
// X86-NEXT:    store float [[A5]], ptr [[A5_ADDR]], align 4
// X86-NEXT:    store float [[A6]], ptr [[A6_ADDR]], align 4
// X86-NEXT:    store float [[A7]], ptr [[A7_ADDR]], align 4
// X86-NEXT:    [[TMP0:%.*]] = load float, ptr [[A0_ADDR]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load float, ptr [[A1_ADDR]], align 4
// X86-NEXT:    [[TMP2:%.*]] = load float, ptr [[A2_ADDR]], align 4
// X86-NEXT:    [[TMP3:%.*]] = load float, ptr [[A3_ADDR]], align 4
// X86-NEXT:    [[TMP4:%.*]] = load float, ptr [[A4_ADDR]], align 4
// X86-NEXT:    [[TMP5:%.*]] = load float, ptr [[A5_ADDR]], align 4
// X86-NEXT:    [[TMP6:%.*]] = load float, ptr [[A6_ADDR]], align 4
// X86-NEXT:    [[TMP7:%.*]] = load float, ptr [[A7_ADDR]], align 4
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META219:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META219]]
// X86-NEXT:    store float [[TMP0]], ptr [[__A_ADDR_I]], align 4, !noalias [[META219]]
// X86-NEXT:    store float [[TMP1]], ptr [[__B_ADDR_I]], align 4, !noalias [[META219]]
// X86-NEXT:    store float [[TMP2]], ptr [[__C_ADDR_I]], align 4, !noalias [[META219]]
// X86-NEXT:    store float [[TMP3]], ptr [[__D_ADDR_I]], align 4, !noalias [[META219]]
// X86-NEXT:    store float [[TMP4]], ptr [[__E_ADDR_I]], align 4, !noalias [[META219]]
// X86-NEXT:    store float [[TMP5]], ptr [[__F_ADDR_I]], align 4, !noalias [[META219]]
// X86-NEXT:    store float [[TMP6]], ptr [[__G_ADDR_I]], align 4, !noalias [[META219]]
// X86-NEXT:    store float [[TMP7]], ptr [[__H_ADDR_I]], align 4, !noalias [[META219]]
// X86-NEXT:    [[TMP8:%.*]] = load float, ptr [[__H_ADDR_I]], align 4, !noalias [[META219]]
// X86-NEXT:    [[VECINIT_I:%.*]] = insertelement <8 x float> poison, float [[TMP8]], i32 0
// X86-NEXT:    [[TMP9:%.*]] = load float, ptr [[__G_ADDR_I]], align 4, !noalias [[META219]]
// X86-NEXT:    [[VECINIT1_I:%.*]] = insertelement <8 x float> [[VECINIT_I]], float [[TMP9]], i32 1
// X86-NEXT:    [[TMP10:%.*]] = load float, ptr [[__F_ADDR_I]], align 4, !noalias [[META219]]
// X86-NEXT:    [[VECINIT2_I:%.*]] = insertelement <8 x float> [[VECINIT1_I]], float [[TMP10]], i32 2
// X86-NEXT:    [[TMP11:%.*]] = load float, ptr [[__E_ADDR_I]], align 4, !noalias [[META219]]
// X86-NEXT:    [[VECINIT3_I:%.*]] = insertelement <8 x float> [[VECINIT2_I]], float [[TMP11]], i32 3
// X86-NEXT:    [[TMP12:%.*]] = load float, ptr [[__D_ADDR_I]], align 4, !noalias [[META219]]
// X86-NEXT:    [[VECINIT4_I:%.*]] = insertelement <8 x float> [[VECINIT3_I]], float [[TMP12]], i32 4
// X86-NEXT:    [[TMP13:%.*]] = load float, ptr [[__C_ADDR_I]], align 4, !noalias [[META219]]
// X86-NEXT:    [[VECINIT5_I:%.*]] = insertelement <8 x float> [[VECINIT4_I]], float [[TMP13]], i32 5
// X86-NEXT:    [[TMP14:%.*]] = load float, ptr [[__B_ADDR_I]], align 4, !noalias [[META219]]
// X86-NEXT:    [[VECINIT6_I:%.*]] = insertelement <8 x float> [[VECINIT5_I]], float [[TMP14]], i32 6
// X86-NEXT:    [[TMP15:%.*]] = load float, ptr [[__A_ADDR_I]], align 4, !noalias [[META219]]
// X86-NEXT:    [[VECINIT7_I:%.*]] = insertelement <8 x float> [[VECINIT6_I]], float [[TMP15]], i32 7
// X86-NEXT:    store <8 x float> [[VECINIT7_I]], ptr [[DOTCOMPOUNDLITERAL_I]], align 32, !noalias [[META219]]
// X86-NEXT:    [[TMP16:%.*]] = load <8 x float>, ptr [[DOTCOMPOUNDLITERAL_I]], align 32, !noalias [[META219]]
// X86-NEXT:    store <8 x float> [[TMP16]], ptr [[TMP]], align 32, !alias.scope [[META219]]
// X86-NEXT:    [[TMP17:%.*]] = load <8 x float>, ptr [[TMP]], align 32, !alias.scope [[META219]]
// X86-NEXT:    store <8 x float> [[TMP17]], ptr [[TMP]], align 32, !alias.scope [[META219]]
// X86-NEXT:    [[TMP18:%.*]] = load <8 x float>, ptr [[TMP]], align 32
// X86-NEXT:    store <8 x float> [[TMP18]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP19:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP19]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_set_ps(float A0, float A1, float A2, float A3, float A4, float A5, float A6, float A7) {
  return _mm256_set_ps(A0, A1, A2, A3, A4, A5, A6, A7);
}

//
__m256i test_mm256_set1_epi8(char A) {
  return _mm256_set1_epi8(A);
}

//
// X86-LABEL: define void @test_mm256_set1_epi16(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x i64>) align 32 [[AGG_RESULT:%.*]], i16 noundef signext [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__W15_ADDR_I_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W14_ADDR_I_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W13_ADDR_I_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W12_ADDR_I_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W11_ADDR_I_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W10_ADDR_I_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W09_ADDR_I_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W08_ADDR_I_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W07_ADDR_I_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W06_ADDR_I_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W05_ADDR_I_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W04_ADDR_I_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W03_ADDR_I_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W02_ADDR_I_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W01_ADDR_I_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W00_ADDR_I_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <16 x i16>, align 32
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__W_ADDR_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[TMP_I:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca i16, align 2
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store i16 [[A]], ptr [[A_ADDR]], align 2
// X86-NEXT:    [[TMP0:%.*]] = load i16, ptr [[A_ADDR]], align 2
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META229:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META229]]
// X86-NEXT:    store i16 [[TMP0]], ptr [[__W_ADDR_I]], align 2, !noalias [[META229]]
// X86-NEXT:    [[TMP1:%.*]] = load i16, ptr [[__W_ADDR_I]], align 2, !noalias [[META229]]
// X86-NEXT:    [[TMP2:%.*]] = load i16, ptr [[__W_ADDR_I]], align 2, !noalias [[META229]]
// X86-NEXT:    [[TMP3:%.*]] = load i16, ptr [[__W_ADDR_I]], align 2, !noalias [[META229]]
// X86-NEXT:    [[TMP4:%.*]] = load i16, ptr [[__W_ADDR_I]], align 2, !noalias [[META229]]
// X86-NEXT:    [[TMP5:%.*]] = load i16, ptr [[__W_ADDR_I]], align 2, !noalias [[META229]]
// X86-NEXT:    [[TMP6:%.*]] = load i16, ptr [[__W_ADDR_I]], align 2, !noalias [[META229]]
// X86-NEXT:    [[TMP7:%.*]] = load i16, ptr [[__W_ADDR_I]], align 2, !noalias [[META229]]
// X86-NEXT:    [[TMP8:%.*]] = load i16, ptr [[__W_ADDR_I]], align 2, !noalias [[META229]]
// X86-NEXT:    [[TMP9:%.*]] = load i16, ptr [[__W_ADDR_I]], align 2, !noalias [[META229]]
// X86-NEXT:    [[TMP10:%.*]] = load i16, ptr [[__W_ADDR_I]], align 2, !noalias [[META229]]
// X86-NEXT:    [[TMP11:%.*]] = load i16, ptr [[__W_ADDR_I]], align 2, !noalias [[META229]]
// X86-NEXT:    [[TMP12:%.*]] = load i16, ptr [[__W_ADDR_I]], align 2, !noalias [[META229]]
// X86-NEXT:    [[TMP13:%.*]] = load i16, ptr [[__W_ADDR_I]], align 2, !noalias [[META229]]
// X86-NEXT:    [[TMP14:%.*]] = load i16, ptr [[__W_ADDR_I]], align 2, !noalias [[META229]]
// X86-NEXT:    [[TMP15:%.*]] = load i16, ptr [[__W_ADDR_I]], align 2, !noalias [[META229]]
// X86-NEXT:    [[TMP16:%.*]] = load i16, ptr [[__W_ADDR_I]], align 2, !noalias [[META229]]
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META232:![0-9]+]])
// X86-NEXT:    store ptr [[TMP_I]], ptr [[RESULT_PTR_I_I]], align 4, !noalias [[META235:![0-9]+]]
// X86-NEXT:    store i16 [[TMP1]], ptr [[__W15_ADDR_I_I]], align 2, !noalias [[META235]]
// X86-NEXT:    store i16 [[TMP2]], ptr [[__W14_ADDR_I_I]], align 2, !noalias [[META235]]
// X86-NEXT:    store i16 [[TMP3]], ptr [[__W13_ADDR_I_I]], align 2, !noalias [[META235]]
// X86-NEXT:    store i16 [[TMP4]], ptr [[__W12_ADDR_I_I]], align 2, !noalias [[META235]]
// X86-NEXT:    store i16 [[TMP5]], ptr [[__W11_ADDR_I_I]], align 2, !noalias [[META235]]
// X86-NEXT:    store i16 [[TMP6]], ptr [[__W10_ADDR_I_I]], align 2, !noalias [[META235]]
// X86-NEXT:    store i16 [[TMP7]], ptr [[__W09_ADDR_I_I]], align 2, !noalias [[META235]]
// X86-NEXT:    store i16 [[TMP8]], ptr [[__W08_ADDR_I_I]], align 2, !noalias [[META235]]
// X86-NEXT:    store i16 [[TMP9]], ptr [[__W07_ADDR_I_I]], align 2, !noalias [[META235]]
// X86-NEXT:    store i16 [[TMP10]], ptr [[__W06_ADDR_I_I]], align 2, !noalias [[META235]]
// X86-NEXT:    store i16 [[TMP11]], ptr [[__W05_ADDR_I_I]], align 2, !noalias [[META235]]
// X86-NEXT:    store i16 [[TMP12]], ptr [[__W04_ADDR_I_I]], align 2, !noalias [[META235]]
// X86-NEXT:    store i16 [[TMP13]], ptr [[__W03_ADDR_I_I]], align 2, !noalias [[META235]]
// X86-NEXT:    store i16 [[TMP14]], ptr [[__W02_ADDR_I_I]], align 2, !noalias [[META235]]
// X86-NEXT:    store i16 [[TMP15]], ptr [[__W01_ADDR_I_I]], align 2, !noalias [[META235]]
// X86-NEXT:    store i16 [[TMP16]], ptr [[__W00_ADDR_I_I]], align 2, !noalias [[META235]]
// X86-NEXT:    [[TMP17:%.*]] = load i16, ptr [[__W00_ADDR_I_I]], align 2, !noalias [[META235]]
// X86-NEXT:    [[VECINIT_I_I:%.*]] = insertelement <16 x i16> poison, i16 [[TMP17]], i32 0
// X86-NEXT:    [[TMP18:%.*]] = load i16, ptr [[__W01_ADDR_I_I]], align 2, !noalias [[META235]]
// X86-NEXT:    [[VECINIT1_I_I:%.*]] = insertelement <16 x i16> [[VECINIT_I_I]], i16 [[TMP18]], i32 1
// X86-NEXT:    [[TMP19:%.*]] = load i16, ptr [[__W02_ADDR_I_I]], align 2, !noalias [[META235]]
// X86-NEXT:    [[VECINIT2_I_I:%.*]] = insertelement <16 x i16> [[VECINIT1_I_I]], i16 [[TMP19]], i32 2
// X86-NEXT:    [[TMP20:%.*]] = load i16, ptr [[__W03_ADDR_I_I]], align 2, !noalias [[META235]]
// X86-NEXT:    [[VECINIT3_I_I:%.*]] = insertelement <16 x i16> [[VECINIT2_I_I]], i16 [[TMP20]], i32 3
// X86-NEXT:    [[TMP21:%.*]] = load i16, ptr [[__W04_ADDR_I_I]], align 2, !noalias [[META235]]
// X86-NEXT:    [[VECINIT4_I_I:%.*]] = insertelement <16 x i16> [[VECINIT3_I_I]], i16 [[TMP21]], i32 4
// X86-NEXT:    [[TMP22:%.*]] = load i16, ptr [[__W05_ADDR_I_I]], align 2, !noalias [[META235]]
// X86-NEXT:    [[VECINIT5_I_I:%.*]] = insertelement <16 x i16> [[VECINIT4_I_I]], i16 [[TMP22]], i32 5
// X86-NEXT:    [[TMP23:%.*]] = load i16, ptr [[__W06_ADDR_I_I]], align 2, !noalias [[META235]]
// X86-NEXT:    [[VECINIT6_I_I:%.*]] = insertelement <16 x i16> [[VECINIT5_I_I]], i16 [[TMP23]], i32 6
// X86-NEXT:    [[TMP24:%.*]] = load i16, ptr [[__W07_ADDR_I_I]], align 2, !noalias [[META235]]
// X86-NEXT:    [[VECINIT7_I_I:%.*]] = insertelement <16 x i16> [[VECINIT6_I_I]], i16 [[TMP24]], i32 7
// X86-NEXT:    [[TMP25:%.*]] = load i16, ptr [[__W08_ADDR_I_I]], align 2, !noalias [[META235]]
// X86-NEXT:    [[VECINIT8_I_I:%.*]] = insertelement <16 x i16> [[VECINIT7_I_I]], i16 [[TMP25]], i32 8
// X86-NEXT:    [[TMP26:%.*]] = load i16, ptr [[__W09_ADDR_I_I]], align 2, !noalias [[META235]]
// X86-NEXT:    [[VECINIT9_I_I:%.*]] = insertelement <16 x i16> [[VECINIT8_I_I]], i16 [[TMP26]], i32 9
// X86-NEXT:    [[TMP27:%.*]] = load i16, ptr [[__W10_ADDR_I_I]], align 2, !noalias [[META235]]
// X86-NEXT:    [[VECINIT10_I_I:%.*]] = insertelement <16 x i16> [[VECINIT9_I_I]], i16 [[TMP27]], i32 10
// X86-NEXT:    [[TMP28:%.*]] = load i16, ptr [[__W11_ADDR_I_I]], align 2, !noalias [[META235]]
// X86-NEXT:    [[VECINIT11_I_I:%.*]] = insertelement <16 x i16> [[VECINIT10_I_I]], i16 [[TMP28]], i32 11
// X86-NEXT:    [[TMP29:%.*]] = load i16, ptr [[__W12_ADDR_I_I]], align 2, !noalias [[META235]]
// X86-NEXT:    [[VECINIT12_I_I:%.*]] = insertelement <16 x i16> [[VECINIT11_I_I]], i16 [[TMP29]], i32 12
// X86-NEXT:    [[TMP30:%.*]] = load i16, ptr [[__W13_ADDR_I_I]], align 2, !noalias [[META235]]
// X86-NEXT:    [[VECINIT13_I_I:%.*]] = insertelement <16 x i16> [[VECINIT12_I_I]], i16 [[TMP30]], i32 13
// X86-NEXT:    [[TMP31:%.*]] = load i16, ptr [[__W14_ADDR_I_I]], align 2, !noalias [[META235]]
// X86-NEXT:    [[VECINIT14_I_I:%.*]] = insertelement <16 x i16> [[VECINIT13_I_I]], i16 [[TMP31]], i32 14
// X86-NEXT:    [[TMP32:%.*]] = load i16, ptr [[__W15_ADDR_I_I]], align 2, !noalias [[META235]]
// X86-NEXT:    [[VECINIT15_I_I:%.*]] = insertelement <16 x i16> [[VECINIT14_I_I]], i16 [[TMP32]], i32 15
// X86-NEXT:    store <16 x i16> [[VECINIT15_I_I]], ptr [[DOTCOMPOUNDLITERAL_I_I]], align 32, !noalias [[META235]]
// X86-NEXT:    [[TMP33:%.*]] = load <16 x i16>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 32, !noalias [[META235]]
// X86-NEXT:    [[TMP34:%.*]] = bitcast <16 x i16> [[TMP33]] to <4 x i64>
// X86-NEXT:    store <4 x i64> [[TMP34]], ptr [[TMP_I]], align 32, !alias.scope [[META232]], !noalias [[META229]]
// X86-NEXT:    [[TMP35:%.*]] = load <4 x i64>, ptr [[TMP_I]], align 32, !alias.scope [[META232]], !noalias [[META229]]
// X86-NEXT:    store <4 x i64> [[TMP35]], ptr [[TMP_I]], align 32, !alias.scope [[META232]], !noalias [[META229]]
// X86-NEXT:    [[TMP36:%.*]] = load <4 x i64>, ptr [[TMP_I]], align 32, !noalias [[META229]]
// X86-NEXT:    store <4 x i64> [[TMP36]], ptr [[TMP]], align 32, !alias.scope [[META229]]
// X86-NEXT:    [[TMP37:%.*]] = load <4 x i64>, ptr [[TMP]], align 32, !alias.scope [[META229]]
// X86-NEXT:    store <4 x i64> [[TMP37]], ptr [[TMP]], align 32, !alias.scope [[META229]]
// X86-NEXT:    [[TMP38:%.*]] = load <4 x i64>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x i64> [[TMP38]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP39:%.*]] = load <4 x i64>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x i64> [[TMP39]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256i test_mm256_set1_epi16(short A) {
  return _mm256_set1_epi16(A);
}

//
// X86-LABEL: define void @test_mm256_set1_epi32(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x i64>) align 32 [[AGG_RESULT:%.*]], i32 noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__I0_ADDR_I_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[__I1_ADDR_I_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[__I2_ADDR_I_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[__I3_ADDR_I_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[__I4_ADDR_I_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[__I5_ADDR_I_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[__I6_ADDR_I_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[__I7_ADDR_I_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x i32>, align 32
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__I_ADDR_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[TMP_I:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store i32 [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR]], align 4
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META236:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META236]]
// X86-NEXT:    store i32 [[TMP0]], ptr [[__I_ADDR_I]], align 4, !noalias [[META236]]
// X86-NEXT:    [[TMP1:%.*]] = load i32, ptr [[__I_ADDR_I]], align 4, !noalias [[META236]]
// X86-NEXT:    [[TMP2:%.*]] = load i32, ptr [[__I_ADDR_I]], align 4, !noalias [[META236]]
// X86-NEXT:    [[TMP3:%.*]] = load i32, ptr [[__I_ADDR_I]], align 4, !noalias [[META236]]
// X86-NEXT:    [[TMP4:%.*]] = load i32, ptr [[__I_ADDR_I]], align 4, !noalias [[META236]]
// X86-NEXT:    [[TMP5:%.*]] = load i32, ptr [[__I_ADDR_I]], align 4, !noalias [[META236]]
// X86-NEXT:    [[TMP6:%.*]] = load i32, ptr [[__I_ADDR_I]], align 4, !noalias [[META236]]
// X86-NEXT:    [[TMP7:%.*]] = load i32, ptr [[__I_ADDR_I]], align 4, !noalias [[META236]]
// X86-NEXT:    [[TMP8:%.*]] = load i32, ptr [[__I_ADDR_I]], align 4, !noalias [[META236]]
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META239:![0-9]+]])
// X86-NEXT:    store ptr [[TMP_I]], ptr [[RESULT_PTR_I_I]], align 4, !noalias [[META242:![0-9]+]]
// X86-NEXT:    store i32 [[TMP1]], ptr [[__I0_ADDR_I_I]], align 4, !noalias [[META242]]
// X86-NEXT:    store i32 [[TMP2]], ptr [[__I1_ADDR_I_I]], align 4, !noalias [[META242]]
// X86-NEXT:    store i32 [[TMP3]], ptr [[__I2_ADDR_I_I]], align 4, !noalias [[META242]]
// X86-NEXT:    store i32 [[TMP4]], ptr [[__I3_ADDR_I_I]], align 4, !noalias [[META242]]
// X86-NEXT:    store i32 [[TMP5]], ptr [[__I4_ADDR_I_I]], align 4, !noalias [[META242]]
// X86-NEXT:    store i32 [[TMP6]], ptr [[__I5_ADDR_I_I]], align 4, !noalias [[META242]]
// X86-NEXT:    store i32 [[TMP7]], ptr [[__I6_ADDR_I_I]], align 4, !noalias [[META242]]
// X86-NEXT:    store i32 [[TMP8]], ptr [[__I7_ADDR_I_I]], align 4, !noalias [[META242]]
// X86-NEXT:    [[TMP9:%.*]] = load i32, ptr [[__I7_ADDR_I_I]], align 4, !noalias [[META242]]
// X86-NEXT:    [[VECINIT_I_I:%.*]] = insertelement <8 x i32> poison, i32 [[TMP9]], i32 0
// X86-NEXT:    [[TMP10:%.*]] = load i32, ptr [[__I6_ADDR_I_I]], align 4, !noalias [[META242]]
// X86-NEXT:    [[VECINIT1_I_I:%.*]] = insertelement <8 x i32> [[VECINIT_I_I]], i32 [[TMP10]], i32 1
// X86-NEXT:    [[TMP11:%.*]] = load i32, ptr [[__I5_ADDR_I_I]], align 4, !noalias [[META242]]
// X86-NEXT:    [[VECINIT2_I_I:%.*]] = insertelement <8 x i32> [[VECINIT1_I_I]], i32 [[TMP11]], i32 2
// X86-NEXT:    [[TMP12:%.*]] = load i32, ptr [[__I4_ADDR_I_I]], align 4, !noalias [[META242]]
// X86-NEXT:    [[VECINIT3_I_I:%.*]] = insertelement <8 x i32> [[VECINIT2_I_I]], i32 [[TMP12]], i32 3
// X86-NEXT:    [[TMP13:%.*]] = load i32, ptr [[__I3_ADDR_I_I]], align 4, !noalias [[META242]]
// X86-NEXT:    [[VECINIT4_I_I:%.*]] = insertelement <8 x i32> [[VECINIT3_I_I]], i32 [[TMP13]], i32 4
// X86-NEXT:    [[TMP14:%.*]] = load i32, ptr [[__I2_ADDR_I_I]], align 4, !noalias [[META242]]
// X86-NEXT:    [[VECINIT5_I_I:%.*]] = insertelement <8 x i32> [[VECINIT4_I_I]], i32 [[TMP14]], i32 5
// X86-NEXT:    [[TMP15:%.*]] = load i32, ptr [[__I1_ADDR_I_I]], align 4, !noalias [[META242]]
// X86-NEXT:    [[VECINIT6_I_I:%.*]] = insertelement <8 x i32> [[VECINIT5_I_I]], i32 [[TMP15]], i32 6
// X86-NEXT:    [[TMP16:%.*]] = load i32, ptr [[__I0_ADDR_I_I]], align 4, !noalias [[META242]]
// X86-NEXT:    [[VECINIT7_I_I:%.*]] = insertelement <8 x i32> [[VECINIT6_I_I]], i32 [[TMP16]], i32 7
// X86-NEXT:    store <8 x i32> [[VECINIT7_I_I]], ptr [[DOTCOMPOUNDLITERAL_I_I]], align 32, !noalias [[META242]]
// X86-NEXT:    [[TMP17:%.*]] = load <8 x i32>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 32, !noalias [[META242]]
// X86-NEXT:    [[TMP18:%.*]] = bitcast <8 x i32> [[TMP17]] to <4 x i64>
// X86-NEXT:    store <4 x i64> [[TMP18]], ptr [[TMP_I]], align 32, !alias.scope [[META239]], !noalias [[META236]]
// X86-NEXT:    [[TMP19:%.*]] = load <4 x i64>, ptr [[TMP_I]], align 32, !alias.scope [[META239]], !noalias [[META236]]
// X86-NEXT:    store <4 x i64> [[TMP19]], ptr [[TMP_I]], align 32, !alias.scope [[META239]], !noalias [[META236]]
// X86-NEXT:    [[TMP20:%.*]] = load <4 x i64>, ptr [[TMP_I]], align 32, !noalias [[META236]]
// X86-NEXT:    store <4 x i64> [[TMP20]], ptr [[TMP]], align 32, !alias.scope [[META236]]
// X86-NEXT:    [[TMP21:%.*]] = load <4 x i64>, ptr [[TMP]], align 32, !alias.scope [[META236]]
// X86-NEXT:    store <4 x i64> [[TMP21]], ptr [[TMP]], align 32, !alias.scope [[META236]]
// X86-NEXT:    [[TMP22:%.*]] = load <4 x i64>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x i64> [[TMP22]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP23:%.*]] = load <4 x i64>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x i64> [[TMP23]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256i test_mm256_set1_epi32(int A) {
  return _mm256_set1_epi32(A);
}

//
// X86-LABEL: define void @test_mm256_set1_epi64x(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x i64>) align 32 [[AGG_RESULT:%.*]], i64 noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I_I:%.*]] = alloca i64, align 8
// X86-NEXT:    [[__B_ADDR_I_I:%.*]] = alloca i64, align 8
// X86-NEXT:    [[__C_ADDR_I_I:%.*]] = alloca i64, align 8
// X86-NEXT:    [[__D_ADDR_I_I:%.*]] = alloca i64, align 8
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__Q_ADDR_I:%.*]] = alloca i64, align 8
// X86-NEXT:    [[TMP_I:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca i64, align 8
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store i64 [[A]], ptr [[A_ADDR]], align 8
// X86-NEXT:    [[TMP0:%.*]] = load i64, ptr [[A_ADDR]], align 8
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META243:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META243]]
// X86-NEXT:    store i64 [[TMP0]], ptr [[__Q_ADDR_I]], align 8, !noalias [[META243]]
// X86-NEXT:    [[TMP1:%.*]] = load i64, ptr [[__Q_ADDR_I]], align 8, !noalias [[META243]]
// X86-NEXT:    [[TMP2:%.*]] = load i64, ptr [[__Q_ADDR_I]], align 8, !noalias [[META243]]
// X86-NEXT:    [[TMP3:%.*]] = load i64, ptr [[__Q_ADDR_I]], align 8, !noalias [[META243]]
// X86-NEXT:    [[TMP4:%.*]] = load i64, ptr [[__Q_ADDR_I]], align 8, !noalias [[META243]]
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META246:![0-9]+]])
// X86-NEXT:    store ptr [[TMP_I]], ptr [[RESULT_PTR_I_I]], align 4, !noalias [[META249:![0-9]+]]
// X86-NEXT:    store i64 [[TMP1]], ptr [[__A_ADDR_I_I]], align 8, !noalias [[META249]]
// X86-NEXT:    store i64 [[TMP2]], ptr [[__B_ADDR_I_I]], align 8, !noalias [[META249]]
// X86-NEXT:    store i64 [[TMP3]], ptr [[__C_ADDR_I_I]], align 8, !noalias [[META249]]
// X86-NEXT:    store i64 [[TMP4]], ptr [[__D_ADDR_I_I]], align 8, !noalias [[META249]]
// X86-NEXT:    [[TMP5:%.*]] = load i64, ptr [[__D_ADDR_I_I]], align 8, !noalias [[META249]]
// X86-NEXT:    [[VECINIT_I_I:%.*]] = insertelement <4 x i64> poison, i64 [[TMP5]], i32 0
// X86-NEXT:    [[TMP6:%.*]] = load i64, ptr [[__C_ADDR_I_I]], align 8, !noalias [[META249]]
// X86-NEXT:    [[VECINIT1_I_I:%.*]] = insertelement <4 x i64> [[VECINIT_I_I]], i64 [[TMP6]], i32 1
// X86-NEXT:    [[TMP7:%.*]] = load i64, ptr [[__B_ADDR_I_I]], align 8, !noalias [[META249]]
// X86-NEXT:    [[VECINIT2_I_I:%.*]] = insertelement <4 x i64> [[VECINIT1_I_I]], i64 [[TMP7]], i32 2
// X86-NEXT:    [[TMP8:%.*]] = load i64, ptr [[__A_ADDR_I_I]], align 8, !noalias [[META249]]
// X86-NEXT:    [[VECINIT3_I_I:%.*]] = insertelement <4 x i64> [[VECINIT2_I_I]], i64 [[TMP8]], i32 3
// X86-NEXT:    store <4 x i64> [[VECINIT3_I_I]], ptr [[DOTCOMPOUNDLITERAL_I_I]], align 32, !noalias [[META249]]
// X86-NEXT:    [[TMP9:%.*]] = load <4 x i64>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 32, !noalias [[META249]]
// X86-NEXT:    store <4 x i64> [[TMP9]], ptr [[TMP_I]], align 32, !alias.scope [[META246]], !noalias [[META243]]
// X86-NEXT:    [[TMP10:%.*]] = load <4 x i64>, ptr [[TMP_I]], align 32, !alias.scope [[META246]], !noalias [[META243]]
// X86-NEXT:    store <4 x i64> [[TMP10]], ptr [[TMP_I]], align 32, !alias.scope [[META246]], !noalias [[META243]]
// X86-NEXT:    [[TMP11:%.*]] = load <4 x i64>, ptr [[TMP_I]], align 32, !noalias [[META243]]
// X86-NEXT:    store <4 x i64> [[TMP11]], ptr [[TMP]], align 32, !alias.scope [[META243]]
// X86-NEXT:    [[TMP12:%.*]] = load <4 x i64>, ptr [[TMP]], align 32, !alias.scope [[META243]]
// X86-NEXT:    store <4 x i64> [[TMP12]], ptr [[TMP]], align 32, !alias.scope [[META243]]
// X86-NEXT:    [[TMP13:%.*]] = load <4 x i64>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x i64> [[TMP13]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP14:%.*]] = load <4 x i64>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x i64> [[TMP14]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256i test_mm256_set1_epi64x(long long A) {
  return _mm256_set1_epi64x(A);
}

//
// X86-LABEL: define void @test_mm256_set1_pd(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], double noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I_I:%.*]] = alloca double, align 8
// X86-NEXT:    [[__B_ADDR_I_I:%.*]] = alloca double, align 8
// X86-NEXT:    [[__C_ADDR_I_I:%.*]] = alloca double, align 8
// X86-NEXT:    [[__D_ADDR_I_I:%.*]] = alloca double, align 8
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__W_ADDR_I:%.*]] = alloca double, align 8
// X86-NEXT:    [[TMP_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca double, align 8
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store double [[A]], ptr [[A_ADDR]], align 8
// X86-NEXT:    [[TMP0:%.*]] = load double, ptr [[A_ADDR]], align 8
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META250:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META250]]
// X86-NEXT:    store double [[TMP0]], ptr [[__W_ADDR_I]], align 8, !noalias [[META250]]
// X86-NEXT:    [[TMP1:%.*]] = load double, ptr [[__W_ADDR_I]], align 8, !noalias [[META250]]
// X86-NEXT:    [[TMP2:%.*]] = load double, ptr [[__W_ADDR_I]], align 8, !noalias [[META250]]
// X86-NEXT:    [[TMP3:%.*]] = load double, ptr [[__W_ADDR_I]], align 8, !noalias [[META250]]
// X86-NEXT:    [[TMP4:%.*]] = load double, ptr [[__W_ADDR_I]], align 8, !noalias [[META250]]
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META253:![0-9]+]])
// X86-NEXT:    store ptr [[TMP_I]], ptr [[RESULT_PTR_I_I]], align 4, !noalias [[META256:![0-9]+]]
// X86-NEXT:    store double [[TMP1]], ptr [[__A_ADDR_I_I]], align 8, !noalias [[META256]]
// X86-NEXT:    store double [[TMP2]], ptr [[__B_ADDR_I_I]], align 8, !noalias [[META256]]
// X86-NEXT:    store double [[TMP3]], ptr [[__C_ADDR_I_I]], align 8, !noalias [[META256]]
// X86-NEXT:    store double [[TMP4]], ptr [[__D_ADDR_I_I]], align 8, !noalias [[META256]]
// X86-NEXT:    [[TMP5:%.*]] = load double, ptr [[__D_ADDR_I_I]], align 8, !noalias [[META256]]
// X86-NEXT:    [[VECINIT_I_I:%.*]] = insertelement <4 x double> poison, double [[TMP5]], i32 0
// X86-NEXT:    [[TMP6:%.*]] = load double, ptr [[__C_ADDR_I_I]], align 8, !noalias [[META256]]
// X86-NEXT:    [[VECINIT1_I_I:%.*]] = insertelement <4 x double> [[VECINIT_I_I]], double [[TMP6]], i32 1
// X86-NEXT:    [[TMP7:%.*]] = load double, ptr [[__B_ADDR_I_I]], align 8, !noalias [[META256]]
// X86-NEXT:    [[VECINIT2_I_I:%.*]] = insertelement <4 x double> [[VECINIT1_I_I]], double [[TMP7]], i32 2
// X86-NEXT:    [[TMP8:%.*]] = load double, ptr [[__A_ADDR_I_I]], align 8, !noalias [[META256]]
// X86-NEXT:    [[VECINIT3_I_I:%.*]] = insertelement <4 x double> [[VECINIT2_I_I]], double [[TMP8]], i32 3
// X86-NEXT:    store <4 x double> [[VECINIT3_I_I]], ptr [[DOTCOMPOUNDLITERAL_I_I]], align 32, !noalias [[META256]]
// X86-NEXT:    [[TMP9:%.*]] = load <4 x double>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 32, !noalias [[META256]]
// X86-NEXT:    store <4 x double> [[TMP9]], ptr [[TMP_I]], align 32, !alias.scope [[META253]], !noalias [[META250]]
// X86-NEXT:    [[TMP10:%.*]] = load <4 x double>, ptr [[TMP_I]], align 32, !alias.scope [[META253]], !noalias [[META250]]
// X86-NEXT:    store <4 x double> [[TMP10]], ptr [[TMP_I]], align 32, !alias.scope [[META253]], !noalias [[META250]]
// X86-NEXT:    [[TMP11:%.*]] = load <4 x double>, ptr [[TMP_I]], align 32, !noalias [[META250]]
// X86-NEXT:    store <4 x double> [[TMP11]], ptr [[TMP]], align 32, !alias.scope [[META250]]
// X86-NEXT:    [[TMP12:%.*]] = load <4 x double>, ptr [[TMP]], align 32, !alias.scope [[META250]]
// X86-NEXT:    store <4 x double> [[TMP12]], ptr [[TMP]], align 32, !alias.scope [[META250]]
// X86-NEXT:    [[TMP13:%.*]] = load <4 x double>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x double> [[TMP13]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP14:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP14]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_set1_pd(double A) {
  return _mm256_set1_pd(A);
}

//
// X86-LABEL: define void @test_mm256_set1_ps(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], float noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I_I:%.*]] = alloca float, align 4
// X86-NEXT:    [[__B_ADDR_I_I:%.*]] = alloca float, align 4
// X86-NEXT:    [[__C_ADDR_I_I:%.*]] = alloca float, align 4
// X86-NEXT:    [[__D_ADDR_I_I:%.*]] = alloca float, align 4
// X86-NEXT:    [[__E_ADDR_I_I:%.*]] = alloca float, align 4
// X86-NEXT:    [[__F_ADDR_I_I:%.*]] = alloca float, align 4
// X86-NEXT:    [[__G_ADDR_I_I:%.*]] = alloca float, align 4
// X86-NEXT:    [[__H_ADDR_I_I:%.*]] = alloca float, align 4
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__W_ADDR_I:%.*]] = alloca float, align 4
// X86-NEXT:    [[TMP_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca float, align 4
// X86-NEXT:    [[TMP:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store float [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP0:%.*]] = load float, ptr [[A_ADDR]], align 4
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META257:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META257]]
// X86-NEXT:    store float [[TMP0]], ptr [[__W_ADDR_I]], align 4, !noalias [[META257]]
// X86-NEXT:    [[TMP1:%.*]] = load float, ptr [[__W_ADDR_I]], align 4, !noalias [[META257]]
// X86-NEXT:    [[TMP2:%.*]] = load float, ptr [[__W_ADDR_I]], align 4, !noalias [[META257]]
// X86-NEXT:    [[TMP3:%.*]] = load float, ptr [[__W_ADDR_I]], align 4, !noalias [[META257]]
// X86-NEXT:    [[TMP4:%.*]] = load float, ptr [[__W_ADDR_I]], align 4, !noalias [[META257]]
// X86-NEXT:    [[TMP5:%.*]] = load float, ptr [[__W_ADDR_I]], align 4, !noalias [[META257]]
// X86-NEXT:    [[TMP6:%.*]] = load float, ptr [[__W_ADDR_I]], align 4, !noalias [[META257]]
// X86-NEXT:    [[TMP7:%.*]] = load float, ptr [[__W_ADDR_I]], align 4, !noalias [[META257]]
// X86-NEXT:    [[TMP8:%.*]] = load float, ptr [[__W_ADDR_I]], align 4, !noalias [[META257]]
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META260:![0-9]+]])
// X86-NEXT:    store ptr [[TMP_I]], ptr [[RESULT_PTR_I_I]], align 4, !noalias [[META263:![0-9]+]]
// X86-NEXT:    store float [[TMP1]], ptr [[__A_ADDR_I_I]], align 4, !noalias [[META263]]
// X86-NEXT:    store float [[TMP2]], ptr [[__B_ADDR_I_I]], align 4, !noalias [[META263]]
// X86-NEXT:    store float [[TMP3]], ptr [[__C_ADDR_I_I]], align 4, !noalias [[META263]]
// X86-NEXT:    store float [[TMP4]], ptr [[__D_ADDR_I_I]], align 4, !noalias [[META263]]
// X86-NEXT:    store float [[TMP5]], ptr [[__E_ADDR_I_I]], align 4, !noalias [[META263]]
// X86-NEXT:    store float [[TMP6]], ptr [[__F_ADDR_I_I]], align 4, !noalias [[META263]]
// X86-NEXT:    store float [[TMP7]], ptr [[__G_ADDR_I_I]], align 4, !noalias [[META263]]
// X86-NEXT:    store float [[TMP8]], ptr [[__H_ADDR_I_I]], align 4, !noalias [[META263]]
// X86-NEXT:    [[TMP9:%.*]] = load float, ptr [[__H_ADDR_I_I]], align 4, !noalias [[META263]]
// X86-NEXT:    [[VECINIT_I_I:%.*]] = insertelement <8 x float> poison, float [[TMP9]], i32 0
// X86-NEXT:    [[TMP10:%.*]] = load float, ptr [[__G_ADDR_I_I]], align 4, !noalias [[META263]]
// X86-NEXT:    [[VECINIT1_I_I:%.*]] = insertelement <8 x float> [[VECINIT_I_I]], float [[TMP10]], i32 1
// X86-NEXT:    [[TMP11:%.*]] = load float, ptr [[__F_ADDR_I_I]], align 4, !noalias [[META263]]
// X86-NEXT:    [[VECINIT2_I_I:%.*]] = insertelement <8 x float> [[VECINIT1_I_I]], float [[TMP11]], i32 2
// X86-NEXT:    [[TMP12:%.*]] = load float, ptr [[__E_ADDR_I_I]], align 4, !noalias [[META263]]
// X86-NEXT:    [[VECINIT3_I_I:%.*]] = insertelement <8 x float> [[VECINIT2_I_I]], float [[TMP12]], i32 3
// X86-NEXT:    [[TMP13:%.*]] = load float, ptr [[__D_ADDR_I_I]], align 4, !noalias [[META263]]
// X86-NEXT:    [[VECINIT4_I_I:%.*]] = insertelement <8 x float> [[VECINIT3_I_I]], float [[TMP13]], i32 4
// X86-NEXT:    [[TMP14:%.*]] = load float, ptr [[__C_ADDR_I_I]], align 4, !noalias [[META263]]
// X86-NEXT:    [[VECINIT5_I_I:%.*]] = insertelement <8 x float> [[VECINIT4_I_I]], float [[TMP14]], i32 5
// X86-NEXT:    [[TMP15:%.*]] = load float, ptr [[__B_ADDR_I_I]], align 4, !noalias [[META263]]
// X86-NEXT:    [[VECINIT6_I_I:%.*]] = insertelement <8 x float> [[VECINIT5_I_I]], float [[TMP15]], i32 6
// X86-NEXT:    [[TMP16:%.*]] = load float, ptr [[__A_ADDR_I_I]], align 4, !noalias [[META263]]
// X86-NEXT:    [[VECINIT7_I_I:%.*]] = insertelement <8 x float> [[VECINIT6_I_I]], float [[TMP16]], i32 7
// X86-NEXT:    store <8 x float> [[VECINIT7_I_I]], ptr [[DOTCOMPOUNDLITERAL_I_I]], align 32, !noalias [[META263]]
// X86-NEXT:    [[TMP17:%.*]] = load <8 x float>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 32, !noalias [[META263]]
// X86-NEXT:    store <8 x float> [[TMP17]], ptr [[TMP_I]], align 32, !alias.scope [[META260]], !noalias [[META257]]
// X86-NEXT:    [[TMP18:%.*]] = load <8 x float>, ptr [[TMP_I]], align 32, !alias.scope [[META260]], !noalias [[META257]]
// X86-NEXT:    store <8 x float> [[TMP18]], ptr [[TMP_I]], align 32, !alias.scope [[META260]], !noalias [[META257]]
// X86-NEXT:    [[TMP19:%.*]] = load <8 x float>, ptr [[TMP_I]], align 32, !noalias [[META257]]
// X86-NEXT:    store <8 x float> [[TMP19]], ptr [[TMP]], align 32, !alias.scope [[META257]]
// X86-NEXT:    [[TMP20:%.*]] = load <8 x float>, ptr [[TMP]], align 32, !alias.scope [[META257]]
// X86-NEXT:    store <8 x float> [[TMP20]], ptr [[TMP]], align 32, !alias.scope [[META257]]
// X86-NEXT:    [[TMP21:%.*]] = load <8 x float>, ptr [[TMP]], align 32
// X86-NEXT:    store <8 x float> [[TMP21]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP22:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP22]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_set1_ps(float A) {
  return _mm256_set1_ps(A);
}

//
__m256i test_mm256_setr_epi8(char A0, char A1, char A2, char A3, char A4, char A5, char A6, char A7,
                             char A8, char A9, char A10, char A11, char A12, char A13, char A14, char A15,
                             char A16, char A17, char A18, char A19, char A20, char A21, char A22, char A23,
                             char A24, char A25, char A26, char A27, char A28, char A29, char A30, char A31) {
  return _mm256_setr_epi8(A0, A1, A2, A3, A4, A5, A6, A7, A8, A9, A10, A11, A12, A13, A14, A15, A16, A17, A18, A19, A20, A21, A22, A23, A24, A25, A26, A27, A28, A29, A30, A31);
}

//
// X86-LABEL: define void @test_mm256_setr_epi16(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x i64>) align 32 [[AGG_RESULT:%.*]], i16 noundef signext [[A0:%.*]], i16 noundef signext [[A1:%.*]], i16 noundef signext [[A2:%.*]], i16 noundef signext [[A3:%.*]], i16 noundef signext [[A4:%.*]], i16 noundef signext [[A5:%.*]], i16 noundef signext [[A6:%.*]], i16 noundef signext [[A7:%.*]], i16 noundef signext [[A8:%.*]], i16 noundef signext [[A9:%.*]], i16 noundef signext [[A10:%.*]], i16 noundef signext [[A11:%.*]], i16 noundef signext [[A12:%.*]], i16 noundef signext [[A13:%.*]], i16 noundef signext [[A14:%.*]], i16 noundef signext [[A15:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__W15_ADDR_I_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W14_ADDR_I_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W13_ADDR_I_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W12_ADDR_I_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W11_ADDR_I_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W10_ADDR_I_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W09_ADDR_I_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W08_ADDR_I_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W07_ADDR_I_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W06_ADDR_I_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W05_ADDR_I_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W04_ADDR_I_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W03_ADDR_I_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W02_ADDR_I_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W01_ADDR_I_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W00_ADDR_I_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <16 x i16>, align 32
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__W15_ADDR_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W14_ADDR_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W13_ADDR_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W12_ADDR_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W11_ADDR_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W10_ADDR_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W09_ADDR_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W08_ADDR_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W07_ADDR_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W06_ADDR_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W05_ADDR_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W04_ADDR_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W03_ADDR_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W02_ADDR_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W01_ADDR_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[__W00_ADDR_I:%.*]] = alloca i16, align 2
// X86-NEXT:    [[TMP_I:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A0_ADDR:%.*]] = alloca i16, align 2
// X86-NEXT:    [[A1_ADDR:%.*]] = alloca i16, align 2
// X86-NEXT:    [[A2_ADDR:%.*]] = alloca i16, align 2
// X86-NEXT:    [[A3_ADDR:%.*]] = alloca i16, align 2
// X86-NEXT:    [[A4_ADDR:%.*]] = alloca i16, align 2
// X86-NEXT:    [[A5_ADDR:%.*]] = alloca i16, align 2
// X86-NEXT:    [[A6_ADDR:%.*]] = alloca i16, align 2
// X86-NEXT:    [[A7_ADDR:%.*]] = alloca i16, align 2
// X86-NEXT:    [[A8_ADDR:%.*]] = alloca i16, align 2
// X86-NEXT:    [[A9_ADDR:%.*]] = alloca i16, align 2
// X86-NEXT:    [[A10_ADDR:%.*]] = alloca i16, align 2
// X86-NEXT:    [[A11_ADDR:%.*]] = alloca i16, align 2
// X86-NEXT:    [[A12_ADDR:%.*]] = alloca i16, align 2
// X86-NEXT:    [[A13_ADDR:%.*]] = alloca i16, align 2
// X86-NEXT:    [[A14_ADDR:%.*]] = alloca i16, align 2
// X86-NEXT:    [[A15_ADDR:%.*]] = alloca i16, align 2
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store i16 [[A0]], ptr [[A0_ADDR]], align 2
// X86-NEXT:    store i16 [[A1]], ptr [[A1_ADDR]], align 2
// X86-NEXT:    store i16 [[A2]], ptr [[A2_ADDR]], align 2
// X86-NEXT:    store i16 [[A3]], ptr [[A3_ADDR]], align 2
// X86-NEXT:    store i16 [[A4]], ptr [[A4_ADDR]], align 2
// X86-NEXT:    store i16 [[A5]], ptr [[A5_ADDR]], align 2
// X86-NEXT:    store i16 [[A6]], ptr [[A6_ADDR]], align 2
// X86-NEXT:    store i16 [[A7]], ptr [[A7_ADDR]], align 2
// X86-NEXT:    store i16 [[A8]], ptr [[A8_ADDR]], align 2
// X86-NEXT:    store i16 [[A9]], ptr [[A9_ADDR]], align 2
// X86-NEXT:    store i16 [[A10]], ptr [[A10_ADDR]], align 2
// X86-NEXT:    store i16 [[A11]], ptr [[A11_ADDR]], align 2
// X86-NEXT:    store i16 [[A12]], ptr [[A12_ADDR]], align 2
// X86-NEXT:    store i16 [[A13]], ptr [[A13_ADDR]], align 2
// X86-NEXT:    store i16 [[A14]], ptr [[A14_ADDR]], align 2
// X86-NEXT:    store i16 [[A15]], ptr [[A15_ADDR]], align 2
// X86-NEXT:    [[TMP0:%.*]] = load i16, ptr [[A0_ADDR]], align 2
// X86-NEXT:    [[TMP1:%.*]] = load i16, ptr [[A1_ADDR]], align 2
// X86-NEXT:    [[TMP2:%.*]] = load i16, ptr [[A2_ADDR]], align 2
// X86-NEXT:    [[TMP3:%.*]] = load i16, ptr [[A3_ADDR]], align 2
// X86-NEXT:    [[TMP4:%.*]] = load i16, ptr [[A4_ADDR]], align 2
// X86-NEXT:    [[TMP5:%.*]] = load i16, ptr [[A5_ADDR]], align 2
// X86-NEXT:    [[TMP6:%.*]] = load i16, ptr [[A6_ADDR]], align 2
// X86-NEXT:    [[TMP7:%.*]] = load i16, ptr [[A7_ADDR]], align 2
// X86-NEXT:    [[TMP8:%.*]] = load i16, ptr [[A8_ADDR]], align 2
// X86-NEXT:    [[TMP9:%.*]] = load i16, ptr [[A9_ADDR]], align 2
// X86-NEXT:    [[TMP10:%.*]] = load i16, ptr [[A10_ADDR]], align 2
// X86-NEXT:    [[TMP11:%.*]] = load i16, ptr [[A11_ADDR]], align 2
// X86-NEXT:    [[TMP12:%.*]] = load i16, ptr [[A12_ADDR]], align 2
// X86-NEXT:    [[TMP13:%.*]] = load i16, ptr [[A13_ADDR]], align 2
// X86-NEXT:    [[TMP14:%.*]] = load i16, ptr [[A14_ADDR]], align 2
// X86-NEXT:    [[TMP15:%.*]] = load i16, ptr [[A15_ADDR]], align 2
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META271:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META271]]
// X86-NEXT:    store i16 [[TMP0]], ptr [[__W15_ADDR_I]], align 2, !noalias [[META271]]
// X86-NEXT:    store i16 [[TMP1]], ptr [[__W14_ADDR_I]], align 2, !noalias [[META271]]
// X86-NEXT:    store i16 [[TMP2]], ptr [[__W13_ADDR_I]], align 2, !noalias [[META271]]
// X86-NEXT:    store i16 [[TMP3]], ptr [[__W12_ADDR_I]], align 2, !noalias [[META271]]
// X86-NEXT:    store i16 [[TMP4]], ptr [[__W11_ADDR_I]], align 2, !noalias [[META271]]
// X86-NEXT:    store i16 [[TMP5]], ptr [[__W10_ADDR_I]], align 2, !noalias [[META271]]
// X86-NEXT:    store i16 [[TMP6]], ptr [[__W09_ADDR_I]], align 2, !noalias [[META271]]
// X86-NEXT:    store i16 [[TMP7]], ptr [[__W08_ADDR_I]], align 2, !noalias [[META271]]
// X86-NEXT:    store i16 [[TMP8]], ptr [[__W07_ADDR_I]], align 2, !noalias [[META271]]
// X86-NEXT:    store i16 [[TMP9]], ptr [[__W06_ADDR_I]], align 2, !noalias [[META271]]
// X86-NEXT:    store i16 [[TMP10]], ptr [[__W05_ADDR_I]], align 2, !noalias [[META271]]
// X86-NEXT:    store i16 [[TMP11]], ptr [[__W04_ADDR_I]], align 2, !noalias [[META271]]
// X86-NEXT:    store i16 [[TMP12]], ptr [[__W03_ADDR_I]], align 2, !noalias [[META271]]
// X86-NEXT:    store i16 [[TMP13]], ptr [[__W02_ADDR_I]], align 2, !noalias [[META271]]
// X86-NEXT:    store i16 [[TMP14]], ptr [[__W01_ADDR_I]], align 2, !noalias [[META271]]
// X86-NEXT:    store i16 [[TMP15]], ptr [[__W00_ADDR_I]], align 2, !noalias [[META271]]
// X86-NEXT:    [[TMP16:%.*]] = load i16, ptr [[__W00_ADDR_I]], align 2, !noalias [[META271]]
// X86-NEXT:    [[TMP17:%.*]] = load i16, ptr [[__W01_ADDR_I]], align 2, !noalias [[META271]]
// X86-NEXT:    [[TMP18:%.*]] = load i16, ptr [[__W02_ADDR_I]], align 2, !noalias [[META271]]
// X86-NEXT:    [[TMP19:%.*]] = load i16, ptr [[__W03_ADDR_I]], align 2, !noalias [[META271]]
// X86-NEXT:    [[TMP20:%.*]] = load i16, ptr [[__W04_ADDR_I]], align 2, !noalias [[META271]]
// X86-NEXT:    [[TMP21:%.*]] = load i16, ptr [[__W05_ADDR_I]], align 2, !noalias [[META271]]
// X86-NEXT:    [[TMP22:%.*]] = load i16, ptr [[__W06_ADDR_I]], align 2, !noalias [[META271]]
// X86-NEXT:    [[TMP23:%.*]] = load i16, ptr [[__W07_ADDR_I]], align 2, !noalias [[META271]]
// X86-NEXT:    [[TMP24:%.*]] = load i16, ptr [[__W08_ADDR_I]], align 2, !noalias [[META271]]
// X86-NEXT:    [[TMP25:%.*]] = load i16, ptr [[__W09_ADDR_I]], align 2, !noalias [[META271]]
// X86-NEXT:    [[TMP26:%.*]] = load i16, ptr [[__W10_ADDR_I]], align 2, !noalias [[META271]]
// X86-NEXT:    [[TMP27:%.*]] = load i16, ptr [[__W11_ADDR_I]], align 2, !noalias [[META271]]
// X86-NEXT:    [[TMP28:%.*]] = load i16, ptr [[__W12_ADDR_I]], align 2, !noalias [[META271]]
// X86-NEXT:    [[TMP29:%.*]] = load i16, ptr [[__W13_ADDR_I]], align 2, !noalias [[META271]]
// X86-NEXT:    [[TMP30:%.*]] = load i16, ptr [[__W14_ADDR_I]], align 2, !noalias [[META271]]
// X86-NEXT:    [[TMP31:%.*]] = load i16, ptr [[__W15_ADDR_I]], align 2, !noalias [[META271]]
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META274:![0-9]+]])
// X86-NEXT:    store ptr [[TMP_I]], ptr [[RESULT_PTR_I_I]], align 4, !noalias [[META277:![0-9]+]]
// X86-NEXT:    store i16 [[TMP16]], ptr [[__W15_ADDR_I_I]], align 2, !noalias [[META277]]
// X86-NEXT:    store i16 [[TMP17]], ptr [[__W14_ADDR_I_I]], align 2, !noalias [[META277]]
// X86-NEXT:    store i16 [[TMP18]], ptr [[__W13_ADDR_I_I]], align 2, !noalias [[META277]]
// X86-NEXT:    store i16 [[TMP19]], ptr [[__W12_ADDR_I_I]], align 2, !noalias [[META277]]
// X86-NEXT:    store i16 [[TMP20]], ptr [[__W11_ADDR_I_I]], align 2, !noalias [[META277]]
// X86-NEXT:    store i16 [[TMP21]], ptr [[__W10_ADDR_I_I]], align 2, !noalias [[META277]]
// X86-NEXT:    store i16 [[TMP22]], ptr [[__W09_ADDR_I_I]], align 2, !noalias [[META277]]
// X86-NEXT:    store i16 [[TMP23]], ptr [[__W08_ADDR_I_I]], align 2, !noalias [[META277]]
// X86-NEXT:    store i16 [[TMP24]], ptr [[__W07_ADDR_I_I]], align 2, !noalias [[META277]]
// X86-NEXT:    store i16 [[TMP25]], ptr [[__W06_ADDR_I_I]], align 2, !noalias [[META277]]
// X86-NEXT:    store i16 [[TMP26]], ptr [[__W05_ADDR_I_I]], align 2, !noalias [[META277]]
// X86-NEXT:    store i16 [[TMP27]], ptr [[__W04_ADDR_I_I]], align 2, !noalias [[META277]]
// X86-NEXT:    store i16 [[TMP28]], ptr [[__W03_ADDR_I_I]], align 2, !noalias [[META277]]
// X86-NEXT:    store i16 [[TMP29]], ptr [[__W02_ADDR_I_I]], align 2, !noalias [[META277]]
// X86-NEXT:    store i16 [[TMP30]], ptr [[__W01_ADDR_I_I]], align 2, !noalias [[META277]]
// X86-NEXT:    store i16 [[TMP31]], ptr [[__W00_ADDR_I_I]], align 2, !noalias [[META277]]
// X86-NEXT:    [[TMP32:%.*]] = load i16, ptr [[__W00_ADDR_I_I]], align 2, !noalias [[META277]]
// X86-NEXT:    [[VECINIT_I_I:%.*]] = insertelement <16 x i16> poison, i16 [[TMP32]], i32 0
// X86-NEXT:    [[TMP33:%.*]] = load i16, ptr [[__W01_ADDR_I_I]], align 2, !noalias [[META277]]
// X86-NEXT:    [[VECINIT1_I_I:%.*]] = insertelement <16 x i16> [[VECINIT_I_I]], i16 [[TMP33]], i32 1
// X86-NEXT:    [[TMP34:%.*]] = load i16, ptr [[__W02_ADDR_I_I]], align 2, !noalias [[META277]]
// X86-NEXT:    [[VECINIT2_I_I:%.*]] = insertelement <16 x i16> [[VECINIT1_I_I]], i16 [[TMP34]], i32 2
// X86-NEXT:    [[TMP35:%.*]] = load i16, ptr [[__W03_ADDR_I_I]], align 2, !noalias [[META277]]
// X86-NEXT:    [[VECINIT3_I_I:%.*]] = insertelement <16 x i16> [[VECINIT2_I_I]], i16 [[TMP35]], i32 3
// X86-NEXT:    [[TMP36:%.*]] = load i16, ptr [[__W04_ADDR_I_I]], align 2, !noalias [[META277]]
// X86-NEXT:    [[VECINIT4_I_I:%.*]] = insertelement <16 x i16> [[VECINIT3_I_I]], i16 [[TMP36]], i32 4
// X86-NEXT:    [[TMP37:%.*]] = load i16, ptr [[__W05_ADDR_I_I]], align 2, !noalias [[META277]]
// X86-NEXT:    [[VECINIT5_I_I:%.*]] = insertelement <16 x i16> [[VECINIT4_I_I]], i16 [[TMP37]], i32 5
// X86-NEXT:    [[TMP38:%.*]] = load i16, ptr [[__W06_ADDR_I_I]], align 2, !noalias [[META277]]
// X86-NEXT:    [[VECINIT6_I_I:%.*]] = insertelement <16 x i16> [[VECINIT5_I_I]], i16 [[TMP38]], i32 6
// X86-NEXT:    [[TMP39:%.*]] = load i16, ptr [[__W07_ADDR_I_I]], align 2, !noalias [[META277]]
// X86-NEXT:    [[VECINIT7_I_I:%.*]] = insertelement <16 x i16> [[VECINIT6_I_I]], i16 [[TMP39]], i32 7
// X86-NEXT:    [[TMP40:%.*]] = load i16, ptr [[__W08_ADDR_I_I]], align 2, !noalias [[META277]]
// X86-NEXT:    [[VECINIT8_I_I:%.*]] = insertelement <16 x i16> [[VECINIT7_I_I]], i16 [[TMP40]], i32 8
// X86-NEXT:    [[TMP41:%.*]] = load i16, ptr [[__W09_ADDR_I_I]], align 2, !noalias [[META277]]
// X86-NEXT:    [[VECINIT9_I_I:%.*]] = insertelement <16 x i16> [[VECINIT8_I_I]], i16 [[TMP41]], i32 9
// X86-NEXT:    [[TMP42:%.*]] = load i16, ptr [[__W10_ADDR_I_I]], align 2, !noalias [[META277]]
// X86-NEXT:    [[VECINIT10_I_I:%.*]] = insertelement <16 x i16> [[VECINIT9_I_I]], i16 [[TMP42]], i32 10
// X86-NEXT:    [[TMP43:%.*]] = load i16, ptr [[__W11_ADDR_I_I]], align 2, !noalias [[META277]]
// X86-NEXT:    [[VECINIT11_I_I:%.*]] = insertelement <16 x i16> [[VECINIT10_I_I]], i16 [[TMP43]], i32 11
// X86-NEXT:    [[TMP44:%.*]] = load i16, ptr [[__W12_ADDR_I_I]], align 2, !noalias [[META277]]
// X86-NEXT:    [[VECINIT12_I_I:%.*]] = insertelement <16 x i16> [[VECINIT11_I_I]], i16 [[TMP44]], i32 12
// X86-NEXT:    [[TMP45:%.*]] = load i16, ptr [[__W13_ADDR_I_I]], align 2, !noalias [[META277]]
// X86-NEXT:    [[VECINIT13_I_I:%.*]] = insertelement <16 x i16> [[VECINIT12_I_I]], i16 [[TMP45]], i32 13
// X86-NEXT:    [[TMP46:%.*]] = load i16, ptr [[__W14_ADDR_I_I]], align 2, !noalias [[META277]]
// X86-NEXT:    [[VECINIT14_I_I:%.*]] = insertelement <16 x i16> [[VECINIT13_I_I]], i16 [[TMP46]], i32 14
// X86-NEXT:    [[TMP47:%.*]] = load i16, ptr [[__W15_ADDR_I_I]], align 2, !noalias [[META277]]
// X86-NEXT:    [[VECINIT15_I_I:%.*]] = insertelement <16 x i16> [[VECINIT14_I_I]], i16 [[TMP47]], i32 15
// X86-NEXT:    store <16 x i16> [[VECINIT15_I_I]], ptr [[DOTCOMPOUNDLITERAL_I_I]], align 32, !noalias [[META277]]
// X86-NEXT:    [[TMP48:%.*]] = load <16 x i16>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 32, !noalias [[META277]]
// X86-NEXT:    [[TMP49:%.*]] = bitcast <16 x i16> [[TMP48]] to <4 x i64>
// X86-NEXT:    store <4 x i64> [[TMP49]], ptr [[TMP_I]], align 32, !alias.scope [[META274]], !noalias [[META271]]
// X86-NEXT:    [[TMP50:%.*]] = load <4 x i64>, ptr [[TMP_I]], align 32, !alias.scope [[META274]], !noalias [[META271]]
// X86-NEXT:    store <4 x i64> [[TMP50]], ptr [[TMP_I]], align 32, !alias.scope [[META274]], !noalias [[META271]]
// X86-NEXT:    [[TMP51:%.*]] = load <4 x i64>, ptr [[TMP_I]], align 32, !noalias [[META271]]
// X86-NEXT:    store <4 x i64> [[TMP51]], ptr [[TMP]], align 32, !alias.scope [[META271]]
// X86-NEXT:    [[TMP52:%.*]] = load <4 x i64>, ptr [[TMP]], align 32, !alias.scope [[META271]]
// X86-NEXT:    store <4 x i64> [[TMP52]], ptr [[TMP]], align 32, !alias.scope [[META271]]
// X86-NEXT:    [[TMP53:%.*]] = load <4 x i64>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x i64> [[TMP53]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP54:%.*]] = load <4 x i64>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x i64> [[TMP54]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256i test_mm256_setr_epi16(short A0, short A1, short A2, short A3, short A4, short A5, short A6, short A7,
                              short A8, short A9, short A10, short A11, short A12, short A13, short A14, short A15) {
  return _mm256_setr_epi16(A0, A1, A2, A3, A4, A5, A6, A7, A8, A9, A10, A11, A12, A13, A14, A15);
}

//
// X86-LABEL: define void @test_mm256_setr_epi32(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x i64>) align 32 [[AGG_RESULT:%.*]], i32 noundef [[A0:%.*]], i32 noundef [[A1:%.*]], i32 noundef [[A2:%.*]], i32 noundef [[A3:%.*]], i32 noundef [[A4:%.*]], i32 noundef [[A5:%.*]], i32 noundef [[A6:%.*]], i32 noundef [[A7:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__I0_ADDR_I_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[__I1_ADDR_I_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[__I2_ADDR_I_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[__I3_ADDR_I_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[__I4_ADDR_I_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[__I5_ADDR_I_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[__I6_ADDR_I_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[__I7_ADDR_I_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x i32>, align 32
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__I0_ADDR_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[__I1_ADDR_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[__I2_ADDR_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[__I3_ADDR_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[__I4_ADDR_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[__I5_ADDR_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[__I6_ADDR_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[__I7_ADDR_I:%.*]] = alloca i32, align 4
// X86-NEXT:    [[TMP_I:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A0_ADDR:%.*]] = alloca i32, align 4
// X86-NEXT:    [[A1_ADDR:%.*]] = alloca i32, align 4
// X86-NEXT:    [[A2_ADDR:%.*]] = alloca i32, align 4
// X86-NEXT:    [[A3_ADDR:%.*]] = alloca i32, align 4
// X86-NEXT:    [[A4_ADDR:%.*]] = alloca i32, align 4
// X86-NEXT:    [[A5_ADDR:%.*]] = alloca i32, align 4
// X86-NEXT:    [[A6_ADDR:%.*]] = alloca i32, align 4
// X86-NEXT:    [[A7_ADDR:%.*]] = alloca i32, align 4
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store i32 [[A0]], ptr [[A0_ADDR]], align 4
// X86-NEXT:    store i32 [[A1]], ptr [[A1_ADDR]], align 4
// X86-NEXT:    store i32 [[A2]], ptr [[A2_ADDR]], align 4
// X86-NEXT:    store i32 [[A3]], ptr [[A3_ADDR]], align 4
// X86-NEXT:    store i32 [[A4]], ptr [[A4_ADDR]], align 4
// X86-NEXT:    store i32 [[A5]], ptr [[A5_ADDR]], align 4
// X86-NEXT:    store i32 [[A6]], ptr [[A6_ADDR]], align 4
// X86-NEXT:    store i32 [[A7]], ptr [[A7_ADDR]], align 4
// X86-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A0_ADDR]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load i32, ptr [[A1_ADDR]], align 4
// X86-NEXT:    [[TMP2:%.*]] = load i32, ptr [[A2_ADDR]], align 4
// X86-NEXT:    [[TMP3:%.*]] = load i32, ptr [[A3_ADDR]], align 4
// X86-NEXT:    [[TMP4:%.*]] = load i32, ptr [[A4_ADDR]], align 4
// X86-NEXT:    [[TMP5:%.*]] = load i32, ptr [[A5_ADDR]], align 4
// X86-NEXT:    [[TMP6:%.*]] = load i32, ptr [[A6_ADDR]], align 4
// X86-NEXT:    [[TMP7:%.*]] = load i32, ptr [[A7_ADDR]], align 4
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META278:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META278]]
// X86-NEXT:    store i32 [[TMP0]], ptr [[__I0_ADDR_I]], align 4, !noalias [[META278]]
// X86-NEXT:    store i32 [[TMP1]], ptr [[__I1_ADDR_I]], align 4, !noalias [[META278]]
// X86-NEXT:    store i32 [[TMP2]], ptr [[__I2_ADDR_I]], align 4, !noalias [[META278]]
// X86-NEXT:    store i32 [[TMP3]], ptr [[__I3_ADDR_I]], align 4, !noalias [[META278]]
// X86-NEXT:    store i32 [[TMP4]], ptr [[__I4_ADDR_I]], align 4, !noalias [[META278]]
// X86-NEXT:    store i32 [[TMP5]], ptr [[__I5_ADDR_I]], align 4, !noalias [[META278]]
// X86-NEXT:    store i32 [[TMP6]], ptr [[__I6_ADDR_I]], align 4, !noalias [[META278]]
// X86-NEXT:    store i32 [[TMP7]], ptr [[__I7_ADDR_I]], align 4, !noalias [[META278]]
// X86-NEXT:    [[TMP8:%.*]] = load i32, ptr [[__I7_ADDR_I]], align 4, !noalias [[META278]]
// X86-NEXT:    [[TMP9:%.*]] = load i32, ptr [[__I6_ADDR_I]], align 4, !noalias [[META278]]
// X86-NEXT:    [[TMP10:%.*]] = load i32, ptr [[__I5_ADDR_I]], align 4, !noalias [[META278]]
// X86-NEXT:    [[TMP11:%.*]] = load i32, ptr [[__I4_ADDR_I]], align 4, !noalias [[META278]]
// X86-NEXT:    [[TMP12:%.*]] = load i32, ptr [[__I3_ADDR_I]], align 4, !noalias [[META278]]
// X86-NEXT:    [[TMP13:%.*]] = load i32, ptr [[__I2_ADDR_I]], align 4, !noalias [[META278]]
// X86-NEXT:    [[TMP14:%.*]] = load i32, ptr [[__I1_ADDR_I]], align 4, !noalias [[META278]]
// X86-NEXT:    [[TMP15:%.*]] = load i32, ptr [[__I0_ADDR_I]], align 4, !noalias [[META278]]
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META281:![0-9]+]])
// X86-NEXT:    store ptr [[TMP_I]], ptr [[RESULT_PTR_I_I]], align 4, !noalias [[META284:![0-9]+]]
// X86-NEXT:    store i32 [[TMP8]], ptr [[__I0_ADDR_I_I]], align 4, !noalias [[META284]]
// X86-NEXT:    store i32 [[TMP9]], ptr [[__I1_ADDR_I_I]], align 4, !noalias [[META284]]
// X86-NEXT:    store i32 [[TMP10]], ptr [[__I2_ADDR_I_I]], align 4, !noalias [[META284]]
// X86-NEXT:    store i32 [[TMP11]], ptr [[__I3_ADDR_I_I]], align 4, !noalias [[META284]]
// X86-NEXT:    store i32 [[TMP12]], ptr [[__I4_ADDR_I_I]], align 4, !noalias [[META284]]
// X86-NEXT:    store i32 [[TMP13]], ptr [[__I5_ADDR_I_I]], align 4, !noalias [[META284]]
// X86-NEXT:    store i32 [[TMP14]], ptr [[__I6_ADDR_I_I]], align 4, !noalias [[META284]]
// X86-NEXT:    store i32 [[TMP15]], ptr [[__I7_ADDR_I_I]], align 4, !noalias [[META284]]
// X86-NEXT:    [[TMP16:%.*]] = load i32, ptr [[__I7_ADDR_I_I]], align 4, !noalias [[META284]]
// X86-NEXT:    [[VECINIT_I_I:%.*]] = insertelement <8 x i32> poison, i32 [[TMP16]], i32 0
// X86-NEXT:    [[TMP17:%.*]] = load i32, ptr [[__I6_ADDR_I_I]], align 4, !noalias [[META284]]
// X86-NEXT:    [[VECINIT1_I_I:%.*]] = insertelement <8 x i32> [[VECINIT_I_I]], i32 [[TMP17]], i32 1
// X86-NEXT:    [[TMP18:%.*]] = load i32, ptr [[__I5_ADDR_I_I]], align 4, !noalias [[META284]]
// X86-NEXT:    [[VECINIT2_I_I:%.*]] = insertelement <8 x i32> [[VECINIT1_I_I]], i32 [[TMP18]], i32 2
// X86-NEXT:    [[TMP19:%.*]] = load i32, ptr [[__I4_ADDR_I_I]], align 4, !noalias [[META284]]
// X86-NEXT:    [[VECINIT3_I_I:%.*]] = insertelement <8 x i32> [[VECINIT2_I_I]], i32 [[TMP19]], i32 3
// X86-NEXT:    [[TMP20:%.*]] = load i32, ptr [[__I3_ADDR_I_I]], align 4, !noalias [[META284]]
// X86-NEXT:    [[VECINIT4_I_I:%.*]] = insertelement <8 x i32> [[VECINIT3_I_I]], i32 [[TMP20]], i32 4
// X86-NEXT:    [[TMP21:%.*]] = load i32, ptr [[__I2_ADDR_I_I]], align 4, !noalias [[META284]]
// X86-NEXT:    [[VECINIT5_I_I:%.*]] = insertelement <8 x i32> [[VECINIT4_I_I]], i32 [[TMP21]], i32 5
// X86-NEXT:    [[TMP22:%.*]] = load i32, ptr [[__I1_ADDR_I_I]], align 4, !noalias [[META284]]
// X86-NEXT:    [[VECINIT6_I_I:%.*]] = insertelement <8 x i32> [[VECINIT5_I_I]], i32 [[TMP22]], i32 6
// X86-NEXT:    [[TMP23:%.*]] = load i32, ptr [[__I0_ADDR_I_I]], align 4, !noalias [[META284]]
// X86-NEXT:    [[VECINIT7_I_I:%.*]] = insertelement <8 x i32> [[VECINIT6_I_I]], i32 [[TMP23]], i32 7
// X86-NEXT:    store <8 x i32> [[VECINIT7_I_I]], ptr [[DOTCOMPOUNDLITERAL_I_I]], align 32, !noalias [[META284]]
// X86-NEXT:    [[TMP24:%.*]] = load <8 x i32>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 32, !noalias [[META284]]
// X86-NEXT:    [[TMP25:%.*]] = bitcast <8 x i32> [[TMP24]] to <4 x i64>
// X86-NEXT:    store <4 x i64> [[TMP25]], ptr [[TMP_I]], align 32, !alias.scope [[META281]], !noalias [[META278]]
// X86-NEXT:    [[TMP26:%.*]] = load <4 x i64>, ptr [[TMP_I]], align 32, !alias.scope [[META281]], !noalias [[META278]]
// X86-NEXT:    store <4 x i64> [[TMP26]], ptr [[TMP_I]], align 32, !alias.scope [[META281]], !noalias [[META278]]
// X86-NEXT:    [[TMP27:%.*]] = load <4 x i64>, ptr [[TMP_I]], align 32, !noalias [[META278]]
// X86-NEXT:    store <4 x i64> [[TMP27]], ptr [[TMP]], align 32, !alias.scope [[META278]]
// X86-NEXT:    [[TMP28:%.*]] = load <4 x i64>, ptr [[TMP]], align 32, !alias.scope [[META278]]
// X86-NEXT:    store <4 x i64> [[TMP28]], ptr [[TMP]], align 32, !alias.scope [[META278]]
// X86-NEXT:    [[TMP29:%.*]] = load <4 x i64>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x i64> [[TMP29]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP30:%.*]] = load <4 x i64>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x i64> [[TMP30]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256i test_mm256_setr_epi32(int A0, int A1, int A2, int A3, int A4, int A5, int A6, int A7) {
  return _mm256_setr_epi32(A0, A1, A2, A3, A4, A5, A6, A7);
}

//
// X86-LABEL: define void @test_mm256_setr_epi64x(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x i64>) align 32 [[AGG_RESULT:%.*]], i64 noundef [[A0:%.*]], i64 noundef [[A1:%.*]], i64 noundef [[A2:%.*]], i64 noundef [[A3:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I_I:%.*]] = alloca i64, align 8
// X86-NEXT:    [[__B_ADDR_I_I:%.*]] = alloca i64, align 8
// X86-NEXT:    [[__C_ADDR_I_I:%.*]] = alloca i64, align 8
// X86-NEXT:    [[__D_ADDR_I_I:%.*]] = alloca i64, align 8
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca i64, align 8
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca i64, align 8
// X86-NEXT:    [[__C_ADDR_I:%.*]] = alloca i64, align 8
// X86-NEXT:    [[__D_ADDR_I:%.*]] = alloca i64, align 8
// X86-NEXT:    [[TMP_I:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A0_ADDR:%.*]] = alloca i64, align 8
// X86-NEXT:    [[A1_ADDR:%.*]] = alloca i64, align 8
// X86-NEXT:    [[A2_ADDR:%.*]] = alloca i64, align 8
// X86-NEXT:    [[A3_ADDR:%.*]] = alloca i64, align 8
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store i64 [[A0]], ptr [[A0_ADDR]], align 8
// X86-NEXT:    store i64 [[A1]], ptr [[A1_ADDR]], align 8
// X86-NEXT:    store i64 [[A2]], ptr [[A2_ADDR]], align 8
// X86-NEXT:    store i64 [[A3]], ptr [[A3_ADDR]], align 8
// X86-NEXT:    [[TMP0:%.*]] = load i64, ptr [[A0_ADDR]], align 8
// X86-NEXT:    [[TMP1:%.*]] = load i64, ptr [[A1_ADDR]], align 8
// X86-NEXT:    [[TMP2:%.*]] = load i64, ptr [[A2_ADDR]], align 8
// X86-NEXT:    [[TMP3:%.*]] = load i64, ptr [[A3_ADDR]], align 8
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META285:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META285]]
// X86-NEXT:    store i64 [[TMP0]], ptr [[__A_ADDR_I]], align 8, !noalias [[META285]]
// X86-NEXT:    store i64 [[TMP1]], ptr [[__B_ADDR_I]], align 8, !noalias [[META285]]
// X86-NEXT:    store i64 [[TMP2]], ptr [[__C_ADDR_I]], align 8, !noalias [[META285]]
// X86-NEXT:    store i64 [[TMP3]], ptr [[__D_ADDR_I]], align 8, !noalias [[META285]]
// X86-NEXT:    [[TMP4:%.*]] = load i64, ptr [[__D_ADDR_I]], align 8, !noalias [[META285]]
// X86-NEXT:    [[TMP5:%.*]] = load i64, ptr [[__C_ADDR_I]], align 8, !noalias [[META285]]
// X86-NEXT:    [[TMP6:%.*]] = load i64, ptr [[__B_ADDR_I]], align 8, !noalias [[META285]]
// X86-NEXT:    [[TMP7:%.*]] = load i64, ptr [[__A_ADDR_I]], align 8, !noalias [[META285]]
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META288:![0-9]+]])
// X86-NEXT:    store ptr [[TMP_I]], ptr [[RESULT_PTR_I_I]], align 4, !noalias [[META291:![0-9]+]]
// X86-NEXT:    store i64 [[TMP4]], ptr [[__A_ADDR_I_I]], align 8, !noalias [[META291]]
// X86-NEXT:    store i64 [[TMP5]], ptr [[__B_ADDR_I_I]], align 8, !noalias [[META291]]
// X86-NEXT:    store i64 [[TMP6]], ptr [[__C_ADDR_I_I]], align 8, !noalias [[META291]]
// X86-NEXT:    store i64 [[TMP7]], ptr [[__D_ADDR_I_I]], align 8, !noalias [[META291]]
// X86-NEXT:    [[TMP8:%.*]] = load i64, ptr [[__D_ADDR_I_I]], align 8, !noalias [[META291]]
// X86-NEXT:    [[VECINIT_I_I:%.*]] = insertelement <4 x i64> poison, i64 [[TMP8]], i32 0
// X86-NEXT:    [[TMP9:%.*]] = load i64, ptr [[__C_ADDR_I_I]], align 8, !noalias [[META291]]
// X86-NEXT:    [[VECINIT1_I_I:%.*]] = insertelement <4 x i64> [[VECINIT_I_I]], i64 [[TMP9]], i32 1
// X86-NEXT:    [[TMP10:%.*]] = load i64, ptr [[__B_ADDR_I_I]], align 8, !noalias [[META291]]
// X86-NEXT:    [[VECINIT2_I_I:%.*]] = insertelement <4 x i64> [[VECINIT1_I_I]], i64 [[TMP10]], i32 2
// X86-NEXT:    [[TMP11:%.*]] = load i64, ptr [[__A_ADDR_I_I]], align 8, !noalias [[META291]]
// X86-NEXT:    [[VECINIT3_I_I:%.*]] = insertelement <4 x i64> [[VECINIT2_I_I]], i64 [[TMP11]], i32 3
// X86-NEXT:    store <4 x i64> [[VECINIT3_I_I]], ptr [[DOTCOMPOUNDLITERAL_I_I]], align 32, !noalias [[META291]]
// X86-NEXT:    [[TMP12:%.*]] = load <4 x i64>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 32, !noalias [[META291]]
// X86-NEXT:    store <4 x i64> [[TMP12]], ptr [[TMP_I]], align 32, !alias.scope [[META288]], !noalias [[META285]]
// X86-NEXT:    [[TMP13:%.*]] = load <4 x i64>, ptr [[TMP_I]], align 32, !alias.scope [[META288]], !noalias [[META285]]
// X86-NEXT:    store <4 x i64> [[TMP13]], ptr [[TMP_I]], align 32, !alias.scope [[META288]], !noalias [[META285]]
// X86-NEXT:    [[TMP14:%.*]] = load <4 x i64>, ptr [[TMP_I]], align 32, !noalias [[META285]]
// X86-NEXT:    store <4 x i64> [[TMP14]], ptr [[TMP]], align 32, !alias.scope [[META285]]
// X86-NEXT:    [[TMP15:%.*]] = load <4 x i64>, ptr [[TMP]], align 32, !alias.scope [[META285]]
// X86-NEXT:    store <4 x i64> [[TMP15]], ptr [[TMP]], align 32, !alias.scope [[META285]]
// X86-NEXT:    [[TMP16:%.*]] = load <4 x i64>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x i64> [[TMP16]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP17:%.*]] = load <4 x i64>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x i64> [[TMP17]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256i test_mm256_setr_epi64x(long long A0, long long A1, long long A2, long long A3) {
  return _mm256_setr_epi64x(A0, A1, A2, A3);
}

//
// X86-LABEL: define void @test_mm256_setr_m128(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <4 x float> noundef [[A:%.*]], <4 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__HI_ADDR_I_I:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[__LO_ADDR_I_I:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__LO_ADDR_I:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[__HI_ADDR_I:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[TMP_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[TMP:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <4 x float> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[B_ADDR]], align 16
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META292:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META292]]
// X86-NEXT:    store <4 x float> [[TMP0]], ptr [[__LO_ADDR_I]], align 16, !noalias [[META292]]
// X86-NEXT:    store <4 x float> [[TMP1]], ptr [[__HI_ADDR_I]], align 16, !noalias [[META292]]
// X86-NEXT:    [[TMP2:%.*]] = load <4 x float>, ptr [[__HI_ADDR_I]], align 16, !noalias [[META292]]
// X86-NEXT:    [[TMP3:%.*]] = load <4 x float>, ptr [[__LO_ADDR_I]], align 16, !noalias [[META292]]
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META295:![0-9]+]])
// X86-NEXT:    store ptr [[TMP_I]], ptr [[RESULT_PTR_I_I]], align 4, !noalias [[META298:![0-9]+]]
// X86-NEXT:    store <4 x float> [[TMP2]], ptr [[__HI_ADDR_I_I]], align 16, !noalias [[META298]]
// X86-NEXT:    store <4 x float> [[TMP3]], ptr [[__LO_ADDR_I_I]], align 16, !noalias [[META298]]
// X86-NEXT:    [[TMP4:%.*]] = load <4 x float>, ptr [[__LO_ADDR_I_I]], align 16, !noalias [[META298]]
// X86-NEXT:    [[TMP5:%.*]] = load <4 x float>, ptr [[__HI_ADDR_I_I]], align 16, !noalias [[META298]]
// X86-NEXT:    [[SHUFFLE_I_I:%.*]] = shufflevector <4 x float> [[TMP4]], <4 x float> [[TMP5]], <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
// X86-NEXT:    store <8 x float> [[SHUFFLE_I_I]], ptr [[TMP_I]], align 32, !alias.scope [[META295]], !noalias [[META292]]
// X86-NEXT:    [[TMP6:%.*]] = load <8 x float>, ptr [[TMP_I]], align 32, !alias.scope [[META295]], !noalias [[META292]]
// X86-NEXT:    store <8 x float> [[TMP6]], ptr [[TMP_I]], align 32, !alias.scope [[META295]], !noalias [[META292]]
// X86-NEXT:    [[TMP7:%.*]] = load <8 x float>, ptr [[TMP_I]], align 32, !noalias [[META292]]
// X86-NEXT:    store <8 x float> [[TMP7]], ptr [[TMP]], align 32, !alias.scope [[META292]]
// X86-NEXT:    [[TMP8:%.*]] = load <8 x float>, ptr [[TMP]], align 32, !alias.scope [[META292]]
// X86-NEXT:    store <8 x float> [[TMP8]], ptr [[TMP]], align 32, !alias.scope [[META292]]
// X86-NEXT:    [[TMP9:%.*]] = load <8 x float>, ptr [[TMP]], align 32
// X86-NEXT:    store <8 x float> [[TMP9]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP10:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP10]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_setr_m128(__m128 A, __m128 B) {
  return _mm256_setr_m128(A, B);
}

//
// X86-LABEL: define void @test_mm256_setr_m128d(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__HI_ADDR_I_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__LO_ADDR_I_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__LO_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__HI_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[TMP_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META299:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META299]]
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__LO_ADDR_I]], align 16, !noalias [[META299]]
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__HI_ADDR_I]], align 16, !noalias [[META299]]
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__HI_ADDR_I]], align 16, !noalias [[META299]]
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__LO_ADDR_I]], align 16, !noalias [[META299]]
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META302:![0-9]+]])
// X86-NEXT:    store ptr [[TMP_I]], ptr [[RESULT_PTR_I_I]], align 4, !noalias [[META305:![0-9]+]]
// X86-NEXT:    store <2 x double> [[TMP2]], ptr [[__HI_ADDR_I_I]], align 16, !noalias [[META305]]
// X86-NEXT:    store <2 x double> [[TMP3]], ptr [[__LO_ADDR_I_I]], align 16, !noalias [[META305]]
// X86-NEXT:    [[TMP4:%.*]] = load <2 x double>, ptr [[__LO_ADDR_I_I]], align 16, !noalias [[META305]]
// X86-NEXT:    [[TMP5:%.*]] = load <2 x double>, ptr [[__HI_ADDR_I_I]], align 16, !noalias [[META305]]
// X86-NEXT:    [[SHUFFLE_I_I:%.*]] = shufflevector <2 x double> [[TMP4]], <2 x double> [[TMP5]], <4 x i32> <i32 0, i32 1, i32 2, i32 3>
// X86-NEXT:    store <4 x double> [[SHUFFLE_I_I]], ptr [[TMP_I]], align 32, !alias.scope [[META302]], !noalias [[META299]]
// X86-NEXT:    [[TMP6:%.*]] = load <4 x double>, ptr [[TMP_I]], align 32, !alias.scope [[META302]], !noalias [[META299]]
// X86-NEXT:    store <4 x double> [[TMP6]], ptr [[TMP_I]], align 32, !alias.scope [[META302]], !noalias [[META299]]
// X86-NEXT:    [[TMP7:%.*]] = load <4 x double>, ptr [[TMP_I]], align 32, !noalias [[META299]]
// X86-NEXT:    store <4 x double> [[TMP7]], ptr [[TMP]], align 32, !alias.scope [[META299]]
// X86-NEXT:    [[TMP8:%.*]] = load <4 x double>, ptr [[TMP]], align 32, !alias.scope [[META299]]
// X86-NEXT:    store <4 x double> [[TMP8]], ptr [[TMP]], align 32, !alias.scope [[META299]]
// X86-NEXT:    [[TMP9:%.*]] = load <4 x double>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x double> [[TMP9]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP10:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP10]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_setr_m128d(__m128d A, __m128d B) {
  return _mm256_setr_m128d(A, B);
}

//
// X86-LABEL: define void @test_mm256_setr_m128i(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x i64>) align 32 [[AGG_RESULT:%.*]], <2 x i64> noundef [[A:%.*]], <2 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__HI_ADDR_I_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__LO_ADDR_I_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__LO_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__HI_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[TMP_I:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x i64> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[B_ADDR]], align 16
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META306:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META306]]
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__LO_ADDR_I]], align 16, !noalias [[META306]]
// X86-NEXT:    store <2 x i64> [[TMP1]], ptr [[__HI_ADDR_I]], align 16, !noalias [[META306]]
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[__HI_ADDR_I]], align 16, !noalias [[META306]]
// X86-NEXT:    [[TMP3:%.*]] = load <2 x i64>, ptr [[__LO_ADDR_I]], align 16, !noalias [[META306]]
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META309:![0-9]+]])
// X86-NEXT:    store ptr [[TMP_I]], ptr [[RESULT_PTR_I_I]], align 4, !noalias [[META312:![0-9]+]]
// X86-NEXT:    store <2 x i64> [[TMP2]], ptr [[__HI_ADDR_I_I]], align 16, !noalias [[META312]]
// X86-NEXT:    store <2 x i64> [[TMP3]], ptr [[__LO_ADDR_I_I]], align 16, !noalias [[META312]]
// X86-NEXT:    [[TMP4:%.*]] = load <2 x i64>, ptr [[__LO_ADDR_I_I]], align 16, !noalias [[META312]]
// X86-NEXT:    [[TMP5:%.*]] = load <2 x i64>, ptr [[__HI_ADDR_I_I]], align 16, !noalias [[META312]]
// X86-NEXT:    [[SHUFFLE_I_I:%.*]] = shufflevector <2 x i64> [[TMP4]], <2 x i64> [[TMP5]], <4 x i32> <i32 0, i32 1, i32 2, i32 3>
// X86-NEXT:    store <4 x i64> [[SHUFFLE_I_I]], ptr [[TMP_I]], align 32, !alias.scope [[META309]], !noalias [[META306]]
// X86-NEXT:    [[TMP6:%.*]] = load <4 x i64>, ptr [[TMP_I]], align 32, !alias.scope [[META309]], !noalias [[META306]]
// X86-NEXT:    store <4 x i64> [[TMP6]], ptr [[TMP_I]], align 32, !alias.scope [[META309]], !noalias [[META306]]
// X86-NEXT:    [[TMP7:%.*]] = load <4 x i64>, ptr [[TMP_I]], align 32, !noalias [[META306]]
// X86-NEXT:    store <4 x i64> [[TMP7]], ptr [[TMP]], align 32, !alias.scope [[META306]]
// X86-NEXT:    [[TMP8:%.*]] = load <4 x i64>, ptr [[TMP]], align 32, !alias.scope [[META306]]
// X86-NEXT:    store <4 x i64> [[TMP8]], ptr [[TMP]], align 32, !alias.scope [[META306]]
// X86-NEXT:    [[TMP9:%.*]] = load <4 x i64>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x i64> [[TMP9]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP10:%.*]] = load <4 x i64>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x i64> [[TMP10]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256i test_mm256_setr_m128i(__m128i A, __m128i B) {
  return _mm256_setr_m128i(A, B);
}

//
// X86-LABEL: define void @test_mm256_setr_pd(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], double noundef [[A0:%.*]], double noundef [[A1:%.*]], double noundef [[A2:%.*]], double noundef [[A3:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I_I:%.*]] = alloca double, align 8
// X86-NEXT:    [[__B_ADDR_I_I:%.*]] = alloca double, align 8
// X86-NEXT:    [[__C_ADDR_I_I:%.*]] = alloca double, align 8
// X86-NEXT:    [[__D_ADDR_I_I:%.*]] = alloca double, align 8
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca double, align 8
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca double, align 8
// X86-NEXT:    [[__C_ADDR_I:%.*]] = alloca double, align 8
// X86-NEXT:    [[__D_ADDR_I:%.*]] = alloca double, align 8
// X86-NEXT:    [[TMP_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A0_ADDR:%.*]] = alloca double, align 8
// X86-NEXT:    [[A1_ADDR:%.*]] = alloca double, align 8
// X86-NEXT:    [[A2_ADDR:%.*]] = alloca double, align 8
// X86-NEXT:    [[A3_ADDR:%.*]] = alloca double, align 8
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store double [[A0]], ptr [[A0_ADDR]], align 8
// X86-NEXT:    store double [[A1]], ptr [[A1_ADDR]], align 8
// X86-NEXT:    store double [[A2]], ptr [[A2_ADDR]], align 8
// X86-NEXT:    store double [[A3]], ptr [[A3_ADDR]], align 8
// X86-NEXT:    [[TMP0:%.*]] = load double, ptr [[A0_ADDR]], align 8
// X86-NEXT:    [[TMP1:%.*]] = load double, ptr [[A1_ADDR]], align 8
// X86-NEXT:    [[TMP2:%.*]] = load double, ptr [[A2_ADDR]], align 8
// X86-NEXT:    [[TMP3:%.*]] = load double, ptr [[A3_ADDR]], align 8
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META313:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META313]]
// X86-NEXT:    store double [[TMP0]], ptr [[__A_ADDR_I]], align 8, !noalias [[META313]]
// X86-NEXT:    store double [[TMP1]], ptr [[__B_ADDR_I]], align 8, !noalias [[META313]]
// X86-NEXT:    store double [[TMP2]], ptr [[__C_ADDR_I]], align 8, !noalias [[META313]]
// X86-NEXT:    store double [[TMP3]], ptr [[__D_ADDR_I]], align 8, !noalias [[META313]]
// X86-NEXT:    [[TMP4:%.*]] = load double, ptr [[__D_ADDR_I]], align 8, !noalias [[META313]]
// X86-NEXT:    [[TMP5:%.*]] = load double, ptr [[__C_ADDR_I]], align 8, !noalias [[META313]]
// X86-NEXT:    [[TMP6:%.*]] = load double, ptr [[__B_ADDR_I]], align 8, !noalias [[META313]]
// X86-NEXT:    [[TMP7:%.*]] = load double, ptr [[__A_ADDR_I]], align 8, !noalias [[META313]]
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META316:![0-9]+]])
// X86-NEXT:    store ptr [[TMP_I]], ptr [[RESULT_PTR_I_I]], align 4, !noalias [[META319:![0-9]+]]
// X86-NEXT:    store double [[TMP4]], ptr [[__A_ADDR_I_I]], align 8, !noalias [[META319]]
// X86-NEXT:    store double [[TMP5]], ptr [[__B_ADDR_I_I]], align 8, !noalias [[META319]]
// X86-NEXT:    store double [[TMP6]], ptr [[__C_ADDR_I_I]], align 8, !noalias [[META319]]
// X86-NEXT:    store double [[TMP7]], ptr [[__D_ADDR_I_I]], align 8, !noalias [[META319]]
// X86-NEXT:    [[TMP8:%.*]] = load double, ptr [[__D_ADDR_I_I]], align 8, !noalias [[META319]]
// X86-NEXT:    [[VECINIT_I_I:%.*]] = insertelement <4 x double> poison, double [[TMP8]], i32 0
// X86-NEXT:    [[TMP9:%.*]] = load double, ptr [[__C_ADDR_I_I]], align 8, !noalias [[META319]]
// X86-NEXT:    [[VECINIT1_I_I:%.*]] = insertelement <4 x double> [[VECINIT_I_I]], double [[TMP9]], i32 1
// X86-NEXT:    [[TMP10:%.*]] = load double, ptr [[__B_ADDR_I_I]], align 8, !noalias [[META319]]
// X86-NEXT:    [[VECINIT2_I_I:%.*]] = insertelement <4 x double> [[VECINIT1_I_I]], double [[TMP10]], i32 2
// X86-NEXT:    [[TMP11:%.*]] = load double, ptr [[__A_ADDR_I_I]], align 8, !noalias [[META319]]
// X86-NEXT:    [[VECINIT3_I_I:%.*]] = insertelement <4 x double> [[VECINIT2_I_I]], double [[TMP11]], i32 3
// X86-NEXT:    store <4 x double> [[VECINIT3_I_I]], ptr [[DOTCOMPOUNDLITERAL_I_I]], align 32, !noalias [[META319]]
// X86-NEXT:    [[TMP12:%.*]] = load <4 x double>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 32, !noalias [[META319]]
// X86-NEXT:    store <4 x double> [[TMP12]], ptr [[TMP_I]], align 32, !alias.scope [[META316]], !noalias [[META313]]
// X86-NEXT:    [[TMP13:%.*]] = load <4 x double>, ptr [[TMP_I]], align 32, !alias.scope [[META316]], !noalias [[META313]]
// X86-NEXT:    store <4 x double> [[TMP13]], ptr [[TMP_I]], align 32, !alias.scope [[META316]], !noalias [[META313]]
// X86-NEXT:    [[TMP14:%.*]] = load <4 x double>, ptr [[TMP_I]], align 32, !noalias [[META313]]
// X86-NEXT:    store <4 x double> [[TMP14]], ptr [[TMP]], align 32, !alias.scope [[META313]]
// X86-NEXT:    [[TMP15:%.*]] = load <4 x double>, ptr [[TMP]], align 32, !alias.scope [[META313]]
// X86-NEXT:    store <4 x double> [[TMP15]], ptr [[TMP]], align 32, !alias.scope [[META313]]
// X86-NEXT:    [[TMP16:%.*]] = load <4 x double>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x double> [[TMP16]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP17:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP17]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_setr_pd(double A0, double A1, double A2, double A3) {
  return _mm256_setr_pd(A0, A1, A2, A3);
}

//
// X86-LABEL: define void @test_mm256_setr_ps(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], float noundef [[A0:%.*]], float noundef [[A1:%.*]], float noundef [[A2:%.*]], float noundef [[A3:%.*]], float noundef [[A4:%.*]], float noundef [[A5:%.*]], float noundef [[A6:%.*]], float noundef [[A7:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I_I:%.*]] = alloca float, align 4
// X86-NEXT:    [[__B_ADDR_I_I:%.*]] = alloca float, align 4
// X86-NEXT:    [[__C_ADDR_I_I:%.*]] = alloca float, align 4
// X86-NEXT:    [[__D_ADDR_I_I:%.*]] = alloca float, align 4
// X86-NEXT:    [[__E_ADDR_I_I:%.*]] = alloca float, align 4
// X86-NEXT:    [[__F_ADDR_I_I:%.*]] = alloca float, align 4
// X86-NEXT:    [[__G_ADDR_I_I:%.*]] = alloca float, align 4
// X86-NEXT:    [[__H_ADDR_I_I:%.*]] = alloca float, align 4
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca float, align 4
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca float, align 4
// X86-NEXT:    [[__C_ADDR_I:%.*]] = alloca float, align 4
// X86-NEXT:    [[__D_ADDR_I:%.*]] = alloca float, align 4
// X86-NEXT:    [[__E_ADDR_I:%.*]] = alloca float, align 4
// X86-NEXT:    [[__F_ADDR_I:%.*]] = alloca float, align 4
// X86-NEXT:    [[__G_ADDR_I:%.*]] = alloca float, align 4
// X86-NEXT:    [[__H_ADDR_I:%.*]] = alloca float, align 4
// X86-NEXT:    [[TMP_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A0_ADDR:%.*]] = alloca float, align 4
// X86-NEXT:    [[A1_ADDR:%.*]] = alloca float, align 4
// X86-NEXT:    [[A2_ADDR:%.*]] = alloca float, align 4
// X86-NEXT:    [[A3_ADDR:%.*]] = alloca float, align 4
// X86-NEXT:    [[A4_ADDR:%.*]] = alloca float, align 4
// X86-NEXT:    [[A5_ADDR:%.*]] = alloca float, align 4
// X86-NEXT:    [[A6_ADDR:%.*]] = alloca float, align 4
// X86-NEXT:    [[A7_ADDR:%.*]] = alloca float, align 4
// X86-NEXT:    [[TMP:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store float [[A0]], ptr [[A0_ADDR]], align 4
// X86-NEXT:    store float [[A1]], ptr [[A1_ADDR]], align 4
// X86-NEXT:    store float [[A2]], ptr [[A2_ADDR]], align 4
// X86-NEXT:    store float [[A3]], ptr [[A3_ADDR]], align 4
// X86-NEXT:    store float [[A4]], ptr [[A4_ADDR]], align 4
// X86-NEXT:    store float [[A5]], ptr [[A5_ADDR]], align 4
// X86-NEXT:    store float [[A6]], ptr [[A6_ADDR]], align 4
// X86-NEXT:    store float [[A7]], ptr [[A7_ADDR]], align 4
// X86-NEXT:    [[TMP0:%.*]] = load float, ptr [[A0_ADDR]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load float, ptr [[A1_ADDR]], align 4
// X86-NEXT:    [[TMP2:%.*]] = load float, ptr [[A2_ADDR]], align 4
// X86-NEXT:    [[TMP3:%.*]] = load float, ptr [[A3_ADDR]], align 4
// X86-NEXT:    [[TMP4:%.*]] = load float, ptr [[A4_ADDR]], align 4
// X86-NEXT:    [[TMP5:%.*]] = load float, ptr [[A5_ADDR]], align 4
// X86-NEXT:    [[TMP6:%.*]] = load float, ptr [[A6_ADDR]], align 4
// X86-NEXT:    [[TMP7:%.*]] = load float, ptr [[A7_ADDR]], align 4
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META320:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META320]]
// X86-NEXT:    store float [[TMP0]], ptr [[__A_ADDR_I]], align 4, !noalias [[META320]]
// X86-NEXT:    store float [[TMP1]], ptr [[__B_ADDR_I]], align 4, !noalias [[META320]]
// X86-NEXT:    store float [[TMP2]], ptr [[__C_ADDR_I]], align 4, !noalias [[META320]]
// X86-NEXT:    store float [[TMP3]], ptr [[__D_ADDR_I]], align 4, !noalias [[META320]]
// X86-NEXT:    store float [[TMP4]], ptr [[__E_ADDR_I]], align 4, !noalias [[META320]]
// X86-NEXT:    store float [[TMP5]], ptr [[__F_ADDR_I]], align 4, !noalias [[META320]]
// X86-NEXT:    store float [[TMP6]], ptr [[__G_ADDR_I]], align 4, !noalias [[META320]]
// X86-NEXT:    store float [[TMP7]], ptr [[__H_ADDR_I]], align 4, !noalias [[META320]]
// X86-NEXT:    [[TMP8:%.*]] = load float, ptr [[__H_ADDR_I]], align 4, !noalias [[META320]]
// X86-NEXT:    [[TMP9:%.*]] = load float, ptr [[__G_ADDR_I]], align 4, !noalias [[META320]]
// X86-NEXT:    [[TMP10:%.*]] = load float, ptr [[__F_ADDR_I]], align 4, !noalias [[META320]]
// X86-NEXT:    [[TMP11:%.*]] = load float, ptr [[__E_ADDR_I]], align 4, !noalias [[META320]]
// X86-NEXT:    [[TMP12:%.*]] = load float, ptr [[__D_ADDR_I]], align 4, !noalias [[META320]]
// X86-NEXT:    [[TMP13:%.*]] = load float, ptr [[__C_ADDR_I]], align 4, !noalias [[META320]]
// X86-NEXT:    [[TMP14:%.*]] = load float, ptr [[__B_ADDR_I]], align 4, !noalias [[META320]]
// X86-NEXT:    [[TMP15:%.*]] = load float, ptr [[__A_ADDR_I]], align 4, !noalias [[META320]]
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META323:![0-9]+]])
// X86-NEXT:    store ptr [[TMP_I]], ptr [[RESULT_PTR_I_I]], align 4, !noalias [[META326:![0-9]+]]
// X86-NEXT:    store float [[TMP8]], ptr [[__A_ADDR_I_I]], align 4, !noalias [[META326]]
// X86-NEXT:    store float [[TMP9]], ptr [[__B_ADDR_I_I]], align 4, !noalias [[META326]]
// X86-NEXT:    store float [[TMP10]], ptr [[__C_ADDR_I_I]], align 4, !noalias [[META326]]
// X86-NEXT:    store float [[TMP11]], ptr [[__D_ADDR_I_I]], align 4, !noalias [[META326]]
// X86-NEXT:    store float [[TMP12]], ptr [[__E_ADDR_I_I]], align 4, !noalias [[META326]]
// X86-NEXT:    store float [[TMP13]], ptr [[__F_ADDR_I_I]], align 4, !noalias [[META326]]
// X86-NEXT:    store float [[TMP14]], ptr [[__G_ADDR_I_I]], align 4, !noalias [[META326]]
// X86-NEXT:    store float [[TMP15]], ptr [[__H_ADDR_I_I]], align 4, !noalias [[META326]]
// X86-NEXT:    [[TMP16:%.*]] = load float, ptr [[__H_ADDR_I_I]], align 4, !noalias [[META326]]
// X86-NEXT:    [[VECINIT_I_I:%.*]] = insertelement <8 x float> poison, float [[TMP16]], i32 0
// X86-NEXT:    [[TMP17:%.*]] = load float, ptr [[__G_ADDR_I_I]], align 4, !noalias [[META326]]
// X86-NEXT:    [[VECINIT1_I_I:%.*]] = insertelement <8 x float> [[VECINIT_I_I]], float [[TMP17]], i32 1
// X86-NEXT:    [[TMP18:%.*]] = load float, ptr [[__F_ADDR_I_I]], align 4, !noalias [[META326]]
// X86-NEXT:    [[VECINIT2_I_I:%.*]] = insertelement <8 x float> [[VECINIT1_I_I]], float [[TMP18]], i32 2
// X86-NEXT:    [[TMP19:%.*]] = load float, ptr [[__E_ADDR_I_I]], align 4, !noalias [[META326]]
// X86-NEXT:    [[VECINIT3_I_I:%.*]] = insertelement <8 x float> [[VECINIT2_I_I]], float [[TMP19]], i32 3
// X86-NEXT:    [[TMP20:%.*]] = load float, ptr [[__D_ADDR_I_I]], align 4, !noalias [[META326]]
// X86-NEXT:    [[VECINIT4_I_I:%.*]] = insertelement <8 x float> [[VECINIT3_I_I]], float [[TMP20]], i32 4
// X86-NEXT:    [[TMP21:%.*]] = load float, ptr [[__C_ADDR_I_I]], align 4, !noalias [[META326]]
// X86-NEXT:    [[VECINIT5_I_I:%.*]] = insertelement <8 x float> [[VECINIT4_I_I]], float [[TMP21]], i32 5
// X86-NEXT:    [[TMP22:%.*]] = load float, ptr [[__B_ADDR_I_I]], align 4, !noalias [[META326]]
// X86-NEXT:    [[VECINIT6_I_I:%.*]] = insertelement <8 x float> [[VECINIT5_I_I]], float [[TMP22]], i32 6
// X86-NEXT:    [[TMP23:%.*]] = load float, ptr [[__A_ADDR_I_I]], align 4, !noalias [[META326]]
// X86-NEXT:    [[VECINIT7_I_I:%.*]] = insertelement <8 x float> [[VECINIT6_I_I]], float [[TMP23]], i32 7
// X86-NEXT:    store <8 x float> [[VECINIT7_I_I]], ptr [[DOTCOMPOUNDLITERAL_I_I]], align 32, !noalias [[META326]]
// X86-NEXT:    [[TMP24:%.*]] = load <8 x float>, ptr [[DOTCOMPOUNDLITERAL_I_I]], align 32, !noalias [[META326]]
// X86-NEXT:    store <8 x float> [[TMP24]], ptr [[TMP_I]], align 32, !alias.scope [[META323]], !noalias [[META320]]
// X86-NEXT:    [[TMP25:%.*]] = load <8 x float>, ptr [[TMP_I]], align 32, !alias.scope [[META323]], !noalias [[META320]]
// X86-NEXT:    store <8 x float> [[TMP25]], ptr [[TMP_I]], align 32, !alias.scope [[META323]], !noalias [[META320]]
// X86-NEXT:    [[TMP26:%.*]] = load <8 x float>, ptr [[TMP_I]], align 32, !noalias [[META320]]
// X86-NEXT:    store <8 x float> [[TMP26]], ptr [[TMP]], align 32, !alias.scope [[META320]]
// X86-NEXT:    [[TMP27:%.*]] = load <8 x float>, ptr [[TMP]], align 32, !alias.scope [[META320]]
// X86-NEXT:    store <8 x float> [[TMP27]], ptr [[TMP]], align 32, !alias.scope [[META320]]
// X86-NEXT:    [[TMP28:%.*]] = load <8 x float>, ptr [[TMP]], align 32
// X86-NEXT:    store <8 x float> [[TMP28]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP29:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP29]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_setr_ps(float A0, float A1, float A2, float A3, float A4, float A5, float A6, float A7) {
  return _mm256_setr_ps(A0, A1, A2, A3, A4, A5, A6, A7);
}

//
// X86-LABEL: define void @test_mm256_setzero_pd(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META327:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META327]]
// X86-NEXT:    store <4 x double> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 32, !noalias [[META327]]
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[DOTCOMPOUNDLITERAL_I]], align 32, !noalias [[META327]]
// X86-NEXT:    store <4 x double> [[TMP0]], ptr [[TMP]], align 32, !alias.scope [[META327]]
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[TMP]], align 32, !alias.scope [[META327]]
// X86-NEXT:    store <4 x double> [[TMP1]], ptr [[TMP]], align 32, !alias.scope [[META327]]
// X86-NEXT:    [[TMP2:%.*]] = load <4 x double>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x double> [[TMP2]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP3:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP3]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_setzero_pd(void) {
  return _mm256_setzero_pd();
}

//
// X86-LABEL: define void @test_mm256_setzero_ps(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[TMP:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META330:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META330]]
// X86-NEXT:    store <8 x float> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 32, !noalias [[META330]]
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[DOTCOMPOUNDLITERAL_I]], align 32, !noalias [[META330]]
// X86-NEXT:    store <8 x float> [[TMP0]], ptr [[TMP]], align 32, !alias.scope [[META330]]
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[TMP]], align 32, !alias.scope [[META330]]
// X86-NEXT:    store <8 x float> [[TMP1]], ptr [[TMP]], align 32, !alias.scope [[META330]]
// X86-NEXT:    [[TMP2:%.*]] = load <8 x float>, ptr [[TMP]], align 32
// X86-NEXT:    store <8 x float> [[TMP2]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP3:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP3]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_setzero_ps(void) {
  return _mm256_setzero_ps();
}

//
// X86-LABEL: define void @test_mm256_setzero_si256(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x i64>) align 32 [[AGG_RESULT:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META333:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META333]]
// X86-NEXT:    store <4 x i64> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 32, !noalias [[META333]]
// X86-NEXT:    [[TMP0:%.*]] = load <4 x i64>, ptr [[DOTCOMPOUNDLITERAL_I]], align 32, !noalias [[META333]]
// X86-NEXT:    store <4 x i64> [[TMP0]], ptr [[TMP]], align 32, !alias.scope [[META333]]
// X86-NEXT:    [[TMP1:%.*]] = load <4 x i64>, ptr [[TMP]], align 32, !alias.scope [[META333]]
// X86-NEXT:    store <4 x i64> [[TMP1]], ptr [[TMP]], align 32, !alias.scope [[META333]]
// X86-NEXT:    [[TMP2:%.*]] = load <4 x i64>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x i64> [[TMP2]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP3:%.*]] = load <4 x i64>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x i64> [[TMP3]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256i test_mm256_setzero_si256(void) {
  return _mm256_setzero_si256();
}

//
// X86-LABEL: define void @test_mm256_shuffle_pd(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[A:%.*]], <4 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[SHUFP:%.*]] = shufflevector <4 x double> [[TMP0]], <4 x double> [[TMP1]], <4 x i32> <i32 0, i32 4, i32 2, i32 6>
// X86-NEXT:    store <4 x double> [[SHUFP]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP2:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP2]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_shuffle_pd(__m256d A, __m256d B) {
  return _mm256_shuffle_pd(A, B, 0);
}

//
// X86-LABEL: define void @test_mm256_shuffle_ps(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]], <8 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[B_ADDR]], align 32
// X86-NEXT:    [[SHUFP:%.*]] = shufflevector <8 x float> [[TMP0]], <8 x float> [[TMP1]], <8 x i32> <i32 0, i32 0, i32 8, i32 8, i32 4, i32 4, i32 12, i32 12>
// X86-NEXT:    store <8 x float> [[SHUFP]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP2:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP2]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_shuffle_ps(__m256 A, __m256 B) {
  return _mm256_shuffle_ps(A, B, 0);
}

//
// X86-LABEL: define void @test_mm256_sqrt_pd(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META336:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META336]]
// X86-NEXT:    store <4 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 32, !noalias [[META336]]
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[__A_ADDR_I]], align 32, !noalias [[META336]]
// X86-NEXT:    [[TMP2:%.*]] = call <4 x double> @llvm.sqrt.v4f64(<4 x double> [[TMP1]])
// X86-NEXT:    store <4 x double> [[TMP2]], ptr [[TMP]], align 32, !alias.scope [[META336]]
// X86-NEXT:    [[TMP3:%.*]] = load <4 x double>, ptr [[TMP]], align 32, !alias.scope [[META336]]
// X86-NEXT:    store <4 x double> [[TMP3]], ptr [[TMP]], align 32, !alias.scope [[META336]]
// X86-NEXT:    [[TMP4:%.*]] = load <4 x double>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x double> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_sqrt_pd(__m256d A) {
  return _mm256_sqrt_pd(A);
}

//
// X86-LABEL: define void @test_mm256_sqrt_ps(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[TMP:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META339:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META339]]
// X86-NEXT:    store <8 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 32, !noalias [[META339]]
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[__A_ADDR_I]], align 32, !noalias [[META339]]
// X86-NEXT:    [[TMP2:%.*]] = call <8 x float> @llvm.sqrt.v8f32(<8 x float> [[TMP1]])
// X86-NEXT:    store <8 x float> [[TMP2]], ptr [[TMP]], align 32, !alias.scope [[META339]]
// X86-NEXT:    [[TMP3:%.*]] = load <8 x float>, ptr [[TMP]], align 32, !alias.scope [[META339]]
// X86-NEXT:    store <8 x float> [[TMP3]], ptr [[TMP]], align 32, !alias.scope [[META339]]
// X86-NEXT:    [[TMP4:%.*]] = load <8 x float>, ptr [[TMP]], align 32
// X86-NEXT:    store <8 x float> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_sqrt_ps(__m256 A) {
  return _mm256_sqrt_ps(A);
}

//
// X86-LABEL: define void @test_mm256_store_pd(
// X86-SAME: ptr noundef [[A:%.*]], <4 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__P_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    store <4 x double> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[B_ADDR]], align 32
// X86-NEXT:    store ptr [[TMP0]], ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    store <4 x double> [[TMP1]], ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    [[TMP2:%.*]] = load <4 x double>, ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    store <4 x double> [[TMP2]], ptr [[TMP3]], align 32
// X86-NEXT:    ret void
//
void test_mm256_store_pd(double* A, __m256d B) {
  _mm256_store_pd(A, B);
}

//
// X86-LABEL: define void @test_mm256_store_ps(
// X86-SAME: ptr noundef [[A:%.*]], <8 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__P_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    store <8 x float> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[B_ADDR]], align 32
// X86-NEXT:    store ptr [[TMP0]], ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    store <8 x float> [[TMP1]], ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    [[TMP2:%.*]] = load <8 x float>, ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    store <8 x float> [[TMP2]], ptr [[TMP3]], align 32
// X86-NEXT:    ret void
//
void test_mm256_store_ps(float* A, __m256 B) {
  _mm256_store_ps(A, B);
}

//
// X86-LABEL: define void @test_mm256_store_si256(
// X86-SAME: ptr noundef [[A:%.*]], <4 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__P_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    store <4 x i64> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load <4 x i64>, ptr [[B_ADDR]], align 32
// X86-NEXT:    store ptr [[TMP0]], ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    store <4 x i64> [[TMP1]], ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    [[TMP2:%.*]] = load <4 x i64>, ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    store <4 x i64> [[TMP2]], ptr [[TMP3]], align 32
// X86-NEXT:    ret void
//
void test_mm256_store_si256(__m256i* A, __m256i B) {
  _mm256_store_si256(A, B);
}

//
// X86-LABEL: define void @test_mm256_storeu_pd(
// X86-SAME: ptr noundef [[A:%.*]], <4 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__P_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    store <4 x double> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[B_ADDR]], align 32
// X86-NEXT:    store ptr [[TMP0]], ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    store <4 x double> [[TMP1]], ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    [[TMP2:%.*]] = load <4 x double>, ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    store <4 x double> [[TMP2]], ptr [[TMP3]], align 1
// X86-NEXT:    ret void
//
void test_mm256_storeu_pd(double* A, __m256d B) {
  _mm256_storeu_pd(A, B);
}

//
// X86-LABEL: define void @test_mm256_storeu_ps(
// X86-SAME: ptr noundef [[A:%.*]], <8 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__P_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    store <8 x float> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[B_ADDR]], align 32
// X86-NEXT:    store ptr [[TMP0]], ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    store <8 x float> [[TMP1]], ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    [[TMP2:%.*]] = load <8 x float>, ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    store <8 x float> [[TMP2]], ptr [[TMP3]], align 1
// X86-NEXT:    ret void
//
void test_mm256_storeu_ps(float* A, __m256 B) {
  // CHECk-NEXT: ret void
  _mm256_storeu_ps(A, B);
}

//
// X86-LABEL: define void @test_mm256_storeu_si256(
// X86-SAME: ptr noundef [[A:%.*]], <4 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__P_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    store <4 x i64> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load <4 x i64>, ptr [[B_ADDR]], align 32
// X86-NEXT:    store ptr [[TMP0]], ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    store <4 x i64> [[TMP1]], ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    [[TMP2:%.*]] = load <4 x i64>, ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    store <4 x i64> [[TMP2]], ptr [[TMP3]], align 1
// X86-NEXT:    ret void
//
void test_mm256_storeu_si256(__m256i* A, __m256i B) {
  // CHECk-NEXT: ret void
  _mm256_storeu_si256(A, B);
}

//
// X86-LABEL: define void @test_mm256_storeu2_m128(
// X86-SAME: ptr noundef [[A:%.*]], ptr noundef [[B:%.*]], <8 x float> noundef [[C:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__P_ADDR_I2:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I3:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[__P_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I1:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[RETVAL_I_I:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[__A_ADDR_I_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[__ADDR_HI_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__ADDR_LO_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[__V128_I:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[COERCE_I:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[B_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[C_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    store ptr [[B]], ptr [[B_ADDR]], align 4
// X86-NEXT:    store <8 x float> [[C]], ptr [[C_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[B_ADDR]], align 4
// X86-NEXT:    [[TMP2:%.*]] = load <8 x float>, ptr [[C_ADDR]], align 32
// X86-NEXT:    store ptr [[TMP0]], ptr [[__ADDR_HI_ADDR_I]], align 4
// X86-NEXT:    store ptr [[TMP1]], ptr [[__ADDR_LO_ADDR_I]], align 4
// X86-NEXT:    store <8 x float> [[TMP2]], ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    [[TMP3:%.*]] = load <8 x float>, ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    store <8 x float> [[TMP3]], ptr [[__A_ADDR_I_I]], align 32
// X86-NEXT:    [[TMP4:%.*]] = load <8 x float>, ptr [[__A_ADDR_I_I]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <8 x float>, ptr [[__A_ADDR_I_I]], align 32
// X86-NEXT:    [[SHUFFLE_I_I:%.*]] = shufflevector <8 x float> [[TMP4]], <8 x float> [[TMP5]], <4 x i32> <i32 0, i32 1, i32 2, i32 3>
// X86-NEXT:    store <4 x float> [[SHUFFLE_I_I]], ptr [[RETVAL_I_I]], align 16
// X86-NEXT:    [[TMP6:%.*]] = load <2 x i64>, ptr [[RETVAL_I_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP6]], ptr [[COERCE_I]], align 16
// X86-NEXT:    [[TMP7:%.*]] = load <4 x float>, ptr [[COERCE_I]], align 16
// X86-NEXT:    store <4 x float> [[TMP7]], ptr [[__V128_I]], align 16
// X86-NEXT:    [[TMP8:%.*]] = load ptr, ptr [[__ADDR_LO_ADDR_I]], align 4
// X86-NEXT:    [[TMP9:%.*]] = load <4 x float>, ptr [[__V128_I]], align 16
// X86-NEXT:    store ptr [[TMP8]], ptr [[__P_ADDR_I2]], align 4
// X86-NEXT:    store <4 x float> [[TMP9]], ptr [[__A_ADDR_I3]], align 16
// X86-NEXT:    [[TMP10:%.*]] = load <4 x float>, ptr [[__A_ADDR_I3]], align 16
// X86-NEXT:    [[TMP11:%.*]] = load ptr, ptr [[__P_ADDR_I2]], align 4
// X86-NEXT:    store <4 x float> [[TMP10]], ptr [[TMP11]], align 1
// X86-NEXT:    [[TMP12:%.*]] = load <8 x float>, ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    [[EXTRACT_I:%.*]] = shufflevector <8 x float> [[TMP12]], <8 x float> poison, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
// X86-NEXT:    store <4 x float> [[EXTRACT_I]], ptr [[__V128_I]], align 16
// X86-NEXT:    [[TMP13:%.*]] = load ptr, ptr [[__ADDR_HI_ADDR_I]], align 4
// X86-NEXT:    [[TMP14:%.*]] = load <4 x float>, ptr [[__V128_I]], align 16
// X86-NEXT:    store ptr [[TMP13]], ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    store <4 x float> [[TMP14]], ptr [[__A_ADDR_I1]], align 16
// X86-NEXT:    [[TMP15:%.*]] = load <4 x float>, ptr [[__A_ADDR_I1]], align 16
// X86-NEXT:    [[TMP16:%.*]] = load ptr, ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    store <4 x float> [[TMP15]], ptr [[TMP16]], align 1
// X86-NEXT:    ret void
//
void test_mm256_storeu2_m128(float* A, float* B, __m256 C) {
  _mm256_storeu2_m128(A, B, C);
}

//
// X86-LABEL: define void @test_mm256_storeu2_m128d(
// X86-SAME: ptr noundef [[A:%.*]], ptr noundef [[B:%.*]], <4 x double> noundef [[C:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__DP_ADDR_I2:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I3:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__DP_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I1:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RETVAL_I_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__A_ADDR_I_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[__ADDR_HI_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__ADDR_LO_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[__V128_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[COERCE_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[B_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[C_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    store ptr [[B]], ptr [[B_ADDR]], align 4
// X86-NEXT:    store <4 x double> [[C]], ptr [[C_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[B_ADDR]], align 4
// X86-NEXT:    [[TMP2:%.*]] = load <4 x double>, ptr [[C_ADDR]], align 32
// X86-NEXT:    store ptr [[TMP0]], ptr [[__ADDR_HI_ADDR_I]], align 4
// X86-NEXT:    store ptr [[TMP1]], ptr [[__ADDR_LO_ADDR_I]], align 4
// X86-NEXT:    store <4 x double> [[TMP2]], ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    [[TMP3:%.*]] = load <4 x double>, ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    store <4 x double> [[TMP3]], ptr [[__A_ADDR_I_I]], align 32
// X86-NEXT:    [[TMP4:%.*]] = load <4 x double>, ptr [[__A_ADDR_I_I]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <4 x double>, ptr [[__A_ADDR_I_I]], align 32
// X86-NEXT:    [[SHUFFLE_I_I:%.*]] = shufflevector <4 x double> [[TMP4]], <4 x double> [[TMP5]], <2 x i32> <i32 0, i32 1>
// X86-NEXT:    store <2 x double> [[SHUFFLE_I_I]], ptr [[RETVAL_I_I]], align 16
// X86-NEXT:    [[TMP6:%.*]] = load <2 x i64>, ptr [[RETVAL_I_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP6]], ptr [[COERCE_I]], align 16
// X86-NEXT:    [[TMP7:%.*]] = load <2 x double>, ptr [[COERCE_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP7]], ptr [[__V128_I]], align 16
// X86-NEXT:    [[TMP8:%.*]] = load ptr, ptr [[__ADDR_LO_ADDR_I]], align 4
// X86-NEXT:    [[TMP9:%.*]] = load <2 x double>, ptr [[__V128_I]], align 16
// X86-NEXT:    store ptr [[TMP8]], ptr [[__DP_ADDR_I2]], align 4
// X86-NEXT:    store <2 x double> [[TMP9]], ptr [[__A_ADDR_I3]], align 16
// X86-NEXT:    [[TMP10:%.*]] = load <2 x double>, ptr [[__A_ADDR_I3]], align 16
// X86-NEXT:    [[TMP11:%.*]] = load ptr, ptr [[__DP_ADDR_I2]], align 4
// X86-NEXT:    store <2 x double> [[TMP10]], ptr [[TMP11]], align 1
// X86-NEXT:    [[TMP12:%.*]] = load <4 x double>, ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    [[EXTRACT_I:%.*]] = shufflevector <4 x double> [[TMP12]], <4 x double> poison, <2 x i32> <i32 2, i32 3>
// X86-NEXT:    store <2 x double> [[EXTRACT_I]], ptr [[__V128_I]], align 16
// X86-NEXT:    [[TMP13:%.*]] = load ptr, ptr [[__ADDR_HI_ADDR_I]], align 4
// X86-NEXT:    [[TMP14:%.*]] = load <2 x double>, ptr [[__V128_I]], align 16
// X86-NEXT:    store ptr [[TMP13]], ptr [[__DP_ADDR_I]], align 4
// X86-NEXT:    store <2 x double> [[TMP14]], ptr [[__A_ADDR_I1]], align 16
// X86-NEXT:    [[TMP15:%.*]] = load <2 x double>, ptr [[__A_ADDR_I1]], align 16
// X86-NEXT:    [[TMP16:%.*]] = load ptr, ptr [[__DP_ADDR_I]], align 4
// X86-NEXT:    store <2 x double> [[TMP15]], ptr [[TMP16]], align 1
// X86-NEXT:    ret void
//
void test_mm256_storeu2_m128d(double* A, double* B, __m256d C) {
  _mm256_storeu2_m128d(A, B, C);
}

//
// X86-LABEL: define void @test_mm256_storeu2_m128i(
// X86-SAME: ptr noundef [[A:%.*]], ptr noundef [[B:%.*]], <4 x i64> noundef [[C:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__P_ADDR_I1:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__B_ADDR_I2:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__P_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[__A_ADDR_I_I:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    [[__ADDR_HI_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__ADDR_LO_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    [[__V128_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[B_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[C_ADDR:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    store ptr [[B]], ptr [[B_ADDR]], align 4
// X86-NEXT:    store <4 x i64> [[C]], ptr [[C_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load ptr, ptr [[B_ADDR]], align 4
// X86-NEXT:    [[TMP2:%.*]] = load <4 x i64>, ptr [[C_ADDR]], align 32
// X86-NEXT:    store ptr [[TMP0]], ptr [[__ADDR_HI_ADDR_I]], align 4
// X86-NEXT:    store ptr [[TMP1]], ptr [[__ADDR_LO_ADDR_I]], align 4
// X86-NEXT:    store <4 x i64> [[TMP2]], ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    [[TMP3:%.*]] = load <4 x i64>, ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    store <4 x i64> [[TMP3]], ptr [[__A_ADDR_I_I]], align 32
// X86-NEXT:    [[TMP4:%.*]] = load <4 x i64>, ptr [[__A_ADDR_I_I]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <4 x i64>, ptr [[__A_ADDR_I_I]], align 32
// X86-NEXT:    [[SHUFFLE_I_I:%.*]] = shufflevector <4 x i64> [[TMP4]], <4 x i64> [[TMP5]], <2 x i32> <i32 0, i32 1>
// X86-NEXT:    store <2 x i64> [[SHUFFLE_I_I]], ptr [[__V128_I]], align 16
// X86-NEXT:    [[TMP6:%.*]] = load ptr, ptr [[__ADDR_LO_ADDR_I]], align 4
// X86-NEXT:    [[TMP7:%.*]] = load <2 x i64>, ptr [[__V128_I]], align 16
// X86-NEXT:    store ptr [[TMP6]], ptr [[__P_ADDR_I1]], align 4
// X86-NEXT:    store <2 x i64> [[TMP7]], ptr [[__B_ADDR_I2]], align 16
// X86-NEXT:    [[TMP8:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I2]], align 16
// X86-NEXT:    [[TMP9:%.*]] = load ptr, ptr [[__P_ADDR_I1]], align 4
// X86-NEXT:    store <2 x i64> [[TMP8]], ptr [[TMP9]], align 1
// X86-NEXT:    [[TMP10:%.*]] = load <4 x i64>, ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    [[TMP11:%.*]] = bitcast <4 x i64> [[TMP10]] to <8 x i32>
// X86-NEXT:    [[EXTRACT_I:%.*]] = shufflevector <8 x i32> [[TMP11]], <8 x i32> poison, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
// X86-NEXT:    [[TMP12:%.*]] = bitcast <4 x i32> [[EXTRACT_I]] to <2 x i64>
// X86-NEXT:    store <2 x i64> [[TMP12]], ptr [[__V128_I]], align 16
// X86-NEXT:    [[TMP13:%.*]] = load ptr, ptr [[__ADDR_HI_ADDR_I]], align 4
// X86-NEXT:    [[TMP14:%.*]] = load <2 x i64>, ptr [[__V128_I]], align 16
// X86-NEXT:    store ptr [[TMP13]], ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    store <2 x i64> [[TMP14]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP15:%.*]] = load <2 x i64>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP16:%.*]] = load ptr, ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    store <2 x i64> [[TMP15]], ptr [[TMP16]], align 1
// X86-NEXT:    ret void
//
void test_mm256_storeu2_m128i(__m128i* A, __m128i* B, __m256i C) {
  _mm256_storeu2_m128i(A, B, C);
}

//
// X86-LABEL: define void @test_mm256_stream_pd(
// X86-SAME: ptr noundef [[A:%.*]], <4 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    store <4 x double> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[B_ADDR]], align 32
// X86-NEXT:    store ptr [[TMP0]], ptr [[__A_ADDR_I]], align 4
// X86-NEXT:    store <4 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 32
// X86-NEXT:    [[TMP2:%.*]] = load <4 x double>, ptr [[__B_ADDR_I]], align 32
// X86-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[__A_ADDR_I]], align 4
// X86-NEXT:    store <4 x double> [[TMP2]], ptr [[TMP3]], align 32, !nontemporal [[META342:![0-9]+]]
// X86-NEXT:    ret void
//
void test_mm256_stream_pd(double* A, __m256d B) {
  _mm256_stream_pd(A, B);
}

//
// X86-LABEL: define void @test_mm256_stream_pd_void(
// X86-SAME: ptr noundef [[A:%.*]], <4 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    store <4 x double> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[B_ADDR]], align 32
// X86-NEXT:    store ptr [[TMP0]], ptr [[__A_ADDR_I]], align 4
// X86-NEXT:    store <4 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 32
// X86-NEXT:    [[TMP2:%.*]] = load <4 x double>, ptr [[__B_ADDR_I]], align 32
// X86-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[__A_ADDR_I]], align 4
// X86-NEXT:    store <4 x double> [[TMP2]], ptr [[TMP3]], align 32, !nontemporal [[META342]]
// X86-NEXT:    ret void
//
void test_mm256_stream_pd_void(void *A, __m256d B) {
  _mm256_stream_pd(A, B);
}

//
// X86-LABEL: define void @test_mm256_stream_ps(
// X86-SAME: ptr noundef [[A:%.*]], <8 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__P_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    store <8 x float> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[B_ADDR]], align 32
// X86-NEXT:    store ptr [[TMP0]], ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    store <8 x float> [[TMP1]], ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    [[TMP2:%.*]] = load <8 x float>, ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    store <8 x float> [[TMP2]], ptr [[TMP3]], align 32, !nontemporal [[META342]]
// X86-NEXT:    ret void
//
void test_mm256_stream_ps(float* A, __m256 B) {
  _mm256_stream_ps(A, B);
}

//
// X86-LABEL: define void @test_mm256_stream_ps_void(
// X86-SAME: ptr noundef [[A:%.*]], <8 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__P_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    store <8 x float> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[B_ADDR]], align 32
// X86-NEXT:    store ptr [[TMP0]], ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    store <8 x float> [[TMP1]], ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    [[TMP2:%.*]] = load <8 x float>, ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[__P_ADDR_I]], align 4
// X86-NEXT:    store <8 x float> [[TMP2]], ptr [[TMP3]], align 32, !nontemporal [[META342]]
// X86-NEXT:    ret void
//
void test_mm256_stream_ps_void(void *A, __m256 B) {
  _mm256_stream_ps(A, B);
}

//
// X86-LABEL: define void @test_mm256_stream_si256(
// X86-SAME: ptr noundef [[A:%.*]], <4 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    store <4 x i64> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load <4 x i64>, ptr [[B_ADDR]], align 32
// X86-NEXT:    store ptr [[TMP0]], ptr [[__A_ADDR_I]], align 4
// X86-NEXT:    store <4 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 32
// X86-NEXT:    [[TMP2:%.*]] = load <4 x i64>, ptr [[__B_ADDR_I]], align 32
// X86-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[__A_ADDR_I]], align 4
// X86-NEXT:    store <4 x i64> [[TMP2]], ptr [[TMP3]], align 32, !nontemporal [[META342]]
// X86-NEXT:    ret void
//
void test_mm256_stream_si256(__m256i* A, __m256i B) {
  _mm256_stream_si256(A, B);
}

//
// X86-LABEL: define void @test_mm256_stream_si256_void(
// X86-SAME: ptr noundef [[A:%.*]], <4 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    [[A_ADDR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    store ptr [[A]], ptr [[A_ADDR]], align 4
// X86-NEXT:    store <4 x i64> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load ptr, ptr [[A_ADDR]], align 4
// X86-NEXT:    [[TMP1:%.*]] = load <4 x i64>, ptr [[B_ADDR]], align 32
// X86-NEXT:    store ptr [[TMP0]], ptr [[__A_ADDR_I]], align 4
// X86-NEXT:    store <4 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 32
// X86-NEXT:    [[TMP2:%.*]] = load <4 x i64>, ptr [[__B_ADDR_I]], align 32
// X86-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[__A_ADDR_I]], align 4
// X86-NEXT:    store <4 x i64> [[TMP2]], ptr [[TMP3]], align 32, !nontemporal [[META342]]
// X86-NEXT:    ret void
//
void test_mm256_stream_si256_void(void *A, __m256i B) {
  _mm256_stream_si256(A, B);
}

//
// X86-LABEL: define void @test_mm256_sub_pd(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[A:%.*]], <4 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[B_ADDR]], align 32
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META343:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META343]]
// X86-NEXT:    store <4 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 32, !noalias [[META343]]
// X86-NEXT:    store <4 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 32, !noalias [[META343]]
// X86-NEXT:    [[TMP2:%.*]] = load <4 x double>, ptr [[__A_ADDR_I]], align 32, !noalias [[META343]]
// X86-NEXT:    [[TMP3:%.*]] = load <4 x double>, ptr [[__B_ADDR_I]], align 32, !noalias [[META343]]
// X86-NEXT:    [[SUB_I:%.*]] = fsub <4 x double> [[TMP2]], [[TMP3]]
// X86-NEXT:    store <4 x double> [[SUB_I]], ptr [[TMP]], align 32, !alias.scope [[META343]]
// X86-NEXT:    [[TMP4:%.*]] = load <4 x double>, ptr [[TMP]], align 32, !alias.scope [[META343]]
// X86-NEXT:    store <4 x double> [[TMP4]], ptr [[TMP]], align 32, !alias.scope [[META343]]
// X86-NEXT:    [[TMP5:%.*]] = load <4 x double>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x double> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP6:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP6]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_sub_pd(__m256d A, __m256d B) {
  return _mm256_sub_pd(A, B);
}

//
// X86-LABEL: define void @test_mm256_sub_ps(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]], <8 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[TMP:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[B_ADDR]], align 32
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META346:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META346]]
// X86-NEXT:    store <8 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 32, !noalias [[META346]]
// X86-NEXT:    store <8 x float> [[TMP1]], ptr [[__B_ADDR_I]], align 32, !noalias [[META346]]
// X86-NEXT:    [[TMP2:%.*]] = load <8 x float>, ptr [[__A_ADDR_I]], align 32, !noalias [[META346]]
// X86-NEXT:    [[TMP3:%.*]] = load <8 x float>, ptr [[__B_ADDR_I]], align 32, !noalias [[META346]]
// X86-NEXT:    [[SUB_I:%.*]] = fsub <8 x float> [[TMP2]], [[TMP3]]
// X86-NEXT:    store <8 x float> [[SUB_I]], ptr [[TMP]], align 32, !alias.scope [[META346]]
// X86-NEXT:    [[TMP4:%.*]] = load <8 x float>, ptr [[TMP]], align 32, !alias.scope [[META346]]
// X86-NEXT:    store <8 x float> [[TMP4]], ptr [[TMP]], align 32, !alias.scope [[META346]]
// X86-NEXT:    [[TMP5:%.*]] = load <8 x float>, ptr [[TMP]], align 32
// X86-NEXT:    store <8 x float> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP6:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP6]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_sub_ps(__m256 A, __m256 B) {
  return _mm256_sub_ps(A, B);
}

//
// X86-LABEL: define i32 @test_mm_testc_pd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = call i32 @llvm.x86.avx.vtestc.pd(<2 x double> [[TMP2]], <2 x double> [[TMP3]])
// X86-NEXT:    ret i32 [[TMP4]]
//
int test_mm_testc_pd(__m128d A, __m128d B) {
  return _mm_testc_pd(A, B);
}

//
// X86-LABEL: define i32 @test_mm256_testc_pd(
// X86-SAME: <4 x double> noundef [[A:%.*]], <4 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[B_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    store <4 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 32
// X86-NEXT:    [[TMP2:%.*]] = load <4 x double>, ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    [[TMP3:%.*]] = load <4 x double>, ptr [[__B_ADDR_I]], align 32
// X86-NEXT:    [[TMP4:%.*]] = call i32 @llvm.x86.avx.vtestc.pd.256(<4 x double> [[TMP2]], <4 x double> [[TMP3]])
// X86-NEXT:    ret i32 [[TMP4]]
//
int test_mm256_testc_pd(__m256d A, __m256d B) {
  return _mm256_testc_pd(A, B);
}

//
// X86-LABEL: define i32 @test_mm_testc_ps(
// X86-SAME: <4 x float> noundef [[A:%.*]], <4 x float> noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <4 x float> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <4 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <4 x float> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <4 x float>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <4 x float>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = call i32 @llvm.x86.avx.vtestc.ps(<4 x float> [[TMP2]], <4 x float> [[TMP3]])
// X86-NEXT:    ret i32 [[TMP4]]
//
int test_mm_testc_ps(__m128 A, __m128 B) {
  return _mm_testc_ps(A, B);
}

//
// X86-LABEL: define i32 @test_mm256_testc_ps(
// X86-SAME: <8 x float> noundef [[A:%.*]], <8 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[B_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    store <8 x float> [[TMP1]], ptr [[__B_ADDR_I]], align 32
// X86-NEXT:    [[TMP2:%.*]] = load <8 x float>, ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    [[TMP3:%.*]] = load <8 x float>, ptr [[__B_ADDR_I]], align 32
// X86-NEXT:    [[TMP4:%.*]] = call i32 @llvm.x86.avx.vtestc.ps.256(<8 x float> [[TMP2]], <8 x float> [[TMP3]])
// X86-NEXT:    ret i32 [[TMP4]]
//
int test_mm256_testc_ps(__m256 A, __m256 B) {
  return _mm256_testc_ps(A, B);
}

//
// X86-LABEL: define i32 @test_mm256_testc_si256(
// X86-SAME: <4 x i64> noundef [[A:%.*]], <4 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    store <4 x i64> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x i64> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x i64>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x i64>, ptr [[B_ADDR]], align 32
// X86-NEXT:    store <4 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    store <4 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 32
// X86-NEXT:    [[TMP2:%.*]] = load <4 x i64>, ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    [[TMP3:%.*]] = load <4 x i64>, ptr [[__B_ADDR_I]], align 32
// X86-NEXT:    [[TMP4:%.*]] = call i32 @llvm.x86.avx.ptestc.256(<4 x i64> [[TMP2]], <4 x i64> [[TMP3]])
// X86-NEXT:    ret i32 [[TMP4]]
//
int test_mm256_testc_si256(__m256i A, __m256i B) {
  return _mm256_testc_si256(A, B);
}

//
// X86-LABEL: define i32 @test_mm_testnzc_pd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = call i32 @llvm.x86.avx.vtestnzc.pd(<2 x double> [[TMP2]], <2 x double> [[TMP3]])
// X86-NEXT:    ret i32 [[TMP4]]
//
int test_mm_testnzc_pd(__m128d A, __m128d B) {
  return _mm_testnzc_pd(A, B);
}

//
// X86-LABEL: define i32 @test_mm256_testnzc_pd(
// X86-SAME: <4 x double> noundef [[A:%.*]], <4 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[B_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    store <4 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 32
// X86-NEXT:    [[TMP2:%.*]] = load <4 x double>, ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    [[TMP3:%.*]] = load <4 x double>, ptr [[__B_ADDR_I]], align 32
// X86-NEXT:    [[TMP4:%.*]] = call i32 @llvm.x86.avx.vtestnzc.pd.256(<4 x double> [[TMP2]], <4 x double> [[TMP3]])
// X86-NEXT:    ret i32 [[TMP4]]
//
int test_mm256_testnzc_pd(__m256d A, __m256d B) {
  return _mm256_testnzc_pd(A, B);
}

//
// X86-LABEL: define i32 @test_mm_testnzc_ps(
// X86-SAME: <4 x float> noundef [[A:%.*]], <4 x float> noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <4 x float> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <4 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <4 x float> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <4 x float>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <4 x float>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = call i32 @llvm.x86.avx.vtestnzc.ps(<4 x float> [[TMP2]], <4 x float> [[TMP3]])
// X86-NEXT:    ret i32 [[TMP4]]
//
int test_mm_testnzc_ps(__m128 A, __m128 B) {
  return _mm_testnzc_ps(A, B);
}

//
// X86-LABEL: define i32 @test_mm256_testnzc_ps(
// X86-SAME: <8 x float> noundef [[A:%.*]], <8 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[B_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    store <8 x float> [[TMP1]], ptr [[__B_ADDR_I]], align 32
// X86-NEXT:    [[TMP2:%.*]] = load <8 x float>, ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    [[TMP3:%.*]] = load <8 x float>, ptr [[__B_ADDR_I]], align 32
// X86-NEXT:    [[TMP4:%.*]] = call i32 @llvm.x86.avx.vtestnzc.ps.256(<8 x float> [[TMP2]], <8 x float> [[TMP3]])
// X86-NEXT:    ret i32 [[TMP4]]
//
int test_mm256_testnzc_ps(__m256 A, __m256 B) {
  return _mm256_testnzc_ps(A, B);
}

//
// X86-LABEL: define i32 @test_mm256_testnzc_si256(
// X86-SAME: <4 x i64> noundef [[A:%.*]], <4 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    store <4 x i64> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x i64> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x i64>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x i64>, ptr [[B_ADDR]], align 32
// X86-NEXT:    store <4 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    store <4 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 32
// X86-NEXT:    [[TMP2:%.*]] = load <4 x i64>, ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    [[TMP3:%.*]] = load <4 x i64>, ptr [[__B_ADDR_I]], align 32
// X86-NEXT:    [[TMP4:%.*]] = call i32 @llvm.x86.avx.ptestnzc.256(<4 x i64> [[TMP2]], <4 x i64> [[TMP3]])
// X86-NEXT:    ret i32 [[TMP4]]
//
int test_mm256_testnzc_si256(__m256i A, __m256i B) {
  return _mm256_testnzc_si256(A, B);
}

//
// X86-LABEL: define i32 @test_mm_testz_pd(
// X86-SAME: <2 x double> noundef [[A:%.*]], <2 x double> noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x double>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = call i32 @llvm.x86.avx.vtestz.pd(<2 x double> [[TMP2]], <2 x double> [[TMP3]])
// X86-NEXT:    ret i32 [[TMP4]]
//
int test_mm_testz_pd(__m128d A, __m128d B) {
  return _mm_testz_pd(A, B);
}

//
// X86-LABEL: define i32 @test_mm256_testz_pd(
// X86-SAME: <4 x double> noundef [[A:%.*]], <4 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[B_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    store <4 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 32
// X86-NEXT:    [[TMP2:%.*]] = load <4 x double>, ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    [[TMP3:%.*]] = load <4 x double>, ptr [[__B_ADDR_I]], align 32
// X86-NEXT:    [[TMP4:%.*]] = call i32 @llvm.x86.avx.vtestz.pd.256(<4 x double> [[TMP2]], <4 x double> [[TMP3]])
// X86-NEXT:    ret i32 [[TMP4]]
//
int test_mm256_testz_pd(__m256d A, __m256d B) {
  return _mm256_testz_pd(A, B);
}

//
// X86-LABEL: define i32 @test_mm_testz_ps(
// X86-SAME: <4 x float> noundef [[A:%.*]], <4 x float> noundef [[B:%.*]]) #[[ATTR1]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    store <4 x float> [[B]], ptr [[B_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[B_ADDR]], align 16
// X86-NEXT:    store <4 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    store <4 x float> [[TMP1]], ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <4 x float>, ptr [[__A_ADDR_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <4 x float>, ptr [[__B_ADDR_I]], align 16
// X86-NEXT:    [[TMP4:%.*]] = call i32 @llvm.x86.avx.vtestz.ps(<4 x float> [[TMP2]], <4 x float> [[TMP3]])
// X86-NEXT:    ret i32 [[TMP4]]
//
int test_mm_testz_ps(__m128 A, __m128 B) {
  return _mm_testz_ps(A, B);
}

//
// X86-LABEL: define i32 @test_mm256_testz_ps(
// X86-SAME: <8 x float> noundef [[A:%.*]], <8 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[B_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    store <8 x float> [[TMP1]], ptr [[__B_ADDR_I]], align 32
// X86-NEXT:    [[TMP2:%.*]] = load <8 x float>, ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    [[TMP3:%.*]] = load <8 x float>, ptr [[__B_ADDR_I]], align 32
// X86-NEXT:    [[TMP4:%.*]] = call i32 @llvm.x86.avx.vtestz.ps.256(<8 x float> [[TMP2]], <8 x float> [[TMP3]])
// X86-NEXT:    ret i32 [[TMP4]]
//
int test_mm256_testz_ps(__m256 A, __m256 B) {
  return _mm256_testz_ps(A, B);
}

//
// X86-LABEL: define i32 @test_mm256_testz_si256(
// X86-SAME: <4 x i64> noundef [[A:%.*]], <4 x i64> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    store <4 x i64> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x i64> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x i64>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x i64>, ptr [[B_ADDR]], align 32
// X86-NEXT:    store <4 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    store <4 x i64> [[TMP1]], ptr [[__B_ADDR_I]], align 32
// X86-NEXT:    [[TMP2:%.*]] = load <4 x i64>, ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    [[TMP3:%.*]] = load <4 x i64>, ptr [[__B_ADDR_I]], align 32
// X86-NEXT:    [[TMP4:%.*]] = call i32 @llvm.x86.avx.ptestz.256(<4 x i64> [[TMP2]], <4 x i64> [[TMP3]])
// X86-NEXT:    ret i32 [[TMP4]]
//
int test_mm256_testz_si256(__m256i A, __m256i B) {
  return _mm256_testz_si256(A, B);
}

//
// X86-LABEL: define void @test_mm256_undefined_ps(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[TMP:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META349:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META349]]
// X86-NEXT:    [[TMP0:%.*]] = freeze <4 x double> undef
// X86-NEXT:    [[TMP1:%.*]] = bitcast <4 x double> [[TMP0]] to <8 x float>
// X86-NEXT:    store <8 x float> [[TMP1]], ptr [[TMP]], align 32, !alias.scope [[META349]]
// X86-NEXT:    [[TMP2:%.*]] = load <8 x float>, ptr [[TMP]], align 32, !alias.scope [[META349]]
// X86-NEXT:    store <8 x float> [[TMP2]], ptr [[TMP]], align 32, !alias.scope [[META349]]
// X86-NEXT:    [[TMP3:%.*]] = load <8 x float>, ptr [[TMP]], align 32
// X86-NEXT:    store <8 x float> [[TMP3]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP4:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_undefined_ps(void) {
  //
  return _mm256_undefined_ps();
}

//
// X86-LABEL: define void @test_mm256_undefined_pd(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META352:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META352]]
// X86-NEXT:    [[TMP0:%.*]] = freeze <4 x double> undef
// X86-NEXT:    store <4 x double> [[TMP0]], ptr [[TMP]], align 32, !alias.scope [[META352]]
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[TMP]], align 32, !alias.scope [[META352]]
// X86-NEXT:    store <4 x double> [[TMP1]], ptr [[TMP]], align 32, !alias.scope [[META352]]
// X86-NEXT:    [[TMP2:%.*]] = load <4 x double>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x double> [[TMP2]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP3:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP3]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_undefined_pd(void) {
  //
  return _mm256_undefined_pd();
}

//
// X86-LABEL: define void @test_mm256_undefined_si256(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x i64>) align 32 [[AGG_RESULT:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META355:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META355]]
// X86-NEXT:    [[TMP0:%.*]] = freeze <4 x double> undef
// X86-NEXT:    [[TMP1:%.*]] = bitcast <4 x double> [[TMP0]] to <4 x i64>
// X86-NEXT:    store <4 x i64> [[TMP1]], ptr [[TMP]], align 32, !alias.scope [[META355]]
// X86-NEXT:    [[TMP2:%.*]] = load <4 x i64>, ptr [[TMP]], align 32, !alias.scope [[META355]]
// X86-NEXT:    store <4 x i64> [[TMP2]], ptr [[TMP]], align 32, !alias.scope [[META355]]
// X86-NEXT:    [[TMP3:%.*]] = load <4 x i64>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x i64> [[TMP3]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP4:%.*]] = load <4 x i64>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x i64> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256i test_mm256_undefined_si256(void) {
  //
  return _mm256_undefined_si256();
}

//
// X86-LABEL: define void @test_mm256_unpackhi_pd(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[A:%.*]], <4 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[B_ADDR]], align 32
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META358:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META358]]
// X86-NEXT:    store <4 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 32, !noalias [[META358]]
// X86-NEXT:    store <4 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 32, !noalias [[META358]]
// X86-NEXT:    [[TMP2:%.*]] = load <4 x double>, ptr [[__A_ADDR_I]], align 32, !noalias [[META358]]
// X86-NEXT:    [[TMP3:%.*]] = load <4 x double>, ptr [[__B_ADDR_I]], align 32, !noalias [[META358]]
// X86-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <4 x double> [[TMP2]], <4 x double> [[TMP3]], <4 x i32> <i32 1, i32 5, i32 3, i32 7>
// X86-NEXT:    store <4 x double> [[SHUFFLE_I]], ptr [[TMP]], align 32, !alias.scope [[META358]]
// X86-NEXT:    [[TMP4:%.*]] = load <4 x double>, ptr [[TMP]], align 32, !alias.scope [[META358]]
// X86-NEXT:    store <4 x double> [[TMP4]], ptr [[TMP]], align 32, !alias.scope [[META358]]
// X86-NEXT:    [[TMP5:%.*]] = load <4 x double>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x double> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP6:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP6]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_unpackhi_pd(__m256d A, __m256d B) {
  return _mm256_unpackhi_pd(A, B);
}

//
// X86-LABEL: define void @test_mm256_unpackhi_ps(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]], <8 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[TMP:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[B_ADDR]], align 32
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META361:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META361]]
// X86-NEXT:    store <8 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 32, !noalias [[META361]]
// X86-NEXT:    store <8 x float> [[TMP1]], ptr [[__B_ADDR_I]], align 32, !noalias [[META361]]
// X86-NEXT:    [[TMP2:%.*]] = load <8 x float>, ptr [[__A_ADDR_I]], align 32, !noalias [[META361]]
// X86-NEXT:    [[TMP3:%.*]] = load <8 x float>, ptr [[__B_ADDR_I]], align 32, !noalias [[META361]]
// X86-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <8 x float> [[TMP2]], <8 x float> [[TMP3]], <8 x i32> <i32 2, i32 10, i32 3, i32 11, i32 6, i32 14, i32 7, i32 15>
// X86-NEXT:    store <8 x float> [[SHUFFLE_I]], ptr [[TMP]], align 32, !alias.scope [[META361]]
// X86-NEXT:    [[TMP4:%.*]] = load <8 x float>, ptr [[TMP]], align 32, !alias.scope [[META361]]
// X86-NEXT:    store <8 x float> [[TMP4]], ptr [[TMP]], align 32, !alias.scope [[META361]]
// X86-NEXT:    [[TMP5:%.*]] = load <8 x float>, ptr [[TMP]], align 32
// X86-NEXT:    store <8 x float> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP6:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP6]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_unpackhi_ps(__m256 A, __m256 B) {
  return _mm256_unpackhi_ps(A, B);
}

//
// X86-LABEL: define void @test_mm256_unpacklo_pd(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[A:%.*]], <4 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[B_ADDR]], align 32
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META364:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META364]]
// X86-NEXT:    store <4 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 32, !noalias [[META364]]
// X86-NEXT:    store <4 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 32, !noalias [[META364]]
// X86-NEXT:    [[TMP2:%.*]] = load <4 x double>, ptr [[__A_ADDR_I]], align 32, !noalias [[META364]]
// X86-NEXT:    [[TMP3:%.*]] = load <4 x double>, ptr [[__B_ADDR_I]], align 32, !noalias [[META364]]
// X86-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <4 x double> [[TMP2]], <4 x double> [[TMP3]], <4 x i32> <i32 0, i32 4, i32 2, i32 6>
// X86-NEXT:    store <4 x double> [[SHUFFLE_I]], ptr [[TMP]], align 32, !alias.scope [[META364]]
// X86-NEXT:    [[TMP4:%.*]] = load <4 x double>, ptr [[TMP]], align 32, !alias.scope [[META364]]
// X86-NEXT:    store <4 x double> [[TMP4]], ptr [[TMP]], align 32, !alias.scope [[META364]]
// X86-NEXT:    [[TMP5:%.*]] = load <4 x double>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x double> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP6:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP6]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_unpacklo_pd(__m256d A, __m256d B) {
  return _mm256_unpacklo_pd(A, B);
}

//
// X86-LABEL: define void @test_mm256_unpacklo_ps(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]], <8 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[TMP:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[B_ADDR]], align 32
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META367:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META367]]
// X86-NEXT:    store <8 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 32, !noalias [[META367]]
// X86-NEXT:    store <8 x float> [[TMP1]], ptr [[__B_ADDR_I]], align 32, !noalias [[META367]]
// X86-NEXT:    [[TMP2:%.*]] = load <8 x float>, ptr [[__A_ADDR_I]], align 32, !noalias [[META367]]
// X86-NEXT:    [[TMP3:%.*]] = load <8 x float>, ptr [[__B_ADDR_I]], align 32, !noalias [[META367]]
// X86-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <8 x float> [[TMP2]], <8 x float> [[TMP3]], <8 x i32> <i32 0, i32 8, i32 1, i32 9, i32 4, i32 12, i32 5, i32 13>
// X86-NEXT:    store <8 x float> [[SHUFFLE_I]], ptr [[TMP]], align 32, !alias.scope [[META367]]
// X86-NEXT:    [[TMP4:%.*]] = load <8 x float>, ptr [[TMP]], align 32, !alias.scope [[META367]]
// X86-NEXT:    store <8 x float> [[TMP4]], ptr [[TMP]], align 32, !alias.scope [[META367]]
// X86-NEXT:    [[TMP5:%.*]] = load <8 x float>, ptr [[TMP]], align 32
// X86-NEXT:    store <8 x float> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP6:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP6]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_unpacklo_ps(__m256 A, __m256 B) {
  return _mm256_unpacklo_ps(A, B);
}

//
// X86-LABEL: define void @test_mm256_xor_pd(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <4 x double> noundef [[A:%.*]], <4 x double> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x double> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[B_ADDR]], align 32
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META370:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META370]]
// X86-NEXT:    store <4 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 32, !noalias [[META370]]
// X86-NEXT:    store <4 x double> [[TMP1]], ptr [[__B_ADDR_I]], align 32, !noalias [[META370]]
// X86-NEXT:    [[TMP2:%.*]] = load <4 x double>, ptr [[__A_ADDR_I]], align 32, !noalias [[META370]]
// X86-NEXT:    [[TMP3:%.*]] = bitcast <4 x double> [[TMP2]] to <4 x i64>
// X86-NEXT:    [[TMP4:%.*]] = load <4 x double>, ptr [[__B_ADDR_I]], align 32, !noalias [[META370]]
// X86-NEXT:    [[TMP5:%.*]] = bitcast <4 x double> [[TMP4]] to <4 x i64>
// X86-NEXT:    [[XOR_I:%.*]] = xor <4 x i64> [[TMP3]], [[TMP5]]
// X86-NEXT:    [[TMP6:%.*]] = bitcast <4 x i64> [[XOR_I]] to <4 x double>
// X86-NEXT:    store <4 x double> [[TMP6]], ptr [[TMP]], align 32, !alias.scope [[META370]]
// X86-NEXT:    [[TMP7:%.*]] = load <4 x double>, ptr [[TMP]], align 32, !alias.scope [[META370]]
// X86-NEXT:    store <4 x double> [[TMP7]], ptr [[TMP]], align 32, !alias.scope [[META370]]
// X86-NEXT:    [[TMP8:%.*]] = load <4 x double>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x double> [[TMP8]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP9:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP9]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_xor_pd(__m256d A, __m256d B) {
  return _mm256_xor_pd(A, B);
}

//
// X86-LABEL: define void @test_mm256_xor_ps(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <8 x float> noundef [[A:%.*]], <8 x float> noundef [[B:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[__B_ADDR_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[B_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[TMP:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <8 x float> [[A]], ptr [[A_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[B]], ptr [[B_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[A_ADDR]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[B_ADDR]], align 32
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META373:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META373]]
// X86-NEXT:    store <8 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 32, !noalias [[META373]]
// X86-NEXT:    store <8 x float> [[TMP1]], ptr [[__B_ADDR_I]], align 32, !noalias [[META373]]
// X86-NEXT:    [[TMP2:%.*]] = load <8 x float>, ptr [[__A_ADDR_I]], align 32, !noalias [[META373]]
// X86-NEXT:    [[TMP3:%.*]] = bitcast <8 x float> [[TMP2]] to <8 x i32>
// X86-NEXT:    [[TMP4:%.*]] = load <8 x float>, ptr [[__B_ADDR_I]], align 32, !noalias [[META373]]
// X86-NEXT:    [[TMP5:%.*]] = bitcast <8 x float> [[TMP4]] to <8 x i32>
// X86-NEXT:    [[XOR_I:%.*]] = xor <8 x i32> [[TMP3]], [[TMP5]]
// X86-NEXT:    [[TMP6:%.*]] = bitcast <8 x i32> [[XOR_I]] to <8 x float>
// X86-NEXT:    store <8 x float> [[TMP6]], ptr [[TMP]], align 32, !alias.scope [[META373]]
// X86-NEXT:    [[TMP7:%.*]] = load <8 x float>, ptr [[TMP]], align 32, !alias.scope [[META373]]
// X86-NEXT:    store <8 x float> [[TMP7]], ptr [[TMP]], align 32, !alias.scope [[META373]]
// X86-NEXT:    [[TMP8:%.*]] = load <8 x float>, ptr [[TMP]], align 32
// X86-NEXT:    store <8 x float> [[TMP8]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP9:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP9]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_xor_ps(__m256 A, __m256 B) {
  return _mm256_xor_ps(A, B);
}

//
// X86-LABEL: define void @test_mm256_zeroall(
// X86-SAME: ) #[[ATTR3:[0-9]+]] {
// X86-NEXT:  entry:
// X86-NEXT:    call void @llvm.x86.avx.vzeroall()
// X86-NEXT:    ret void
//
void test_mm256_zeroall(void) {
  return _mm256_zeroall();
}

//
// X86-LABEL: define void @test_mm256_zeroupper(
// X86-SAME: ) #[[ATTR3]] {
// X86-NEXT:  entry:
// X86-NEXT:    call void @llvm.x86.avx.vzeroupper()
// X86-NEXT:    ret void
//
void test_mm256_zeroupper(void) {
  return _mm256_zeroupper();
}

//
// X86-LABEL: define void @test_mm256_zextpd128_pd256(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x double>) align 32 [[AGG_RESULT:%.*]], <2 x double> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[COERCE_I:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x double>, align 16
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <2 x double> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x double>, ptr [[A_ADDR]], align 16
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META376:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META376]]
// X86-NEXT:    store <2 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 16, !noalias [[META376]]
// X86-NEXT:    [[TMP1:%.*]] = load <2 x double>, ptr [[__A_ADDR_I]], align 16, !noalias [[META376]]
// X86-NEXT:    store <2 x double> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    store <2 x double> [[TMP2]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP3]], ptr [[COERCE_I]], align 16, !noalias [[META376]]
// X86-NEXT:    [[TMP4:%.*]] = load <2 x double>, ptr [[COERCE_I]], align 16, !noalias [[META376]]
// X86-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <2 x double> [[TMP1]], <2 x double> [[TMP4]], <4 x i32> <i32 0, i32 1, i32 2, i32 3>
// X86-NEXT:    store <4 x double> [[SHUFFLE_I]], ptr [[TMP]], align 32, !alias.scope [[META376]]
// X86-NEXT:    [[TMP5:%.*]] = load <4 x double>, ptr [[TMP]], align 32, !alias.scope [[META376]]
// X86-NEXT:    store <4 x double> [[TMP5]], ptr [[TMP]], align 32, !alias.scope [[META376]]
// X86-NEXT:    [[TMP6:%.*]] = load <4 x double>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x double> [[TMP6]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP7:%.*]] = load <4 x double>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x double> [[TMP7]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256d test_mm256_zextpd128_pd256(__m128d A) {
  return _mm256_zextpd128_pd256(A);
}

//
// X86-LABEL: define void @test_mm256_zextps128_ps256(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<8 x float>) align 32 [[AGG_RESULT:%.*]], <4 x float> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[RETVAL_I:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[COERCE_I:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <4 x float>, align 16
// X86-NEXT:    [[TMP:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <4 x float> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A_ADDR]], align 16
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META379:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META379]]
// X86-NEXT:    store <4 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 16, !noalias [[META379]]
// X86-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr [[__A_ADDR_I]], align 16, !noalias [[META379]]
// X86-NEXT:    store <4 x float> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <4 x float>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    store <4 x float> [[TMP2]], ptr [[RETVAL_I]], align 16
// X86-NEXT:    [[TMP3:%.*]] = load <2 x i64>, ptr [[RETVAL_I]], align 16
// X86-NEXT:    store <2 x i64> [[TMP3]], ptr [[COERCE_I]], align 16, !noalias [[META379]]
// X86-NEXT:    [[TMP4:%.*]] = load <4 x float>, ptr [[COERCE_I]], align 16, !noalias [[META379]]
// X86-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <4 x float> [[TMP1]], <4 x float> [[TMP4]], <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
// X86-NEXT:    store <8 x float> [[SHUFFLE_I]], ptr [[TMP]], align 32, !alias.scope [[META379]]
// X86-NEXT:    [[TMP5:%.*]] = load <8 x float>, ptr [[TMP]], align 32, !alias.scope [[META379]]
// X86-NEXT:    store <8 x float> [[TMP5]], ptr [[TMP]], align 32, !alias.scope [[META379]]
// X86-NEXT:    [[TMP6:%.*]] = load <8 x float>, ptr [[TMP]], align 32
// X86-NEXT:    store <8 x float> [[TMP6]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP7:%.*]] = load <8 x float>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <8 x float> [[TMP7]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256 test_mm256_zextps128_ps256(__m128 A) {
  return _mm256_zextps128_ps256(A);
}

//
// X86-LABEL: define void @test_mm256_zextsi128_si256(
// X86-SAME: ptr dead_on_unwind noalias writable sret(<4 x i64>) align 32 [[AGG_RESULT:%.*]], <2 x i64> noundef [[A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[DOTCOMPOUNDLITERAL_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[RESULT_PTR_I:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[RESULT_PTR:%.*]] = alloca ptr, align 4
// X86-NEXT:    [[A_ADDR:%.*]] = alloca <2 x i64>, align 16
// X86-NEXT:    [[TMP:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    store ptr [[AGG_RESULT]], ptr [[RESULT_PTR]], align 4
// X86-NEXT:    store <2 x i64> [[A]], ptr [[A_ADDR]], align 16
// X86-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[A_ADDR]], align 16
// X86-NEXT:    call void @llvm.experimental.noalias.scope.decl(metadata [[META382:![0-9]+]])
// X86-NEXT:    store ptr [[TMP]], ptr [[RESULT_PTR_I]], align 4, !noalias [[META382]]
// X86-NEXT:    store <2 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 16, !noalias [[META382]]
// X86-NEXT:    [[TMP1:%.*]] = load <2 x i64>, ptr [[__A_ADDR_I]], align 16, !noalias [[META382]]
// X86-NEXT:    store <2 x i64> zeroinitializer, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    [[TMP2:%.*]] = load <2 x i64>, ptr [[DOTCOMPOUNDLITERAL_I]], align 16
// X86-NEXT:    [[SHUFFLE_I:%.*]] = shufflevector <2 x i64> [[TMP1]], <2 x i64> [[TMP2]], <4 x i32> <i32 0, i32 1, i32 2, i32 3>
// X86-NEXT:    store <4 x i64> [[SHUFFLE_I]], ptr [[TMP]], align 32, !alias.scope [[META382]]
// X86-NEXT:    [[TMP3:%.*]] = load <4 x i64>, ptr [[TMP]], align 32, !alias.scope [[META382]]
// X86-NEXT:    store <4 x i64> [[TMP3]], ptr [[TMP]], align 32, !alias.scope [[META382]]
// X86-NEXT:    [[TMP4:%.*]] = load <4 x i64>, ptr [[TMP]], align 32
// X86-NEXT:    store <4 x i64> [[TMP4]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    [[TMP5:%.*]] = load <4 x i64>, ptr [[AGG_RESULT]], align 32
// X86-NEXT:    store <4 x i64> [[TMP5]], ptr [[AGG_RESULT]], align 32
// X86-NEXT:    ret void
//
__m256i test_mm256_zextsi128_si256(__m128i A) {
  return _mm256_zextsi128_si256(A);
}

//
// X86-LABEL: define double @test_mm256_cvtsd_f64(
// X86-SAME: <4 x double> noundef [[__A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    [[__A_ADDR:%.*]] = alloca <4 x double>, align 32
// X86-NEXT:    store <4 x double> [[__A]], ptr [[__A_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x double>, ptr [[__A_ADDR]], align 32
// X86-NEXT:    store <4 x double> [[TMP0]], ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x double>, ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    [[VECEXT_I:%.*]] = extractelement <4 x double> [[TMP1]], i32 0
// X86-NEXT:    ret double [[VECEXT_I]]
//
double test_mm256_cvtsd_f64(__m256d __a)
{
  return _mm256_cvtsd_f64(__a);
}

//
// X86-LABEL: define i32 @test_mm256_cvtsi256_si32(
// X86-SAME: <4 x i64> noundef [[__A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    [[__B_I:%.*]] = alloca <8 x i32>, align 32
// X86-NEXT:    [[__A_ADDR:%.*]] = alloca <4 x i64>, align 32
// X86-NEXT:    store <4 x i64> [[__A]], ptr [[__A_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <4 x i64>, ptr [[__A_ADDR]], align 32
// X86-NEXT:    store <4 x i64> [[TMP0]], ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <4 x i64>, ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    [[TMP2:%.*]] = bitcast <4 x i64> [[TMP1]] to <8 x i32>
// X86-NEXT:    store <8 x i32> [[TMP2]], ptr [[__B_I]], align 32
// X86-NEXT:    [[TMP3:%.*]] = load <8 x i32>, ptr [[__B_I]], align 32
// X86-NEXT:    [[VECEXT_I:%.*]] = extractelement <8 x i32> [[TMP3]], i32 0
// X86-NEXT:    ret i32 [[VECEXT_I]]
//
int test_mm256_cvtsi256_si32(__m256i __a)
{
  return _mm256_cvtsi256_si32(__a);
}

//
// X86-LABEL: define float @test_mm256_cvtss_f32(
// X86-SAME: <8 x float> noundef [[__A:%.*]]) #[[ATTR0]] {
// X86-NEXT:  entry:
// X86-NEXT:    [[__A_ADDR_I:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    [[__A_ADDR:%.*]] = alloca <8 x float>, align 32
// X86-NEXT:    store <8 x float> [[__A]], ptr [[__A_ADDR]], align 32
// X86-NEXT:    [[TMP0:%.*]] = load <8 x float>, ptr [[__A_ADDR]], align 32
// X86-NEXT:    store <8 x float> [[TMP0]], ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    [[TMP1:%.*]] = load <8 x float>, ptr [[__A_ADDR_I]], align 32
// X86-NEXT:    [[VECEXT_I:%.*]] = extractelement <8 x float> [[TMP1]], i32 0
// X86-NEXT:    ret float [[VECEXT_I]]
//
float test_mm256_cvtss_f32(__m256 __a)
{
  return _mm256_cvtss_f32(__a);
}
//.
// X86: [[META3]] = !{[[META4:![0-9]+]]}
// X86: [[META4]] = distinct !{[[META4]], [[META5:![0-9]+]], !"_mm256_add_pd: %agg.result"}
// X86: [[META5]] = distinct !{[[META5]], !"_mm256_add_pd"}
// X86: [[META6]] = !{[[META7:![0-9]+]]}
// X86: [[META7]] = distinct !{[[META7]], [[META8:![0-9]+]], !"_mm256_add_ps: %agg.result"}
// X86: [[META8]] = distinct !{[[META8]], !"_mm256_add_ps"}
// X86: [[META9]] = !{[[META10:![0-9]+]]}
// X86: [[META10]] = distinct !{[[META10]], [[META11:![0-9]+]], !"_mm256_addsub_pd: %agg.result"}
// X86: [[META11]] = distinct !{[[META11]], !"_mm256_addsub_pd"}
// X86: [[META12]] = !{[[META13:![0-9]+]]}
// X86: [[META13]] = distinct !{[[META13]], [[META14:![0-9]+]], !"_mm256_addsub_ps: %agg.result"}
// X86: [[META14]] = distinct !{[[META14]], !"_mm256_addsub_ps"}
// X86: [[META15]] = !{[[META16:![0-9]+]]}
// X86: [[META16]] = distinct !{[[META16]], [[META17:![0-9]+]], !"_mm256_and_pd: %agg.result"}
// X86: [[META17]] = distinct !{[[META17]], !"_mm256_and_pd"}
// X86: [[META18]] = !{[[META19:![0-9]+]]}
// X86: [[META19]] = distinct !{[[META19]], [[META20:![0-9]+]], !"_mm256_and_ps: %agg.result"}
// X86: [[META20]] = distinct !{[[META20]], !"_mm256_and_ps"}
// X86: [[META21]] = !{[[META22:![0-9]+]]}
// X86: [[META22]] = distinct !{[[META22]], [[META23:![0-9]+]], !"_mm256_andnot_pd: %agg.result"}
// X86: [[META23]] = distinct !{[[META23]], !"_mm256_andnot_pd"}
// X86: [[META24]] = !{[[META25:![0-9]+]]}
// X86: [[META25]] = distinct !{[[META25]], [[META26:![0-9]+]], !"_mm256_andnot_ps: %agg.result"}
// X86: [[META26]] = distinct !{[[META26]], !"_mm256_andnot_ps"}
// X86: [[META27]] = !{[[META28:![0-9]+]]}
// X86: [[META28]] = distinct !{[[META28]], [[META29:![0-9]+]], !"_mm256_blendv_pd: %agg.result"}
// X86: [[META29]] = distinct !{[[META29]], !"_mm256_blendv_pd"}
// X86: [[META30]] = !{[[META31:![0-9]+]]}
// X86: [[META31]] = distinct !{[[META31]], [[META32:![0-9]+]], !"_mm256_blendv_ps: %agg.result"}
// X86: [[META32]] = distinct !{[[META32]], !"_mm256_blendv_ps"}
// X86: [[META33]] = !{[[META34:![0-9]+]]}
// X86: [[META34]] = distinct !{[[META34]], [[META35:![0-9]+]], !"_mm256_broadcast_pd: %agg.result"}
// X86: [[META35]] = distinct !{[[META35]], !"_mm256_broadcast_pd"}
// X86: [[META36]] = !{[[META37:![0-9]+]]}
// X86: [[META37]] = distinct !{[[META37]], [[META38:![0-9]+]], !"_mm256_broadcast_ps: %agg.result"}
// X86: [[META38]] = distinct !{[[META38]], !"_mm256_broadcast_ps"}
// X86: [[META39]] = !{[[META40:![0-9]+]]}
// X86: [[META40]] = distinct !{[[META40]], [[META41:![0-9]+]], !"_mm256_broadcast_sd: %agg.result"}
// X86: [[META41]] = distinct !{[[META41]], !"_mm256_broadcast_sd"}
// X86: [[META42]] = !{[[META43:![0-9]+]]}
// X86: [[META43]] = distinct !{[[META43]], [[META44:![0-9]+]], !"_mm256_broadcast_ss: %agg.result"}
// X86: [[META44]] = distinct !{[[META44]], !"_mm256_broadcast_ss"}
// X86: [[META45]] = !{[[META46:![0-9]+]]}
// X86: [[META46]] = distinct !{[[META46]], [[META47:![0-9]+]], !"_mm256_castpd_ps: %agg.result"}
// X86: [[META47]] = distinct !{[[META47]], !"_mm256_castpd_ps"}
// X86: [[META48]] = !{[[META49:![0-9]+]]}
// X86: [[META49]] = distinct !{[[META49]], [[META50:![0-9]+]], !"_mm256_castpd_si256: %agg.result"}
// X86: [[META50]] = distinct !{[[META50]], !"_mm256_castpd_si256"}
// X86: [[META51]] = !{[[META52:![0-9]+]]}
// X86: [[META52]] = distinct !{[[META52]], [[META53:![0-9]+]], !"_mm256_castpd128_pd256: %agg.result"}
// X86: [[META53]] = distinct !{[[META53]], !"_mm256_castpd128_pd256"}
// X86: [[META54]] = !{[[META55:![0-9]+]]}
// X86: [[META55]] = distinct !{[[META55]], [[META56:![0-9]+]], !"_mm256_castps_pd: %agg.result"}
// X86: [[META56]] = distinct !{[[META56]], !"_mm256_castps_pd"}
// X86: [[META57]] = !{[[META58:![0-9]+]]}
// X86: [[META58]] = distinct !{[[META58]], [[META59:![0-9]+]], !"_mm256_castps_si256: %agg.result"}
// X86: [[META59]] = distinct !{[[META59]], !"_mm256_castps_si256"}
// X86: [[META60]] = !{[[META61:![0-9]+]]}
// X86: [[META61]] = distinct !{[[META61]], [[META62:![0-9]+]], !"_mm256_castps128_ps256: %agg.result"}
// X86: [[META62]] = distinct !{[[META62]], !"_mm256_castps128_ps256"}
// X86: [[META63]] = !{[[META64:![0-9]+]]}
// X86: [[META64]] = distinct !{[[META64]], [[META65:![0-9]+]], !"_mm256_castsi128_si256: %agg.result"}
// X86: [[META65]] = distinct !{[[META65]], !"_mm256_castsi128_si256"}
// X86: [[META66]] = !{[[META67:![0-9]+]]}
// X86: [[META67]] = distinct !{[[META67]], [[META68:![0-9]+]], !"_mm256_castsi256_pd: %agg.result"}
// X86: [[META68]] = distinct !{[[META68]], !"_mm256_castsi256_pd"}
// X86: [[META69]] = !{[[META70:![0-9]+]]}
// X86: [[META70]] = distinct !{[[META70]], [[META71:![0-9]+]], !"_mm256_castsi256_ps: %agg.result"}
// X86: [[META71]] = distinct !{[[META71]], !"_mm256_castsi256_ps"}
// X86: [[META72]] = !{[[META73:![0-9]+]]}
// X86: [[META73]] = distinct !{[[META73]], [[META74:![0-9]+]], !"_mm256_cvtepi32_pd: %agg.result"}
// X86: [[META74]] = distinct !{[[META74]], !"_mm256_cvtepi32_pd"}
// X86: [[META75]] = !{[[META76:![0-9]+]]}
// X86: [[META76]] = distinct !{[[META76]], [[META77:![0-9]+]], !"_mm256_cvtepi32_ps: %agg.result"}
// X86: [[META77]] = distinct !{[[META77]], !"_mm256_cvtepi32_ps"}
// X86: [[META78]] = !{[[META79:![0-9]+]]}
// X86: [[META79]] = distinct !{[[META79]], [[META80:![0-9]+]], !"_mm256_cvtps_epi32: %agg.result"}
// X86: [[META80]] = distinct !{[[META80]], !"_mm256_cvtps_epi32"}
// X86: [[META81]] = !{[[META82:![0-9]+]]}
// X86: [[META82]] = distinct !{[[META82]], [[META83:![0-9]+]], !"_mm256_cvtps_pd: %agg.result"}
// X86: [[META83]] = distinct !{[[META83]], !"_mm256_cvtps_pd"}
// X86: [[META84]] = !{[[META85:![0-9]+]]}
// X86: [[META85]] = distinct !{[[META85]], [[META86:![0-9]+]], !"_mm256_cvttps_epi32: %agg.result"}
// X86: [[META86]] = distinct !{[[META86]], !"_mm256_cvttps_epi32"}
// X86: [[META87]] = !{[[META88:![0-9]+]]}
// X86: [[META88]] = distinct !{[[META88]], [[META89:![0-9]+]], !"_mm256_div_pd: %agg.result"}
// X86: [[META89]] = distinct !{[[META89]], !"_mm256_div_pd"}
// X86: [[META90]] = !{[[META91:![0-9]+]]}
// X86: [[META91]] = distinct !{[[META91]], [[META92:![0-9]+]], !"_mm256_div_ps: %agg.result"}
// X86: [[META92]] = distinct !{[[META92]], !"_mm256_div_ps"}
// X86: [[META93]] = !{[[META94:![0-9]+]]}
// X86: [[META94]] = distinct !{[[META94]], [[META95:![0-9]+]], !"_mm256_hadd_pd: %agg.result"}
// X86: [[META95]] = distinct !{[[META95]], !"_mm256_hadd_pd"}
// X86: [[META96]] = !{[[META97:![0-9]+]]}
// X86: [[META97]] = distinct !{[[META97]], [[META98:![0-9]+]], !"_mm256_hadd_ps: %agg.result"}
// X86: [[META98]] = distinct !{[[META98]], !"_mm256_hadd_ps"}
// X86: [[META99]] = !{[[META100:![0-9]+]]}
// X86: [[META100]] = distinct !{[[META100]], [[META101:![0-9]+]], !"_mm256_hsub_pd: %agg.result"}
// X86: [[META101]] = distinct !{[[META101]], !"_mm256_hsub_pd"}
// X86: [[META102]] = !{[[META103:![0-9]+]]}
// X86: [[META103]] = distinct !{[[META103]], [[META104:![0-9]+]], !"_mm256_hsub_ps: %agg.result"}
// X86: [[META104]] = distinct !{[[META104]], !"_mm256_hsub_ps"}
// X86: [[META105]] = !{[[META106:![0-9]+]]}
// X86: [[META106]] = distinct !{[[META106]], [[META107:![0-9]+]], !"_mm256_lddqu_si256: %agg.result"}
// X86: [[META107]] = distinct !{[[META107]], !"_mm256_lddqu_si256"}
// X86: [[META108]] = !{[[META109:![0-9]+]]}
// X86: [[META109]] = distinct !{[[META109]], [[META110:![0-9]+]], !"_mm256_load_pd: %agg.result"}
// X86: [[META110]] = distinct !{[[META110]], !"_mm256_load_pd"}
// X86: [[META111]] = !{[[META112:![0-9]+]]}
// X86: [[META112]] = distinct !{[[META112]], [[META113:![0-9]+]], !"_mm256_load_ps: %agg.result"}
// X86: [[META113]] = distinct !{[[META113]], !"_mm256_load_ps"}
// X86: [[META114]] = !{[[META115:![0-9]+]]}
// X86: [[META115]] = distinct !{[[META115]], [[META116:![0-9]+]], !"_mm256_load_si256: %agg.result"}
// X86: [[META116]] = distinct !{[[META116]], !"_mm256_load_si256"}
// X86: [[META117]] = !{[[META118:![0-9]+]]}
// X86: [[META118]] = distinct !{[[META118]], [[META119:![0-9]+]], !"_mm256_loadu_pd: %agg.result"}
// X86: [[META119]] = distinct !{[[META119]], !"_mm256_loadu_pd"}
// X86: [[META120]] = !{[[META121:![0-9]+]]}
// X86: [[META121]] = distinct !{[[META121]], [[META122:![0-9]+]], !"_mm256_loadu_ps: %agg.result"}
// X86: [[META122]] = distinct !{[[META122]], !"_mm256_loadu_ps"}
// X86: [[META123]] = !{[[META124:![0-9]+]]}
// X86: [[META124]] = distinct !{[[META124]], [[META125:![0-9]+]], !"_mm256_loadu_si256: %agg.result"}
// X86: [[META125]] = distinct !{[[META125]], !"_mm256_loadu_si256"}
// X86: [[META126]] = !{[[META127:![0-9]+]]}
// X86: [[META127]] = distinct !{[[META127]], [[META128:![0-9]+]], !"_mm256_loadu2_m128: %agg.result"}
// X86: [[META128]] = distinct !{[[META128]], !"_mm256_loadu2_m128"}
// X86: [[META129]] = !{[[META130:![0-9]+]]}
// X86: [[META130]] = distinct !{[[META130]], [[META131:![0-9]+]], !"_mm256_set_m128: %agg.result"}
// X86: [[META131]] = distinct !{[[META131]], !"_mm256_set_m128"}
// X86: [[META132]] = !{[[META133:![0-9]+]]}
// X86: [[META133]] = distinct !{[[META133]], [[META134:![0-9]+]], !"_mm256_loadu2_m128d: %agg.result"}
// X86: [[META134]] = distinct !{[[META134]], !"_mm256_loadu2_m128d"}
// X86: [[META135]] = !{[[META136:![0-9]+]]}
// X86: [[META136]] = distinct !{[[META136]], [[META137:![0-9]+]], !"_mm256_set_m128d: %agg.result"}
// X86: [[META137]] = distinct !{[[META137]], !"_mm256_set_m128d"}
// X86: [[META138]] = !{[[META139:![0-9]+]]}
// X86: [[META139]] = distinct !{[[META139]], [[META140:![0-9]+]], !"_mm256_loadu2_m128i: %agg.result"}
// X86: [[META140]] = distinct !{[[META140]], !"_mm256_loadu2_m128i"}
// X86: [[META141]] = !{[[META142:![0-9]+]]}
// X86: [[META142]] = distinct !{[[META142]], [[META143:![0-9]+]], !"_mm256_set_m128i: %agg.result"}
// X86: [[META143]] = distinct !{[[META143]], !"_mm256_set_m128i"}
// X86: [[META144]] = !{[[META145:![0-9]+]]}
// X86: [[META145]] = distinct !{[[META145]], [[META146:![0-9]+]], !"_mm256_maskload_pd: %agg.result"}
// X86: [[META146]] = distinct !{[[META146]], !"_mm256_maskload_pd"}
// X86: [[META147]] = !{[[META148:![0-9]+]]}
// X86: [[META148]] = distinct !{[[META148]], [[META149:![0-9]+]], !"_mm256_maskload_ps: %agg.result"}
// X86: [[META149]] = distinct !{[[META149]], !"_mm256_maskload_ps"}
// X86: [[META150]] = !{[[META151:![0-9]+]]}
// X86: [[META151]] = distinct !{[[META151]], [[META152:![0-9]+]], !"_mm256_max_pd: %agg.result"}
// X86: [[META152]] = distinct !{[[META152]], !"_mm256_max_pd"}
// X86: [[META153]] = !{[[META154:![0-9]+]]}
// X86: [[META154]] = distinct !{[[META154]], [[META155:![0-9]+]], !"_mm256_max_ps: %agg.result"}
// X86: [[META155]] = distinct !{[[META155]], !"_mm256_max_ps"}
// X86: [[META156]] = !{[[META157:![0-9]+]]}
// X86: [[META157]] = distinct !{[[META157]], [[META158:![0-9]+]], !"_mm256_min_pd: %agg.result"}
// X86: [[META158]] = distinct !{[[META158]], !"_mm256_min_pd"}
// X86: [[META159]] = !{[[META160:![0-9]+]]}
// X86: [[META160]] = distinct !{[[META160]], [[META161:![0-9]+]], !"_mm256_min_ps: %agg.result"}
// X86: [[META161]] = distinct !{[[META161]], !"_mm256_min_ps"}
// X86: [[META162]] = !{[[META163:![0-9]+]]}
// X86: [[META163]] = distinct !{[[META163]], [[META164:![0-9]+]], !"_mm256_movedup_pd: %agg.result"}
// X86: [[META164]] = distinct !{[[META164]], !"_mm256_movedup_pd"}
// X86: [[META165]] = !{[[META166:![0-9]+]]}
// X86: [[META166]] = distinct !{[[META166]], [[META167:![0-9]+]], !"_mm256_movehdup_ps: %agg.result"}
// X86: [[META167]] = distinct !{[[META167]], !"_mm256_movehdup_ps"}
// X86: [[META168]] = !{[[META169:![0-9]+]]}
// X86: [[META169]] = distinct !{[[META169]], [[META170:![0-9]+]], !"_mm256_moveldup_ps: %agg.result"}
// X86: [[META170]] = distinct !{[[META170]], !"_mm256_moveldup_ps"}
// X86: [[META171]] = !{[[META172:![0-9]+]]}
// X86: [[META172]] = distinct !{[[META172]], [[META173:![0-9]+]], !"_mm256_mul_pd: %agg.result"}
// X86: [[META173]] = distinct !{[[META173]], !"_mm256_mul_pd"}
// X86: [[META174]] = !{[[META175:![0-9]+]]}
// X86: [[META175]] = distinct !{[[META175]], [[META176:![0-9]+]], !"_mm256_mul_ps: %agg.result"}
// X86: [[META176]] = distinct !{[[META176]], !"_mm256_mul_ps"}
// X86: [[META177]] = !{[[META178:![0-9]+]]}
// X86: [[META178]] = distinct !{[[META178]], [[META179:![0-9]+]], !"_mm256_or_pd: %agg.result"}
// X86: [[META179]] = distinct !{[[META179]], !"_mm256_or_pd"}
// X86: [[META180]] = !{[[META181:![0-9]+]]}
// X86: [[META181]] = distinct !{[[META181]], [[META182:![0-9]+]], !"_mm256_or_ps: %agg.result"}
// X86: [[META182]] = distinct !{[[META182]], !"_mm256_or_ps"}
// X86: [[META183]] = !{[[META184:![0-9]+]]}
// X86: [[META184]] = distinct !{[[META184]], [[META185:![0-9]+]], !"_mm256_permutevar_pd: %agg.result"}
// X86: [[META185]] = distinct !{[[META185]], !"_mm256_permutevar_pd"}
// X86: [[META186]] = !{[[META187:![0-9]+]]}
// X86: [[META187]] = distinct !{[[META187]], [[META188:![0-9]+]], !"_mm256_permutevar_ps: %agg.result"}
// X86: [[META188]] = distinct !{[[META188]], !"_mm256_permutevar_ps"}
// X86: [[META189]] = !{[[META190:![0-9]+]]}
// X86: [[META190]] = distinct !{[[META190]], [[META191:![0-9]+]], !"_mm256_rcp_ps: %agg.result"}
// X86: [[META191]] = distinct !{[[META191]], !"_mm256_rcp_ps"}
// X86: [[META192]] = !{[[META193:![0-9]+]]}
// X86: [[META193]] = distinct !{[[META193]], [[META194:![0-9]+]], !"_mm256_rsqrt_ps: %agg.result"}
// X86: [[META194]] = distinct !{[[META194]], !"_mm256_rsqrt_ps"}
// X86: [[META198]] = !{[[META199:![0-9]+]]}
// X86: [[META199]] = distinct !{[[META199]], [[META200:![0-9]+]], !"_mm256_set_epi16: %agg.result"}
// X86: [[META200]] = distinct !{[[META200]], !"_mm256_set_epi16"}
// X86: [[META201]] = !{[[META202:![0-9]+]]}
// X86: [[META202]] = distinct !{[[META202]], [[META203:![0-9]+]], !"_mm256_set_epi32: %agg.result"}
// X86: [[META203]] = distinct !{[[META203]], !"_mm256_set_epi32"}
// X86: [[META204]] = !{[[META205:![0-9]+]]}
// X86: [[META205]] = distinct !{[[META205]], [[META206:![0-9]+]], !"_mm256_set_epi64x: %agg.result"}
// X86: [[META206]] = distinct !{[[META206]], !"_mm256_set_epi64x"}
// X86: [[META207]] = !{[[META208:![0-9]+]]}
// X86: [[META208]] = distinct !{[[META208]], [[META209:![0-9]+]], !"_mm256_set_m128: %agg.result"}
// X86: [[META209]] = distinct !{[[META209]], !"_mm256_set_m128"}
// X86: [[META210]] = !{[[META211:![0-9]+]]}
// X86: [[META211]] = distinct !{[[META211]], [[META212:![0-9]+]], !"_mm256_set_m128d: %agg.result"}
// X86: [[META212]] = distinct !{[[META212]], !"_mm256_set_m128d"}
// X86: [[META213]] = !{[[META214:![0-9]+]]}
// X86: [[META214]] = distinct !{[[META214]], [[META215:![0-9]+]], !"_mm256_set_m128i: %agg.result"}
// X86: [[META215]] = distinct !{[[META215]], !"_mm256_set_m128i"}
// X86: [[META216]] = !{[[META217:![0-9]+]]}
// X86: [[META217]] = distinct !{[[META217]], [[META218:![0-9]+]], !"_mm256_set_pd: %agg.result"}
// X86: [[META218]] = distinct !{[[META218]], !"_mm256_set_pd"}
// X86: [[META219]] = !{[[META220:![0-9]+]]}
// X86: [[META220]] = distinct !{[[META220]], [[META221:![0-9]+]], !"_mm256_set_ps: %agg.result"}
// X86: [[META221]] = distinct !{[[META221]], !"_mm256_set_ps"}
// X86: [[META229]] = !{[[META230:![0-9]+]]}
// X86: [[META230]] = distinct !{[[META230]], [[META231:![0-9]+]], !"_mm256_set1_epi16: %agg.result"}
// X86: [[META231]] = distinct !{[[META231]], !"_mm256_set1_epi16"}
// X86: [[META232]] = !{[[META233:![0-9]+]]}
// X86: [[META233]] = distinct !{[[META233]], [[META234:![0-9]+]], !"_mm256_set_epi16: %agg.result"}
// X86: [[META234]] = distinct !{[[META234]], !"_mm256_set_epi16"}
// X86: [[META235]] = !{[[META233]], [[META230]]}
// X86: [[META236]] = !{[[META237:![0-9]+]]}
// X86: [[META237]] = distinct !{[[META237]], [[META238:![0-9]+]], !"_mm256_set1_epi32: %agg.result"}
// X86: [[META238]] = distinct !{[[META238]], !"_mm256_set1_epi32"}
// X86: [[META239]] = !{[[META240:![0-9]+]]}
// X86: [[META240]] = distinct !{[[META240]], [[META241:![0-9]+]], !"_mm256_set_epi32: %agg.result"}
// X86: [[META241]] = distinct !{[[META241]], !"_mm256_set_epi32"}
// X86: [[META242]] = !{[[META240]], [[META237]]}
// X86: [[META243]] = !{[[META244:![0-9]+]]}
// X86: [[META244]] = distinct !{[[META244]], [[META245:![0-9]+]], !"_mm256_set1_epi64x: %agg.result"}
// X86: [[META245]] = distinct !{[[META245]], !"_mm256_set1_epi64x"}
// X86: [[META246]] = !{[[META247:![0-9]+]]}
// X86: [[META247]] = distinct !{[[META247]], [[META248:![0-9]+]], !"_mm256_set_epi64x: %agg.result"}
// X86: [[META248]] = distinct !{[[META248]], !"_mm256_set_epi64x"}
// X86: [[META249]] = !{[[META247]], [[META244]]}
// X86: [[META250]] = !{[[META251:![0-9]+]]}
// X86: [[META251]] = distinct !{[[META251]], [[META252:![0-9]+]], !"_mm256_set1_pd: %agg.result"}
// X86: [[META252]] = distinct !{[[META252]], !"_mm256_set1_pd"}
// X86: [[META253]] = !{[[META254:![0-9]+]]}
// X86: [[META254]] = distinct !{[[META254]], [[META255:![0-9]+]], !"_mm256_set_pd: %agg.result"}
// X86: [[META255]] = distinct !{[[META255]], !"_mm256_set_pd"}
// X86: [[META256]] = !{[[META254]], [[META251]]}
// X86: [[META257]] = !{[[META258:![0-9]+]]}
// X86: [[META258]] = distinct !{[[META258]], [[META259:![0-9]+]], !"_mm256_set1_ps: %agg.result"}
// X86: [[META259]] = distinct !{[[META259]], !"_mm256_set1_ps"}
// X86: [[META260]] = !{[[META261:![0-9]+]]}
// X86: [[META261]] = distinct !{[[META261]], [[META262:![0-9]+]], !"_mm256_set_ps: %agg.result"}
// X86: [[META262]] = distinct !{[[META262]], !"_mm256_set_ps"}
// X86: [[META263]] = !{[[META261]], [[META258]]}
// X86: [[META271]] = !{[[META272:![0-9]+]]}
// X86: [[META272]] = distinct !{[[META272]], [[META273:![0-9]+]], !"_mm256_setr_epi16: %agg.result"}
// X86: [[META273]] = distinct !{[[META273]], !"_mm256_setr_epi16"}
// X86: [[META274]] = !{[[META275:![0-9]+]]}
// X86: [[META275]] = distinct !{[[META275]], [[META276:![0-9]+]], !"_mm256_set_epi16: %agg.result"}
// X86: [[META276]] = distinct !{[[META276]], !"_mm256_set_epi16"}
// X86: [[META277]] = !{[[META275]], [[META272]]}
// X86: [[META278]] = !{[[META279:![0-9]+]]}
// X86: [[META279]] = distinct !{[[META279]], [[META280:![0-9]+]], !"_mm256_setr_epi32: %agg.result"}
// X86: [[META280]] = distinct !{[[META280]], !"_mm256_setr_epi32"}
// X86: [[META281]] = !{[[META282:![0-9]+]]}
// X86: [[META282]] = distinct !{[[META282]], [[META283:![0-9]+]], !"_mm256_set_epi32: %agg.result"}
// X86: [[META283]] = distinct !{[[META283]], !"_mm256_set_epi32"}
// X86: [[META284]] = !{[[META282]], [[META279]]}
// X86: [[META285]] = !{[[META286:![0-9]+]]}
// X86: [[META286]] = distinct !{[[META286]], [[META287:![0-9]+]], !"_mm256_setr_epi64x: %agg.result"}
// X86: [[META287]] = distinct !{[[META287]], !"_mm256_setr_epi64x"}
// X86: [[META288]] = !{[[META289:![0-9]+]]}
// X86: [[META289]] = distinct !{[[META289]], [[META290:![0-9]+]], !"_mm256_set_epi64x: %agg.result"}
// X86: [[META290]] = distinct !{[[META290]], !"_mm256_set_epi64x"}
// X86: [[META291]] = !{[[META289]], [[META286]]}
// X86: [[META292]] = !{[[META293:![0-9]+]]}
// X86: [[META293]] = distinct !{[[META293]], [[META294:![0-9]+]], !"_mm256_setr_m128: %agg.result"}
// X86: [[META294]] = distinct !{[[META294]], !"_mm256_setr_m128"}
// X86: [[META295]] = !{[[META296:![0-9]+]]}
// X86: [[META296]] = distinct !{[[META296]], [[META297:![0-9]+]], !"_mm256_set_m128: %agg.result"}
// X86: [[META297]] = distinct !{[[META297]], !"_mm256_set_m128"}
// X86: [[META298]] = !{[[META296]], [[META293]]}
// X86: [[META299]] = !{[[META300:![0-9]+]]}
// X86: [[META300]] = distinct !{[[META300]], [[META301:![0-9]+]], !"_mm256_setr_m128d: %agg.result"}
// X86: [[META301]] = distinct !{[[META301]], !"_mm256_setr_m128d"}
// X86: [[META302]] = !{[[META303:![0-9]+]]}
// X86: [[META303]] = distinct !{[[META303]], [[META304:![0-9]+]], !"_mm256_set_m128d: %agg.result"}
// X86: [[META304]] = distinct !{[[META304]], !"_mm256_set_m128d"}
// X86: [[META305]] = !{[[META303]], [[META300]]}
// X86: [[META306]] = !{[[META307:![0-9]+]]}
// X86: [[META307]] = distinct !{[[META307]], [[META308:![0-9]+]], !"_mm256_setr_m128i: %agg.result"}
// X86: [[META308]] = distinct !{[[META308]], !"_mm256_setr_m128i"}
// X86: [[META309]] = !{[[META310:![0-9]+]]}
// X86: [[META310]] = distinct !{[[META310]], [[META311:![0-9]+]], !"_mm256_set_m128i: %agg.result"}
// X86: [[META311]] = distinct !{[[META311]], !"_mm256_set_m128i"}
// X86: [[META312]] = !{[[META310]], [[META307]]}
// X86: [[META313]] = !{[[META314:![0-9]+]]}
// X86: [[META314]] = distinct !{[[META314]], [[META315:![0-9]+]], !"_mm256_setr_pd: %agg.result"}
// X86: [[META315]] = distinct !{[[META315]], !"_mm256_setr_pd"}
// X86: [[META316]] = !{[[META317:![0-9]+]]}
// X86: [[META317]] = distinct !{[[META317]], [[META318:![0-9]+]], !"_mm256_set_pd: %agg.result"}
// X86: [[META318]] = distinct !{[[META318]], !"_mm256_set_pd"}
// X86: [[META319]] = !{[[META317]], [[META314]]}
// X86: [[META320]] = !{[[META321:![0-9]+]]}
// X86: [[META321]] = distinct !{[[META321]], [[META322:![0-9]+]], !"_mm256_setr_ps: %agg.result"}
// X86: [[META322]] = distinct !{[[META322]], !"_mm256_setr_ps"}
// X86: [[META323]] = !{[[META324:![0-9]+]]}
// X86: [[META324]] = distinct !{[[META324]], [[META325:![0-9]+]], !"_mm256_set_ps: %agg.result"}
// X86: [[META325]] = distinct !{[[META325]], !"_mm256_set_ps"}
// X86: [[META326]] = !{[[META324]], [[META321]]}
// X86: [[META327]] = !{[[META328:![0-9]+]]}
// X86: [[META328]] = distinct !{[[META328]], [[META329:![0-9]+]], !"_mm256_setzero_pd: %agg.result"}
// X86: [[META329]] = distinct !{[[META329]], !"_mm256_setzero_pd"}
// X86: [[META330]] = !{[[META331:![0-9]+]]}
// X86: [[META331]] = distinct !{[[META331]], [[META332:![0-9]+]], !"_mm256_setzero_ps: %agg.result"}
// X86: [[META332]] = distinct !{[[META332]], !"_mm256_setzero_ps"}
// X86: [[META333]] = !{[[META334:![0-9]+]]}
// X86: [[META334]] = distinct !{[[META334]], [[META335:![0-9]+]], !"_mm256_setzero_si256: %agg.result"}
// X86: [[META335]] = distinct !{[[META335]], !"_mm256_setzero_si256"}
// X86: [[META336]] = !{[[META337:![0-9]+]]}
// X86: [[META337]] = distinct !{[[META337]], [[META338:![0-9]+]], !"_mm256_sqrt_pd: %agg.result"}
// X86: [[META338]] = distinct !{[[META338]], !"_mm256_sqrt_pd"}
// X86: [[META339]] = !{[[META340:![0-9]+]]}
// X86: [[META340]] = distinct !{[[META340]], [[META341:![0-9]+]], !"_mm256_sqrt_ps: %agg.result"}
// X86: [[META341]] = distinct !{[[META341]], !"_mm256_sqrt_ps"}
// X86: [[META342]] = !{i32 1}
// X86: [[META343]] = !{[[META344:![0-9]+]]}
// X86: [[META344]] = distinct !{[[META344]], [[META345:![0-9]+]], !"_mm256_sub_pd: %agg.result"}
// X86: [[META345]] = distinct !{[[META345]], !"_mm256_sub_pd"}
// X86: [[META346]] = !{[[META347:![0-9]+]]}
// X86: [[META347]] = distinct !{[[META347]], [[META348:![0-9]+]], !"_mm256_sub_ps: %agg.result"}
// X86: [[META348]] = distinct !{[[META348]], !"_mm256_sub_ps"}
// X86: [[META349]] = !{[[META350:![0-9]+]]}
// X86: [[META350]] = distinct !{[[META350]], [[META351:![0-9]+]], !"_mm256_undefined_ps: %agg.result"}
// X86: [[META351]] = distinct !{[[META351]], !"_mm256_undefined_ps"}
// X86: [[META352]] = !{[[META353:![0-9]+]]}
// X86: [[META353]] = distinct !{[[META353]], [[META354:![0-9]+]], !"_mm256_undefined_pd: %agg.result"}
// X86: [[META354]] = distinct !{[[META354]], !"_mm256_undefined_pd"}
// X86: [[META355]] = !{[[META356:![0-9]+]]}
// X86: [[META356]] = distinct !{[[META356]], [[META357:![0-9]+]], !"_mm256_undefined_si256: %agg.result"}
// X86: [[META357]] = distinct !{[[META357]], !"_mm256_undefined_si256"}
// X86: [[META358]] = !{[[META359:![0-9]+]]}
// X86: [[META359]] = distinct !{[[META359]], [[META360:![0-9]+]], !"_mm256_unpackhi_pd: %agg.result"}
// X86: [[META360]] = distinct !{[[META360]], !"_mm256_unpackhi_pd"}
// X86: [[META361]] = !{[[META362:![0-9]+]]}
// X86: [[META362]] = distinct !{[[META362]], [[META363:![0-9]+]], !"_mm256_unpackhi_ps: %agg.result"}
// X86: [[META363]] = distinct !{[[META363]], !"_mm256_unpackhi_ps"}
// X86: [[META364]] = !{[[META365:![0-9]+]]}
// X86: [[META365]] = distinct !{[[META365]], [[META366:![0-9]+]], !"_mm256_unpacklo_pd: %agg.result"}
// X86: [[META366]] = distinct !{[[META366]], !"_mm256_unpacklo_pd"}
// X86: [[META367]] = !{[[META368:![0-9]+]]}
// X86: [[META368]] = distinct !{[[META368]], [[META369:![0-9]+]], !"_mm256_unpacklo_ps: %agg.result"}
// X86: [[META369]] = distinct !{[[META369]], !"_mm256_unpacklo_ps"}
// X86: [[META370]] = !{[[META371:![0-9]+]]}
// X86: [[META371]] = distinct !{[[META371]], [[META372:![0-9]+]], !"_mm256_xor_pd: %agg.result"}
// X86: [[META372]] = distinct !{[[META372]], !"_mm256_xor_pd"}
// X86: [[META373]] = !{[[META374:![0-9]+]]}
// X86: [[META374]] = distinct !{[[META374]], [[META375:![0-9]+]], !"_mm256_xor_ps: %agg.result"}
// X86: [[META375]] = distinct !{[[META375]], !"_mm256_xor_ps"}
// X86: [[META376]] = !{[[META377:![0-9]+]]}
// X86: [[META377]] = distinct !{[[META377]], [[META378:![0-9]+]], !"_mm256_zextpd128_pd256: %agg.result"}
// X86: [[META378]] = distinct !{[[META378]], !"_mm256_zextpd128_pd256"}
// X86: [[META379]] = !{[[META380:![0-9]+]]}
// X86: [[META380]] = distinct !{[[META380]], [[META381:![0-9]+]], !"_mm256_zextps128_ps256: %agg.result"}
// X86: [[META381]] = distinct !{[[META381]], !"_mm256_zextps128_ps256"}
// X86: [[META382]] = !{[[META383:![0-9]+]]}
// X86: [[META383]] = distinct !{[[META383]], [[META384:![0-9]+]], !"_mm256_zextsi128_si256: %agg.result"}
// X86: [[META384]] = distinct !{[[META384]], !"_mm256_zextsi128_si256"}
//.
//// NOTE: These prefixes are unused and the list is autogenerated. Do not add tests below this line:
// CHECK: {{.*}}
// X64: {{.*}}
