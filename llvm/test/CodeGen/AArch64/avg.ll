; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 4
; RUN: llc -mtriple=aarch64 -mattr=+sve < %s | FileCheck %s

define <16 x i16> @zext_avgflooru(<16 x i8> %a0, <16 x i8> %a1) {
; CHECK-LABEL: zext_avgflooru:
; CHECK:       // %bb.0:
; CHECK-NEXT:    uhadd v0.16b, v0.16b, v1.16b
; CHECK-NEXT:    ushll2 v1.8h, v0.16b, #0
; CHECK-NEXT:    ushll v0.8h, v0.8b, #0
; CHECK-NEXT:    ret
  %x0 = zext <16 x i8> %a0 to <16 x i16>
  %x1 = zext <16 x i8> %a1 to <16 x i16>
  %and = and <16 x i16> %x0, %x1
  %xor = xor <16 x i16> %x0, %x1
  %shift = lshr <16 x i16> %xor, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %avg = add <16 x i16> %and, %shift
  ret <16 x i16> %avg
}

define void @zext_mload_avgflooru(ptr %p1, ptr %p2, <vscale x 8 x i1> %mask) {
; CHECK-LABEL: zext_mload_avgflooru:
; CHECK:       // %bb.0:
; CHECK-NEXT:    ld1b { z0.h }, p0/z, [x0]
; CHECK-NEXT:    ld1b { z1.h }, p0/z, [x1]
; CHECK-NEXT:    and z0.h, z0.h, #0xff
; CHECK-NEXT:    and z1.h, z1.h, #0xff
; CHECK-NEXT:    add z0.h, z0.h, z1.h
; CHECK-NEXT:    lsr z0.h, z0.h, #1
; CHECK-NEXT:    st1h { z0.h }, p0, [x0]
; CHECK-NEXT:    ret
  %ld1 = call <vscale x 8 x i8> @llvm.masked.load(ptr %p1, i32 16, <vscale x 8 x i1> %mask, <vscale x 8 x i8> zeroinitializer)
  %ld2 = call <vscale x 8 x i8> @llvm.masked.load(ptr %p2, i32 16, <vscale x 8 x i1> %mask, <vscale x 8 x i8> zeroinitializer)
  %and = and <vscale x 8 x i8> %ld1, %ld2
  %xor = xor <vscale x 8 x i8> %ld1, %ld2
  %shift = lshr <vscale x 8 x i8> %xor, splat(i8 1)
  %avg = add <vscale x 8 x i8> %and, %shift
  %avgext = zext <vscale x 8 x i8> %avg to <vscale x 8 x i16>
  call void @llvm.masked.store.nxv8i16(<vscale x 8 x i16> %avgext, ptr %p1, i32 16, <vscale x 8 x i1> %mask)
  ret void
}

define <16 x i16> @zext_avgflooru_mismatch(<16 x i8> %a0, <16 x i4> %a1) {
; CHECK-LABEL: zext_avgflooru_mismatch:
; CHECK:       // %bb.0:
; CHECK-NEXT:    movi v2.16b, #15
; CHECK-NEXT:    and v1.16b, v1.16b, v2.16b
; CHECK-NEXT:    uhadd v0.16b, v0.16b, v1.16b
; CHECK-NEXT:    ushll2 v1.8h, v0.16b, #0
; CHECK-NEXT:    ushll v0.8h, v0.8b, #0
; CHECK-NEXT:    ret
  %x0 = zext <16 x i8> %a0 to <16 x i16>
  %x1 = zext <16 x i4> %a1 to <16 x i16>
  %and = and <16 x i16> %x0, %x1
  %xor = xor <16 x i16> %x0, %x1
  %shift = lshr <16 x i16> %xor, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %avg = add <16 x i16> %and, %shift
  ret <16 x i16> %avg
}

define <16 x i16> @zext_avgceilu(<16 x i8> %a0, <16 x i8> %a1) {
; CHECK-LABEL: zext_avgceilu:
; CHECK:       // %bb.0:
; CHECK-NEXT:    urhadd v0.16b, v0.16b, v1.16b
; CHECK-NEXT:    ushll2 v1.8h, v0.16b, #0
; CHECK-NEXT:    ushll v0.8h, v0.8b, #0
; CHECK-NEXT:    ret
  %x0 = zext <16 x i8> %a0 to <16 x i16>
  %x1 = zext <16 x i8> %a1 to <16 x i16>
  %or = or <16 x i16> %x0, %x1
  %xor = xor <16 x i16> %x0, %x1
  %shift = lshr <16 x i16> %xor, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %avg = sub <16 x i16> %or, %shift
  ret <16 x i16> %avg
}

define void @zext_mload_avgceilu(ptr %p1, ptr %p2, <vscale x 8 x i1> %mask) {
; CHECK-LABEL: zext_mload_avgceilu:
; CHECK:       // %bb.0:
; CHECK-NEXT:    ld1b { z0.h }, p0/z, [x0]
; CHECK-NEXT:    ld1b { z1.h }, p0/z, [x1]
; CHECK-NEXT:    mov z2.h, #-1 // =0xffffffffffffffff
; CHECK-NEXT:    and z0.h, z0.h, #0xff
; CHECK-NEXT:    and z1.h, z1.h, #0xff
; CHECK-NEXT:    eor z0.d, z0.d, z2.d
; CHECK-NEXT:    sub z0.h, z1.h, z0.h
; CHECK-NEXT:    lsr z0.h, z0.h, #1
; CHECK-NEXT:    st1b { z0.h }, p0, [x0]
; CHECK-NEXT:    ret
  %ld1 = call <vscale x 8 x i8> @llvm.masked.load(ptr %p1, i32 16, <vscale x 8 x i1> %mask, <vscale x 8 x i8> zeroinitializer)
  %ld2 = call <vscale x 8 x i8> @llvm.masked.load(ptr %p2, i32 16, <vscale x 8 x i1> %mask, <vscale x 8 x i8> zeroinitializer)
  %zext1 = zext <vscale x 8 x i8> %ld1 to <vscale x 8 x i16>
  %zext2 = zext <vscale x 8 x i8> %ld2 to <vscale x 8 x i16>
  %add1 = add nuw nsw <vscale x 8 x i16> %zext1, splat(i16 1)
  %add2 = add nuw nsw <vscale x 8 x i16> %add1, %zext2
  %shift = lshr <vscale x 8 x i16> %add2, splat(i16 1)
  %trunc = trunc <vscale x 8 x i16> %shift to <vscale x 8 x i8>
  call void @llvm.masked.store.nxv8i8(<vscale x 8 x i8> %trunc, ptr %p1, i32 16, <vscale x 8 x i1> %mask)
  ret void
}


define <16 x i16> @zext_avgceilu_mismatch(<16 x i4> %a0, <16 x i8> %a1) {
; CHECK-LABEL: zext_avgceilu_mismatch:
; CHECK:       // %bb.0:
; CHECK-NEXT:    movi v2.16b, #15
; CHECK-NEXT:    and v0.16b, v0.16b, v2.16b
; CHECK-NEXT:    urhadd v0.16b, v0.16b, v1.16b
; CHECK-NEXT:    ushll2 v1.8h, v0.16b, #0
; CHECK-NEXT:    ushll v0.8h, v0.8b, #0
; CHECK-NEXT:    ret
  %x0 = zext <16 x i4> %a0 to <16 x i16>
  %x1 = zext <16 x i8> %a1 to <16 x i16>
  %or = or <16 x i16> %x0, %x1
  %xor = xor <16 x i16> %x0, %x1
  %shift = lshr <16 x i16> %xor, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %avg = sub <16 x i16> %or, %shift
  ret <16 x i16> %avg
}

define <16 x i16> @sext_avgfloors(<16 x i8> %a0, <16 x i8> %a1) {
; CHECK-LABEL: sext_avgfloors:
; CHECK:       // %bb.0:
; CHECK-NEXT:    shadd v0.16b, v0.16b, v1.16b
; CHECK-NEXT:    sshll2 v1.8h, v0.16b, #0
; CHECK-NEXT:    sshll v0.8h, v0.8b, #0
; CHECK-NEXT:    ret
  %x0 = sext <16 x i8> %a0 to <16 x i16>
  %x1 = sext <16 x i8> %a1 to <16 x i16>
  %and = and <16 x i16> %x0, %x1
  %xor = xor <16 x i16> %x0, %x1
  %shift = ashr <16 x i16> %xor, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %avg = add <16 x i16> %and, %shift
  ret <16 x i16> %avg
}

define <16 x i16> @sext_avgfloors_mismatch(<16 x i8> %a0, <16 x i4> %a1) {
; CHECK-LABEL: sext_avgfloors_mismatch:
; CHECK:       // %bb.0:
; CHECK-NEXT:    ushll2 v2.8h, v1.16b, #0
; CHECK-NEXT:    ushll v1.8h, v1.8b, #0
; CHECK-NEXT:    sshll v3.8h, v0.8b, #0
; CHECK-NEXT:    sshll2 v0.8h, v0.16b, #0
; CHECK-NEXT:    shl v1.8h, v1.8h, #12
; CHECK-NEXT:    shl v2.8h, v2.8h, #12
; CHECK-NEXT:    sshr v4.8h, v1.8h, #12
; CHECK-NEXT:    sshr v1.8h, v2.8h, #12
; CHECK-NEXT:    shadd v1.8h, v0.8h, v1.8h
; CHECK-NEXT:    shadd v0.8h, v3.8h, v4.8h
; CHECK-NEXT:    ret
  %x0 = sext <16 x i8> %a0 to <16 x i16>
  %x1 = sext <16 x i4> %a1 to <16 x i16>
  %and = and <16 x i16> %x0, %x1
  %xor = xor <16 x i16> %x0, %x1
  %shift = ashr <16 x i16> %xor, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %avg = add <16 x i16> %and, %shift
  ret <16 x i16> %avg
}

define <16 x i16> @sext_avgceils(<16 x i8> %a0, <16 x i8> %a1) {
; CHECK-LABEL: sext_avgceils:
; CHECK:       // %bb.0:
; CHECK-NEXT:    srhadd v0.16b, v0.16b, v1.16b
; CHECK-NEXT:    sshll2 v1.8h, v0.16b, #0
; CHECK-NEXT:    sshll v0.8h, v0.8b, #0
; CHECK-NEXT:    ret
  %x0 = sext <16 x i8> %a0 to <16 x i16>
  %x1 = sext <16 x i8> %a1 to <16 x i16>
  %or = or <16 x i16> %x0, %x1
  %xor = xor <16 x i16> %x0, %x1
  %shift = ashr <16 x i16> %xor, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %avg = sub <16 x i16> %or, %shift
  ret <16 x i16> %avg
}

define <16 x i16> @sext_avgceils_mismatch(<16 x i4> %a0, <16 x i8> %a1) {
; CHECK-LABEL: sext_avgceils_mismatch:
; CHECK:       // %bb.0:
; CHECK-NEXT:    ushll v2.8h, v0.8b, #0
; CHECK-NEXT:    ushll2 v0.8h, v0.16b, #0
; CHECK-NEXT:    sshll v3.8h, v1.8b, #0
; CHECK-NEXT:    sshll2 v1.8h, v1.16b, #0
; CHECK-NEXT:    shl v2.8h, v2.8h, #12
; CHECK-NEXT:    shl v0.8h, v0.8h, #12
; CHECK-NEXT:    sshr v2.8h, v2.8h, #12
; CHECK-NEXT:    sshr v0.8h, v0.8h, #12
; CHECK-NEXT:    srhadd v1.8h, v0.8h, v1.8h
; CHECK-NEXT:    srhadd v0.8h, v2.8h, v3.8h
; CHECK-NEXT:    ret
  %x0 = sext <16 x i4> %a0 to <16 x i16>
  %x1 = sext <16 x i8> %a1 to <16 x i16>
  %or = or <16 x i16> %x0, %x1
  %xor = xor <16 x i16> %x0, %x1
  %shift = ashr <16 x i16> %xor, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %avg = sub <16 x i16> %or, %shift
  ret <16 x i16> %avg
}

define <8 x i16> @add_avgflooru(<8 x i16> %a0, <8 x i16> %a1) {
; CHECK-LABEL: add_avgflooru:
; CHECK:       // %bb.0:
; CHECK-NEXT:    uhadd v0.8h, v0.8h, v1.8h
; CHECK-NEXT:    ret
  %add = add nuw <8 x i16> %a0, %a1
  %avg = lshr <8 x i16> %add, splat(i16 1)
  ret <8 x i16> %avg
}

define <8 x i16> @add_avgflooru_mismatch(<8 x i16> %a0, <8 x i16> %a1) {
; CHECK-LABEL: add_avgflooru_mismatch:
; CHECK:       // %bb.0:
; CHECK-NEXT:    add v0.8h, v0.8h, v1.8h
; CHECK-NEXT:    ushr v0.8h, v0.8h, #1
; CHECK-NEXT:    ret
  %add = add <8 x i16> %a0, %a1
  %avg = lshr <8 x i16> %add, splat(i16 1)
  ret <8 x i16> %avg
}

define <8 x i16> @add_avgceilu(<8 x i16> %a0, <8 x i16> %a1) {
; CHECK-LABEL: add_avgceilu:
; CHECK:       // %bb.0:
; CHECK-NEXT:    urhadd v0.8h, v0.8h, v1.8h
; CHECK-NEXT:    ret
  %add0 = add nuw <8 x i16> %a0, splat(i16 1)
  %add = add nuw <8 x i16> %a1, %add0
  %avg = lshr <8 x i16> %add, splat(i16 1)
  ret <8 x i16> %avg
}

define <8 x i16> @add_avgceilu2(<8 x i16> %a0, <8 x i16> %a1) {
; CHECK-LABEL: add_avgceilu2:
; CHECK:       // %bb.0:
; CHECK-NEXT:    urhadd v0.8h, v1.8h, v0.8h
; CHECK-NEXT:    ret
  %add0 = add nuw <8 x i16> %a1, %a0
  %add = add nuw <8 x i16> %add0, splat(i16 1)
  %avg = lshr <8 x i16> %add, splat(i16 1)
  ret <8 x i16> %avg
}

define <8 x i16> @add_avgceilu_mismatch1(<8 x i16> %a0, <8 x i16> %a1) {
; CHECK-LABEL: add_avgceilu_mismatch1:
; CHECK:       // %bb.0:
; CHECK-NEXT:    movi v2.8h, #1
; CHECK-NEXT:    add v0.8h, v1.8h, v0.8h
; CHECK-NEXT:    uhadd v0.8h, v0.8h, v2.8h
; CHECK-NEXT:    ret
  %add0 = add <8 x i16> %a1, %a0
  %add = add nuw <8 x i16> %add0, splat(i16 1)
  %avg = lshr <8 x i16> %add, splat(i16 1)
  ret <8 x i16> %avg
}

define <8 x i16> @add_avgceilu_mismatch2(<8 x i16> %a0, <8 x i16> %a1) {
; CHECK-LABEL: add_avgceilu_mismatch2:
; CHECK:       // %bb.0:
; CHECK-NEXT:    mvn v1.16b, v1.16b
; CHECK-NEXT:    sub v0.8h, v0.8h, v1.8h
; CHECK-NEXT:    ushr v0.8h, v0.8h, #1
; CHECK-NEXT:    ret
  %add0 = add nuw <8 x i16> %a1, %a0
  %add = add <8 x i16> %add0, splat(i16 1)
  %avg = lshr <8 x i16> %add, splat(i16 1)
  ret <8 x i16> %avg
}

define <8 x i16> @add_avgceilu_mismatch3(<8 x i16> %a0, <8 x i16> %a1) {
; CHECK-LABEL: add_avgceilu_mismatch3:
; CHECK:       // %bb.0:
; CHECK-NEXT:    mvn v1.16b, v1.16b
; CHECK-NEXT:    sub v0.8h, v0.8h, v1.8h
; CHECK-NEXT:    ushr v0.8h, v0.8h, #1
; CHECK-NEXT:    ret
  %add0 = add nuw <8 x i16> %a1, %a0
  %add = add <8 x i16> %add0, splat(i16 1)
  %avg = lshr <8 x i16> %add, splat(i16 1)
  ret <8 x i16> %avg
}

define <8 x i16> @add_avgfloors(<8 x i16> %a0, <8 x i16> %a1) {
; CHECK-LABEL: add_avgfloors:
; CHECK:       // %bb.0:
; CHECK-NEXT:    shadd v0.8h, v0.8h, v1.8h
; CHECK-NEXT:    ret
  %add = add nsw <8 x i16> %a0, %a1
  %avg = ashr <8 x i16> %add, splat(i16 1)
  ret <8 x i16> %avg
}

define <8 x i16> @add_avgfloors_mismatch(<8 x i16> %a0, <8 x i16> %a1) {
; CHECK-LABEL: add_avgfloors_mismatch:
; CHECK:       // %bb.0:
; CHECK-NEXT:    add v0.8h, v0.8h, v1.8h
; CHECK-NEXT:    sshr v0.8h, v0.8h, #1
; CHECK-NEXT:    ret
  %add = add <8 x i16> %a0, %a1
  %avg = ashr <8 x i16> %add, splat(i16 1)
  ret <8 x i16> %avg
}

define <8 x i16> @add_avgfoor_mismatch2(<8 x i16> %a0, <8 x i16> %a1) {
; CHECK-LABEL: add_avgfoor_mismatch2:
; CHECK:       // %bb.0:
; CHECK-NEXT:    add v0.8h, v0.8h, v1.8h
; CHECK-NEXT:    sshr v0.8h, v0.8h, #2
; CHECK-NEXT:    ret
  %add = add nsw <8 x i16> %a0, %a1
  %avg = ashr <8 x i16> %add, splat(i16 2)
  ret <8 x i16> %avg
}

define <8 x i16> @add_avgceils(<8 x i16> %a0, <8 x i16> %a1) {
; CHECK-LABEL: add_avgceils:
; CHECK:       // %bb.0:
; CHECK-NEXT:    srhadd v0.8h, v0.8h, v1.8h
; CHECK-NEXT:    ret
  %add0 = add nsw <8 x i16> %a0, splat(i16 1)
  %add = add nsw <8 x i16> %a1, %add0
  %avg = ashr <8 x i16> %add, splat(i16 1)
  ret <8 x i16> %avg
}

define <8 x i16> @add_avgceils2(<8 x i16> %a0, <8 x i16> %a1) {
; CHECK-LABEL: add_avgceils2:
; CHECK:       // %bb.0:
; CHECK-NEXT:    srhadd v0.8h, v1.8h, v0.8h
; CHECK-NEXT:    ret
  %add0 = add nsw <8 x i16> %a1, %a0
  %add = add nsw <8 x i16> %add0, splat(i16 1)
  %avg = ashr <8 x i16> %add, splat(i16 1)
  ret <8 x i16> %avg
}

define <8 x i16> @add_avgceils_mismatch1(<8 x i16> %a0, <8 x i16> %a1) {
; CHECK-LABEL: add_avgceils_mismatch1:
; CHECK:       // %bb.0:
; CHECK-NEXT:    movi v2.8h, #1
; CHECK-NEXT:    add v0.8h, v1.8h, v0.8h
; CHECK-NEXT:    shadd v0.8h, v0.8h, v2.8h
; CHECK-NEXT:    ret
  %add0 = add <8 x i16> %a1, %a0
  %add = add nsw <8 x i16> %add0, splat(i16 1)
  %avg = ashr <8 x i16> %add, splat(i16 1)
  ret <8 x i16> %avg
}

define <8 x i16> @add_avgceils_mismatch2(<8 x i16> %a0, <8 x i16> %a1) {
; CHECK-LABEL: add_avgceils_mismatch2:
; CHECK:       // %bb.0:
; CHECK-NEXT:    mvn v1.16b, v1.16b
; CHECK-NEXT:    sub v0.8h, v0.8h, v1.8h
; CHECK-NEXT:    sshr v0.8h, v0.8h, #1
; CHECK-NEXT:    ret
  %add0 = add nsw <8 x i16> %a1, %a0
  %add = add <8 x i16> %add0, splat(i16 1)
  %avg = ashr <8 x i16> %add, splat(i16 1)
  ret <8 x i16> %avg
}

define <8 x i16> @add_avgceils_mismatch3(<8 x i16> %a0, <8 x i16> %a1) {
; CHECK-LABEL: add_avgceils_mismatch3:
; CHECK:       // %bb.0:
; CHECK-NEXT:    mvn v1.16b, v1.16b
; CHECK-NEXT:    sub v0.8h, v0.8h, v1.8h
; CHECK-NEXT:    sshr v0.8h, v0.8h, #1
; CHECK-NEXT:    ret
  %add0 = add nsw <8 x i16> %a1, %a0
  %add = add <8 x i16> %add0, splat(i16 1)
  %avg = ashr <8 x i16> %add, splat(i16 1)
  ret <8 x i16> %avg
}

define <8 x i16> @add_avgceils_mismatch4(<8 x i16> %a0, <8 x i16> %a1) {
; CHECK-LABEL: add_avgceils_mismatch4:
; CHECK:       // %bb.0:
; CHECK-NEXT:    mvn v0.16b, v0.16b
; CHECK-NEXT:    sub v0.8h, v1.8h, v0.8h
; CHECK-NEXT:    sshr v0.8h, v0.8h, #2
; CHECK-NEXT:    ret
  %add0 = add nsw <8 x i16> %a0, splat(i16 1)
  %add = add nsw <8 x i16> %a1, %add0
  %avg = ashr <8 x i16> %add, splat(i16 2)
  ret <8 x i16> %avg
}

define <8 x i16> @add_avgceilu_mismatch(<8 x i16> %a0, <8 x i16> %a1) {
; CHECK-LABEL: add_avgceilu_mismatch:
; CHECK:       // %bb.0:
; CHECK-NEXT:    movi v2.8h, #1
; CHECK-NEXT:    add v0.8h, v1.8h, v0.8h
; CHECK-NEXT:    add v0.8h, v0.8h, v2.8h
; CHECK-NEXT:    ushr v0.8h, v0.8h, #2
; CHECK-NEXT:    ret
  %add0 = add nuw <8 x i16> %a1, %a0
  %add = add nuw <8 x i16> %add0, splat(i16 1)
  %avg = lshr <8 x i16> %add, splat(i16 2)
  ret <8 x i16> %avg
}
