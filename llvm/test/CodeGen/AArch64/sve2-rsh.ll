; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 4
; RUN: llc -mtriple=aarch64 -mattr=+sve < %s -o - | FileCheck --check-prefixes=CHECK,SVE %s
; RUN: llc -mtriple=aarch64 -mattr=+sve2 < %s -o - | FileCheck --check-prefixes=CHECK,SVE2 %s

; Wrong add/shift amount. Should be 32 for shift of 6.
define <vscale x 2 x i64> @neg_urshr_1(<vscale x 2 x i64> %x) {
; CHECK-LABEL: neg_urshr_1:
; CHECK:       // %bb.0:
; CHECK-NEXT:    add z0.d, z0.d, #16 // =0x10
; CHECK-NEXT:    lsr z0.d, z0.d, #6
; CHECK-NEXT:    ret
  %add = add <vscale x 2 x i64> %x, splat (i64 16)
  %sh = lshr <vscale x 2 x i64> %add, splat (i64 6)
  ret <vscale x 2 x i64> %sh
}

; Vector Shift.
define <vscale x 2 x i64> @neg_urshr_2(<vscale x 2 x i64> %x, <vscale x 2 x i64> %y) {
; CHECK-LABEL: neg_urshr_2:
; CHECK:       // %bb.0:
; CHECK-NEXT:    ptrue p0.d
; CHECK-NEXT:    add z0.d, z0.d, #32 // =0x20
; CHECK-NEXT:    lsr z0.d, p0/m, z0.d, z1.d
; CHECK-NEXT:    ret
  %add = add <vscale x 2 x i64> %x, splat (i64 32)
  %sh = lshr <vscale x 2 x i64> %add, %y
  ret <vscale x 2 x i64> %sh
}

; Vector Add.
define <vscale x 2 x i64> @neg_urshr_3(<vscale x 2 x i64> %x, <vscale x 2 x i64> %y) {
; CHECK-LABEL: neg_urshr_3:
; CHECK:       // %bb.0:
; CHECK-NEXT:    add z0.d, z0.d, z1.d
; CHECK-NEXT:    lsr z0.d, z0.d, #6
; CHECK-NEXT:    ret
  %add = add <vscale x 2 x i64> %x, %y
  %sh = lshr <vscale x 2 x i64> %add, splat (i64 6)
  ret <vscale x 2 x i64> %sh
}

; Add has two uses.
define <vscale x 2 x i64> @neg_urshr_4(<vscale x 2 x i64> %x) {
; CHECK-LABEL: neg_urshr_4:
; CHECK:       // %bb.0:
; CHECK-NEXT:    stp x29, x30, [sp, #-16]! // 16-byte Folded Spill
; CHECK-NEXT:    addvl sp, sp, #-1
; CHECK-NEXT:    str z8, [sp] // 16-byte Folded Spill
; CHECK-NEXT:    .cfi_escape 0x0f, 0x0c, 0x8f, 0x00, 0x11, 0x10, 0x22, 0x11, 0x08, 0x92, 0x2e, 0x00, 0x1e, 0x22 // sp + 16 + 8 * VG
; CHECK-NEXT:    .cfi_offset w30, -8
; CHECK-NEXT:    .cfi_offset w29, -16
; CHECK-NEXT:    .cfi_escape 0x10, 0x48, 0x0a, 0x11, 0x70, 0x22, 0x11, 0x78, 0x92, 0x2e, 0x00, 0x1e, 0x22 // $d8 @ cfa - 16 - 8 * VG
; CHECK-NEXT:    add z0.d, z0.d, #32 // =0x20
; CHECK-NEXT:    lsr z8.d, z0.d, #6
; CHECK-NEXT:    bl use
; CHECK-NEXT:    mov z0.d, z8.d
; CHECK-NEXT:    ldr z8, [sp] // 16-byte Folded Reload
; CHECK-NEXT:    addvl sp, sp, #1
; CHECK-NEXT:    ldp x29, x30, [sp], #16 // 16-byte Folded Reload
; CHECK-NEXT:    ret
  %add = add <vscale x 2 x i64> %x, splat (i64 32)
  %sh = lshr <vscale x 2 x i64> %add, splat (i64 6)
  call void @use(<vscale x 2 x i64> %add)
  ret <vscale x 2 x i64> %sh
}

define <vscale x 16 x i8> @urshr_i8(<vscale x 16 x i8> %x) {
; SVE-LABEL: urshr_i8:
; SVE:       // %bb.0:
; SVE-NEXT:    add z0.b, z0.b, #32 // =0x20
; SVE-NEXT:    lsr z0.b, z0.b, #6
; SVE-NEXT:    ret
;
; SVE2-LABEL: urshr_i8:
; SVE2:       // %bb.0:
; SVE2-NEXT:    ptrue p0.b
; SVE2-NEXT:    urshr z0.b, p0/m, z0.b, #6
; SVE2-NEXT:    ret
  %add = add <vscale x 16 x i8> %x, splat (i8 32)
  %sh = lshr <vscale x 16 x i8> %add, splat (i8 6)
  ret <vscale x 16 x i8> %sh
}

define <vscale x 8 x i16> @urshr_i16(<vscale x 8 x i16> %x) {
; SVE-LABEL: urshr_i16:
; SVE:       // %bb.0:
; SVE-NEXT:    add z0.h, z0.h, #32 // =0x20
; SVE-NEXT:    lsr z0.h, z0.h, #6
; SVE-NEXT:    ret
;
; SVE2-LABEL: urshr_i16:
; SVE2:       // %bb.0:
; SVE2-NEXT:    ptrue p0.h
; SVE2-NEXT:    urshr z0.h, p0/m, z0.h, #6
; SVE2-NEXT:    ret
  %add = add <vscale x 8 x i16> %x, splat (i16 32)
  %sh = lshr <vscale x 8 x i16> %add, splat (i16 6)
  ret <vscale x 8 x i16> %sh
}

define <vscale x 4 x i32> @urshr_i32(<vscale x 4 x i32> %x) {
; SVE-LABEL: urshr_i32:
; SVE:       // %bb.0:
; SVE-NEXT:    add z0.s, z0.s, #32 // =0x20
; SVE-NEXT:    lsr z0.s, z0.s, #6
; SVE-NEXT:    ret
;
; SVE2-LABEL: urshr_i32:
; SVE2:       // %bb.0:
; SVE2-NEXT:    ptrue p0.s
; SVE2-NEXT:    urshr z0.s, p0/m, z0.s, #6
; SVE2-NEXT:    ret
  %add = add <vscale x 4 x i32> %x, splat (i32 32)
  %sh = lshr <vscale x 4 x i32> %add, splat (i32 6)
  ret <vscale x 4 x i32> %sh
}

define <vscale x 2 x i64> @urshr_i64(<vscale x 2 x i64> %x) {
; SVE-LABEL: urshr_i64:
; SVE:       // %bb.0:
; SVE-NEXT:    add z0.d, z0.d, #32 // =0x20
; SVE-NEXT:    lsr z0.d, z0.d, #6
; SVE-NEXT:    ret
;
; SVE2-LABEL: urshr_i64:
; SVE2:       // %bb.0:
; SVE2-NEXT:    ptrue p0.d
; SVE2-NEXT:    urshr z0.d, p0/m, z0.d, #6
; SVE2-NEXT:    ret
  %add = add <vscale x 2 x i64> %x, splat (i64 32)
  %sh = lshr <vscale x 2 x i64> %add, splat (i64 6)
  ret <vscale x 2 x i64> %sh
}

define <vscale x 16 x i8> @srshr_i8(<vscale x 16 x i8> %x) {
; SVE-LABEL: srshr_i8:
; SVE:       // %bb.0:
; SVE-NEXT:    add z0.b, z0.b, #32 // =0x20
; SVE-NEXT:    asr z0.b, z0.b, #6
; SVE-NEXT:    ret
;
; SVE2-LABEL: srshr_i8:
; SVE2:       // %bb.0:
; SVE2-NEXT:    ptrue p0.b
; SVE2-NEXT:    srshr z0.b, p0/m, z0.b, #6
; SVE2-NEXT:    ret
  %add = add <vscale x 16 x i8> %x, splat (i8 32)
  %sh = ashr <vscale x 16 x i8> %add, splat (i8 6)
  ret <vscale x 16 x i8> %sh
}

define <vscale x 8 x i16> @srshr_i16(<vscale x 8 x i16> %x) {
; SVE-LABEL: srshr_i16:
; SVE:       // %bb.0:
; SVE-NEXT:    add z0.h, z0.h, #32 // =0x20
; SVE-NEXT:    asr z0.h, z0.h, #6
; SVE-NEXT:    ret
;
; SVE2-LABEL: srshr_i16:
; SVE2:       // %bb.0:
; SVE2-NEXT:    ptrue p0.h
; SVE2-NEXT:    srshr z0.h, p0/m, z0.h, #6
; SVE2-NEXT:    ret
  %add = add <vscale x 8 x i16> %x, splat (i16 32)
  %sh = ashr <vscale x 8 x i16> %add, splat (i16 6)
  ret <vscale x 8 x i16> %sh
}

define <vscale x 4 x i32> @srshr_i32(<vscale x 4 x i32> %x) {
; SVE-LABEL: srshr_i32:
; SVE:       // %bb.0:
; SVE-NEXT:    add z0.s, z0.s, #32 // =0x20
; SVE-NEXT:    asr z0.s, z0.s, #6
; SVE-NEXT:    ret
;
; SVE2-LABEL: srshr_i32:
; SVE2:       // %bb.0:
; SVE2-NEXT:    ptrue p0.s
; SVE2-NEXT:    srshr z0.s, p0/m, z0.s, #6
; SVE2-NEXT:    ret
  %add = add <vscale x 4 x i32> %x, splat (i32 32)
  %sh = ashr <vscale x 4 x i32> %add, splat (i32 6)
  ret <vscale x 4 x i32> %sh
}

define <vscale x 2 x i64> @srshr_i64(<vscale x 2 x i64> %x) {
; SVE-LABEL: srshr_i64:
; SVE:       // %bb.0:
; SVE-NEXT:    add z0.d, z0.d, #32 // =0x20
; SVE-NEXT:    asr z0.d, z0.d, #6
; SVE-NEXT:    ret
;
; SVE2-LABEL: srshr_i64:
; SVE2:       // %bb.0:
; SVE2-NEXT:    ptrue p0.d
; SVE2-NEXT:    srshr z0.d, p0/m, z0.d, #6
; SVE2-NEXT:    ret
  %add = add <vscale x 2 x i64> %x, splat (i64 32)
  %sh = ashr <vscale x 2 x i64> %add, splat (i64 6)
  ret <vscale x 2 x i64> %sh
}

declare void @use(<vscale x 2 x i64>)
