; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=aarch64-linux-gnu -mattr=+sme2 -verify-machineinstrs < %s | FileCheck %s

@dst = global [512 x i8] zeroinitializer, align 1
@src = global [512 x i8] zeroinitializer, align 1

define void @sc_memcpy(i64 noundef %n) "aarch64_pstate_sm_compatible" {
; CHECK-LABEL: sc_memcpy:
; CHECK:       // %bb.0: // %entry
; CHECK-NEXT:    str x30, [sp, #-16]! // 8-byte Folded Spill
; CHECK-NEXT:    .cfi_def_cfa_offset 16
; CHECK-NEXT:    .cfi_offset w30, -16
; CHECK-NEXT:    mov x2, x0
; CHECK-NEXT:    adrp x0, :got:dst
; CHECK-NEXT:    adrp x1, :got:src
; CHECK-NEXT:    ldr x0, [x0, :got_lo12:dst]
; CHECK-NEXT:    ldr x1, [x1, :got_lo12:src]
; CHECK-NEXT:    bl __arm_sc_memcpy
; CHECK-NEXT:    ldr x30, [sp], #16 // 8-byte Folded Reload
; CHECK-NEXT:    ret
entry:
  tail call void @llvm.memcpy.p0.p0.i64(ptr align 1 @dst, ptr nonnull align 1 @src, i64 %n, i1 false)
  ret void
}

define void @sc_memset(i64 noundef %n) "aarch64_pstate_sm_compatible" {
; CHECK-LABEL: sc_memset:
; CHECK:       // %bb.0: // %entry
; CHECK-NEXT:    str x30, [sp, #-16]! // 8-byte Folded Spill
; CHECK-NEXT:    .cfi_def_cfa_offset 16
; CHECK-NEXT:    .cfi_offset w30, -16
; CHECK-NEXT:    mov x2, x0
; CHECK-NEXT:    adrp x0, :got:dst
; CHECK-NEXT:    mov w1, #2 // =0x2
; CHECK-NEXT:    ldr x0, [x0, :got_lo12:dst]
; CHECK-NEXT:    // kill: def $w2 killed $w2 killed $x2
; CHECK-NEXT:    bl __arm_sc_memset
; CHECK-NEXT:    ldr x30, [sp], #16 // 8-byte Folded Reload
; CHECK-NEXT:    ret
entry:
  tail call void @llvm.memset.p0.i64(ptr align 1 @dst, i8 2, i64 %n, i1 false)
  ret void
}

define void @sc_memmove(i64 noundef %n) "aarch64_pstate_sm_compatible" {
; CHECK-LABEL: sc_memmove:
; CHECK:       // %bb.0: // %entry
; CHECK-NEXT:    str x30, [sp, #-16]! // 8-byte Folded Spill
; CHECK-NEXT:    .cfi_def_cfa_offset 16
; CHECK-NEXT:    .cfi_offset w30, -16
; CHECK-NEXT:    mov x2, x0
; CHECK-NEXT:    adrp x0, :got:dst
; CHECK-NEXT:    adrp x1, :got:src
; CHECK-NEXT:    ldr x0, [x0, :got_lo12:dst]
; CHECK-NEXT:    ldr x1, [x1, :got_lo12:src]
; CHECK-NEXT:    bl __arm_sc_memmove
; CHECK-NEXT:    ldr x30, [sp], #16 // 8-byte Folded Reload
; CHECK-NEXT:    ret
entry:
  tail call void @llvm.memmove.p0.p0.i64(ptr align 1 @dst, ptr nonnull align 1 @src, i64 %n, i1 false)
  ret void
}

declare void @llvm.memset.p0.i64(ptr nocapture writeonly, i8, i64, i1 immarg)
declare void @llvm.memcpy.p0.p0.i64(ptr nocapture writeonly, ptr nocapture readonly, i64, i1 immarg)
declare void @llvm.memmove.p0.p0.i64(ptr nocapture writeonly, ptr nocapture readonly, i64, i1 immarg)
