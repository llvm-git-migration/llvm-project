; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 4
; RUN: llc < %s | FileCheck %s

target triple = "aarch64-unknown-linux"

; Byte elements, UF=0
define void @f0(ptr noalias %dst, ptr readonly %src, i32 %n) #0 {
; CHECK-LABEL: f0:
; CHECK:       // %bb.0: // %entry
; CHECK-NEXT:    cmp w2, #1
; CHECK-NEXT:    b.lt .LBB0_3
; CHECK-NEXT:  // %bb.1: // %for.body.preheader
; CHECK-NEXT:    mov w9, w2
; CHECK-NEXT:    rdvl x10, #1
; CHECK-NEXT:    mov x8, xzr
; CHECK-NEXT:    whilelo p0.b, xzr, x9
; CHECK-NEXT:    .p2align 5, , 16
; CHECK-NEXT:  .LBB0_2: // %vector.body
; CHECK-NEXT:    // =>This Inner Loop Header: Depth=1
; CHECK-NEXT:    ld1b { z0.b }, p0/z, [x1, x8]
; CHECK-NEXT:    mul z0.b, z0.b, #3
; CHECK-NEXT:    st1b { z0.b }, p0, [x0, x8]
; CHECK-NEXT:    add x8, x8, x10
; CHECK-NEXT:    whilelo p0.b, x8, x9
; CHECK-NEXT:    b.mi .LBB0_2
; CHECK-NEXT:  .LBB0_3: // %for.cond.cleanup
; CHECK-NEXT:    ret
entry:
  %cmp7 = icmp sgt i32 %n, 0
  br i1 %cmp7, label %for.body.preheader, label %for.cond.cleanup

for.body.preheader:
  %wide.trip.count = zext nneg i32 %n to i64
  %0 = tail call i64 @llvm.vscale.i64()
  %1 = shl nuw nsw i64 %0, 4
  %active.lane.mask.entry = tail call <vscale x 16 x i1> @llvm.get.active.lane.mask.nxv16i1.i64(i64 0, i64 %wide.trip.count)
  br label %vector.body

vector.body:
  %index = phi i64 [ 0, %for.body.preheader ], [ %index.next, %vector.body ]
  %active.lane.mask = phi <vscale x 16 x i1> [ %active.lane.mask.entry, %for.body.preheader ], [ %active.lane.mask.next, %vector.body ]
  %2 = getelementptr inbounds i8, ptr %src, i64 %index
  %wide.masked.load = tail call <vscale x 16 x i8> @llvm.masked.load.nxv16i8.p0(ptr %2, i32 1, <vscale x 16 x i1> %active.lane.mask, <vscale x 16 x i8> poison)
  %3 = mul <vscale x 16 x i8> %wide.masked.load, shufflevector (<vscale x 16 x i8> insertelement (<vscale x 16 x i8> poison, i8 3, i64 0), <vscale x 16 x i8> poison, <vscale x 16 x i32> zeroinitializer)
  %4 = getelementptr inbounds i8, ptr %dst, i64 %index
  tail call void @llvm.masked.store.nxv16i8.p0(<vscale x 16 x i8> %3, ptr %4, i32 1, <vscale x 16 x i1> %active.lane.mask)
  %index.next = add i64 %index, %1
  %active.lane.mask.next = tail call <vscale x 16 x i1> @llvm.get.active.lane.mask.nxv16i1.i64(i64 %index.next, i64 %wide.trip.count)
  %5 = extractelement <vscale x 16 x i1> %active.lane.mask.next, i64 0
  br i1 %5, label %vector.body, label %for.cond.cleanup

for.cond.cleanup:
  ret void
}

; Double-precision float elements, UF=0
define void @f1(ptr noalias %dst, ptr readonly %src, i32 %n) #0 {
; CHECK-LABEL: f1:
; CHECK:       // %bb.0: // %entry
; CHECK-NEXT:    cmp w2, #1
; CHECK-NEXT:    b.lt .LBB1_3
; CHECK-NEXT:  // %bb.1: // %for.body.preheader
; CHECK-NEXT:    mov w9, w2
; CHECK-NEXT:    fmov z0.d, #3.00000000
; CHECK-NEXT:    cntd x10
; CHECK-NEXT:    mov x8, xzr
; CHECK-NEXT:    whilelo p0.d, xzr, x9
; CHECK-NEXT:    .p2align 5, , 16
; CHECK-NEXT:  .LBB1_2: // %vector.body
; CHECK-NEXT:    // =>This Inner Loop Header: Depth=1
; CHECK-NEXT:    ld1d { z1.d }, p0/z, [x1, x8, lsl #3]
; CHECK-NEXT:    fmul z1.d, z1.d, z0.d
; CHECK-NEXT:    st1d { z1.d }, p0, [x0, x8, lsl #3]
; CHECK-NEXT:    add x8, x8, x10
; CHECK-NEXT:    whilelo p0.d, x8, x9
; CHECK-NEXT:    b.mi .LBB1_2
; CHECK-NEXT:  .LBB1_3: // %for.cond.cleanup
; CHECK-NEXT:    ret
entry:
  %cmp6 = icmp sgt i32 %n, 0
  br i1 %cmp6, label %for.body.preheader, label %for.cond.cleanup

for.body.preheader:
  %wide.trip.count = zext nneg i32 %n to i64
  %0 = tail call i64 @llvm.vscale.i64()
  %1 = shl nuw nsw i64 %0, 1
  %active.lane.mask.entry = tail call <vscale x 2 x i1> @llvm.get.active.lane.mask.nxv2i1.i64(i64 0, i64 %wide.trip.count)
  br label %vector.body

vector.body:
  %index = phi i64 [ 0, %for.body.preheader ], [ %index.next, %vector.body ]
  %active.lane.mask = phi <vscale x 2 x i1> [ %active.lane.mask.entry, %for.body.preheader ], [ %active.lane.mask.next, %vector.body ]
  %2 = getelementptr inbounds double, ptr %src, i64 %index
  %wide.masked.load = tail call <vscale x 2 x double> @llvm.masked.load.nxv2f64.p0(ptr %2, i32 8, <vscale x 2 x i1> %active.lane.mask, <vscale x 2 x double> poison)
  %3 = fmul <vscale x 2 x double> %wide.masked.load, shufflevector (<vscale x 2 x double> insertelement (<vscale x 2 x double> poison, double 3.000000e+00, i64 0), <vscale x 2 x double> poison, <vscale x 2 x i32> zeroinitializer)
  %4 = getelementptr inbounds double, ptr %dst, i64 %index
  tail call void @llvm.masked.store.nxv2f64.p0(<vscale x 2 x double> %3, ptr %4, i32 8, <vscale x 2 x i1> %active.lane.mask)
  %index.next = add i64 %index, %1
  %active.lane.mask.next = tail call <vscale x 2 x i1> @llvm.get.active.lane.mask.nxv2i1.i64(i64 %index.next, i64 %wide.trip.count)
  %5 = extractelement <vscale x 2 x i1> %active.lane.mask.next, i64 0
  br i1 %5, label %vector.body, label %for.cond.cleanup

for.cond.cleanup:
  ret void
}

; Byte elements, UF=2
define void @f2(ptr noalias %dst, ptr readonly %src, i32 %n) #0 {
; CHECK-LABEL: f2:
; CHECK:       // %bb.0: // %entry
; CHECK-NEXT:    cmp w2, #1
; CHECK-NEXT:    b.lt .LBB2_3
; CHECK-NEXT:  // %bb.1: // %for.body.preheader
; CHECK-NEXT:    rdvl x11, #1
; CHECK-NEXT:    mov w8, w2
; CHECK-NEXT:    mov x9, xzr
; CHECK-NEXT:    add x10, x0, x11
; CHECK-NEXT:    whilelo p0.b, x11, x8
; CHECK-NEXT:    add x11, x1, x11
; CHECK-NEXT:    rdvl x12, #2
; CHECK-NEXT:    rdvl x13, #3
; CHECK-NEXT:    whilelo p1.b, xzr, x8
; CHECK-NEXT:    .p2align 5, , 16
; CHECK-NEXT:  .LBB2_2: // %vector.body
; CHECK-NEXT:    // =>This Inner Loop Header: Depth=1
; CHECK-NEXT:    ld1b { z0.b }, p1/z, [x1, x9]
; CHECK-NEXT:    ld1b { z1.b }, p0/z, [x11, x9]
; CHECK-NEXT:    add x14, x13, x9
; CHECK-NEXT:    mul z0.b, z0.b, #3
; CHECK-NEXT:    mul z1.b, z1.b, #3
; CHECK-NEXT:    st1b { z0.b }, p1, [x0, x9]
; CHECK-NEXT:    st1b { z1.b }, p0, [x10, x9]
; CHECK-NEXT:    add x9, x12, x9
; CHECK-NEXT:    whilelo p0.b, x14, x8
; CHECK-NEXT:    whilelo p1.b, x9, x8
; CHECK-NEXT:    b.mi .LBB2_2
; CHECK-NEXT:  .LBB2_3: // %for.cond.cleanup
; CHECK-NEXT:    ret
entry:
  %cmp7 = icmp sgt i32 %n, 0
  br i1 %cmp7, label %for.body.preheader, label %for.cond.cleanup

for.body.preheader:
  %wide.trip.count = zext nneg i32 %n to i64
  %0 = tail call i64 @llvm.vscale.i64()
  %1 = shl nuw nsw i64 %0, 5
  %2 = tail call i64 @llvm.vscale.i64()
  %3 = shl nuw nsw i64 %2, 4
  %active.lane.mask.entry = tail call <vscale x 16 x i1> @llvm.get.active.lane.mask.nxv16i1.i64(i64 %3, i64 %wide.trip.count)
  %active.lane.mask.entry1 = tail call <vscale x 16 x i1> @llvm.get.active.lane.mask.nxv16i1.i64(i64 0, i64 %wide.trip.count)
  %4 = tail call i64 @llvm.vscale.i64()
  %5 = shl nuw nsw i64 %4, 4
  %6 = tail call i64 @llvm.vscale.i64()
  %7 = shl nuw nsw i64 %6, 4
  %8 = tail call i64 @llvm.vscale.i64()
  %9 = shl nuw nsw i64 %8, 4
  br label %vector.body

vector.body:
  %index = phi i64 [ 0, %for.body.preheader ], [ %index.next4, %vector.body ]
  %active.lane.mask = phi <vscale x 16 x i1> [ %active.lane.mask.entry1, %for.body.preheader ], [ %active.lane.mask.next5, %vector.body ]
  %active.lane.mask2 = phi <vscale x 16 x i1> [ %active.lane.mask.entry, %for.body.preheader ], [ %active.lane.mask.next, %vector.body ]
  %10 = getelementptr inbounds i8, ptr %src, i64 %index
  %wide.masked.load = tail call <vscale x 16 x i8> @llvm.masked.load.nxv16i8.p0(ptr %10, i32 1, <vscale x 16 x i1> %active.lane.mask, <vscale x 16 x i8> poison)
  %11 = getelementptr inbounds i8, ptr %10, i64 %5
  %wide.masked.load3 = tail call <vscale x 16 x i8> @llvm.masked.load.nxv16i8.p0(ptr nonnull %11, i32 1, <vscale x 16 x i1> %active.lane.mask2, <vscale x 16 x i8> poison)
  %12 = mul <vscale x 16 x i8> %wide.masked.load, shufflevector (<vscale x 16 x i8> insertelement (<vscale x 16 x i8> poison, i8 3, i64 0), <vscale x 16 x i8> poison, <vscale x 16 x i32> zeroinitializer)
  %13 = mul <vscale x 16 x i8> %wide.masked.load3, shufflevector (<vscale x 16 x i8> insertelement (<vscale x 16 x i8> poison, i8 3, i64 0), <vscale x 16 x i8> poison, <vscale x 16 x i32> zeroinitializer)
  %14 = getelementptr inbounds i8, ptr %dst, i64 %index
  tail call void @llvm.masked.store.nxv16i8.p0(<vscale x 16 x i8> %12, ptr %14, i32 1, <vscale x 16 x i1> %active.lane.mask)
  %15 = getelementptr inbounds i8, ptr %14, i64 %7
  tail call void @llvm.masked.store.nxv16i8.p0(<vscale x 16 x i8> %13, ptr %15, i32 1, <vscale x 16 x i1> %active.lane.mask2)
  %index.next = add i64 %index, %1
  %index.next4 = add i64 %index, %1
  %16 = add i64 %index.next, %9
  %active.lane.mask.next = tail call <vscale x 16 x i1> @llvm.get.active.lane.mask.nxv16i1.i64(i64 %16, i64 %wide.trip.count)
  %active.lane.mask.next5 = tail call <vscale x 16 x i1> @llvm.get.active.lane.mask.nxv16i1.i64(i64 %index.next, i64 %wide.trip.count)
  %17 = extractelement <vscale x 16 x i1> %active.lane.mask.next5, i64 0
  br i1 %17, label %vector.body, label %for.cond.cleanup

for.cond.cleanup:
  ret void
}

; Double-precision float elements, UF=2
define void @f3(ptr noalias %dst, ptr readonly %src, i32 %n) #0 {
; CHECK-LABEL: f3:
; CHECK:       // %bb.0: // %entry
; CHECK-NEXT:    cmp w2, #1
; CHECK-NEXT:    b.lt .LBB3_3
; CHECK-NEXT:  // %bb.1: // %for.body.preheader
; CHECK-NEXT:    mov w9, w2
; CHECK-NEXT:    rdvl x11, #1
; CHECK-NEXT:    fmov z0.d, #3.00000000
; CHECK-NEXT:    mov x8, xzr
; CHECK-NEXT:    add x10, x0, x11
; CHECK-NEXT:    whilelo p1.s, xzr, x9
; CHECK-NEXT:    add x11, x1, x11
; CHECK-NEXT:    cntw x12
; CHECK-NEXT:    punpkhi p0.h, p1.b
; CHECK-NEXT:    punpklo p1.h, p1.b
; CHECK-NEXT:    .p2align 5, , 16
; CHECK-NEXT:  .LBB3_2: // %vector.body
; CHECK-NEXT:    // =>This Inner Loop Header: Depth=1
; CHECK-NEXT:    ld1d { z1.d }, p1/z, [x1, x8, lsl #3]
; CHECK-NEXT:    ld1d { z2.d }, p0/z, [x11, x8, lsl #3]
; CHECK-NEXT:    fmul z1.d, z1.d, z0.d
; CHECK-NEXT:    fmul z2.d, z2.d, z0.d
; CHECK-NEXT:    st1d { z1.d }, p1, [x0, x8, lsl #3]
; CHECK-NEXT:    st1d { z2.d }, p0, [x10, x8, lsl #3]
; CHECK-NEXT:    add x8, x12, x8
; CHECK-NEXT:    whilelo p1.s, x8, x9
; CHECK-NEXT:    punpkhi p0.h, p1.b
; CHECK-NEXT:    punpklo p1.h, p1.b
; CHECK-NEXT:    b.mi .LBB3_2
; CHECK-NEXT:  .LBB3_3: // %for.cond.cleanup
; CHECK-NEXT:    ret
entry:
  %cmp6 = icmp sgt i32 %n, 0
  br i1 %cmp6, label %for.body.preheader, label %for.cond.cleanup

for.body.preheader:
  %wide.trip.count = zext nneg i32 %n to i64
  %0 = tail call i64 @llvm.vscale.i64()
  %1 = shl nuw nsw i64 %0, 2
  %active.lane.mask.entry = tail call <vscale x 4 x i1> @llvm.get.active.lane.mask.nxv4i1.i64(i64 0, i64 %wide.trip.count)
  %active.lane.mask.entry1 = tail call <vscale x 2 x i1> @llvm.vector.extract.nxv2i1.nxv4i1(<vscale x 4 x i1> %active.lane.mask.entry, i64 2)
  %active.lane.mask.entry2 = tail call <vscale x 2 x i1> @llvm.vector.extract.nxv2i1.nxv4i1(<vscale x 4 x i1> %active.lane.mask.entry, i64 0)
  %2 = tail call i64 @llvm.vscale.i64()
  %3 = shl nuw nsw i64 %2, 1
  %4 = tail call i64 @llvm.vscale.i64()
  %5 = shl nuw nsw i64 %4, 1
  br label %vector.body

vector.body:
  %index = phi i64 [ 0, %for.body.preheader ], [ %index.next5, %vector.body ]
  %active.lane.mask = phi <vscale x 2 x i1> [ %active.lane.mask.entry2, %for.body.preheader ], [ %active.lane.mask.next7, %vector.body ]
  %active.lane.mask3 = phi <vscale x 2 x i1> [ %active.lane.mask.entry1, %for.body.preheader ], [ %active.lane.mask.next6, %vector.body ]
  %6 = getelementptr inbounds double, ptr %src, i64 %index
  %wide.masked.load = tail call <vscale x 2 x double> @llvm.masked.load.nxv2f64.p0(ptr %6, i32 8, <vscale x 2 x i1> %active.lane.mask, <vscale x 2 x double> poison)
  %7 = getelementptr inbounds double, ptr %6, i64 %3
  %wide.masked.load4 = tail call <vscale x 2 x double> @llvm.masked.load.nxv2f64.p0(ptr nonnull %7, i32 8, <vscale x 2 x i1> %active.lane.mask3, <vscale x 2 x double> poison)
  %8 = fmul <vscale x 2 x double> %wide.masked.load, shufflevector (<vscale x 2 x double> insertelement (<vscale x 2 x double> poison, double 3.000000e+00, i64 0), <vscale x 2 x double> poison, <vscale x 2 x i32> zeroinitializer)
  %9 = fmul <vscale x 2 x double> %wide.masked.load4, shufflevector (<vscale x 2 x double> insertelement (<vscale x 2 x double> poison, double 3.000000e+00, i64 0), <vscale x 2 x double> poison, <vscale x 2 x i32> zeroinitializer)
  %10 = getelementptr inbounds double, ptr %dst, i64 %index
  tail call void @llvm.masked.store.nxv2f64.p0(<vscale x 2 x double> %8, ptr %10, i32 8, <vscale x 2 x i1> %active.lane.mask)
  %11 = getelementptr inbounds double, ptr %10, i64 %5
  tail call void @llvm.masked.store.nxv2f64.p0(<vscale x 2 x double> %9, ptr %11, i32 8, <vscale x 2 x i1> %active.lane.mask3)
  %index.next = add i64 %index, %1
  %index.next5 = add i64 %index, %1
  %active.lane.mask.next = tail call <vscale x 4 x i1> @llvm.get.active.lane.mask.nxv4i1.i64(i64 %index.next, i64 %wide.trip.count)
  %active.lane.mask.next6 = tail call <vscale x 2 x i1> @llvm.vector.extract.nxv2i1.nxv4i1(<vscale x 4 x i1> %active.lane.mask.next, i64 2)
  %active.lane.mask.next7 = tail call <vscale x 2 x i1> @llvm.vector.extract.nxv2i1.nxv4i1(<vscale x 4 x i1> %active.lane.mask.next, i64 0)
  %12 = extractelement <vscale x 2 x i1> %active.lane.mask.next7, i64 0
  br i1 %12, label %vector.body, label %for.cond.cleanup

for.cond.cleanup:
  ret void
}

; Byte elements, UF=4
define void @f4(ptr noalias %dst, ptr readonly %src, i32 %n) #0 {
; CHECK-LABEL: f4:
; CHECK:       // %bb.0: // %entry
; CHECK-NEXT:    cmp w2, #1
; CHECK-NEXT:    b.lt .LBB4_3
; CHECK-NEXT:  // %bb.1: // %for.body.preheader
; CHECK-NEXT:    rdvl x14, #2
; CHECK-NEXT:    mov w8, w2
; CHECK-NEXT:    mov x9, xzr
; CHECK-NEXT:    add x11, x0, x14
; CHECK-NEXT:    rdvl x13, #3
; CHECK-NEXT:    rdvl x15, #1
; CHECK-NEXT:    add x10, x0, x13
; CHECK-NEXT:    add x12, x0, x15
; CHECK-NEXT:    rdvl x16, #4
; CHECK-NEXT:    rdvl x17, #5
; CHECK-NEXT:    rdvl x18, #6
; CHECK-NEXT:    rdvl x2, #7
; CHECK-NEXT:    whilelo p0.b, x13, x8
; CHECK-NEXT:    add x13, x1, x13
; CHECK-NEXT:    whilelo p1.b, x14, x8
; CHECK-NEXT:    add x14, x1, x14
; CHECK-NEXT:    whilelo p2.b, x15, x8
; CHECK-NEXT:    add x15, x1, x15
; CHECK-NEXT:    whilelo p3.b, xzr, x8
; CHECK-NEXT:    .p2align 5, , 16
; CHECK-NEXT:  .LBB4_2: // %vector.body
; CHECK-NEXT:    // =>This Inner Loop Header: Depth=1
; CHECK-NEXT:    ld1b { z0.b }, p3/z, [x1, x9]
; CHECK-NEXT:    ld1b { z1.b }, p2/z, [x15, x9]
; CHECK-NEXT:    add x3, x2, x9
; CHECK-NEXT:    mul z0.b, z0.b, #3
; CHECK-NEXT:    ld1b { z2.b }, p1/z, [x14, x9]
; CHECK-NEXT:    ld1b { z3.b }, p0/z, [x13, x9]
; CHECK-NEXT:    mul z1.b, z1.b, #3
; CHECK-NEXT:    mul z2.b, z2.b, #3
; CHECK-NEXT:    mul z3.b, z3.b, #3
; CHECK-NEXT:    st1b { z0.b }, p3, [x0, x9]
; CHECK-NEXT:    st1b { z1.b }, p2, [x12, x9]
; CHECK-NEXT:    st1b { z2.b }, p1, [x11, x9]
; CHECK-NEXT:    st1b { z3.b }, p0, [x10, x9]
; CHECK-NEXT:    whilelo p0.b, x3, x8
; CHECK-NEXT:    add x3, x18, x9
; CHECK-NEXT:    whilelo p1.b, x3, x8
; CHECK-NEXT:    add x3, x17, x9
; CHECK-NEXT:    add x9, x16, x9
; CHECK-NEXT:    whilelo p2.b, x3, x8
; CHECK-NEXT:    whilelo p3.b, x9, x8
; CHECK-NEXT:    b.mi .LBB4_2
; CHECK-NEXT:  .LBB4_3: // %for.cond.cleanup
; CHECK-NEXT:    ret
entry:
  %cmp7 = icmp sgt i32 %n, 0
  br i1 %cmp7, label %for.body.preheader, label %for.cond.cleanup

for.body.preheader:
  %wide.trip.count = zext nneg i32 %n to i64
  %0 = tail call i64 @llvm.vscale.i64()
  %1 = shl nuw nsw i64 %0, 6
  %2 = tail call i64 @llvm.vscale.i64()
  %3 = shl nuw nsw i64 %2, 4
  %4 = tail call i64 @llvm.vscale.i64()
  %5 = shl nuw nsw i64 %4, 5
  %6 = tail call i64 @llvm.vscale.i64()
  %7 = mul nuw nsw i64 %6, 48
  %active.lane.mask.entry = tail call <vscale x 16 x i1> @llvm.get.active.lane.mask.nxv16i1.i64(i64 %7, i64 %wide.trip.count)
  %active.lane.mask.entry3 = tail call <vscale x 16 x i1> @llvm.get.active.lane.mask.nxv16i1.i64(i64 %5, i64 %wide.trip.count)
  %active.lane.mask.entry4 = tail call <vscale x 16 x i1> @llvm.get.active.lane.mask.nxv16i1.i64(i64 %3, i64 %wide.trip.count)
  %active.lane.mask.entry5 = tail call <vscale x 16 x i1> @llvm.get.active.lane.mask.nxv16i1.i64(i64 0, i64 %wide.trip.count)
  %8 = tail call i64 @llvm.vscale.i64()
  %9 = shl nuw nsw i64 %8, 4
  %10 = tail call i64 @llvm.vscale.i64()
  %11 = shl nuw nsw i64 %10, 5
  %12 = tail call i64 @llvm.vscale.i64()
  %13 = mul nuw nsw i64 %12, 48
  %14 = tail call i64 @llvm.vscale.i64()
  %15 = shl nuw nsw i64 %14, 4
  %16 = tail call i64 @llvm.vscale.i64()
  %17 = shl nuw nsw i64 %16, 5
  %18 = tail call i64 @llvm.vscale.i64()
  %19 = mul nuw nsw i64 %18, 48
  %20 = tail call i64 @llvm.vscale.i64()
  %21 = shl nuw nsw i64 %20, 4
  %22 = tail call i64 @llvm.vscale.i64()
  %23 = shl nuw nsw i64 %22, 5
  %24 = tail call i64 @llvm.vscale.i64()
  %25 = mul nuw nsw i64 %24, 48
  br label %vector.body

vector.body:
  %index = phi i64 [ 0, %for.body.preheader ], [ %index.next14, %vector.body ]
  %active.lane.mask = phi <vscale x 16 x i1> [ %active.lane.mask.entry5, %for.body.preheader ], [ %active.lane.mask.next17, %vector.body ]
  %active.lane.mask6 = phi <vscale x 16 x i1> [ %active.lane.mask.entry4, %for.body.preheader ], [ %active.lane.mask.next16, %vector.body ]
  %active.lane.mask7 = phi <vscale x 16 x i1> [ %active.lane.mask.entry3, %for.body.preheader ], [ %active.lane.mask.next15, %vector.body ]
  %active.lane.mask8 = phi <vscale x 16 x i1> [ %active.lane.mask.entry, %for.body.preheader ], [ %active.lane.mask.next, %vector.body ]
  %26 = getelementptr inbounds i8, ptr %src, i64 %index
  %wide.masked.load = tail call <vscale x 16 x i8> @llvm.masked.load.nxv16i8.p0(ptr %26, i32 1, <vscale x 16 x i1> %active.lane.mask, <vscale x 16 x i8> poison)
  %27 = getelementptr inbounds i8, ptr %26, i64 %9
  %wide.masked.load9 = tail call <vscale x 16 x i8> @llvm.masked.load.nxv16i8.p0(ptr nonnull %27, i32 1, <vscale x 16 x i1> %active.lane.mask6, <vscale x 16 x i8> poison)
  %28 = getelementptr inbounds i8, ptr %26, i64 %11
  %wide.masked.load10 = tail call <vscale x 16 x i8> @llvm.masked.load.nxv16i8.p0(ptr nonnull %28, i32 1, <vscale x 16 x i1> %active.lane.mask7, <vscale x 16 x i8> poison)
  %29 = getelementptr inbounds i8, ptr %26, i64 %13
  %wide.masked.load11 = tail call <vscale x 16 x i8> @llvm.masked.load.nxv16i8.p0(ptr nonnull %29, i32 1, <vscale x 16 x i1> %active.lane.mask8, <vscale x 16 x i8> poison)
  %30 = mul <vscale x 16 x i8> %wide.masked.load, shufflevector (<vscale x 16 x i8> insertelement (<vscale x 16 x i8> poison, i8 3, i64 0), <vscale x 16 x i8> poison, <vscale x 16 x i32> zeroinitializer)
  %31 = mul <vscale x 16 x i8> %wide.masked.load9, shufflevector (<vscale x 16 x i8> insertelement (<vscale x 16 x i8> poison, i8 3, i64 0), <vscale x 16 x i8> poison, <vscale x 16 x i32> zeroinitializer)
  %32 = mul <vscale x 16 x i8> %wide.masked.load10, shufflevector (<vscale x 16 x i8> insertelement (<vscale x 16 x i8> poison, i8 3, i64 0), <vscale x 16 x i8> poison, <vscale x 16 x i32> zeroinitializer)
  %33 = mul <vscale x 16 x i8> %wide.masked.load11, shufflevector (<vscale x 16 x i8> insertelement (<vscale x 16 x i8> poison, i8 3, i64 0), <vscale x 16 x i8> poison, <vscale x 16 x i32> zeroinitializer)
  %34 = getelementptr inbounds i8, ptr %dst, i64 %index
  tail call void @llvm.masked.store.nxv16i8.p0(<vscale x 16 x i8> %30, ptr %34, i32 1, <vscale x 16 x i1> %active.lane.mask)
  %35 = getelementptr inbounds i8, ptr %34, i64 %15
  tail call void @llvm.masked.store.nxv16i8.p0(<vscale x 16 x i8> %31, ptr %35, i32 1, <vscale x 16 x i1> %active.lane.mask6)
  %36 = getelementptr inbounds i8, ptr %34, i64 %17
  tail call void @llvm.masked.store.nxv16i8.p0(<vscale x 16 x i8> %32, ptr %36, i32 1, <vscale x 16 x i1> %active.lane.mask7)
  %37 = getelementptr inbounds i8, ptr %34, i64 %19
  tail call void @llvm.masked.store.nxv16i8.p0(<vscale x 16 x i8> %33, ptr %37, i32 1, <vscale x 16 x i1> %active.lane.mask8)
  %index.next = add i64 %index, %1
  %index.next14 = add i64 %index, %1
  %38 = add i64 %index.next, %21
  %39 = add i64 %index.next, %23
  %40 = add i64 %index.next, %25
  %active.lane.mask.next = tail call <vscale x 16 x i1> @llvm.get.active.lane.mask.nxv16i1.i64(i64 %40, i64 %wide.trip.count)
  %active.lane.mask.next15 = tail call <vscale x 16 x i1> @llvm.get.active.lane.mask.nxv16i1.i64(i64 %39, i64 %wide.trip.count)
  %active.lane.mask.next16 = tail call <vscale x 16 x i1> @llvm.get.active.lane.mask.nxv16i1.i64(i64 %38, i64 %wide.trip.count)
  %active.lane.mask.next17 = tail call <vscale x 16 x i1> @llvm.get.active.lane.mask.nxv16i1.i64(i64 %index.next, i64 %wide.trip.count)
  %41 = extractelement <vscale x 16 x i1> %active.lane.mask.next17, i64 0
  br i1 %41, label %vector.body, label %for.cond.cleanup

for.cond.cleanup:
  ret void
}

; Double-precision float elements, UF=4
define void @f5(ptr noalias %dst, ptr readonly %src, i32 %n) #0 {
; CHECK-LABEL: f5:
; CHECK:       // %bb.0: // %entry
; CHECK-NEXT:    cmp w2, #1
; CHECK-NEXT:    b.lt .LBB5_3
; CHECK-NEXT:  // %bb.1: // %for.body.preheader
; CHECK-NEXT:    cntw x10
; CHECK-NEXT:    mov w8, w2
; CHECK-NEXT:    fmov z0.d, #3.00000000
; CHECK-NEXT:    mov x9, xzr
; CHECK-NEXT:    rdvl x13, #3
; CHECK-NEXT:    rdvl x14, #2
; CHECK-NEXT:    add x11, x0, x14
; CHECK-NEXT:    add x14, x1, x14
; CHECK-NEXT:    rdvl x15, #1
; CHECK-NEXT:    add x12, x0, x15
; CHECK-NEXT:    add x15, x1, x15
; CHECK-NEXT:    cnth x16
; CHECK-NEXT:    cntw x17, all, mul #3
; CHECK-NEXT:    whilelo p1.s, x10, x8
; CHECK-NEXT:    add x10, x0, x13
; CHECK-NEXT:    add x13, x1, x13
; CHECK-NEXT:    punpkhi p0.h, p1.b
; CHECK-NEXT:    punpklo p1.h, p1.b
; CHECK-NEXT:    whilelo p3.s, xzr, x8
; CHECK-NEXT:    punpkhi p2.h, p3.b
; CHECK-NEXT:    punpklo p3.h, p3.b
; CHECK-NEXT:    .p2align 5, , 16
; CHECK-NEXT:  .LBB5_2: // %vector.body
; CHECK-NEXT:    // =>This Inner Loop Header: Depth=1
; CHECK-NEXT:    ld1d { z1.d }, p3/z, [x1, x9, lsl #3]
; CHECK-NEXT:    ld1d { z2.d }, p2/z, [x15, x9, lsl #3]
; CHECK-NEXT:    add x18, x17, x9
; CHECK-NEXT:    fmul z1.d, z1.d, z0.d
; CHECK-NEXT:    ld1d { z3.d }, p1/z, [x14, x9, lsl #3]
; CHECK-NEXT:    ld1d { z4.d }, p0/z, [x13, x9, lsl #3]
; CHECK-NEXT:    fmul z2.d, z2.d, z0.d
; CHECK-NEXT:    fmul z3.d, z3.d, z0.d
; CHECK-NEXT:    st1d { z1.d }, p3, [x0, x9, lsl #3]
; CHECK-NEXT:    fmul z1.d, z4.d, z0.d
; CHECK-NEXT:    st1d { z2.d }, p2, [x12, x9, lsl #3]
; CHECK-NEXT:    st1d { z3.d }, p1, [x11, x9, lsl #3]
; CHECK-NEXT:    whilelo p3.s, x18, x8
; CHECK-NEXT:    st1d { z1.d }, p0, [x10, x9, lsl #3]
; CHECK-NEXT:    add x9, x16, x9
; CHECK-NEXT:    punpkhi p0.h, p3.b
; CHECK-NEXT:    punpklo p1.h, p3.b
; CHECK-NEXT:    whilelo p4.s, x9, x8
; CHECK-NEXT:    punpkhi p2.h, p4.b
; CHECK-NEXT:    punpklo p3.h, p4.b
; CHECK-NEXT:    b.mi .LBB5_2
; CHECK-NEXT:  .LBB5_3: // %for.cond.cleanup
; CHECK-NEXT:    ret
entry:
  %cmp6 = icmp sgt i32 %n, 0
  br i1 %cmp6, label %for.body.preheader, label %for.cond.cleanup

for.body.preheader:
  %wide.trip.count = zext nneg i32 %n to i64
  %0 = tail call i64 @llvm.vscale.i64()
  %1 = shl nuw nsw i64 %0, 3
  %2 = tail call i64 @llvm.vscale.i64()
  %3 = shl nuw nsw i64 %2, 2
  %active.lane.mask.entry = tail call <vscale x 4 x i1> @llvm.get.active.lane.mask.nxv4i1.i64(i64 %3, i64 %wide.trip.count)
  %active.lane.mask.entry3 = tail call <vscale x 4 x i1> @llvm.get.active.lane.mask.nxv4i1.i64(i64 0, i64 %wide.trip.count)
  %active.lane.mask.entry4 = tail call <vscale x 2 x i1> @llvm.vector.extract.nxv2i1.nxv4i1(<vscale x 4 x i1> %active.lane.mask.entry, i64 2)
  %active.lane.mask.entry5 = tail call <vscale x 2 x i1> @llvm.vector.extract.nxv2i1.nxv4i1(<vscale x 4 x i1> %active.lane.mask.entry, i64 0)
  %active.lane.mask.entry6 = tail call <vscale x 2 x i1> @llvm.vector.extract.nxv2i1.nxv4i1(<vscale x 4 x i1> %active.lane.mask.entry3, i64 2)
  %active.lane.mask.entry7 = tail call <vscale x 2 x i1> @llvm.vector.extract.nxv2i1.nxv4i1(<vscale x 4 x i1> %active.lane.mask.entry3, i64 0)
  %4 = tail call i64 @llvm.vscale.i64()
  %5 = shl nuw nsw i64 %4, 1
  %6 = tail call i64 @llvm.vscale.i64()
  %7 = shl nuw nsw i64 %6, 2
  %8 = tail call i64 @llvm.vscale.i64()
  %9 = mul nuw nsw i64 %8, 6
  %10 = tail call i64 @llvm.vscale.i64()
  %11 = shl nuw nsw i64 %10, 1
  %12 = tail call i64 @llvm.vscale.i64()
  %13 = shl nuw nsw i64 %12, 2
  %14 = tail call i64 @llvm.vscale.i64()
  %15 = mul nuw nsw i64 %14, 6
  %16 = tail call i64 @llvm.vscale.i64()
  %17 = shl nuw nsw i64 %16, 2
  br label %vector.body

vector.body:
  %index = phi i64 [ 0, %for.body.preheader ], [ %index.next16, %vector.body ]
  %active.lane.mask = phi <vscale x 2 x i1> [ %active.lane.mask.entry7, %for.body.preheader ], [ %active.lane.mask.next21, %vector.body ]
  %active.lane.mask8 = phi <vscale x 2 x i1> [ %active.lane.mask.entry6, %for.body.preheader ], [ %active.lane.mask.next20, %vector.body ]
  %active.lane.mask9 = phi <vscale x 2 x i1> [ %active.lane.mask.entry5, %for.body.preheader ], [ %active.lane.mask.next19, %vector.body ]
  %active.lane.mask10 = phi <vscale x 2 x i1> [ %active.lane.mask.entry4, %for.body.preheader ], [ %active.lane.mask.next18, %vector.body ]
  %18 = getelementptr inbounds double, ptr %src, i64 %index
  %wide.masked.load = tail call <vscale x 2 x double> @llvm.masked.load.nxv2f64.p0(ptr %18, i32 8, <vscale x 2 x i1> %active.lane.mask, <vscale x 2 x double> poison)
  %19 = getelementptr inbounds double, ptr %18, i64 %5
  %wide.masked.load11 = tail call <vscale x 2 x double> @llvm.masked.load.nxv2f64.p0(ptr nonnull %19, i32 8, <vscale x 2 x i1> %active.lane.mask8, <vscale x 2 x double> poison)
  %20 = getelementptr inbounds double, ptr %18, i64 %7
  %wide.masked.load12 = tail call <vscale x 2 x double> @llvm.masked.load.nxv2f64.p0(ptr nonnull %20, i32 8, <vscale x 2 x i1> %active.lane.mask9, <vscale x 2 x double> poison)
  %21 = getelementptr inbounds double, ptr %18, i64 %9
  %wide.masked.load13 = tail call <vscale x 2 x double> @llvm.masked.load.nxv2f64.p0(ptr nonnull %21, i32 8, <vscale x 2 x i1> %active.lane.mask10, <vscale x 2 x double> poison)
  %22 = fmul <vscale x 2 x double> %wide.masked.load, shufflevector (<vscale x 2 x double> insertelement (<vscale x 2 x double> poison, double 3.000000e+00, i64 0), <vscale x 2 x double> poison, <vscale x 2 x i32> zeroinitializer)
  %23 = fmul <vscale x 2 x double> %wide.masked.load11, shufflevector (<vscale x 2 x double> insertelement (<vscale x 2 x double> poison, double 3.000000e+00, i64 0), <vscale x 2 x double> poison, <vscale x 2 x i32> zeroinitializer)
  %24 = fmul <vscale x 2 x double> %wide.masked.load12, shufflevector (<vscale x 2 x double> insertelement (<vscale x 2 x double> poison, double 3.000000e+00, i64 0), <vscale x 2 x double> poison, <vscale x 2 x i32> zeroinitializer)
  %25 = fmul <vscale x 2 x double> %wide.masked.load13, shufflevector (<vscale x 2 x double> insertelement (<vscale x 2 x double> poison, double 3.000000e+00, i64 0), <vscale x 2 x double> poison, <vscale x 2 x i32> zeroinitializer)
  %26 = getelementptr inbounds double, ptr %dst, i64 %index
  tail call void @llvm.masked.store.nxv2f64.p0(<vscale x 2 x double> %22, ptr %26, i32 8, <vscale x 2 x i1> %active.lane.mask)
  %27 = getelementptr inbounds double, ptr %26, i64 %11
  tail call void @llvm.masked.store.nxv2f64.p0(<vscale x 2 x double> %23, ptr %27, i32 8, <vscale x 2 x i1> %active.lane.mask8)
  %28 = getelementptr inbounds double, ptr %26, i64 %13
  tail call void @llvm.masked.store.nxv2f64.p0(<vscale x 2 x double> %24, ptr %28, i32 8, <vscale x 2 x i1> %active.lane.mask9)
  %29 = getelementptr inbounds double, ptr %26, i64 %15
  tail call void @llvm.masked.store.nxv2f64.p0(<vscale x 2 x double> %25, ptr %29, i32 8, <vscale x 2 x i1> %active.lane.mask10)
  %index.next = add i64 %index, %1
  %index.next16 = add i64 %index, %1
  %30 = add i64 %index.next, %17
  %active.lane.mask.next = tail call <vscale x 4 x i1> @llvm.get.active.lane.mask.nxv4i1.i64(i64 %30, i64 %wide.trip.count)
  %active.lane.mask.next17 = tail call <vscale x 4 x i1> @llvm.get.active.lane.mask.nxv4i1.i64(i64 %index.next, i64 %wide.trip.count)
  %active.lane.mask.next18 = tail call <vscale x 2 x i1> @llvm.vector.extract.nxv2i1.nxv4i1(<vscale x 4 x i1> %active.lane.mask.next, i64 2)
  %active.lane.mask.next19 = tail call <vscale x 2 x i1> @llvm.vector.extract.nxv2i1.nxv4i1(<vscale x 4 x i1> %active.lane.mask.next, i64 0)
  %active.lane.mask.next20 = tail call <vscale x 2 x i1> @llvm.vector.extract.nxv2i1.nxv4i1(<vscale x 4 x i1> %active.lane.mask.next17, i64 2)
  %active.lane.mask.next21 = tail call <vscale x 2 x i1> @llvm.vector.extract.nxv2i1.nxv4i1(<vscale x 4 x i1> %active.lane.mask.next17, i64 0)
  %31 = extractelement <vscale x 2 x i1> %active.lane.mask.next21, i64 0
  br i1 %31, label %vector.body, label %for.cond.cleanup

for.cond.cleanup:
  ret void
}

; SVE 2.1, byte elements, UF=0
define void @f6(ptr noalias %dst, ptr readonly %src, i32 %n) #1 {
; CHECK-LABEL: f6:
; CHECK:       // %bb.0: // %entry
; CHECK-NEXT:    cmp w2, #1
; CHECK-NEXT:    b.lt .LBB6_3
; CHECK-NEXT:  // %bb.1: // %for.body.preheader
; CHECK-NEXT:    mov w9, w2
; CHECK-NEXT:    mov x8, xzr
; CHECK-NEXT:    whilelo p0.b, xzr, x9
; CHECK-NEXT:    .p2align 5, , 16
; CHECK-NEXT:  .LBB6_2: // %vector.body
; CHECK-NEXT:    // =>This Inner Loop Header: Depth=1
; CHECK-NEXT:    ld1b { z0.b }, p0/z, [x1, x8]
; CHECK-NEXT:    mul z0.b, z0.b, #3
; CHECK-NEXT:    st1b { z0.b }, p0, [x0, x8]
; CHECK-NEXT:    addvl x8, x8, #1
; CHECK-NEXT:    whilelo p0.b, x8, x9
; CHECK-NEXT:    b.mi .LBB6_2
; CHECK-NEXT:  .LBB6_3: // %for.cond.cleanup
; CHECK-NEXT:    ret
entry:
  %cmp7 = icmp sgt i32 %n, 0
  br i1 %cmp7, label %for.body.preheader, label %for.cond.cleanup

for.body.preheader:
  %wide.trip.count = zext nneg i32 %n to i64
  %0 = tail call i64 @llvm.vscale.i64()
  %1 = shl nuw nsw i64 %0, 4
  %active.lane.mask.entry = tail call <vscale x 16 x i1> @llvm.get.active.lane.mask.nxv16i1.i64(i64 0, i64 %wide.trip.count)
  br label %vector.body

vector.body:
  %index = phi i64 [ 0, %for.body.preheader ], [ %index.next, %vector.body ]
  %active.lane.mask = phi <vscale x 16 x i1> [ %active.lane.mask.entry, %for.body.preheader ], [ %active.lane.mask.next, %vector.body ]
  %2 = getelementptr inbounds i8, ptr %src, i64 %index
  %wide.masked.load = tail call <vscale x 16 x i8> @llvm.masked.load.nxv16i8.p0(ptr %2, i32 1, <vscale x 16 x i1> %active.lane.mask, <vscale x 16 x i8> poison)
  %3 = mul <vscale x 16 x i8> %wide.masked.load, shufflevector (<vscale x 16 x i8> insertelement (<vscale x 16 x i8> poison, i8 3, i64 0), <vscale x 16 x i8> poison, <vscale x 16 x i32> zeroinitializer)
  %4 = getelementptr inbounds i8, ptr %dst, i64 %index
  tail call void @llvm.masked.store.nxv16i8.p0(<vscale x 16 x i8> %3, ptr %4, i32 1, <vscale x 16 x i1> %active.lane.mask)
  %index.next = add i64 %index, %1
  %active.lane.mask.next = tail call <vscale x 16 x i1> @llvm.get.active.lane.mask.nxv16i1.i64(i64 %index.next, i64 %wide.trip.count)
  %5 = extractelement <vscale x 16 x i1> %active.lane.mask.next, i64 0
  br i1 %5, label %vector.body, label %for.cond.cleanup

for.cond.cleanup:
  ret void
}

; SVE2.1, double-precision float elements, UF=0
define void @f7(ptr noalias %dst, ptr readonly %src, i32 %n) #1 {
; CHECK-LABEL: f7:
; CHECK:       // %bb.0: // %entry
; CHECK-NEXT:    cmp w2, #1
; CHECK-NEXT:    b.lt .LBB7_3
; CHECK-NEXT:  // %bb.1: // %for.body.preheader
; CHECK-NEXT:    mov w9, w2
; CHECK-NEXT:    fmov z0.d, #3.00000000
; CHECK-NEXT:    mov x8, xzr
; CHECK-NEXT:    whilelo p0.d, xzr, x9
; CHECK-NEXT:    .p2align 5, , 16
; CHECK-NEXT:  .LBB7_2: // %vector.body
; CHECK-NEXT:    // =>This Inner Loop Header: Depth=1
; CHECK-NEXT:    ld1d { z1.d }, p0/z, [x1, x8, lsl #3]
; CHECK-NEXT:    fmul z1.d, z1.d, z0.d
; CHECK-NEXT:    st1d { z1.d }, p0, [x0, x8, lsl #3]
; CHECK-NEXT:    incd x8
; CHECK-NEXT:    whilelo p0.d, x8, x9
; CHECK-NEXT:    b.mi .LBB7_2
; CHECK-NEXT:  .LBB7_3: // %for.cond.cleanup
; CHECK-NEXT:    ret
entry:
  %cmp6 = icmp sgt i32 %n, 0
  br i1 %cmp6, label %for.body.preheader, label %for.cond.cleanup

for.body.preheader:
  %wide.trip.count = zext nneg i32 %n to i64
  %0 = tail call i64 @llvm.vscale.i64()
  %1 = shl nuw nsw i64 %0, 1
  %active.lane.mask.entry = tail call <vscale x 2 x i1> @llvm.get.active.lane.mask.nxv2i1.i64(i64 0, i64 %wide.trip.count)
  br label %vector.body

vector.body:
  %index = phi i64 [ 0, %for.body.preheader ], [ %index.next, %vector.body ]
  %active.lane.mask = phi <vscale x 2 x i1> [ %active.lane.mask.entry, %for.body.preheader ], [ %active.lane.mask.next, %vector.body ]
  %2 = getelementptr inbounds double, ptr %src, i64 %index
  %wide.masked.load = tail call <vscale x 2 x double> @llvm.masked.load.nxv2f64.p0(ptr %2, i32 8, <vscale x 2 x i1> %active.lane.mask, <vscale x 2 x double> poison)
  %3 = fmul <vscale x 2 x double> %wide.masked.load, shufflevector (<vscale x 2 x double> insertelement (<vscale x 2 x double> poison, double 3.000000e+00, i64 0), <vscale x 2 x double> poison, <vscale x 2 x i32> zeroinitializer)
  %4 = getelementptr inbounds double, ptr %dst, i64 %index
  tail call void @llvm.masked.store.nxv2f64.p0(<vscale x 2 x double> %3, ptr %4, i32 8, <vscale x 2 x i1> %active.lane.mask)
  %index.next = add i64 %index, %1
  %active.lane.mask.next = tail call <vscale x 2 x i1> @llvm.get.active.lane.mask.nxv2i1.i64(i64 %index.next, i64 %wide.trip.count)
  %5 = extractelement <vscale x 2 x i1> %active.lane.mask.next, i64 0
  br i1 %5, label %vector.body, label %for.cond.cleanup

for.cond.cleanup:
  ret void
}

; SVE 2.1, byte elements, UF=2
define void @f8(ptr noalias %dst, ptr readonly %src, i32 %n) #1 {
; CHECK-LABEL: f8:
; CHECK:       // %bb.0: // %entry
; CHECK-NEXT:    cmp w2, #1
; CHECK-NEXT:    b.lt .LBB8_3
; CHECK-NEXT:  // %bb.1: // %for.body.preheader
; CHECK-NEXT:    mov w9, w2
; CHECK-NEXT:    addvl x10, x0, #1
; CHECK-NEXT:    mov x8, xzr
; CHECK-NEXT:    whilelo { p0.b, p1.b }, xzr, x9
; CHECK-NEXT:    addvl x11, x1, #1
; CHECK-NEXT:    .p2align 5, , 16
; CHECK-NEXT:  .LBB8_2: // %vector.body
; CHECK-NEXT:    // =>This Inner Loop Header: Depth=1
; CHECK-NEXT:    ld1b { z0.b }, p0/z, [x1, x8]
; CHECK-NEXT:    ld1b { z1.b }, p1/z, [x11, x8]
; CHECK-NEXT:    mul z0.b, z0.b, #3
; CHECK-NEXT:    mul z1.b, z1.b, #3
; CHECK-NEXT:    st1b { z0.b }, p0, [x0, x8]
; CHECK-NEXT:    st1b { z1.b }, p1, [x10, x8]
; CHECK-NEXT:    addvl x8, x8, #2
; CHECK-NEXT:    whilelo { p0.b, p1.b }, x8, x9
; CHECK-NEXT:    b.mi .LBB8_2
; CHECK-NEXT:  .LBB8_3: // %for.cond.cleanup
; CHECK-NEXT:    ret
entry:
  %cmp7 = icmp sgt i32 %n, 0
  br i1 %cmp7, label %for.body.preheader, label %for.cond.cleanup

for.body.preheader:
  %wide.trip.count = zext nneg i32 %n to i64
  %0 = tail call i64 @llvm.vscale.i64()
  %1 = shl nuw nsw i64 %0, 5
  %active.lane.mask.entry = tail call <vscale x 32 x i1> @llvm.get.active.lane.mask.nxv32i1.i64(i64 0, i64 %wide.trip.count)
  %active.lane.mask.entry1 = tail call <vscale x 16 x i1> @llvm.vector.extract.nxv16i1.nxv32i1(<vscale x 32 x i1> %active.lane.mask.entry, i64 16)
  %active.lane.mask.entry2 = tail call <vscale x 16 x i1> @llvm.vector.extract.nxv16i1.nxv32i1(<vscale x 32 x i1> %active.lane.mask.entry, i64 0)
  %2 = tail call i64 @llvm.vscale.i64()
  %3 = shl nuw nsw i64 %2, 4
  %4 = tail call i64 @llvm.vscale.i64()
  %5 = shl nuw nsw i64 %4, 4
  br label %vector.body

vector.body:
  %index = phi i64 [ 0, %for.body.preheader ], [ %index.next5, %vector.body ]
  %active.lane.mask = phi <vscale x 16 x i1> [ %active.lane.mask.entry2, %for.body.preheader ], [ %active.lane.mask.next7, %vector.body ]
  %active.lane.mask3 = phi <vscale x 16 x i1> [ %active.lane.mask.entry1, %for.body.preheader ], [ %active.lane.mask.next6, %vector.body ]
  %6 = getelementptr inbounds i8, ptr %src, i64 %index
  %wide.masked.load = tail call <vscale x 16 x i8> @llvm.masked.load.nxv16i8.p0(ptr %6, i32 1, <vscale x 16 x i1> %active.lane.mask, <vscale x 16 x i8> poison)
  %7 = getelementptr inbounds i8, ptr %6, i64 %3
  %wide.masked.load4 = tail call <vscale x 16 x i8> @llvm.masked.load.nxv16i8.p0(ptr nonnull %7, i32 1, <vscale x 16 x i1> %active.lane.mask3, <vscale x 16 x i8> poison)
  %8 = mul <vscale x 16 x i8> %wide.masked.load, shufflevector (<vscale x 16 x i8> insertelement (<vscale x 16 x i8> poison, i8 3, i64 0), <vscale x 16 x i8> poison, <vscale x 16 x i32> zeroinitializer)
  %9 = mul <vscale x 16 x i8> %wide.masked.load4, shufflevector (<vscale x 16 x i8> insertelement (<vscale x 16 x i8> poison, i8 3, i64 0), <vscale x 16 x i8> poison, <vscale x 16 x i32> zeroinitializer)
  %10 = getelementptr inbounds i8, ptr %dst, i64 %index
  tail call void @llvm.masked.store.nxv16i8.p0(<vscale x 16 x i8> %8, ptr %10, i32 1, <vscale x 16 x i1> %active.lane.mask)
  %11 = getelementptr inbounds i8, ptr %10, i64 %5
  tail call void @llvm.masked.store.nxv16i8.p0(<vscale x 16 x i8> %9, ptr %11, i32 1, <vscale x 16 x i1> %active.lane.mask3)
  %index.next = add i64 %index, %1
  %index.next5 = add i64 %index, %1
  %active.lane.mask.next = tail call <vscale x 32 x i1> @llvm.get.active.lane.mask.nxv32i1.i64(i64 %index.next, i64 %wide.trip.count)
  %active.lane.mask.next6 = tail call <vscale x 16 x i1> @llvm.vector.extract.nxv16i1.nxv32i1(<vscale x 32 x i1> %active.lane.mask.next, i64 16)
  %active.lane.mask.next7 = tail call <vscale x 16 x i1> @llvm.vector.extract.nxv16i1.nxv32i1(<vscale x 32 x i1> %active.lane.mask.next, i64 0)
  %12 = extractelement <vscale x 16 x i1> %active.lane.mask.next7, i64 0
  br i1 %12, label %vector.body, label %for.cond.cleanup

for.cond.cleanup:
  ret void
}

; SVE2.1, double-precision float elements, UF=2
define void @f9(ptr noalias %dst, ptr readonly %src, i32 %n) #1 {
; CHECK-LABEL: f9:
; CHECK:       // %bb.0: // %entry
; CHECK-NEXT:    cmp w2, #1
; CHECK-NEXT:    b.lt .LBB9_3
; CHECK-NEXT:  // %bb.1: // %for.body.preheader
; CHECK-NEXT:    mov w9, w2
; CHECK-NEXT:    addvl x10, x0, #1
; CHECK-NEXT:    fmov z0.d, #3.00000000
; CHECK-NEXT:    mov x8, xzr
; CHECK-NEXT:    whilelo { p0.d, p1.d }, xzr, x9
; CHECK-NEXT:    addvl x11, x1, #1
; CHECK-NEXT:    .p2align 5, , 16
; CHECK-NEXT:  .LBB9_2: // %vector.body
; CHECK-NEXT:    // =>This Inner Loop Header: Depth=1
; CHECK-NEXT:    ld1d { z1.d }, p0/z, [x1, x8, lsl #3]
; CHECK-NEXT:    ld1d { z2.d }, p1/z, [x11, x8, lsl #3]
; CHECK-NEXT:    fmul z1.d, z1.d, z0.d
; CHECK-NEXT:    fmul z2.d, z2.d, z0.d
; CHECK-NEXT:    st1d { z1.d }, p0, [x0, x8, lsl #3]
; CHECK-NEXT:    st1d { z2.d }, p1, [x10, x8, lsl #3]
; CHECK-NEXT:    incw x8
; CHECK-NEXT:    whilelo { p0.d, p1.d }, x8, x9
; CHECK-NEXT:    b.mi .LBB9_2
; CHECK-NEXT:  .LBB9_3: // %for.cond.cleanup
; CHECK-NEXT:    ret
entry:
  %cmp6 = icmp sgt i32 %n, 0
  br i1 %cmp6, label %for.body.preheader, label %for.cond.cleanup

for.body.preheader:
  %wide.trip.count = zext nneg i32 %n to i64
  %0 = tail call i64 @llvm.vscale.i64()
  %1 = shl nuw nsw i64 %0, 2
  %active.lane.mask.entry = tail call <vscale x 4 x i1> @llvm.get.active.lane.mask.nxv4i1.i64(i64 0, i64 %wide.trip.count)
  %active.lane.mask.entry1 = tail call <vscale x 2 x i1> @llvm.vector.extract.nxv2i1.nxv4i1(<vscale x 4 x i1> %active.lane.mask.entry, i64 2)
  %active.lane.mask.entry2 = tail call <vscale x 2 x i1> @llvm.vector.extract.nxv2i1.nxv4i1(<vscale x 4 x i1> %active.lane.mask.entry, i64 0)
  %2 = tail call i64 @llvm.vscale.i64()
  %3 = shl nuw nsw i64 %2, 1
  %4 = tail call i64 @llvm.vscale.i64()
  %5 = shl nuw nsw i64 %4, 1
  br label %vector.body

vector.body:
  %index = phi i64 [ 0, %for.body.preheader ], [ %index.next5, %vector.body ]
  %active.lane.mask = phi <vscale x 2 x i1> [ %active.lane.mask.entry2, %for.body.preheader ], [ %active.lane.mask.next7, %vector.body ]
  %active.lane.mask3 = phi <vscale x 2 x i1> [ %active.lane.mask.entry1, %for.body.preheader ], [ %active.lane.mask.next6, %vector.body ]
  %6 = getelementptr inbounds double, ptr %src, i64 %index
  %wide.masked.load = tail call <vscale x 2 x double> @llvm.masked.load.nxv2f64.p0(ptr %6, i32 8, <vscale x 2 x i1> %active.lane.mask, <vscale x 2 x double> poison)
  %7 = getelementptr inbounds double, ptr %6, i64 %3
  %wide.masked.load4 = tail call <vscale x 2 x double> @llvm.masked.load.nxv2f64.p0(ptr nonnull %7, i32 8, <vscale x 2 x i1> %active.lane.mask3, <vscale x 2 x double> poison)
  %8 = fmul <vscale x 2 x double> %wide.masked.load, shufflevector (<vscale x 2 x double> insertelement (<vscale x 2 x double> poison, double 3.000000e+00, i64 0), <vscale x 2 x double> poison, <vscale x 2 x i32> zeroinitializer)
  %9 = fmul <vscale x 2 x double> %wide.masked.load4, shufflevector (<vscale x 2 x double> insertelement (<vscale x 2 x double> poison, double 3.000000e+00, i64 0), <vscale x 2 x double> poison, <vscale x 2 x i32> zeroinitializer)
  %10 = getelementptr inbounds double, ptr %dst, i64 %index
  tail call void @llvm.masked.store.nxv2f64.p0(<vscale x 2 x double> %8, ptr %10, i32 8, <vscale x 2 x i1> %active.lane.mask)
  %11 = getelementptr inbounds double, ptr %10, i64 %5
  tail call void @llvm.masked.store.nxv2f64.p0(<vscale x 2 x double> %9, ptr %11, i32 8, <vscale x 2 x i1> %active.lane.mask3)
  %index.next = add i64 %index, %1
  %index.next5 = add i64 %index, %1
  %active.lane.mask.next = tail call <vscale x 4 x i1> @llvm.get.active.lane.mask.nxv4i1.i64(i64 %index.next, i64 %wide.trip.count)
  %active.lane.mask.next6 = tail call <vscale x 2 x i1> @llvm.vector.extract.nxv2i1.nxv4i1(<vscale x 4 x i1> %active.lane.mask.next, i64 2)
  %active.lane.mask.next7 = tail call <vscale x 2 x i1> @llvm.vector.extract.nxv2i1.nxv4i1(<vscale x 4 x i1> %active.lane.mask.next, i64 0)
  %12 = extractelement <vscale x 2 x i1> %active.lane.mask.next7, i64 0
  br i1 %12, label %vector.body, label %for.cond.cleanup

for.cond.cleanup:
  ret void
}

; SVE 2.1, byte elements, UF=4
define void @f10(ptr noalias %dst, ptr readonly %src, i32 %n) #1 {
; CHECK-LABEL: f10:
; CHECK:       // %bb.0: // %entry
; CHECK-NEXT:    cmp w2, #1
; CHECK-NEXT:    b.lt .LBB10_3
; CHECK-NEXT:  // %bb.1: // %for.body.preheader
; CHECK-NEXT:    rdvl x10, #2
; CHECK-NEXT:    mov w8, w2
; CHECK-NEXT:    mov x9, xzr
; CHECK-NEXT:    whilelo { p2.b, p3.b }, xzr, x8
; CHECK-NEXT:    addvl x11, x0, #2
; CHECK-NEXT:    whilelo { p0.b, p1.b }, x10, x8
; CHECK-NEXT:    addvl x10, x0, #3
; CHECK-NEXT:    addvl x12, x0, #1
; CHECK-NEXT:    addvl x13, x1, #3
; CHECK-NEXT:    addvl x14, x1, #2
; CHECK-NEXT:    addvl x15, x1, #1
; CHECK-NEXT:    .p2align 5, , 16
; CHECK-NEXT:  .LBB10_2: // %vector.body
; CHECK-NEXT:    // =>This Inner Loop Header: Depth=1
; CHECK-NEXT:    ld1b { z0.b }, p2/z, [x1, x9]
; CHECK-NEXT:    ld1b { z1.b }, p3/z, [x15, x9]
; CHECK-NEXT:    addvl x16, x9, #6
; CHECK-NEXT:    mul z0.b, z0.b, #3
; CHECK-NEXT:    ld1b { z2.b }, p0/z, [x14, x9]
; CHECK-NEXT:    ld1b { z3.b }, p1/z, [x13, x9]
; CHECK-NEXT:    mul z1.b, z1.b, #3
; CHECK-NEXT:    mul z2.b, z2.b, #3
; CHECK-NEXT:    mul z3.b, z3.b, #3
; CHECK-NEXT:    st1b { z0.b }, p2, [x0, x9]
; CHECK-NEXT:    st1b { z1.b }, p3, [x12, x9]
; CHECK-NEXT:    st1b { z2.b }, p0, [x11, x9]
; CHECK-NEXT:    st1b { z3.b }, p1, [x10, x9]
; CHECK-NEXT:    addvl x9, x9, #4
; CHECK-NEXT:    whilelo { p0.b, p1.b }, x16, x8
; CHECK-NEXT:    whilelo { p2.b, p3.b }, x9, x8
; CHECK-NEXT:    b.mi .LBB10_2
; CHECK-NEXT:  .LBB10_3: // %for.cond.cleanup
; CHECK-NEXT:    ret
entry:
  %cmp7 = icmp sgt i32 %n, 0
  br i1 %cmp7, label %for.body.preheader, label %for.cond.cleanup

for.body.preheader:
  %wide.trip.count = zext nneg i32 %n to i64
  %0 = tail call i64 @llvm.vscale.i64()
  %1 = shl nuw nsw i64 %0, 6
  %2 = tail call i64 @llvm.vscale.i64()
  %3 = shl nuw nsw i64 %2, 5
  %active.lane.mask.entry = tail call <vscale x 32 x i1> @llvm.get.active.lane.mask.nxv32i1.i64(i64 %3, i64 %wide.trip.count)
  %active.lane.mask.entry3 = tail call <vscale x 32 x i1> @llvm.get.active.lane.mask.nxv32i1.i64(i64 0, i64 %wide.trip.count)
  %active.lane.mask.entry4 = tail call <vscale x 16 x i1> @llvm.vector.extract.nxv16i1.nxv32i1(<vscale x 32 x i1> %active.lane.mask.entry, i64 16)
  %active.lane.mask.entry5 = tail call <vscale x 16 x i1> @llvm.vector.extract.nxv16i1.nxv32i1(<vscale x 32 x i1> %active.lane.mask.entry, i64 0)
  %active.lane.mask.entry6 = tail call <vscale x 16 x i1> @llvm.vector.extract.nxv16i1.nxv32i1(<vscale x 32 x i1> %active.lane.mask.entry3, i64 16)
  %active.lane.mask.entry7 = tail call <vscale x 16 x i1> @llvm.vector.extract.nxv16i1.nxv32i1(<vscale x 32 x i1> %active.lane.mask.entry3, i64 0)
  %4 = tail call i64 @llvm.vscale.i64()
  %5 = shl nuw nsw i64 %4, 4
  %6 = tail call i64 @llvm.vscale.i64()
  %7 = shl nuw nsw i64 %6, 5
  %8 = tail call i64 @llvm.vscale.i64()
  %9 = mul nuw nsw i64 %8, 48
  %10 = tail call i64 @llvm.vscale.i64()
  %11 = shl nuw nsw i64 %10, 4
  %12 = tail call i64 @llvm.vscale.i64()
  %13 = shl nuw nsw i64 %12, 5
  %14 = tail call i64 @llvm.vscale.i64()
  %15 = mul nuw nsw i64 %14, 48
  %16 = tail call i64 @llvm.vscale.i64()
  %17 = shl nuw nsw i64 %16, 5
  br label %vector.body

vector.body:
  %index = phi i64 [ 0, %for.body.preheader ], [ %index.next16, %vector.body ]
  %active.lane.mask = phi <vscale x 16 x i1> [ %active.lane.mask.entry7, %for.body.preheader ], [ %active.lane.mask.next21, %vector.body ]
  %active.lane.mask8 = phi <vscale x 16 x i1> [ %active.lane.mask.entry6, %for.body.preheader ], [ %active.lane.mask.next20, %vector.body ]
  %active.lane.mask9 = phi <vscale x 16 x i1> [ %active.lane.mask.entry5, %for.body.preheader ], [ %active.lane.mask.next19, %vector.body ]
  %active.lane.mask10 = phi <vscale x 16 x i1> [ %active.lane.mask.entry4, %for.body.preheader ], [ %active.lane.mask.next18, %vector.body ]
  %18 = getelementptr inbounds i8, ptr %src, i64 %index
  %wide.masked.load = tail call <vscale x 16 x i8> @llvm.masked.load.nxv16i8.p0(ptr %18, i32 1, <vscale x 16 x i1> %active.lane.mask, <vscale x 16 x i8> poison)
  %19 = getelementptr inbounds i8, ptr %18, i64 %5
  %wide.masked.load11 = tail call <vscale x 16 x i8> @llvm.masked.load.nxv16i8.p0(ptr nonnull %19, i32 1, <vscale x 16 x i1> %active.lane.mask8, <vscale x 16 x i8> poison)
  %20 = getelementptr inbounds i8, ptr %18, i64 %7
  %wide.masked.load12 = tail call <vscale x 16 x i8> @llvm.masked.load.nxv16i8.p0(ptr nonnull %20, i32 1, <vscale x 16 x i1> %active.lane.mask9, <vscale x 16 x i8> poison)
  %21 = getelementptr inbounds i8, ptr %18, i64 %9
  %wide.masked.load13 = tail call <vscale x 16 x i8> @llvm.masked.load.nxv16i8.p0(ptr nonnull %21, i32 1, <vscale x 16 x i1> %active.lane.mask10, <vscale x 16 x i8> poison)
  %22 = mul <vscale x 16 x i8> %wide.masked.load, shufflevector (<vscale x 16 x i8> insertelement (<vscale x 16 x i8> poison, i8 3, i64 0), <vscale x 16 x i8> poison, <vscale x 16 x i32> zeroinitializer)
  %23 = mul <vscale x 16 x i8> %wide.masked.load11, shufflevector (<vscale x 16 x i8> insertelement (<vscale x 16 x i8> poison, i8 3, i64 0), <vscale x 16 x i8> poison, <vscale x 16 x i32> zeroinitializer)
  %24 = mul <vscale x 16 x i8> %wide.masked.load12, shufflevector (<vscale x 16 x i8> insertelement (<vscale x 16 x i8> poison, i8 3, i64 0), <vscale x 16 x i8> poison, <vscale x 16 x i32> zeroinitializer)
  %25 = mul <vscale x 16 x i8> %wide.masked.load13, shufflevector (<vscale x 16 x i8> insertelement (<vscale x 16 x i8> poison, i8 3, i64 0), <vscale x 16 x i8> poison, <vscale x 16 x i32> zeroinitializer)
  %26 = getelementptr inbounds i8, ptr %dst, i64 %index
  tail call void @llvm.masked.store.nxv16i8.p0(<vscale x 16 x i8> %22, ptr %26, i32 1, <vscale x 16 x i1> %active.lane.mask)
  %27 = getelementptr inbounds i8, ptr %26, i64 %11
  tail call void @llvm.masked.store.nxv16i8.p0(<vscale x 16 x i8> %23, ptr %27, i32 1, <vscale x 16 x i1> %active.lane.mask8)
  %28 = getelementptr inbounds i8, ptr %26, i64 %13
  tail call void @llvm.masked.store.nxv16i8.p0(<vscale x 16 x i8> %24, ptr %28, i32 1, <vscale x 16 x i1> %active.lane.mask9)
  %29 = getelementptr inbounds i8, ptr %26, i64 %15
  tail call void @llvm.masked.store.nxv16i8.p0(<vscale x 16 x i8> %25, ptr %29, i32 1, <vscale x 16 x i1> %active.lane.mask10)
  %index.next = add i64 %index, %1
  %index.next16 = add i64 %index, %1
  %30 = add i64 %index.next, %17
  %active.lane.mask.next = tail call <vscale x 32 x i1> @llvm.get.active.lane.mask.nxv32i1.i64(i64 %30, i64 %wide.trip.count)
  %active.lane.mask.next17 = tail call <vscale x 32 x i1> @llvm.get.active.lane.mask.nxv32i1.i64(i64 %index.next, i64 %wide.trip.count)
  %active.lane.mask.next18 = tail call <vscale x 16 x i1> @llvm.vector.extract.nxv16i1.nxv32i1(<vscale x 32 x i1> %active.lane.mask.next, i64 16)
  %active.lane.mask.next19 = tail call <vscale x 16 x i1> @llvm.vector.extract.nxv16i1.nxv32i1(<vscale x 32 x i1> %active.lane.mask.next, i64 0)
  %active.lane.mask.next20 = tail call <vscale x 16 x i1> @llvm.vector.extract.nxv16i1.nxv32i1(<vscale x 32 x i1> %active.lane.mask.next17, i64 16)
  %active.lane.mask.next21 = tail call <vscale x 16 x i1> @llvm.vector.extract.nxv16i1.nxv32i1(<vscale x 32 x i1> %active.lane.mask.next17, i64 0)
  %31 = extractelement <vscale x 16 x i1> %active.lane.mask.next21, i64 0
  br i1 %31, label %vector.body, label %for.cond.cleanup

for.cond.cleanup:
  ret void
}

; SVE2.1, double-precision float elements, UF=4
define void @f11(ptr noalias %dst, ptr readonly %src, i32 %n) #1 {
; CHECK-LABEL: f11:
; CHECK:       // %bb.0: // %entry
; CHECK-NEXT:    cmp w2, #1
; CHECK-NEXT:    b.lt .LBB11_3
; CHECK-NEXT:  // %bb.1: // %for.body.preheader
; CHECK-NEXT:    cntw x10
; CHECK-NEXT:    mov w8, w2
; CHECK-NEXT:    fmov z0.d, #3.00000000
; CHECK-NEXT:    mov x9, xzr
; CHECK-NEXT:    mov x15, xzr
; CHECK-NEXT:    whilelo { p2.d, p3.d }, xzr, x8
; CHECK-NEXT:    addvl x11, x0, #2
; CHECK-NEXT:    whilelo { p0.d, p1.d }, x10, x8
; CHECK-NEXT:    addvl x10, x0, #3
; CHECK-NEXT:    addvl x12, x0, #1
; CHECK-NEXT:    addvl x13, x1, #3
; CHECK-NEXT:    addvl x14, x1, #2
; CHECK-NEXT:    addvl x16, x1, #1
; CHECK-NEXT:    .p2align 5, , 16
; CHECK-NEXT:  .LBB11_2: // %vector.body
; CHECK-NEXT:    // =>This Inner Loop Header: Depth=1
; CHECK-NEXT:    ld1d { z1.d }, p2/z, [x1, x9, lsl #3]
; CHECK-NEXT:    ld1d { z2.d }, p3/z, [x16, x9, lsl #3]
; CHECK-NEXT:    inch x15
; CHECK-NEXT:    fmul z1.d, z1.d, z0.d
; CHECK-NEXT:    ld1d { z3.d }, p0/z, [x14, x9, lsl #3]
; CHECK-NEXT:    ld1d { z4.d }, p1/z, [x13, x9, lsl #3]
; CHECK-NEXT:    fmul z2.d, z2.d, z0.d
; CHECK-NEXT:    fmul z3.d, z3.d, z0.d
; CHECK-NEXT:    fmul z4.d, z4.d, z0.d
; CHECK-NEXT:    st1d { z1.d }, p2, [x0, x9, lsl #3]
; CHECK-NEXT:    st1d { z2.d }, p3, [x12, x9, lsl #3]
; CHECK-NEXT:    st1d { z3.d }, p0, [x11, x9, lsl #3]
; CHECK-NEXT:    st1d { z4.d }, p1, [x10, x9, lsl #3]
; CHECK-NEXT:    incw x9, all, mul #3
; CHECK-NEXT:    whilelo { p0.d, p1.d }, x9, x8
; CHECK-NEXT:    whilelo { p2.d, p3.d }, x15, x8
; CHECK-NEXT:    mov x9, x15
; CHECK-NEXT:    b.mi .LBB11_2
; CHECK-NEXT:  .LBB11_3: // %for.cond.cleanup
; CHECK-NEXT:    ret
entry:
  %cmp6 = icmp sgt i32 %n, 0
  br i1 %cmp6, label %for.body.preheader, label %for.cond.cleanup

for.body.preheader:
  %wide.trip.count = zext nneg i32 %n to i64
  %0 = tail call i64 @llvm.vscale.i64()
  %1 = shl nuw nsw i64 %0, 3
  %2 = tail call i64 @llvm.vscale.i64()
  %3 = shl nuw nsw i64 %2, 2
  %active.lane.mask.entry = tail call <vscale x 4 x i1> @llvm.get.active.lane.mask.nxv4i1.i64(i64 %3, i64 %wide.trip.count)
  %active.lane.mask.entry3 = tail call <vscale x 4 x i1> @llvm.get.active.lane.mask.nxv4i1.i64(i64 0, i64 %wide.trip.count)
  %active.lane.mask.entry4 = tail call <vscale x 2 x i1> @llvm.vector.extract.nxv2i1.nxv4i1(<vscale x 4 x i1> %active.lane.mask.entry, i64 2)
  %active.lane.mask.entry5 = tail call <vscale x 2 x i1> @llvm.vector.extract.nxv2i1.nxv4i1(<vscale x 4 x i1> %active.lane.mask.entry, i64 0)
  %active.lane.mask.entry6 = tail call <vscale x 2 x i1> @llvm.vector.extract.nxv2i1.nxv4i1(<vscale x 4 x i1> %active.lane.mask.entry3, i64 2)
  %active.lane.mask.entry7 = tail call <vscale x 2 x i1> @llvm.vector.extract.nxv2i1.nxv4i1(<vscale x 4 x i1> %active.lane.mask.entry3, i64 0)
  %4 = tail call i64 @llvm.vscale.i64()
  %5 = shl nuw nsw i64 %4, 1
  %6 = tail call i64 @llvm.vscale.i64()
  %7 = shl nuw nsw i64 %6, 2
  %8 = tail call i64 @llvm.vscale.i64()
  %9 = mul nuw nsw i64 %8, 6
  %10 = tail call i64 @llvm.vscale.i64()
  %11 = shl nuw nsw i64 %10, 1
  %12 = tail call i64 @llvm.vscale.i64()
  %13 = shl nuw nsw i64 %12, 2
  %14 = tail call i64 @llvm.vscale.i64()
  %15 = mul nuw nsw i64 %14, 6
  %16 = tail call i64 @llvm.vscale.i64()
  %17 = shl nuw nsw i64 %16, 2
  br label %vector.body

vector.body:
  %index = phi i64 [ 0, %for.body.preheader ], [ %index.next16, %vector.body ]
  %active.lane.mask = phi <vscale x 2 x i1> [ %active.lane.mask.entry7, %for.body.preheader ], [ %active.lane.mask.next21, %vector.body ]
  %active.lane.mask8 = phi <vscale x 2 x i1> [ %active.lane.mask.entry6, %for.body.preheader ], [ %active.lane.mask.next20, %vector.body ]
  %active.lane.mask9 = phi <vscale x 2 x i1> [ %active.lane.mask.entry5, %for.body.preheader ], [ %active.lane.mask.next19, %vector.body ]
  %active.lane.mask10 = phi <vscale x 2 x i1> [ %active.lane.mask.entry4, %for.body.preheader ], [ %active.lane.mask.next18, %vector.body ]
  %18 = getelementptr inbounds double, ptr %src, i64 %index
  %wide.masked.load = tail call <vscale x 2 x double> @llvm.masked.load.nxv2f64.p0(ptr %18, i32 8, <vscale x 2 x i1> %active.lane.mask, <vscale x 2 x double> poison)
  %19 = getelementptr inbounds double, ptr %18, i64 %5
  %wide.masked.load11 = tail call <vscale x 2 x double> @llvm.masked.load.nxv2f64.p0(ptr nonnull %19, i32 8, <vscale x 2 x i1> %active.lane.mask8, <vscale x 2 x double> poison)
  %20 = getelementptr inbounds double, ptr %18, i64 %7
  %wide.masked.load12 = tail call <vscale x 2 x double> @llvm.masked.load.nxv2f64.p0(ptr nonnull %20, i32 8, <vscale x 2 x i1> %active.lane.mask9, <vscale x 2 x double> poison)
  %21 = getelementptr inbounds double, ptr %18, i64 %9
  %wide.masked.load13 = tail call <vscale x 2 x double> @llvm.masked.load.nxv2f64.p0(ptr nonnull %21, i32 8, <vscale x 2 x i1> %active.lane.mask10, <vscale x 2 x double> poison)
  %22 = fmul <vscale x 2 x double> %wide.masked.load, shufflevector (<vscale x 2 x double> insertelement (<vscale x 2 x double> poison, double 3.000000e+00, i64 0), <vscale x 2 x double> poison, <vscale x 2 x i32> zeroinitializer)
  %23 = fmul <vscale x 2 x double> %wide.masked.load11, shufflevector (<vscale x 2 x double> insertelement (<vscale x 2 x double> poison, double 3.000000e+00, i64 0), <vscale x 2 x double> poison, <vscale x 2 x i32> zeroinitializer)
  %24 = fmul <vscale x 2 x double> %wide.masked.load12, shufflevector (<vscale x 2 x double> insertelement (<vscale x 2 x double> poison, double 3.000000e+00, i64 0), <vscale x 2 x double> poison, <vscale x 2 x i32> zeroinitializer)
  %25 = fmul <vscale x 2 x double> %wide.masked.load13, shufflevector (<vscale x 2 x double> insertelement (<vscale x 2 x double> poison, double 3.000000e+00, i64 0), <vscale x 2 x double> poison, <vscale x 2 x i32> zeroinitializer)
  %26 = getelementptr inbounds double, ptr %dst, i64 %index
  tail call void @llvm.masked.store.nxv2f64.p0(<vscale x 2 x double> %22, ptr %26, i32 8, <vscale x 2 x i1> %active.lane.mask)
  %27 = getelementptr inbounds double, ptr %26, i64 %11
  tail call void @llvm.masked.store.nxv2f64.p0(<vscale x 2 x double> %23, ptr %27, i32 8, <vscale x 2 x i1> %active.lane.mask8)
  %28 = getelementptr inbounds double, ptr %26, i64 %13
  tail call void @llvm.masked.store.nxv2f64.p0(<vscale x 2 x double> %24, ptr %28, i32 8, <vscale x 2 x i1> %active.lane.mask9)
  %29 = getelementptr inbounds double, ptr %26, i64 %15
  tail call void @llvm.masked.store.nxv2f64.p0(<vscale x 2 x double> %25, ptr %29, i32 8, <vscale x 2 x i1> %active.lane.mask10)
  %index.next = add i64 %index, %1
  %index.next16 = add i64 %index, %1
  %30 = add i64 %index.next, %17
  %active.lane.mask.next = tail call <vscale x 4 x i1> @llvm.get.active.lane.mask.nxv4i1.i64(i64 %30, i64 %wide.trip.count)
  %active.lane.mask.next17 = tail call <vscale x 4 x i1> @llvm.get.active.lane.mask.nxv4i1.i64(i64 %index.next, i64 %wide.trip.count)
  %active.lane.mask.next18 = tail call <vscale x 2 x i1> @llvm.vector.extract.nxv2i1.nxv4i1(<vscale x 4 x i1> %active.lane.mask.next, i64 2)
  %active.lane.mask.next19 = tail call <vscale x 2 x i1> @llvm.vector.extract.nxv2i1.nxv4i1(<vscale x 4 x i1> %active.lane.mask.next, i64 0)
  %active.lane.mask.next20 = tail call <vscale x 2 x i1> @llvm.vector.extract.nxv2i1.nxv4i1(<vscale x 4 x i1> %active.lane.mask.next17, i64 2)
  %active.lane.mask.next21 = tail call <vscale x 2 x i1> @llvm.vector.extract.nxv2i1.nxv4i1(<vscale x 4 x i1> %active.lane.mask.next17, i64 0)
  %31 = extractelement <vscale x 2 x i1> %active.lane.mask.next21, i64 0
  br i1 %31, label %vector.body, label %for.cond.cleanup

for.cond.cleanup:
  ret void
}

declare <vscale x 16 x i1> @llvm.get.active.lane.mask.nxv16i1.i64(i64, i64)
declare <vscale x 16 x i1> @llvm.vector.extract.nxv16i1.nxv32i1(<vscale x 32 x i1>, i64 immarg)
declare <vscale x 16 x i8> @llvm.masked.load.nxv16i8.p0(ptr nocapture, i32 immarg, <vscale x 16 x i1>, <vscale x 16 x i8>)
declare <vscale x 2 x double> @llvm.masked.load.nxv2f64.p0(ptr nocapture, i32 immarg, <vscale x 2 x i1>, <vscale x 2 x double>)
declare <vscale x 2 x i1> @llvm.get.active.lane.mask.nxv2i1.i64(i64, i64)
declare <vscale x 2 x i1> @llvm.vector.extract.nxv2i1.nxv4i1(<vscale x 4 x i1>, i64 immarg)
declare <vscale x 32 x i1> @llvm.get.active.lane.mask.nxv32i1.i64(i64, i64)
declare <vscale x 4 x i1> @llvm.get.active.lane.mask.nxv4i1.i64(i64, i64)
declare i64 @llvm.vscale.i64()
declare void @llvm.masked.store.nxv16i8.p0(<vscale x 16 x i8>, ptr nocapture, i32 immarg, <vscale x 16 x i1>)
declare void @llvm.masked.store.nxv2f64.p0(<vscale x 2 x double>, ptr nocapture, i32 immarg, <vscale x 2 x i1>)

attributes #0 = { nounwind vscale_range(1,16) "target-cpu"="neoverse-v1" }
attributes #1 = { nounwind vscale_range(1,16) "target-cpu"="neoverse-v1" "target-features"="+sve2p1" }
