; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 4
; RUN: llc < %s -mtriple=x86_64-unknown -mattr=+avx2 | FileCheck %s

define <4 x i16> @hadd_trunc_v4i32(<4 x i32> %x, <4 x i32> %y) {
; CHECK-LABEL: hadd_trunc_v4i32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vpbroadcastd {{.*#+}} xmm2 = [3,3,3,3]
; CHECK-NEXT:    vpand %xmm2, %xmm0, %xmm0
; CHECK-NEXT:    vpand %xmm2, %xmm1, %xmm1
; CHECK-NEXT:    vphaddd %xmm1, %xmm0, %xmm0
; CHECK-NEXT:    vpackusdw %xmm0, %xmm0, %xmm0
; CHECK-NEXT:    retq
entry:
  %0 = and <4 x i32> %x, <i32 3, i32 3, i32 3, i32 3>
  %1 = and <4 x i32> %y, <i32 3, i32 3, i32 3, i32 3>
  %2 = tail call <4 x i32> @llvm.x86.ssse3.phadd.d.128(<4 x i32> %0, <4 x i32> %1)
  %conv = trunc <4 x i32> %2 to <4 x i16>
  ret <4 x i16> %conv
}

define <8 x i8> @hadd_trunc_v8i16(<8 x i16> %x, <8 x i16> %y) {
; CHECK-LABEL: hadd_trunc_v8i16:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vpbroadcastw {{.*#+}} xmm2 = [3,3,3,3,3,3,3,3]
; CHECK-NEXT:    vpand %xmm2, %xmm0, %xmm0
; CHECK-NEXT:    vpand %xmm2, %xmm1, %xmm1
; CHECK-NEXT:    vphaddw %xmm1, %xmm0, %xmm0
; CHECK-NEXT:    vpackuswb %xmm0, %xmm0, %xmm0
; CHECK-NEXT:    retq
entry:
  %0 = and <8 x i16> %x, <i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3>
  %1 = and <8 x i16> %y, <i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3>
  %2 = tail call <8 x i16> @llvm.x86.ssse3.phadd.w.128(<8 x i16> %0, <8 x i16> %1)
  %conv = trunc <8 x i16> %2 to <8 x i8>
  ret <8 x i8> %conv
}

define <8 x i8> @hadd_trunc_v8i16_sat(<8 x i16> %x, <8 x i16> %y) {
; CHECK-LABEL: hadd_trunc_v8i16_sat:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vpbroadcastw {{.*#+}} xmm2 = [3,3,3,3,3,3,3,3]
; CHECK-NEXT:    vpand %xmm2, %xmm0, %xmm0
; CHECK-NEXT:    vpand %xmm2, %xmm1, %xmm1
; CHECK-NEXT:    vphaddsw %xmm1, %xmm0, %xmm0
; CHECK-NEXT:    vpackuswb %xmm0, %xmm0, %xmm0
; CHECK-NEXT:    retq
entry:
  %0 = and <8 x i16> %x, <i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3>
  %1 = and <8 x i16> %y, <i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3>
  %2 = tail call <8 x i16> @llvm.x86.ssse3.phadd.sw.128(<8 x i16> %0, <8 x i16> %1)
  %conv = trunc <8 x i16> %2 to <8 x i8>
  ret <8 x i8> %conv
}

define <8 x i16> @hadd_trunc_v8i32(<8 x i32> %x, <8 x i32> %y) {
; CHECK-LABEL: hadd_trunc_v8i32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vpbroadcastd {{.*#+}} ymm2 = [3,3,3,3,3,3,3,3]
; CHECK-NEXT:    vpand %ymm2, %ymm0, %ymm0
; CHECK-NEXT:    vpand %ymm2, %ymm1, %ymm1
; CHECK-NEXT:    vphaddd %ymm1, %ymm0, %ymm0
; CHECK-NEXT:    vextracti128 $1, %ymm0, %xmm1
; CHECK-NEXT:    vpackusdw %xmm1, %xmm0, %xmm0
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    retq
entry:
  %0 = and <8 x i32> %x, <i32 3, i32 3, i32 3, i32 3, i32 3, i32 3, i32 3, i32 3>
  %1 = and <8 x i32> %y, <i32 3, i32 3, i32 3, i32 3, i32 3, i32 3, i32 3, i32 3>
  %2 = tail call <8 x i32> @llvm.x86.avx2.phadd.d(<8 x i32> %0, <8 x i32> %1)
  %conv = trunc <8 x i32> %2 to <8 x i16>
  ret <8 x i16> %conv
}

define <16 x i8> @hadd_trunc_v16i16(<16 x i16> %x, <16 x i16> %y) {
; CHECK-LABEL: hadd_trunc_v16i16:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vpbroadcastw {{.*#+}} ymm2 = [3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3]
; CHECK-NEXT:    vpand %ymm2, %ymm0, %ymm0
; CHECK-NEXT:    vpand %ymm2, %ymm1, %ymm1
; CHECK-NEXT:    vphaddw %ymm1, %ymm0, %ymm0
; CHECK-NEXT:    vextracti128 $1, %ymm0, %xmm1
; CHECK-NEXT:    vpackuswb %xmm1, %xmm0, %xmm0
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    retq
entry:
  %0 = and <16 x i16> %x, <i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3>
  %1 = and <16 x i16> %y, <i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3>
  %2 = tail call <16 x i16> @llvm.x86.avx2.phadd.w(<16 x i16> %0, <16 x i16> %1)
  %conv = trunc <16 x i16> %2 to <16 x i8>
  ret <16 x i8> %conv
}

define <16 x i8> @hadd_trunc_v16i16_sat(<16 x i16> %x, <16 x i16> %y) {
; CHECK-LABEL: hadd_trunc_v16i16_sat:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vpbroadcastw {{.*#+}} ymm2 = [3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3]
; CHECK-NEXT:    vpand %ymm2, %ymm0, %ymm0
; CHECK-NEXT:    vpand %ymm2, %ymm1, %ymm1
; CHECK-NEXT:    vphaddsw %ymm1, %ymm0, %ymm0
; CHECK-NEXT:    vextracti128 $1, %ymm0, %xmm1
; CHECK-NEXT:    vpackuswb %xmm1, %xmm0, %xmm0
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    retq
entry:
  %0 = and <16 x i16> %x, <i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3>
  %1 = and <16 x i16> %y, <i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3, i16 3>
  %2 = tail call <16 x i16> @llvm.x86.avx2.phadd.sw(<16 x i16> %0, <16 x i16> %1)
  %conv = trunc <16 x i16> %2 to <16 x i8>
  ret <16 x i8> %conv
}

define <4 x i16> @hsub_trunc_v4i32(<4 x i32> %x, <4 x i32> %y) {
; CHECK-LABEL: hsub_trunc_v4i32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vxorps %xmm0, %xmm0, %xmm0
; CHECK-NEXT:    retq
entry:
  %0 = or <4 x i32> %x, <i32 65535, i32 65535, i32 65535, i32 65535>
  %1 = or <4 x i32> %y, <i32 65535, i32 65535, i32 65535, i32 65535>
  %2 = tail call <4 x i32> @llvm.x86.ssse3.phsub.d.128(<4 x i32> %0, <4 x i32> %1)
  %conv = trunc <4 x i32> %2 to <4 x i16>
  ret <4 x i16> %conv
}

define <8 x i8> @hsub_trunc_v8i16(<8 x i16> %x, <8 x i16> %y) {
; CHECK-LABEL: hsub_trunc_v8i16:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vxorps %xmm0, %xmm0, %xmm0
; CHECK-NEXT:    retq
entry:
  %0 = or <8 x i16> %x, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %1 = or <8 x i16> %y, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %2 = tail call <8 x i16> @llvm.x86.ssse3.phsub.w.128(<8 x i16> %0, <8 x i16> %1)
  %conv = trunc <8 x i16> %2 to <8 x i8>
  ret <8 x i8> %conv
}

define <8 x i8> @hsub_and_trunc_v8i16_sat(<8 x i16> %x, <8 x i16> %y) {
; CHECK-LABEL: hsub_and_trunc_v8i16_sat:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vpbroadcastw {{.*#+}} xmm2 = [7,7,7,7,7,7,7,7]
; CHECK-NEXT:    vpor %xmm2, %xmm0, %xmm0
; CHECK-NEXT:    vpor %xmm2, %xmm1, %xmm1
; CHECK-NEXT:    vphsubsw %xmm1, %xmm0, %xmm0
; CHECK-NEXT:    vpshufb {{.*#+}} xmm0 = xmm0[0,2,4,6,8,10,12,14,u,u,u,u,u,u,u,u]
; CHECK-NEXT:    retq
entry:
  %0 = or <8 x i16> %x, <i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7>
  %1 = or <8 x i16> %y, <i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7>
  %2 = tail call <8 x i16> @llvm.x86.ssse3.phsub.sw.128(<8 x i16> %0, <8 x i16> %1)
  %3 = and <8 x i16> %2, <i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7>
  %4 = trunc <8 x i16> %2 to <8 x i8>
  ret <8 x i8> %4
}

define <8 x i16> @hsub_trunc_v8i32(<8 x i32> %x, <8 x i32> %y) {
; CHECK-LABEL: hsub_trunc_v8i32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vxorps %xmm0, %xmm0, %xmm0
; CHECK-NEXT:    retq
entry:
  %0 = or <8 x i32> %x, <i32 65535, i32 65535, i32 65535, i32 65535, i32 65535, i32 65535, i32 65535, i32 65535>
  %1 = or <8 x i32> %y, <i32 65535, i32 65535, i32 65535, i32 65535, i32 65535, i32 65535, i32 65535, i32 65535>
  %2 = tail call <8 x i32> @llvm.x86.avx2.phsub.d(<8 x i32> %0, <8 x i32> %1)
  %conv = trunc <8 x i32> %2 to <8 x i16>
  ret <8 x i16> %conv
}

define <16 x i8> @hsub_trunc_v16i16(<16 x i16> %x, <16 x i16> %y) {
; CHECK-LABEL: hsub_trunc_v16i16:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vxorps %xmm0, %xmm0, %xmm0
; CHECK-NEXT:    retq
entry:
  %0 = or <16 x i16> %x, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %1 = or <16 x i16> %y, <i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255, i16 255>
  %2 = tail call <16 x i16> @llvm.x86.avx2.phsub.w(<16 x i16> %0, <16 x i16> %1)
  %conv = trunc <16 x i16> %2 to <16 x i8>
  ret <16 x i8> %conv
}

define <16 x i8> @hsub_and_trunc_v16i16_sat(<16 x i16> %x, <16 x i16> %y) {
; CHECK-LABEL: hsub_and_trunc_v16i16_sat:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vpbroadcastw {{.*#+}} ymm2 = [7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7]
; CHECK-NEXT:    vpor %ymm2, %ymm0, %ymm0
; CHECK-NEXT:    vpor %ymm2, %ymm1, %ymm1
; CHECK-NEXT:    vphsubsw %ymm1, %ymm0, %ymm0
; CHECK-NEXT:    vpand {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %ymm0, %ymm0
; CHECK-NEXT:    vextracti128 $1, %ymm0, %xmm1
; CHECK-NEXT:    vpackuswb %xmm1, %xmm0, %xmm0
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    retq
entry:
  %0 = or <16 x i16> %x, <i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7>
  %1 = or <16 x i16> %y, <i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7>
  %2 = tail call <16 x i16> @llvm.x86.avx2.phsub.sw(<16 x i16> %0, <16 x i16> %1)
  %3 = and <16 x i16> %2, <i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7, i16 7>
  %4 = trunc <16 x i16> %2 to <16 x i8>
  ret <16 x i8> %4
}
