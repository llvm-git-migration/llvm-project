; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: opt < %s -passes=instcombine -mtriple=x86_64-unknown-unknown -S | llc -mtriple=x86_64-unknown-unknown -mattr=+avx512f,+avx512vl | FileCheck %s
; RUN: opt < %s -passes=instcombine -mtriple=x86_64-unknown-unknown -S | llc -mtriple=x86_64-unknown-unknown -mattr=+avx512f,+avx512vl -early-live-intervals | FileCheck %s

;; This is just a simple test to make sure there are no regressions
;; cause by splitting/recombining ternlog intrinsics.

declare <4 x i64> @use_merge_4xi64(<4 x i64>, <4 x i64>)

declare <4 x i32> @llvm.x86.avx512.pternlog.d.128(<4 x i32>, <4 x i32>, <4 x i32>, i32 immarg)
declare <4 x i64> @llvm.x86.avx512.pternlog.q.256(<4 x i64>, <4 x i64>, <4 x i64>, i32 immarg)
declare <8 x i64> @llvm.x86.avx512.pternlog.q.512(<8 x i64>, <8 x i64>, <8 x i64>, i32 immarg)
declare <2 x i64> @llvm.x86.avx512.pternlog.q.128(<2 x i64>, <2 x i64>, <2 x i64>, i32 immarg)
declare <16 x i32> @llvm.x86.avx512.pternlog.d.512(<16 x i32>, <16 x i32>, <16 x i32>, i32 immarg)
declare <8 x i32> @llvm.x86.avx512.pternlog.d.256(<8 x i32>, <8 x i32>, <8 x i32>, i32 immarg)

define <16 x i32> @vpternlog_d_v512_imm0(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2) nounwind {
  %r = tail call <16 x i32> @llvm.x86.avx512.pternlog.d.512(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2, i32 0)
  ret <16 x i32> %r
}

define <2 x i64> @vpternlog_q_v128_imm1(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2) nounwind {
  %r = tail call <2 x i64> @llvm.x86.avx512.pternlog.q.128(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2, i32 1)
  ret <2 x i64> %r
}

define <8 x i32> @vpternlog_d_v256_imm2(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2) nounwind {
  %r = tail call <8 x i32> @llvm.x86.avx512.pternlog.d.256(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2, i32 2)
  ret <8 x i32> %r
}

define <8 x i64> @vpternlog_q_v512_imm3(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2) nounwind {
  %r = tail call <8 x i64> @llvm.x86.avx512.pternlog.q.512(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2, i32 3)
  ret <8 x i64> %r
}

define <4 x i32> @vpternlog_d_v128_imm4(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2) nounwind {
  %r = tail call <4 x i32> @llvm.x86.avx512.pternlog.d.128(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2, i32 4)
  ret <4 x i32> %r
}

define <4 x i64> @vpternlog_q_v256_imm5(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2) nounwind {
  %r = tail call <4 x i64> @llvm.x86.avx512.pternlog.q.256(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2, i32 5)
  ret <4 x i64> %r
}

define <16 x i32> @vpternlog_d_v512_imm6(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2) nounwind {
  %r = tail call <16 x i32> @llvm.x86.avx512.pternlog.d.512(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2, i32 6)
  ret <16 x i32> %r
}

define <2 x i64> @vpternlog_q_v128_imm7(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2) nounwind {
  %r = tail call <2 x i64> @llvm.x86.avx512.pternlog.q.128(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2, i32 7)
  ret <2 x i64> %r
}

define <8 x i32> @vpternlog_d_v256_imm8(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2) nounwind {
  %r = tail call <8 x i32> @llvm.x86.avx512.pternlog.d.256(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2, i32 8)
  ret <8 x i32> %r
}

define <8 x i64> @vpternlog_q_v512_imm9(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2) nounwind {
  %r = tail call <8 x i64> @llvm.x86.avx512.pternlog.q.512(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2, i32 9)
  ret <8 x i64> %r
}

define <4 x i32> @vpternlog_d_v128_imm10(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2) nounwind {
  %r = tail call <4 x i32> @llvm.x86.avx512.pternlog.d.128(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2, i32 10)
  ret <4 x i32> %r
}

define <4 x i64> @vpternlog_q_v256_imm11(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2) nounwind {
  %r = tail call <4 x i64> @llvm.x86.avx512.pternlog.q.256(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2, i32 11)
  ret <4 x i64> %r
}

define <16 x i32> @vpternlog_d_v512_imm12(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2) nounwind {
  %r = tail call <16 x i32> @llvm.x86.avx512.pternlog.d.512(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2, i32 12)
  ret <16 x i32> %r
}

define <2 x i64> @vpternlog_q_v128_imm13(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2) nounwind {
  %r = tail call <2 x i64> @llvm.x86.avx512.pternlog.q.128(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2, i32 13)
  ret <2 x i64> %r
}

define <8 x i32> @vpternlog_d_v256_imm14(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2) nounwind {
  %r = tail call <8 x i32> @llvm.x86.avx512.pternlog.d.256(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2, i32 14)
  ret <8 x i32> %r
}

define <8 x i64> @vpternlog_q_v512_imm15(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2) nounwind {
  %r = tail call <8 x i64> @llvm.x86.avx512.pternlog.q.512(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2, i32 15)
  ret <8 x i64> %r
}

define <4 x i32> @vpternlog_d_v128_imm16(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2) nounwind {
  %r = tail call <4 x i32> @llvm.x86.avx512.pternlog.d.128(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2, i32 16)
  ret <4 x i32> %r
}

define <4 x i64> @vpternlog_q_v256_imm17(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2) nounwind {
  %r = tail call <4 x i64> @llvm.x86.avx512.pternlog.q.256(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2, i32 17)
  ret <4 x i64> %r
}

define <16 x i32> @vpternlog_d_v512_imm18(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2) nounwind {
  %r = tail call <16 x i32> @llvm.x86.avx512.pternlog.d.512(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2, i32 18)
  ret <16 x i32> %r
}

define <2 x i64> @vpternlog_q_v128_imm19(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2) nounwind {
  %r = tail call <2 x i64> @llvm.x86.avx512.pternlog.q.128(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2, i32 19)
  ret <2 x i64> %r
}

define <8 x i32> @vpternlog_d_v256_imm20(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2) nounwind {
  %r = tail call <8 x i32> @llvm.x86.avx512.pternlog.d.256(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2, i32 20)
  ret <8 x i32> %r
}

define <8 x i64> @vpternlog_q_v512_imm21(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2) nounwind {
  %r = tail call <8 x i64> @llvm.x86.avx512.pternlog.q.512(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2, i32 21)
  ret <8 x i64> %r
}

;; This is one_bit_set pattern
define <4 x i32> @vpternlog_d_v128_imm22(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2) nounwind {
  %r = tail call <4 x i32> @llvm.x86.avx512.pternlog.d.128(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2, i32 22)
  ret <4 x i32> %r
}

define <4 x i64> @vpternlog_q_v256_imm23(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2) nounwind {
  %r = tail call <4 x i64> @llvm.x86.avx512.pternlog.q.256(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2, i32 23)
  ret <4 x i64> %r
}

define <16 x i32> @vpternlog_d_v512_imm24(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2) nounwind {
  %r = tail call <16 x i32> @llvm.x86.avx512.pternlog.d.512(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2, i32 24)
  ret <16 x i32> %r
}

define <2 x i64> @vpternlog_q_v128_imm25(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2) nounwind {
  %r = tail call <2 x i64> @llvm.x86.avx512.pternlog.q.128(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2, i32 25)
  ret <2 x i64> %r
}

define <8 x i32> @vpternlog_d_v256_imm26(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2) nounwind {
  %r = tail call <8 x i32> @llvm.x86.avx512.pternlog.d.256(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2, i32 26)
  ret <8 x i32> %r
}

define <8 x i64> @vpternlog_q_v512_imm27(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2) nounwind {
  %r = tail call <8 x i64> @llvm.x86.avx512.pternlog.q.512(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2, i32 27)
  ret <8 x i64> %r
}

define <4 x i32> @vpternlog_d_v128_imm28(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2) nounwind {
  %r = tail call <4 x i32> @llvm.x86.avx512.pternlog.d.128(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2, i32 28)
  ret <4 x i32> %r
}

define <4 x i64> @vpternlog_q_v256_imm29(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2) nounwind {
  %r = tail call <4 x i64> @llvm.x86.avx512.pternlog.q.256(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2, i32 29)
  ret <4 x i64> %r
}

define <16 x i32> @vpternlog_d_v512_imm30(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2) nounwind {
  %r = tail call <16 x i32> @llvm.x86.avx512.pternlog.d.512(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2, i32 30)
  ret <16 x i32> %r
}

define <2 x i64> @vpternlog_q_v128_imm31(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2) nounwind {
  %r = tail call <2 x i64> @llvm.x86.avx512.pternlog.q.128(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2, i32 31)
  ret <2 x i64> %r
}

define <8 x i32> @vpternlog_d_v256_imm32(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2) nounwind {
  %r = tail call <8 x i32> @llvm.x86.avx512.pternlog.d.256(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2, i32 32)
  ret <8 x i32> %r
}

define <8 x i64> @vpternlog_q_v512_imm33(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2) nounwind {
  %r = tail call <8 x i64> @llvm.x86.avx512.pternlog.q.512(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2, i32 33)
  ret <8 x i64> %r
}

define <4 x i32> @vpternlog_d_v128_imm34(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2) nounwind {
  %r = tail call <4 x i32> @llvm.x86.avx512.pternlog.d.128(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2, i32 34)
  ret <4 x i32> %r
}

define <4 x i64> @vpternlog_q_v256_imm35(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2) nounwind {
  %r = tail call <4 x i64> @llvm.x86.avx512.pternlog.q.256(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2, i32 35)
  ret <4 x i64> %r
}

define <16 x i32> @vpternlog_d_v512_imm36(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2) nounwind {
  %r = tail call <16 x i32> @llvm.x86.avx512.pternlog.d.512(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2, i32 36)
  ret <16 x i32> %r
}

define <2 x i64> @vpternlog_q_v128_imm37(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2) nounwind {
  %r = tail call <2 x i64> @llvm.x86.avx512.pternlog.q.128(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2, i32 37)
  ret <2 x i64> %r
}

define <8 x i32> @vpternlog_d_v256_imm38(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2) nounwind {
  %r = tail call <8 x i32> @llvm.x86.avx512.pternlog.d.256(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2, i32 38)
  ret <8 x i32> %r
}

define <8 x i64> @vpternlog_q_v512_imm39(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2) nounwind {
  %r = tail call <8 x i64> @llvm.x86.avx512.pternlog.q.512(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2, i32 39)
  ret <8 x i64> %r
}

define <4 x i32> @vpternlog_d_v128_imm40(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2) nounwind {
  %r = tail call <4 x i32> @llvm.x86.avx512.pternlog.d.128(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2, i32 40)
  ret <4 x i32> %r
}

define <4 x i64> @vpternlog_q_v256_imm41(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2) nounwind {
  %r = tail call <4 x i64> @llvm.x86.avx512.pternlog.q.256(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2, i32 41)
  ret <4 x i64> %r
}

define <16 x i32> @vpternlog_d_v512_imm42(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2) nounwind {
  %r = tail call <16 x i32> @llvm.x86.avx512.pternlog.d.512(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2, i32 42)
  ret <16 x i32> %r
}

define <2 x i64> @vpternlog_q_v128_imm43(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2) nounwind {
  %r = tail call <2 x i64> @llvm.x86.avx512.pternlog.q.128(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2, i32 43)
  ret <2 x i64> %r
}

define <8 x i32> @vpternlog_d_v256_imm44(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2) nounwind {
  %r = tail call <8 x i32> @llvm.x86.avx512.pternlog.d.256(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2, i32 44)
  ret <8 x i32> %r
}

define <8 x i64> @vpternlog_q_v512_imm45(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2) nounwind {
  %r = tail call <8 x i64> @llvm.x86.avx512.pternlog.q.512(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2, i32 45)
  ret <8 x i64> %r
}

define <4 x i32> @vpternlog_d_v128_imm46(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2) nounwind {
  %r = tail call <4 x i32> @llvm.x86.avx512.pternlog.d.128(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2, i32 46)
  ret <4 x i32> %r
}

define <4 x i64> @vpternlog_q_v256_imm47(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2) nounwind {
  %r = tail call <4 x i64> @llvm.x86.avx512.pternlog.q.256(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2, i32 47)
  ret <4 x i64> %r
}

define <16 x i32> @vpternlog_d_v512_imm48(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2) nounwind {
  %r = tail call <16 x i32> @llvm.x86.avx512.pternlog.d.512(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2, i32 48)
  ret <16 x i32> %r
}

define <2 x i64> @vpternlog_q_v128_imm49(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2) nounwind {
  %r = tail call <2 x i64> @llvm.x86.avx512.pternlog.q.128(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2, i32 49)
  ret <2 x i64> %r
}

define <8 x i32> @vpternlog_d_v256_imm50(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2) nounwind {
  %r = tail call <8 x i32> @llvm.x86.avx512.pternlog.d.256(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2, i32 50)
  ret <8 x i32> %r
}

define <8 x i64> @vpternlog_q_v512_imm51(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2) nounwind {
  %r = tail call <8 x i64> @llvm.x86.avx512.pternlog.q.512(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2, i32 51)
  ret <8 x i64> %r
}

define <4 x i32> @vpternlog_d_v128_imm52(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2) nounwind {
  %r = tail call <4 x i32> @llvm.x86.avx512.pternlog.d.128(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2, i32 52)
  ret <4 x i32> %r
}

define <4 x i64> @vpternlog_q_v256_imm53(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2) nounwind {
  %r = tail call <4 x i64> @llvm.x86.avx512.pternlog.q.256(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2, i32 53)
  ret <4 x i64> %r
}

define <16 x i32> @vpternlog_d_v512_imm54(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2) nounwind {
  %r = tail call <16 x i32> @llvm.x86.avx512.pternlog.d.512(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2, i32 54)
  ret <16 x i32> %r
}

define <2 x i64> @vpternlog_q_v128_imm55(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2) nounwind {
  %r = tail call <2 x i64> @llvm.x86.avx512.pternlog.q.128(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2, i32 55)
  ret <2 x i64> %r
}

define <8 x i32> @vpternlog_d_v256_imm56(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2) nounwind {
  %r = tail call <8 x i32> @llvm.x86.avx512.pternlog.d.256(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2, i32 56)
  ret <8 x i32> %r
}

define <8 x i64> @vpternlog_q_v512_imm57(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2) nounwind {
  %r = tail call <8 x i64> @llvm.x86.avx512.pternlog.q.512(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2, i32 57)
  ret <8 x i64> %r
}

define <4 x i32> @vpternlog_d_v128_imm58(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2) nounwind {
  %r = tail call <4 x i32> @llvm.x86.avx512.pternlog.d.128(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2, i32 58)
  ret <4 x i32> %r
}

define <4 x i64> @vpternlog_q_v256_imm59(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2) nounwind {
  %r = tail call <4 x i64> @llvm.x86.avx512.pternlog.q.256(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2, i32 59)
  ret <4 x i64> %r
}

define <16 x i32> @vpternlog_d_v512_imm60(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2) nounwind {
  %r = tail call <16 x i32> @llvm.x86.avx512.pternlog.d.512(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2, i32 60)
  ret <16 x i32> %r
}

define <2 x i64> @vpternlog_q_v128_imm61(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2) nounwind {
  %r = tail call <2 x i64> @llvm.x86.avx512.pternlog.q.128(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2, i32 61)
  ret <2 x i64> %r
}

define <8 x i32> @vpternlog_d_v256_imm62(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2) nounwind {
  %r = tail call <8 x i32> @llvm.x86.avx512.pternlog.d.256(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2, i32 62)
  ret <8 x i32> %r
}

define <8 x i64> @vpternlog_q_v512_imm63(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2) nounwind {
  %r = tail call <8 x i64> @llvm.x86.avx512.pternlog.q.512(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2, i32 63)
  ret <8 x i64> %r
}

define <4 x i32> @vpternlog_d_v128_imm64(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2) nounwind {
  %r = tail call <4 x i32> @llvm.x86.avx512.pternlog.d.128(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2, i32 64)
  ret <4 x i32> %r
}

define <4 x i64> @vpternlog_q_v256_imm65(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2) nounwind {
  %r = tail call <4 x i64> @llvm.x86.avx512.pternlog.q.256(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2, i32 65)
  ret <4 x i64> %r
}

define <16 x i32> @vpternlog_d_v512_imm66(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2) nounwind {
  %r = tail call <16 x i32> @llvm.x86.avx512.pternlog.d.512(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2, i32 66)
  ret <16 x i32> %r
}

define <2 x i64> @vpternlog_q_v128_imm67(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2) nounwind {
  %r = tail call <2 x i64> @llvm.x86.avx512.pternlog.q.128(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2, i32 67)
  ret <2 x i64> %r
}

define <8 x i32> @vpternlog_d_v256_imm68(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2) nounwind {
  %r = tail call <8 x i32> @llvm.x86.avx512.pternlog.d.256(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2, i32 68)
  ret <8 x i32> %r
}

define <8 x i64> @vpternlog_q_v512_imm69(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2) nounwind {
  %r = tail call <8 x i64> @llvm.x86.avx512.pternlog.q.512(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2, i32 69)
  ret <8 x i64> %r
}

define <4 x i32> @vpternlog_d_v128_imm70(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2) nounwind {
  %r = tail call <4 x i32> @llvm.x86.avx512.pternlog.d.128(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2, i32 70)
  ret <4 x i32> %r
}

define <4 x i64> @vpternlog_q_v256_imm71(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2) nounwind {
  %r = tail call <4 x i64> @llvm.x86.avx512.pternlog.q.256(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2, i32 71)
  ret <4 x i64> %r
}

define <16 x i32> @vpternlog_d_v512_imm72(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2) nounwind {
  %r = tail call <16 x i32> @llvm.x86.avx512.pternlog.d.512(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2, i32 72)
  ret <16 x i32> %r
}

define <2 x i64> @vpternlog_q_v128_imm73(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2) nounwind {
  %r = tail call <2 x i64> @llvm.x86.avx512.pternlog.q.128(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2, i32 73)
  ret <2 x i64> %r
}

define <8 x i32> @vpternlog_d_v256_imm74(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2) nounwind {
  %r = tail call <8 x i32> @llvm.x86.avx512.pternlog.d.256(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2, i32 74)
  ret <8 x i32> %r
}

define <8 x i64> @vpternlog_q_v512_imm75(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2) nounwind {
  %r = tail call <8 x i64> @llvm.x86.avx512.pternlog.q.512(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2, i32 75)
  ret <8 x i64> %r
}

define <4 x i32> @vpternlog_d_v128_imm76(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2) nounwind {
  %r = tail call <4 x i32> @llvm.x86.avx512.pternlog.d.128(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2, i32 76)
  ret <4 x i32> %r
}

define <4 x i64> @vpternlog_q_v256_imm77(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2) nounwind {
  %r = tail call <4 x i64> @llvm.x86.avx512.pternlog.q.256(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2, i32 77)
  ret <4 x i64> %r
}

define <16 x i32> @vpternlog_d_v512_imm78(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2) nounwind {
  %r = tail call <16 x i32> @llvm.x86.avx512.pternlog.d.512(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2, i32 78)
  ret <16 x i32> %r
}

define <2 x i64> @vpternlog_q_v128_imm79(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2) nounwind {
  %r = tail call <2 x i64> @llvm.x86.avx512.pternlog.q.128(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2, i32 79)
  ret <2 x i64> %r
}

define <8 x i32> @vpternlog_d_v256_imm80(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2) nounwind {
  %r = tail call <8 x i32> @llvm.x86.avx512.pternlog.d.256(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2, i32 80)
  ret <8 x i32> %r
}

define <8 x i64> @vpternlog_q_v512_imm81(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2) nounwind {
  %r = tail call <8 x i64> @llvm.x86.avx512.pternlog.q.512(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2, i32 81)
  ret <8 x i64> %r
}

define <4 x i32> @vpternlog_d_v128_imm82(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2) nounwind {
  %r = tail call <4 x i32> @llvm.x86.avx512.pternlog.d.128(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2, i32 82)
  ret <4 x i32> %r
}

define <4 x i64> @vpternlog_q_v256_imm83(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2) nounwind {
  %r = tail call <4 x i64> @llvm.x86.avx512.pternlog.q.256(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2, i32 83)
  ret <4 x i64> %r
}

define <16 x i32> @vpternlog_d_v512_imm84(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2) nounwind {
  %r = tail call <16 x i32> @llvm.x86.avx512.pternlog.d.512(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2, i32 84)
  ret <16 x i32> %r
}

define <2 x i64> @vpternlog_q_v128_imm85(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2) nounwind {
  %r = tail call <2 x i64> @llvm.x86.avx512.pternlog.q.128(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2, i32 85)
  ret <2 x i64> %r
}

define <8 x i32> @vpternlog_d_v256_imm86(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2) nounwind {
  %r = tail call <8 x i32> @llvm.x86.avx512.pternlog.d.256(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2, i32 86)
  ret <8 x i32> %r
}

define <8 x i64> @vpternlog_q_v512_imm87(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2) nounwind {
  %r = tail call <8 x i64> @llvm.x86.avx512.pternlog.q.512(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2, i32 87)
  ret <8 x i64> %r
}

define <4 x i32> @vpternlog_d_v128_imm88(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2) nounwind {
  %r = tail call <4 x i32> @llvm.x86.avx512.pternlog.d.128(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2, i32 88)
  ret <4 x i32> %r
}

define <4 x i64> @vpternlog_q_v256_imm89(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2) nounwind {
  %r = tail call <4 x i64> @llvm.x86.avx512.pternlog.q.256(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2, i32 89)
  ret <4 x i64> %r
}

define <16 x i32> @vpternlog_d_v512_imm90(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2) nounwind {
  %r = tail call <16 x i32> @llvm.x86.avx512.pternlog.d.512(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2, i32 90)
  ret <16 x i32> %r
}

define <2 x i64> @vpternlog_q_v128_imm91(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2) nounwind {
  %r = tail call <2 x i64> @llvm.x86.avx512.pternlog.q.128(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2, i32 91)
  ret <2 x i64> %r
}

define <8 x i32> @vpternlog_d_v256_imm92(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2) nounwind {
  %r = tail call <8 x i32> @llvm.x86.avx512.pternlog.d.256(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2, i32 92)
  ret <8 x i32> %r
}

define <8 x i64> @vpternlog_q_v512_imm93(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2) nounwind {
  %r = tail call <8 x i64> @llvm.x86.avx512.pternlog.q.512(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2, i32 93)
  ret <8 x i64> %r
}

define <4 x i32> @vpternlog_d_v128_imm94(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2) nounwind {
  %r = tail call <4 x i32> @llvm.x86.avx512.pternlog.d.128(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2, i32 94)
  ret <4 x i32> %r
}

define <4 x i64> @vpternlog_q_v256_imm95(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2) nounwind {
  %r = tail call <4 x i64> @llvm.x86.avx512.pternlog.q.256(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2, i32 95)
  ret <4 x i64> %r
}

define <16 x i32> @vpternlog_d_v512_imm96(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2) nounwind {
  %r = tail call <16 x i32> @llvm.x86.avx512.pternlog.d.512(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2, i32 96)
  ret <16 x i32> %r
}

define <2 x i64> @vpternlog_q_v128_imm97(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2) nounwind {
  %r = tail call <2 x i64> @llvm.x86.avx512.pternlog.q.128(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2, i32 97)
  ret <2 x i64> %r
}

define <8 x i32> @vpternlog_d_v256_imm98(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2) nounwind {
  %r = tail call <8 x i32> @llvm.x86.avx512.pternlog.d.256(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2, i32 98)
  ret <8 x i32> %r
}

define <8 x i64> @vpternlog_q_v512_imm99(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2) nounwind {
  %r = tail call <8 x i64> @llvm.x86.avx512.pternlog.q.512(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2, i32 99)
  ret <8 x i64> %r
}

define <4 x i32> @vpternlog_d_v128_imm100(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2) nounwind {
  %r = tail call <4 x i32> @llvm.x86.avx512.pternlog.d.128(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2, i32 100)
  ret <4 x i32> %r
}

define <4 x i64> @vpternlog_q_v256_imm101(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2) nounwind {
  %r = tail call <4 x i64> @llvm.x86.avx512.pternlog.q.256(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2, i32 101)
  ret <4 x i64> %r
}

define <16 x i32> @vpternlog_d_v512_imm102(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2) nounwind {
  %r = tail call <16 x i32> @llvm.x86.avx512.pternlog.d.512(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2, i32 102)
  ret <16 x i32> %r
}

define <2 x i64> @vpternlog_q_v128_imm103(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2) nounwind {
  %r = tail call <2 x i64> @llvm.x86.avx512.pternlog.q.128(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2, i32 103)
  ret <2 x i64> %r
}

; This is two_bits_set pattern
define <8 x i32> @vpternlog_d_v256_imm104(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2) nounwind {
  %r = tail call <8 x i32> @llvm.x86.avx512.pternlog.d.256(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2, i32 104)
  ret <8 x i32> %r
}

define <8 x i64> @vpternlog_q_v512_imm105(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2) nounwind {
  %r = tail call <8 x i64> @llvm.x86.avx512.pternlog.q.512(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2, i32 105)
  ret <8 x i64> %r
}

define <4 x i32> @vpternlog_d_v128_imm106(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2) nounwind {
  %r = tail call <4 x i32> @llvm.x86.avx512.pternlog.d.128(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2, i32 106)
  ret <4 x i32> %r
}

define <4 x i64> @vpternlog_q_v256_imm107(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2) nounwind {
  %r = tail call <4 x i64> @llvm.x86.avx512.pternlog.q.256(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2, i32 107)
  ret <4 x i64> %r
}

define <16 x i32> @vpternlog_d_v512_imm108(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2) nounwind {
  %r = tail call <16 x i32> @llvm.x86.avx512.pternlog.d.512(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2, i32 108)
  ret <16 x i32> %r
}

define <2 x i64> @vpternlog_q_v128_imm109(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2) nounwind {
  %r = tail call <2 x i64> @llvm.x86.avx512.pternlog.q.128(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2, i32 109)
  ret <2 x i64> %r
}

define <8 x i32> @vpternlog_d_v256_imm110(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2) nounwind {
  %r = tail call <8 x i32> @llvm.x86.avx512.pternlog.d.256(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2, i32 110)
  ret <8 x i32> %r
}

define <8 x i64> @vpternlog_q_v512_imm111(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2) nounwind {
  %r = tail call <8 x i64> @llvm.x86.avx512.pternlog.q.512(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2, i32 111)
  ret <8 x i64> %r
}

define <4 x i32> @vpternlog_d_v128_imm112(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2) nounwind {
  %r = tail call <4 x i32> @llvm.x86.avx512.pternlog.d.128(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2, i32 112)
  ret <4 x i32> %r
}

define <4 x i64> @vpternlog_q_v256_imm113(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2) nounwind {
  %r = tail call <4 x i64> @llvm.x86.avx512.pternlog.q.256(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2, i32 113)
  ret <4 x i64> %r
}

define <16 x i32> @vpternlog_d_v512_imm114(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2) nounwind {
  %r = tail call <16 x i32> @llvm.x86.avx512.pternlog.d.512(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2, i32 114)
  ret <16 x i32> %r
}

define <2 x i64> @vpternlog_q_v128_imm115(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2) nounwind {
  %r = tail call <2 x i64> @llvm.x86.avx512.pternlog.q.128(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2, i32 115)
  ret <2 x i64> %r
}

define <8 x i32> @vpternlog_d_v256_imm116(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2) nounwind {
  %r = tail call <8 x i32> @llvm.x86.avx512.pternlog.d.256(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2, i32 116)
  ret <8 x i32> %r
}

define <8 x i64> @vpternlog_q_v512_imm117(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2) nounwind {
  %r = tail call <8 x i64> @llvm.x86.avx512.pternlog.q.512(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2, i32 117)
  ret <8 x i64> %r
}

define <4 x i32> @vpternlog_d_v128_imm118(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2) nounwind {
  %r = tail call <4 x i32> @llvm.x86.avx512.pternlog.d.128(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2, i32 118)
  ret <4 x i32> %r
}

define <4 x i64> @vpternlog_q_v256_imm119(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2) nounwind {
  %r = tail call <4 x i64> @llvm.x86.avx512.pternlog.q.256(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2, i32 119)
  ret <4 x i64> %r
}

define <16 x i32> @vpternlog_d_v512_imm120(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2) nounwind {
  %r = tail call <16 x i32> @llvm.x86.avx512.pternlog.d.512(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2, i32 120)
  ret <16 x i32> %r
}

define <2 x i64> @vpternlog_q_v128_imm121(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2) nounwind {
  %r = tail call <2 x i64> @llvm.x86.avx512.pternlog.q.128(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2, i32 121)
  ret <2 x i64> %r
}

define <8 x i32> @vpternlog_d_v256_imm122(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2) nounwind {
  %r = tail call <8 x i32> @llvm.x86.avx512.pternlog.d.256(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2, i32 122)
  ret <8 x i32> %r
}

define <8 x i64> @vpternlog_q_v512_imm123(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2) nounwind {
  %r = tail call <8 x i64> @llvm.x86.avx512.pternlog.q.512(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2, i32 123)
  ret <8 x i64> %r
}

define <4 x i32> @vpternlog_d_v128_imm124(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2) nounwind {
  %r = tail call <4 x i32> @llvm.x86.avx512.pternlog.d.128(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2, i32 124)
  ret <4 x i32> %r
}

define <4 x i64> @vpternlog_q_v256_imm125(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2) nounwind {
  %r = tail call <4 x i64> @llvm.x86.avx512.pternlog.q.256(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2, i32 125)
  ret <4 x i64> %r
}

define <16 x i32> @vpternlog_d_v512_imm126(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2) nounwind {
  %r = tail call <16 x i32> @llvm.x86.avx512.pternlog.d.512(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2, i32 126)
  ret <16 x i32> %r
}

define <2 x i64> @vpternlog_q_v128_imm127(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2) nounwind {
  %r = tail call <2 x i64> @llvm.x86.avx512.pternlog.q.128(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2, i32 127)
  ret <2 x i64> %r
}

define <8 x i32> @vpternlog_d_v256_imm128(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2) nounwind {
  %r = tail call <8 x i32> @llvm.x86.avx512.pternlog.d.256(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2, i32 128)
  ret <8 x i32> %r
}

define <8 x i64> @vpternlog_q_v512_imm129(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2) nounwind {
  %r = tail call <8 x i64> @llvm.x86.avx512.pternlog.q.512(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2, i32 129)
  ret <8 x i64> %r
}

define <4 x i32> @vpternlog_d_v128_imm130(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2) nounwind {
  %r = tail call <4 x i32> @llvm.x86.avx512.pternlog.d.128(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2, i32 130)
  ret <4 x i32> %r
}

define <4 x i64> @vpternlog_q_v256_imm131(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2) nounwind {
  %r = tail call <4 x i64> @llvm.x86.avx512.pternlog.q.256(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2, i32 131)
  ret <4 x i64> %r
}

define <16 x i32> @vpternlog_d_v512_imm132(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2) nounwind {
  %r = tail call <16 x i32> @llvm.x86.avx512.pternlog.d.512(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2, i32 132)
  ret <16 x i32> %r
}

define <2 x i64> @vpternlog_q_v128_imm133(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2) nounwind {
  %r = tail call <2 x i64> @llvm.x86.avx512.pternlog.q.128(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2, i32 133)
  ret <2 x i64> %r
}

define <8 x i32> @vpternlog_d_v256_imm134(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2) nounwind {
  %r = tail call <8 x i32> @llvm.x86.avx512.pternlog.d.256(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2, i32 134)
  ret <8 x i32> %r
}

define <8 x i64> @vpternlog_q_v512_imm135(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2) nounwind {
  %r = tail call <8 x i64> @llvm.x86.avx512.pternlog.q.512(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2, i32 135)
  ret <8 x i64> %r
}

define <4 x i32> @vpternlog_d_v128_imm136(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2) nounwind {
  %r = tail call <4 x i32> @llvm.x86.avx512.pternlog.d.128(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2, i32 136)
  ret <4 x i32> %r
}

define <4 x i64> @vpternlog_q_v256_imm137(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2) nounwind {
  %r = tail call <4 x i64> @llvm.x86.avx512.pternlog.q.256(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2, i32 137)
  ret <4 x i64> %r
}

define <16 x i32> @vpternlog_d_v512_imm138(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2) nounwind {
  %r = tail call <16 x i32> @llvm.x86.avx512.pternlog.d.512(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2, i32 138)
  ret <16 x i32> %r
}

define <2 x i64> @vpternlog_q_v128_imm139(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2) nounwind {
  %r = tail call <2 x i64> @llvm.x86.avx512.pternlog.q.128(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2, i32 139)
  ret <2 x i64> %r
}

define <8 x i32> @vpternlog_d_v256_imm140(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2) nounwind {
  %r = tail call <8 x i32> @llvm.x86.avx512.pternlog.d.256(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2, i32 140)
  ret <8 x i32> %r
}

define <8 x i64> @vpternlog_q_v512_imm141(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2) nounwind {
  %r = tail call <8 x i64> @llvm.x86.avx512.pternlog.q.512(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2, i32 141)
  ret <8 x i64> %r
}

define <4 x i32> @vpternlog_d_v128_imm142(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2) nounwind {
  %r = tail call <4 x i32> @llvm.x86.avx512.pternlog.d.128(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2, i32 142)
  ret <4 x i32> %r
}

define <4 x i64> @vpternlog_q_v256_imm143(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2) nounwind {
  %r = tail call <4 x i64> @llvm.x86.avx512.pternlog.q.256(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2, i32 143)
  ret <4 x i64> %r
}

define <16 x i32> @vpternlog_d_v512_imm144(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2) nounwind {
  %r = tail call <16 x i32> @llvm.x86.avx512.pternlog.d.512(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2, i32 144)
  ret <16 x i32> %r
}

define <2 x i64> @vpternlog_q_v128_imm145(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2) nounwind {
  %r = tail call <2 x i64> @llvm.x86.avx512.pternlog.q.128(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2, i32 145)
  ret <2 x i64> %r
}

define <8 x i32> @vpternlog_d_v256_imm146(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2) nounwind {
  %r = tail call <8 x i32> @llvm.x86.avx512.pternlog.d.256(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2, i32 146)
  ret <8 x i32> %r
}

define <8 x i64> @vpternlog_q_v512_imm147(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2) nounwind {
  %r = tail call <8 x i64> @llvm.x86.avx512.pternlog.q.512(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2, i32 147)
  ret <8 x i64> %r
}

define <4 x i32> @vpternlog_d_v128_imm148(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2) nounwind {
  %r = tail call <4 x i32> @llvm.x86.avx512.pternlog.d.128(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2, i32 148)
  ret <4 x i32> %r
}

define <4 x i64> @vpternlog_q_v256_imm149(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2) nounwind {
  %r = tail call <4 x i64> @llvm.x86.avx512.pternlog.q.256(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2, i32 149)
  ret <4 x i64> %r
}

define <16 x i32> @vpternlog_d_v512_imm150(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2) nounwind {
  %r = tail call <16 x i32> @llvm.x86.avx512.pternlog.d.512(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2, i32 150)
  ret <16 x i32> %r
}

define <2 x i64> @vpternlog_q_v128_imm151(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2) nounwind {
  %r = tail call <2 x i64> @llvm.x86.avx512.pternlog.q.128(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2, i32 151)
  ret <2 x i64> %r
}

define <8 x i32> @vpternlog_d_v256_imm152(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2) nounwind {
  %r = tail call <8 x i32> @llvm.x86.avx512.pternlog.d.256(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2, i32 152)
  ret <8 x i32> %r
}

define <8 x i64> @vpternlog_q_v512_imm153(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2) nounwind {
  %r = tail call <8 x i64> @llvm.x86.avx512.pternlog.q.512(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2, i32 153)
  ret <8 x i64> %r
}

define <4 x i32> @vpternlog_d_v128_imm154(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2) nounwind {
  %r = tail call <4 x i32> @llvm.x86.avx512.pternlog.d.128(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2, i32 154)
  ret <4 x i32> %r
}

define <4 x i64> @vpternlog_q_v256_imm155(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2) nounwind {
  %r = tail call <4 x i64> @llvm.x86.avx512.pternlog.q.256(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2, i32 155)
  ret <4 x i64> %r
}

define <16 x i32> @vpternlog_d_v512_imm156(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2) nounwind {
  %r = tail call <16 x i32> @llvm.x86.avx512.pternlog.d.512(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2, i32 156)
  ret <16 x i32> %r
}

define <2 x i64> @vpternlog_q_v128_imm157(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2) nounwind {
  %r = tail call <2 x i64> @llvm.x86.avx512.pternlog.q.128(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2, i32 157)
  ret <2 x i64> %r
}

define <8 x i32> @vpternlog_d_v256_imm158(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2) nounwind {
  %r = tail call <8 x i32> @llvm.x86.avx512.pternlog.d.256(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2, i32 158)
  ret <8 x i32> %r
}

define <8 x i64> @vpternlog_q_v512_imm159(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2) nounwind {
  %r = tail call <8 x i64> @llvm.x86.avx512.pternlog.q.512(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2, i32 159)
  ret <8 x i64> %r
}

define <4 x i32> @vpternlog_d_v128_imm160(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2) nounwind {
  %r = tail call <4 x i32> @llvm.x86.avx512.pternlog.d.128(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2, i32 160)
  ret <4 x i32> %r
}

define <4 x i64> @vpternlog_q_v256_imm161(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2) nounwind {
  %r = tail call <4 x i64> @llvm.x86.avx512.pternlog.q.256(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2, i32 161)
  ret <4 x i64> %r
}

define <16 x i32> @vpternlog_d_v512_imm162(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2) nounwind {
  %r = tail call <16 x i32> @llvm.x86.avx512.pternlog.d.512(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2, i32 162)
  ret <16 x i32> %r
}

define <2 x i64> @vpternlog_q_v128_imm163(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2) nounwind {
  %r = tail call <2 x i64> @llvm.x86.avx512.pternlog.q.128(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2, i32 163)
  ret <2 x i64> %r
}

define <8 x i32> @vpternlog_d_v256_imm164(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2) nounwind {
  %r = tail call <8 x i32> @llvm.x86.avx512.pternlog.d.256(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2, i32 164)
  ret <8 x i32> %r
}

define <8 x i64> @vpternlog_q_v512_imm165(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2) nounwind {
  %r = tail call <8 x i64> @llvm.x86.avx512.pternlog.q.512(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2, i32 165)
  ret <8 x i64> %r
}

define <4 x i32> @vpternlog_d_v128_imm166(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2) nounwind {
  %r = tail call <4 x i32> @llvm.x86.avx512.pternlog.d.128(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2, i32 166)
  ret <4 x i32> %r
}

define <4 x i64> @vpternlog_q_v256_imm167(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2) nounwind {
  %r = tail call <4 x i64> @llvm.x86.avx512.pternlog.q.256(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2, i32 167)
  ret <4 x i64> %r
}

define <16 x i32> @vpternlog_d_v512_imm168(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2) nounwind {
  %r = tail call <16 x i32> @llvm.x86.avx512.pternlog.d.512(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2, i32 168)
  ret <16 x i32> %r
}

define <2 x i64> @vpternlog_q_v128_imm169(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2) nounwind {
  %r = tail call <2 x i64> @llvm.x86.avx512.pternlog.q.128(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2, i32 169)
  ret <2 x i64> %r
}

define <8 x i32> @vpternlog_d_v256_imm170(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2) nounwind {
  %r = tail call <8 x i32> @llvm.x86.avx512.pternlog.d.256(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2, i32 170)
  ret <8 x i32> %r
}

define <8 x i64> @vpternlog_q_v512_imm171(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2) nounwind {
  %r = tail call <8 x i64> @llvm.x86.avx512.pternlog.q.512(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2, i32 171)
  ret <8 x i64> %r
}

define <4 x i32> @vpternlog_d_v128_imm172(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2) nounwind {
  %r = tail call <4 x i32> @llvm.x86.avx512.pternlog.d.128(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2, i32 172)
  ret <4 x i32> %r
}

define <4 x i64> @vpternlog_q_v256_imm173(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2) nounwind {
  %r = tail call <4 x i64> @llvm.x86.avx512.pternlog.q.256(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2, i32 173)
  ret <4 x i64> %r
}

define <16 x i32> @vpternlog_d_v512_imm174(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2) nounwind {
  %r = tail call <16 x i32> @llvm.x86.avx512.pternlog.d.512(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2, i32 174)
  ret <16 x i32> %r
}

define <2 x i64> @vpternlog_q_v128_imm175(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2) nounwind {
  %r = tail call <2 x i64> @llvm.x86.avx512.pternlog.q.128(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2, i32 175)
  ret <2 x i64> %r
}

define <8 x i32> @vpternlog_d_v256_imm176(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2) nounwind {
  %r = tail call <8 x i32> @llvm.x86.avx512.pternlog.d.256(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2, i32 176)
  ret <8 x i32> %r
}

define <8 x i64> @vpternlog_q_v512_imm177(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2) nounwind {
  %r = tail call <8 x i64> @llvm.x86.avx512.pternlog.q.512(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2, i32 177)
  ret <8 x i64> %r
}

define <4 x i32> @vpternlog_d_v128_imm178(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2) nounwind {
  %r = tail call <4 x i32> @llvm.x86.avx512.pternlog.d.128(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2, i32 178)
  ret <4 x i32> %r
}

define <4 x i64> @vpternlog_q_v256_imm179(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2) nounwind {
  %r = tail call <4 x i64> @llvm.x86.avx512.pternlog.q.256(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2, i32 179)
  ret <4 x i64> %r
}

define <16 x i32> @vpternlog_d_v512_imm180(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2) nounwind {
  %r = tail call <16 x i32> @llvm.x86.avx512.pternlog.d.512(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2, i32 180)
  ret <16 x i32> %r
}

define <2 x i64> @vpternlog_q_v128_imm181(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2) nounwind {
  %r = tail call <2 x i64> @llvm.x86.avx512.pternlog.q.128(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2, i32 181)
  ret <2 x i64> %r
}

define <8 x i32> @vpternlog_d_v256_imm182(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2) nounwind {
  %r = tail call <8 x i32> @llvm.x86.avx512.pternlog.d.256(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2, i32 182)
  ret <8 x i32> %r
}

define <8 x i64> @vpternlog_q_v512_imm183(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2) nounwind {
  %r = tail call <8 x i64> @llvm.x86.avx512.pternlog.q.512(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2, i32 183)
  ret <8 x i64> %r
}

define <4 x i32> @vpternlog_d_v128_imm184(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2) nounwind {
  %r = tail call <4 x i32> @llvm.x86.avx512.pternlog.d.128(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2, i32 184)
  ret <4 x i32> %r
}

define <4 x i64> @vpternlog_q_v256_imm185(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2) nounwind {
  %r = tail call <4 x i64> @llvm.x86.avx512.pternlog.q.256(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2, i32 185)
  ret <4 x i64> %r
}

define <16 x i32> @vpternlog_d_v512_imm186(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2) nounwind {
  %r = tail call <16 x i32> @llvm.x86.avx512.pternlog.d.512(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2, i32 186)
  ret <16 x i32> %r
}

define <2 x i64> @vpternlog_q_v128_imm187(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2) nounwind {
  %r = tail call <2 x i64> @llvm.x86.avx512.pternlog.q.128(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2, i32 187)
  ret <2 x i64> %r
}

define <8 x i32> @vpternlog_d_v256_imm188(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2) nounwind {
  %r = tail call <8 x i32> @llvm.x86.avx512.pternlog.d.256(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2, i32 188)
  ret <8 x i32> %r
}

define <8 x i64> @vpternlog_q_v512_imm189(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2) nounwind {
  %r = tail call <8 x i64> @llvm.x86.avx512.pternlog.q.512(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2, i32 189)
  ret <8 x i64> %r
}

define <4 x i32> @vpternlog_d_v128_imm190(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2) nounwind {
  %r = tail call <4 x i32> @llvm.x86.avx512.pternlog.d.128(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2, i32 190)
  ret <4 x i32> %r
}

define <4 x i64> @vpternlog_q_v256_imm191(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2) nounwind {
  %r = tail call <4 x i64> @llvm.x86.avx512.pternlog.q.256(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2, i32 191)
  ret <4 x i64> %r
}

define <16 x i32> @vpternlog_d_v512_imm192(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2) nounwind {
  %r = tail call <16 x i32> @llvm.x86.avx512.pternlog.d.512(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2, i32 192)
  ret <16 x i32> %r
}

define <2 x i64> @vpternlog_q_v128_imm193(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2) nounwind {
  %r = tail call <2 x i64> @llvm.x86.avx512.pternlog.q.128(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2, i32 193)
  ret <2 x i64> %r
}

define <8 x i32> @vpternlog_d_v256_imm194(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2) nounwind {
  %r = tail call <8 x i32> @llvm.x86.avx512.pternlog.d.256(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2, i32 194)
  ret <8 x i32> %r
}

define <8 x i64> @vpternlog_q_v512_imm195(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2) nounwind {
  %r = tail call <8 x i64> @llvm.x86.avx512.pternlog.q.512(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2, i32 195)
  ret <8 x i64> %r
}

define <4 x i32> @vpternlog_d_v128_imm196(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2) nounwind {
  %r = tail call <4 x i32> @llvm.x86.avx512.pternlog.d.128(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2, i32 196)
  ret <4 x i32> %r
}

define <4 x i64> @vpternlog_q_v256_imm197(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2) nounwind {
  %r = tail call <4 x i64> @llvm.x86.avx512.pternlog.q.256(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2, i32 197)
  ret <4 x i64> %r
}

define <16 x i32> @vpternlog_d_v512_imm198(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2) nounwind {
  %r = tail call <16 x i32> @llvm.x86.avx512.pternlog.d.512(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2, i32 198)
  ret <16 x i32> %r
}

define <2 x i64> @vpternlog_q_v128_imm199(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2) nounwind {
  %r = tail call <2 x i64> @llvm.x86.avx512.pternlog.q.128(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2, i32 199)
  ret <2 x i64> %r
}

define <8 x i32> @vpternlog_d_v256_imm200(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2) nounwind {
  %r = tail call <8 x i32> @llvm.x86.avx512.pternlog.d.256(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2, i32 200)
  ret <8 x i32> %r
}

define <8 x i64> @vpternlog_q_v512_imm201(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2) nounwind {
  %r = tail call <8 x i64> @llvm.x86.avx512.pternlog.q.512(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2, i32 201)
  ret <8 x i64> %r
}

define <4 x i32> @vpternlog_d_v128_imm202(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2) nounwind {
  %r = tail call <4 x i32> @llvm.x86.avx512.pternlog.d.128(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2, i32 202)
  ret <4 x i32> %r
}

define <4 x i64> @vpternlog_q_v256_imm203(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2) nounwind {
  %r = tail call <4 x i64> @llvm.x86.avx512.pternlog.q.256(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2, i32 203)
  ret <4 x i64> %r
}

define <16 x i32> @vpternlog_d_v512_imm204(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2) nounwind {
  %r = tail call <16 x i32> @llvm.x86.avx512.pternlog.d.512(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2, i32 204)
  ret <16 x i32> %r
}

define <2 x i64> @vpternlog_q_v128_imm205(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2) nounwind {
  %r = tail call <2 x i64> @llvm.x86.avx512.pternlog.q.128(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2, i32 205)
  ret <2 x i64> %r
}

define <8 x i32> @vpternlog_d_v256_imm206(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2) nounwind {
  %r = tail call <8 x i32> @llvm.x86.avx512.pternlog.d.256(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2, i32 206)
  ret <8 x i32> %r
}

define <8 x i64> @vpternlog_q_v512_imm207(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2) nounwind {
  %r = tail call <8 x i64> @llvm.x86.avx512.pternlog.q.512(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2, i32 207)
  ret <8 x i64> %r
}

define <4 x i32> @vpternlog_d_v128_imm208(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2) nounwind {
  %r = tail call <4 x i32> @llvm.x86.avx512.pternlog.d.128(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2, i32 208)
  ret <4 x i32> %r
}

define <4 x i64> @vpternlog_q_v256_imm209(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2) nounwind {
  %r = tail call <4 x i64> @llvm.x86.avx512.pternlog.q.256(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2, i32 209)
  ret <4 x i64> %r
}

define <16 x i32> @vpternlog_d_v512_imm210(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2) nounwind {
  %r = tail call <16 x i32> @llvm.x86.avx512.pternlog.d.512(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2, i32 210)
  ret <16 x i32> %r
}

define <2 x i64> @vpternlog_q_v128_imm211(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2) nounwind {
  %r = tail call <2 x i64> @llvm.x86.avx512.pternlog.q.128(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2, i32 211)
  ret <2 x i64> %r
}

define <8 x i32> @vpternlog_d_v256_imm212(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2) nounwind {
  %r = tail call <8 x i32> @llvm.x86.avx512.pternlog.d.256(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2, i32 212)
  ret <8 x i32> %r
}

define <8 x i64> @vpternlog_q_v512_imm213(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2) nounwind {
  %r = tail call <8 x i64> @llvm.x86.avx512.pternlog.q.512(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2, i32 213)
  ret <8 x i64> %r
}

define <4 x i32> @vpternlog_d_v128_imm214(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2) nounwind {
  %r = tail call <4 x i32> @llvm.x86.avx512.pternlog.d.128(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2, i32 214)
  ret <4 x i32> %r
}

define <4 x i64> @vpternlog_q_v256_imm215(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2) nounwind {
  %r = tail call <4 x i64> @llvm.x86.avx512.pternlog.q.256(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2, i32 215)
  ret <4 x i64> %r
}

define <16 x i32> @vpternlog_d_v512_imm216(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2) nounwind {
  %r = tail call <16 x i32> @llvm.x86.avx512.pternlog.d.512(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2, i32 216)
  ret <16 x i32> %r
}

define <2 x i64> @vpternlog_q_v128_imm217(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2) nounwind {
  %r = tail call <2 x i64> @llvm.x86.avx512.pternlog.q.128(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2, i32 217)
  ret <2 x i64> %r
}

define <8 x i32> @vpternlog_d_v256_imm218(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2) nounwind {
  %r = tail call <8 x i32> @llvm.x86.avx512.pternlog.d.256(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2, i32 218)
  ret <8 x i32> %r
}

define <8 x i64> @vpternlog_q_v512_imm219(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2) nounwind {
  %r = tail call <8 x i64> @llvm.x86.avx512.pternlog.q.512(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2, i32 219)
  ret <8 x i64> %r
}

define <4 x i32> @vpternlog_d_v128_imm220(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2) nounwind {
  %r = tail call <4 x i32> @llvm.x86.avx512.pternlog.d.128(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2, i32 220)
  ret <4 x i32> %r
}

define <4 x i64> @vpternlog_q_v256_imm221(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2) nounwind {
  %r = tail call <4 x i64> @llvm.x86.avx512.pternlog.q.256(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2, i32 221)
  ret <4 x i64> %r
}

define <16 x i32> @vpternlog_d_v512_imm222(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2) nounwind {
  %r = tail call <16 x i32> @llvm.x86.avx512.pternlog.d.512(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2, i32 222)
  ret <16 x i32> %r
}

define <2 x i64> @vpternlog_q_v128_imm223(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2) nounwind {
  %r = tail call <2 x i64> @llvm.x86.avx512.pternlog.q.128(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2, i32 223)
  ret <2 x i64> %r
}

define <8 x i32> @vpternlog_d_v256_imm224(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2) nounwind {
  %r = tail call <8 x i32> @llvm.x86.avx512.pternlog.d.256(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2, i32 224)
  ret <8 x i32> %r
}

define <8 x i64> @vpternlog_q_v512_imm225(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2) nounwind {
  %r = tail call <8 x i64> @llvm.x86.avx512.pternlog.q.512(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2, i32 225)
  ret <8 x i64> %r
}

define <4 x i32> @vpternlog_d_v128_imm226(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2) nounwind {
  %r = tail call <4 x i32> @llvm.x86.avx512.pternlog.d.128(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2, i32 226)
  ret <4 x i32> %r
}

define <4 x i64> @vpternlog_q_v256_imm227(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2) nounwind {
  %r = tail call <4 x i64> @llvm.x86.avx512.pternlog.q.256(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2, i32 227)
  ret <4 x i64> %r
}

define <16 x i32> @vpternlog_d_v512_imm228(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2) nounwind {
  %r = tail call <16 x i32> @llvm.x86.avx512.pternlog.d.512(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2, i32 228)
  ret <16 x i32> %r
}

define <2 x i64> @vpternlog_q_v128_imm229(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2) nounwind {
  %r = tail call <2 x i64> @llvm.x86.avx512.pternlog.q.128(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2, i32 229)
  ret <2 x i64> %r
}

define <8 x i32> @vpternlog_d_v256_imm230(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2) nounwind {
  %r = tail call <8 x i32> @llvm.x86.avx512.pternlog.d.256(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2, i32 230)
  ret <8 x i32> %r
}

define <8 x i64> @vpternlog_q_v512_imm231(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2) nounwind {
  %r = tail call <8 x i64> @llvm.x86.avx512.pternlog.q.512(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2, i32 231)
  ret <8 x i64> %r
}

define <4 x i32> @vpternlog_d_v128_imm232(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2) nounwind {
  %r = tail call <4 x i32> @llvm.x86.avx512.pternlog.d.128(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2, i32 232)
  ret <4 x i32> %r
}

define <4 x i64> @vpternlog_q_v256_imm233(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2) nounwind {
  %r = tail call <4 x i64> @llvm.x86.avx512.pternlog.q.256(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2, i32 233)
  ret <4 x i64> %r
}

define <16 x i32> @vpternlog_d_v512_imm234(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2) nounwind {
  %r = tail call <16 x i32> @llvm.x86.avx512.pternlog.d.512(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2, i32 234)
  ret <16 x i32> %r
}

define <2 x i64> @vpternlog_q_v128_imm235(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2) nounwind {
  %r = tail call <2 x i64> @llvm.x86.avx512.pternlog.q.128(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2, i32 235)
  ret <2 x i64> %r
}

define <8 x i32> @vpternlog_d_v256_imm236(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2) nounwind {
  %r = tail call <8 x i32> @llvm.x86.avx512.pternlog.d.256(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2, i32 236)
  ret <8 x i32> %r
}

define <8 x i64> @vpternlog_q_v512_imm237(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2) nounwind {
  %r = tail call <8 x i64> @llvm.x86.avx512.pternlog.q.512(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2, i32 237)
  ret <8 x i64> %r
}

define <4 x i32> @vpternlog_d_v128_imm238(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2) nounwind {
  %r = tail call <4 x i32> @llvm.x86.avx512.pternlog.d.128(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2, i32 238)
  ret <4 x i32> %r
}

define <4 x i64> @vpternlog_q_v256_imm239(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2) nounwind {
  %r = tail call <4 x i64> @llvm.x86.avx512.pternlog.q.256(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2, i32 239)
  ret <4 x i64> %r
}

define <16 x i32> @vpternlog_d_v512_imm240(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2) nounwind {
  %r = tail call <16 x i32> @llvm.x86.avx512.pternlog.d.512(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2, i32 240)
  ret <16 x i32> %r
}

define <2 x i64> @vpternlog_q_v128_imm241(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2) nounwind {
  %r = tail call <2 x i64> @llvm.x86.avx512.pternlog.q.128(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2, i32 241)
  ret <2 x i64> %r
}

define <8 x i32> @vpternlog_d_v256_imm242(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2) nounwind {
  %r = tail call <8 x i32> @llvm.x86.avx512.pternlog.d.256(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2, i32 242)
  ret <8 x i32> %r
}

define <8 x i64> @vpternlog_q_v512_imm243(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2) nounwind {
  %r = tail call <8 x i64> @llvm.x86.avx512.pternlog.q.512(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2, i32 243)
  ret <8 x i64> %r
}

define <4 x i32> @vpternlog_d_v128_imm244(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2) nounwind {
  %r = tail call <4 x i32> @llvm.x86.avx512.pternlog.d.128(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2, i32 244)
  ret <4 x i32> %r
}

define <4 x i64> @vpternlog_q_v256_imm245(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2) nounwind {
  %r = tail call <4 x i64> @llvm.x86.avx512.pternlog.q.256(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2, i32 245)
  ret <4 x i64> %r
}

define <16 x i32> @vpternlog_d_v512_imm246(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2) nounwind {
  %r = tail call <16 x i32> @llvm.x86.avx512.pternlog.d.512(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2, i32 246)
  ret <16 x i32> %r
}

define <2 x i64> @vpternlog_q_v128_imm247(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2) nounwind {
  %r = tail call <2 x i64> @llvm.x86.avx512.pternlog.q.128(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2, i32 247)
  ret <2 x i64> %r
}

define <8 x i32> @vpternlog_d_v256_imm248(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2) nounwind {
  %r = tail call <8 x i32> @llvm.x86.avx512.pternlog.d.256(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2, i32 248)
  ret <8 x i32> %r
}

define <8 x i64> @vpternlog_q_v512_imm249(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2) nounwind {
  %r = tail call <8 x i64> @llvm.x86.avx512.pternlog.q.512(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2, i32 249)
  ret <8 x i64> %r
}

define <4 x i32> @vpternlog_d_v128_imm250(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2) nounwind {
  %r = tail call <4 x i32> @llvm.x86.avx512.pternlog.d.128(<4 x i32> %v0, <4 x i32> %v1, <4 x i32> %v2, i32 250)
  ret <4 x i32> %r
}

define <4 x i64> @vpternlog_q_v256_imm251(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2) nounwind {
  %r = tail call <4 x i64> @llvm.x86.avx512.pternlog.q.256(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2, i32 251)
  ret <4 x i64> %r
}

define <16 x i32> @vpternlog_d_v512_imm252(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2) nounwind {
  %r = tail call <16 x i32> @llvm.x86.avx512.pternlog.d.512(<16 x i32> %v0, <16 x i32> %v1, <16 x i32> %v2, i32 252)
  ret <16 x i32> %r
}

; This is bitselect pattern
define <2 x i64> @vpternlog_q_v128_imm253(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2) nounwind {
  %r = tail call <2 x i64> @llvm.x86.avx512.pternlog.q.128(<2 x i64> %v0, <2 x i64> %v1, <2 x i64> %v2, i32 253)
  ret <2 x i64> %r
}

define <8 x i32> @vpternlog_d_v256_imm254(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2) nounwind {
  %r = tail call <8 x i32> @llvm.x86.avx512.pternlog.d.256(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2, i32 254)
  ret <8 x i32> %r
}

define <8 x i64> @vpternlog_q_v512_imm255(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2) nounwind {
  %r = tail call <8 x i64> @llvm.x86.avx512.pternlog.q.512(<8 x i64> %v0, <8 x i64> %v1, <8 x i64> %v2, i32 255)
  ret <8 x i64> %r
}

define <8 x i32> @vpternlog_vselect_8xi32(<8 x i32> %v0, <8 x i32> %v1, <8 x i32> %v2, <8 x i32> %v3) nounwind {
  %vcmp = icmp eq <8 x i32> %v0, %v1
  %vmask = sext <8 x i1> %vcmp to <8 x i32>
  %r = tail call <8 x i32> @llvm.x86.avx512.pternlog.d.256(<8 x i32> %vmask, <8 x i32> %v1, <8 x i32> %v2, i32 202)
  ret <8 x i32> %r
}

define <4 x i64> @vpternlog_carry_save_add_4xi64(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2) nounwind {
  %l = tail call <4 x i64> @llvm.x86.avx512.pternlog.q.256(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2, i32 150)
  %h = tail call <4 x i64> @llvm.x86.avx512.pternlog.q.256(<4 x i64> %v0, <4 x i64> %v1, <4 x i64> %v2, i32 232)
  %r = tail call <4 x i64> @use_merge_4xi64(<4 x i64> %l, <4 x i64> %h)
  ret <4 x i64> %r
}
;; NOTE: These prefixes are unused and the list is autogenerated. Do not add tests below this line:
; CHECK: {{.*}}
