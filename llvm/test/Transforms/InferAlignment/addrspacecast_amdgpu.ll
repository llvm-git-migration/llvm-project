; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --version 3
; RUN: opt -passes='infer-alignment' -mtriple=amdgcn -mcpu=gfx90a -S < %s  | FileCheck -check-prefix=AMDGPU %s
; RUN: opt -passes='infer-alignment' -mcpu=corei7 -mtriple=x86_64-linux -S < %s  | FileCheck -check-prefix=X86 %s

$globalArrayAS0 = comdat any
$globalArrayAS1 = comdat any
$globalArrayAS2 = comdat any
$globalArrayAS3 = comdat any
$globalArrayAS4 = comdat any
$globalArrayAS5 = comdat any
$globalArrayAS6 = comdat any
$globalArrayAS7 = comdat any
$globalArrayAS8 = comdat any
@globalArrayAS0 = linkonce_odr hidden addrspace(0) global [4096 x i8] undef, comdat, align 16
@globalArrayAS1 = linkonce_odr hidden addrspace(1) global [4096 x i8] undef, comdat, align 16
@globalArrayAS2 = linkonce_odr hidden addrspace(2) global [4096 x i8] undef, comdat, align 16
@globalArrayAS3 = linkonce_odr hidden addrspace(3) global [4096 x i8] undef, comdat, align 16
@globalArrayAS4 = linkonce_odr hidden addrspace(4) global [4096 x i8] undef, comdat, align 16
@globalArrayAS5 = linkonce_odr hidden addrspace(5) global [4096 x i8] undef, comdat, align 16
@globalArrayAS6 = linkonce_odr hidden addrspace(6) global [4096 x i8] undef, comdat, align 16
@globalArrayAS7 = linkonce_odr hidden addrspace(7) global [4096 x i8] undef, comdat, align 16
@globalArrayAS8 = linkonce_odr hidden addrspace(8) global [4096 x i8] undef, comdat, align 16

; Function Attrs: alwaysinline convergent mustprogress nounwind
define amdgpu_kernel void @infer_AS10(i32 %idx) unnamed_addr align 2 {
; AMDGPU-LABEL: define amdgpu_kernel void @infer_AS10(
; AMDGPU-SAME: i32 [[IDX:%.*]]) unnamed_addr #[[ATTR0:[0-9]+]] align 2 {
; AMDGPU-NEXT:  entry:
; AMDGPU-NEXT:    [[MUL32_I:%.*]] = shl nuw nsw i32 [[IDX]], 8
; AMDGPU-NEXT:    [[ADD36_I:%.*]] = add nuw nsw i32 [[MUL32_I]], 1024
; AMDGPU-NEXT:    [[IDXPROM37_I21:%.*]] = zext i32 [[ADD36_I]] to i64
; AMDGPU-NEXT:    [[ARRAYIDX38_I22:%.*]] = getelementptr inbounds float, ptr addrspacecast (ptr addrspace(1) @globalArrayAS1 to ptr), i64 [[IDXPROM37_I21]]
; AMDGPU-NEXT:    [[L1:%.*]] = load float, ptr [[ARRAYIDX38_I22]], align 16
; AMDGPU-NEXT:    ret void
;
; X86-LABEL: define amdgpu_kernel void @infer_AS10(
; X86-SAME: i32 [[IDX:%.*]]) unnamed_addr #[[ATTR0:[0-9]+]] align 2 {
; X86-NEXT:  entry:
; X86-NEXT:    [[MUL32_I:%.*]] = shl nuw nsw i32 [[IDX]], 8
; X86-NEXT:    [[ADD36_I:%.*]] = add nuw nsw i32 [[MUL32_I]], 1024
; X86-NEXT:    [[IDXPROM37_I21:%.*]] = zext i32 [[ADD36_I]] to i64
; X86-NEXT:    [[ARRAYIDX38_I22:%.*]] = getelementptr inbounds float, ptr addrspacecast (ptr addrspace(1) @globalArrayAS1 to ptr), i64 [[IDXPROM37_I21]]
; X86-NEXT:    [[L1:%.*]] = load float, ptr [[ARRAYIDX38_I22]], align 16
; X86-NEXT:    ret void
;
entry:
  %mul32.i = shl nuw nsw i32 %idx, 8
  %add36.i = add nuw nsw i32 %mul32.i, 1024
  %idxprom37.i21 = zext i32 %add36.i to i64
  %arrayidx38.i22 = getelementptr inbounds float, ptr addrspacecast (ptr addrspace(1) @globalArrayAS1 to ptr), i64 %idxprom37.i21
  %l1 = load float, ptr  %arrayidx38.i22, align 4
  ret void
}

; Function Attrs: alwaysinline convergent mustprogress nounwind
define amdgpu_kernel void @infer_AS20(i32 %idx) unnamed_addr align 2 {
; AMDGPU-LABEL: define amdgpu_kernel void @infer_AS20(
; AMDGPU-SAME: i32 [[IDX:%.*]]) unnamed_addr #[[ATTR0]] align 2 {
; AMDGPU-NEXT:  entry:
; AMDGPU-NEXT:    [[MUL32_I:%.*]] = shl nuw nsw i32 [[IDX]], 8
; AMDGPU-NEXT:    [[ADD36_I:%.*]] = add nuw nsw i32 [[MUL32_I]], 1024
; AMDGPU-NEXT:    [[IDXPROM37_I21:%.*]] = zext i32 [[ADD36_I]] to i64
; AMDGPU-NEXT:    [[ARRAYIDX38_I22:%.*]] = getelementptr inbounds float, ptr addrspacecast (ptr addrspace(2) @globalArrayAS2 to ptr), i64 [[IDXPROM37_I21]]
; AMDGPU-NEXT:    [[L1:%.*]] = load float, ptr [[ARRAYIDX38_I22]], align 16
; AMDGPU-NEXT:    ret void
;
; X86-LABEL: define amdgpu_kernel void @infer_AS20(
; X86-SAME: i32 [[IDX:%.*]]) unnamed_addr #[[ATTR0]] align 2 {
; X86-NEXT:  entry:
; X86-NEXT:    [[MUL32_I:%.*]] = shl nuw nsw i32 [[IDX]], 8
; X86-NEXT:    [[ADD36_I:%.*]] = add nuw nsw i32 [[MUL32_I]], 1024
; X86-NEXT:    [[IDXPROM37_I21:%.*]] = zext i32 [[ADD36_I]] to i64
; X86-NEXT:    [[ARRAYIDX38_I22:%.*]] = getelementptr inbounds float, ptr addrspacecast (ptr addrspace(2) @globalArrayAS2 to ptr), i64 [[IDXPROM37_I21]]
; X86-NEXT:    [[L1:%.*]] = load float, ptr [[ARRAYIDX38_I22]], align 16
; X86-NEXT:    ret void
;
entry:
  %mul32.i = shl nuw nsw i32 %idx, 8
  %add36.i = add nuw nsw i32 %mul32.i, 1024
  %idxprom37.i21 = zext i32 %add36.i to i64
  %arrayidx38.i22 = getelementptr inbounds float, ptr addrspacecast (ptr addrspace(2) @globalArrayAS2 to ptr), i64 %idxprom37.i21
  %l1 = load float, ptr  %arrayidx38.i22, align 4
  ret void
}

; Function Attrs: alwaysinline convergent mustprogress nounwind
define amdgpu_kernel void @infer_AS30(i32 %idx) unnamed_addr align 2 {
; AMDGPU-LABEL: define amdgpu_kernel void @infer_AS30(
; AMDGPU-SAME: i32 [[IDX:%.*]]) unnamed_addr #[[ATTR0]] align 2 {
; AMDGPU-NEXT:  entry:
; AMDGPU-NEXT:    [[MUL32_I:%.*]] = shl nuw nsw i32 [[IDX]], 8
; AMDGPU-NEXT:    [[ADD36_I:%.*]] = add nuw nsw i32 [[MUL32_I]], 1024
; AMDGPU-NEXT:    [[IDXPROM37_I21:%.*]] = zext i32 [[ADD36_I]] to i64
; AMDGPU-NEXT:    [[ARRAYIDX38_I22:%.*]] = getelementptr inbounds float, ptr addrspacecast (ptr addrspace(3) @globalArrayAS3 to ptr), i64 [[IDXPROM37_I21]]
; AMDGPU-NEXT:    [[L1:%.*]] = load float, ptr [[ARRAYIDX38_I22]], align 16
; AMDGPU-NEXT:    ret void
;
; X86-LABEL: define amdgpu_kernel void @infer_AS30(
; X86-SAME: i32 [[IDX:%.*]]) unnamed_addr #[[ATTR0]] align 2 {
; X86-NEXT:  entry:
; X86-NEXT:    [[MUL32_I:%.*]] = shl nuw nsw i32 [[IDX]], 8
; X86-NEXT:    [[ADD36_I:%.*]] = add nuw nsw i32 [[MUL32_I]], 1024
; X86-NEXT:    [[IDXPROM37_I21:%.*]] = zext i32 [[ADD36_I]] to i64
; X86-NEXT:    [[ARRAYIDX38_I22:%.*]] = getelementptr inbounds float, ptr addrspacecast (ptr addrspace(3) @globalArrayAS3 to ptr), i64 [[IDXPROM37_I21]]
; X86-NEXT:    [[L1:%.*]] = load float, ptr [[ARRAYIDX38_I22]], align 16
; X86-NEXT:    ret void
;
entry:
  %mul32.i = shl nuw nsw i32 %idx, 8
  %add36.i = add nuw nsw i32 %mul32.i, 1024
  %idxprom37.i21 = zext i32 %add36.i to i64
  %arrayidx38.i22 = getelementptr inbounds float, ptr addrspacecast (ptr addrspace(3) @globalArrayAS3 to ptr), i64 %idxprom37.i21
  %l1 = load float, ptr  %arrayidx38.i22, align 4
  ret void
}

; Function Attrs: alwaysinline convergent mustprogress nounwind
define amdgpu_kernel void @infer_AS40(i32 %idx) unnamed_addr align 2 {
; AMDGPU-LABEL: define amdgpu_kernel void @infer_AS40(
; AMDGPU-SAME: i32 [[IDX:%.*]]) unnamed_addr #[[ATTR0]] align 2 {
; AMDGPU-NEXT:  entry:
; AMDGPU-NEXT:    [[MUL32_I:%.*]] = shl nuw nsw i32 [[IDX]], 8
; AMDGPU-NEXT:    [[ADD36_I:%.*]] = add nuw nsw i32 [[MUL32_I]], 1024
; AMDGPU-NEXT:    [[IDXPROM37_I21:%.*]] = zext i32 [[ADD36_I]] to i64
; AMDGPU-NEXT:    [[ARRAYIDX38_I22:%.*]] = getelementptr inbounds float, ptr addrspacecast (ptr addrspace(4) @globalArrayAS4 to ptr), i64 [[IDXPROM37_I21]]
; AMDGPU-NEXT:    [[L1:%.*]] = load float, ptr [[ARRAYIDX38_I22]], align 16
; AMDGPU-NEXT:    ret void
;
; X86-LABEL: define amdgpu_kernel void @infer_AS40(
; X86-SAME: i32 [[IDX:%.*]]) unnamed_addr #[[ATTR0]] align 2 {
; X86-NEXT:  entry:
; X86-NEXT:    [[MUL32_I:%.*]] = shl nuw nsw i32 [[IDX]], 8
; X86-NEXT:    [[ADD36_I:%.*]] = add nuw nsw i32 [[MUL32_I]], 1024
; X86-NEXT:    [[IDXPROM37_I21:%.*]] = zext i32 [[ADD36_I]] to i64
; X86-NEXT:    [[ARRAYIDX38_I22:%.*]] = getelementptr inbounds float, ptr addrspacecast (ptr addrspace(4) @globalArrayAS4 to ptr), i64 [[IDXPROM37_I21]]
; X86-NEXT:    [[L1:%.*]] = load float, ptr [[ARRAYIDX38_I22]], align 16
; X86-NEXT:    ret void
;
entry:
  %mul32.i = shl nuw nsw i32 %idx, 8
  %add36.i = add nuw nsw i32 %mul32.i, 1024
  %idxprom37.i21 = zext i32 %add36.i to i64
  %arrayidx38.i22 = getelementptr inbounds float, ptr addrspacecast (ptr addrspace(4) @globalArrayAS4 to ptr), i64 %idxprom37.i21
  %l1 = load float, ptr  %arrayidx38.i22, align 4
  ret void
}

; Function Attrs: alwaysinline convergent mustprogress nounwind
define amdgpu_kernel void @infer_AS50(i32 %idx) unnamed_addr align 2 {
; AMDGPU-LABEL: define amdgpu_kernel void @infer_AS50(
; AMDGPU-SAME: i32 [[IDX:%.*]]) unnamed_addr #[[ATTR0]] align 2 {
; AMDGPU-NEXT:  entry:
; AMDGPU-NEXT:    [[MUL32_I:%.*]] = shl nuw nsw i32 [[IDX]], 8
; AMDGPU-NEXT:    [[ADD36_I:%.*]] = add nuw nsw i32 [[MUL32_I]], 1024
; AMDGPU-NEXT:    [[IDXPROM37_I21:%.*]] = zext i32 [[ADD36_I]] to i64
; AMDGPU-NEXT:    [[ARRAYIDX38_I22:%.*]] = getelementptr inbounds float, ptr addrspacecast (ptr addrspace(5) @globalArrayAS5 to ptr), i64 [[IDXPROM37_I21]]
; AMDGPU-NEXT:    [[L1:%.*]] = load float, ptr [[ARRAYIDX38_I22]], align 16
; AMDGPU-NEXT:    ret void
;
; X86-LABEL: define amdgpu_kernel void @infer_AS50(
; X86-SAME: i32 [[IDX:%.*]]) unnamed_addr #[[ATTR0]] align 2 {
; X86-NEXT:  entry:
; X86-NEXT:    [[MUL32_I:%.*]] = shl nuw nsw i32 [[IDX]], 8
; X86-NEXT:    [[ADD36_I:%.*]] = add nuw nsw i32 [[MUL32_I]], 1024
; X86-NEXT:    [[IDXPROM37_I21:%.*]] = zext i32 [[ADD36_I]] to i64
; X86-NEXT:    [[ARRAYIDX38_I22:%.*]] = getelementptr inbounds float, ptr addrspacecast (ptr addrspace(5) @globalArrayAS5 to ptr), i64 [[IDXPROM37_I21]]
; X86-NEXT:    [[L1:%.*]] = load float, ptr [[ARRAYIDX38_I22]], align 16
; X86-NEXT:    ret void
;
entry:
  %mul32.i = shl nuw nsw i32 %idx, 8
  %add36.i = add nuw nsw i32 %mul32.i, 1024
  %idxprom37.i21 = zext i32 %add36.i to i64
  %arrayidx38.i22 = getelementptr inbounds float, ptr addrspacecast (ptr addrspace(5) @globalArrayAS5 to ptr), i64 %idxprom37.i21
  %l1 = load float, ptr  %arrayidx38.i22, align 4
  ret void
}

; Function Attrs: alwaysinline convergent mustprogress nounwind
define amdgpu_kernel void @infer_AS60(i32 %idx) unnamed_addr align 2 {
; AMDGPU-LABEL: define amdgpu_kernel void @infer_AS60(
; AMDGPU-SAME: i32 [[IDX:%.*]]) unnamed_addr #[[ATTR0]] align 2 {
; AMDGPU-NEXT:  entry:
; AMDGPU-NEXT:    [[MUL32_I:%.*]] = shl nuw nsw i32 [[IDX]], 8
; AMDGPU-NEXT:    [[ADD36_I:%.*]] = add nuw nsw i32 [[MUL32_I]], 1024
; AMDGPU-NEXT:    [[IDXPROM37_I21:%.*]] = zext i32 [[ADD36_I]] to i64
; AMDGPU-NEXT:    [[ARRAYIDX38_I22:%.*]] = getelementptr inbounds float, ptr addrspacecast (ptr addrspace(6) @globalArrayAS6 to ptr), i64 [[IDXPROM37_I21]]
; AMDGPU-NEXT:    [[L1:%.*]] = load float, ptr [[ARRAYIDX38_I22]], align 16
; AMDGPU-NEXT:    ret void
;
; X86-LABEL: define amdgpu_kernel void @infer_AS60(
; X86-SAME: i32 [[IDX:%.*]]) unnamed_addr #[[ATTR0]] align 2 {
; X86-NEXT:  entry:
; X86-NEXT:    [[MUL32_I:%.*]] = shl nuw nsw i32 [[IDX]], 8
; X86-NEXT:    [[ADD36_I:%.*]] = add nuw nsw i32 [[MUL32_I]], 1024
; X86-NEXT:    [[IDXPROM37_I21:%.*]] = zext i32 [[ADD36_I]] to i64
; X86-NEXT:    [[ARRAYIDX38_I22:%.*]] = getelementptr inbounds float, ptr addrspacecast (ptr addrspace(6) @globalArrayAS6 to ptr), i64 [[IDXPROM37_I21]]
; X86-NEXT:    [[L1:%.*]] = load float, ptr [[ARRAYIDX38_I22]], align 16
; X86-NEXT:    ret void
;
entry:
  %mul32.i = shl nuw nsw i32 %idx, 8
  %add36.i = add nuw nsw i32 %mul32.i, 1024
  %idxprom37.i21 = zext i32 %add36.i to i64
  %arrayidx38.i22 = getelementptr inbounds float, ptr addrspacecast (ptr addrspace(6) @globalArrayAS6 to ptr), i64 %idxprom37.i21
  %l1 = load float, ptr  %arrayidx38.i22, align 4
  ret void
}

; Function Attrs: alwaysinline convergent mustprogress nounwind
define amdgpu_kernel void @infer_AS70(i32 %idx) unnamed_addr align 2 {
; AMDGPU-LABEL: define amdgpu_kernel void @infer_AS70(
; AMDGPU-SAME: i32 [[IDX:%.*]]) unnamed_addr #[[ATTR0]] align 2 {
; AMDGPU-NEXT:  entry:
; AMDGPU-NEXT:    [[MUL32_I:%.*]] = shl nuw nsw i32 [[IDX]], 8
; AMDGPU-NEXT:    [[ADD36_I:%.*]] = add nuw nsw i32 [[MUL32_I]], 1024
; AMDGPU-NEXT:    [[IDXPROM37_I21:%.*]] = zext i32 [[ADD36_I]] to i64
; AMDGPU-NEXT:    [[ARRAYIDX38_I22:%.*]] = getelementptr inbounds float, ptr addrspacecast (ptr addrspace(7) @globalArrayAS7 to ptr), i64 [[IDXPROM37_I21]]
; AMDGPU-NEXT:    [[L1:%.*]] = load float, ptr [[ARRAYIDX38_I22]], align 4
; AMDGPU-NEXT:    ret void
;
; X86-LABEL: define amdgpu_kernel void @infer_AS70(
; X86-SAME: i32 [[IDX:%.*]]) unnamed_addr #[[ATTR0]] align 2 {
; X86-NEXT:  entry:
; X86-NEXT:    [[MUL32_I:%.*]] = shl nuw nsw i32 [[IDX]], 8
; X86-NEXT:    [[ADD36_I:%.*]] = add nuw nsw i32 [[MUL32_I]], 1024
; X86-NEXT:    [[IDXPROM37_I21:%.*]] = zext i32 [[ADD36_I]] to i64
; X86-NEXT:    [[ARRAYIDX38_I22:%.*]] = getelementptr inbounds float, ptr addrspacecast (ptr addrspace(7) @globalArrayAS7 to ptr), i64 [[IDXPROM37_I21]]
; X86-NEXT:    [[L1:%.*]] = load float, ptr [[ARRAYIDX38_I22]], align 16
; X86-NEXT:    ret void
;
entry:
  %mul32.i = shl nuw nsw i32 %idx, 8
  %add36.i = add nuw nsw i32 %mul32.i, 1024
  %idxprom37.i21 = zext i32 %add36.i to i64
  %arrayidx38.i22 = getelementptr inbounds float, ptr addrspacecast (ptr addrspace(7) @globalArrayAS7 to ptr), i64 %idxprom37.i21
  %l1 = load float, ptr  %arrayidx38.i22, align 4
  ret void
}

; Function Attrs: alwaysinline convergent mustprogress nounwind
define amdgpu_kernel void @infer_AS80(i32 %idx) unnamed_addr align 2 {
; AMDGPU-LABEL: define amdgpu_kernel void @infer_AS80(
; AMDGPU-SAME: i32 [[IDX:%.*]]) unnamed_addr #[[ATTR0]] align 2 {
; AMDGPU-NEXT:  entry:
; AMDGPU-NEXT:    [[MUL32_I:%.*]] = shl nuw nsw i32 [[IDX]], 8
; AMDGPU-NEXT:    [[ADD36_I:%.*]] = add nuw nsw i32 [[MUL32_I]], 1024
; AMDGPU-NEXT:    [[IDXPROM37_I21:%.*]] = zext i32 [[ADD36_I]] to i64
; AMDGPU-NEXT:    [[ARRAYIDX38_I22:%.*]] = getelementptr inbounds float, ptr addrspacecast (ptr addrspace(8) @globalArrayAS8 to ptr), i64 [[IDXPROM37_I21]]
; AMDGPU-NEXT:    [[L1:%.*]] = load float, ptr [[ARRAYIDX38_I22]], align 4
; AMDGPU-NEXT:    ret void
;
; X86-LABEL: define amdgpu_kernel void @infer_AS80(
; X86-SAME: i32 [[IDX:%.*]]) unnamed_addr #[[ATTR0]] align 2 {
; X86-NEXT:  entry:
; X86-NEXT:    [[MUL32_I:%.*]] = shl nuw nsw i32 [[IDX]], 8
; X86-NEXT:    [[ADD36_I:%.*]] = add nuw nsw i32 [[MUL32_I]], 1024
; X86-NEXT:    [[IDXPROM37_I21:%.*]] = zext i32 [[ADD36_I]] to i64
; X86-NEXT:    [[ARRAYIDX38_I22:%.*]] = getelementptr inbounds float, ptr addrspacecast (ptr addrspace(8) @globalArrayAS8 to ptr), i64 [[IDXPROM37_I21]]
; X86-NEXT:    [[L1:%.*]] = load float, ptr [[ARRAYIDX38_I22]], align 16
; X86-NEXT:    ret void
;
entry:
  %mul32.i = shl nuw nsw i32 %idx, 8
  %add36.i = add nuw nsw i32 %mul32.i, 1024
  %idxprom37.i21 = zext i32 %add36.i to i64
  %arrayidx38.i22 = getelementptr inbounds float, ptr addrspacecast (ptr addrspace(8) @globalArrayAS8 to ptr), i64 %idxprom37.i21
  %l1 = load float, ptr  %arrayidx38.i22, align 4
  ret void
}


; Function Attrs: alwaysinline convergent mustprogress nounwind
define amdgpu_kernel void @infer_AS01(i32 %idx) unnamed_addr align 2 {
; AMDGPU-LABEL: define amdgpu_kernel void @infer_AS01(
; AMDGPU-SAME: i32 [[IDX:%.*]]) unnamed_addr #[[ATTR0]] align 2 {
; AMDGPU-NEXT:  entry:
; AMDGPU-NEXT:    [[MUL32_I:%.*]] = shl nuw nsw i32 [[IDX]], 8
; AMDGPU-NEXT:    [[ADD36_I:%.*]] = add nuw nsw i32 [[MUL32_I]], 1024
; AMDGPU-NEXT:    [[IDXPROM37_I21:%.*]] = zext i32 [[ADD36_I]] to i64
; AMDGPU-NEXT:    [[ARRAYIDX38_I22:%.*]] = getelementptr inbounds float, ptr addrspace(1) addrspacecast (ptr @globalArrayAS0 to ptr addrspace(1)), i64 [[IDXPROM37_I21]]
; AMDGPU-NEXT:    [[L1:%.*]] = load float, ptr addrspace(1) [[ARRAYIDX38_I22]], align 16
; AMDGPU-NEXT:    ret void
;
; X86-LABEL: define amdgpu_kernel void @infer_AS01(
; X86-SAME: i32 [[IDX:%.*]]) unnamed_addr #[[ATTR0]] align 2 {
; X86-NEXT:  entry:
; X86-NEXT:    [[MUL32_I:%.*]] = shl nuw nsw i32 [[IDX]], 8
; X86-NEXT:    [[ADD36_I:%.*]] = add nuw nsw i32 [[MUL32_I]], 1024
; X86-NEXT:    [[IDXPROM37_I21:%.*]] = zext i32 [[ADD36_I]] to i64
; X86-NEXT:    [[ARRAYIDX38_I22:%.*]] = getelementptr inbounds float, ptr addrspace(1) addrspacecast (ptr @globalArrayAS0 to ptr addrspace(1)), i64 [[IDXPROM37_I21]]
; X86-NEXT:    [[L1:%.*]] = load float, ptr addrspace(1) [[ARRAYIDX38_I22]], align 16
; X86-NEXT:    ret void
;
entry:
  %mul32.i = shl nuw nsw i32 %idx, 8
  %add36.i = add nuw nsw i32 %mul32.i, 1024
  %idxprom37.i21 = zext i32 %add36.i to i64
  %arrayidx38.i22 = getelementptr inbounds float, ptr addrspace(1) addrspacecast (ptr @globalArrayAS0 to ptr addrspace(1)), i64 %idxprom37.i21
  %l1 = load float, ptr addrspace(1) %arrayidx38.i22, align 4
  ret void
}

; Function Attrs: alwaysinline convergent mustprogress nounwind
define amdgpu_kernel void @infer_AS02(i32 %idx) unnamed_addr align 2 {
; AMDGPU-LABEL: define amdgpu_kernel void @infer_AS02(
; AMDGPU-SAME: i32 [[IDX:%.*]]) unnamed_addr #[[ATTR0]] align 2 {
; AMDGPU-NEXT:  entry:
; AMDGPU-NEXT:    [[MUL32_I:%.*]] = shl nuw nsw i32 [[IDX]], 8
; AMDGPU-NEXT:    [[ADD36_I:%.*]] = add nuw nsw i32 [[MUL32_I]], 1024
; AMDGPU-NEXT:    [[IDXPROM37_I21:%.*]] = zext i32 [[ADD36_I]] to i64
; AMDGPU-NEXT:    [[ARRAYIDX38_I22:%.*]] = getelementptr inbounds float, ptr addrspace(2) addrspacecast (ptr @globalArrayAS0 to ptr addrspace(2)), i64 [[IDXPROM37_I21]]
; AMDGPU-NEXT:    [[L1:%.*]] = load float, ptr addrspace(2) [[ARRAYIDX38_I22]], align 16
; AMDGPU-NEXT:    ret void
;
; X86-LABEL: define amdgpu_kernel void @infer_AS02(
; X86-SAME: i32 [[IDX:%.*]]) unnamed_addr #[[ATTR0]] align 2 {
; X86-NEXT:  entry:
; X86-NEXT:    [[MUL32_I:%.*]] = shl nuw nsw i32 [[IDX]], 8
; X86-NEXT:    [[ADD36_I:%.*]] = add nuw nsw i32 [[MUL32_I]], 1024
; X86-NEXT:    [[IDXPROM37_I21:%.*]] = zext i32 [[ADD36_I]] to i64
; X86-NEXT:    [[ARRAYIDX38_I22:%.*]] = getelementptr inbounds float, ptr addrspace(2) addrspacecast (ptr @globalArrayAS0 to ptr addrspace(2)), i64 [[IDXPROM37_I21]]
; X86-NEXT:    [[L1:%.*]] = load float, ptr addrspace(2) [[ARRAYIDX38_I22]], align 16
; X86-NEXT:    ret void
;
entry:
  %mul32.i = shl nuw nsw i32 %idx, 8
  %add36.i = add nuw nsw i32 %mul32.i, 1024
  %idxprom37.i21 = zext i32 %add36.i to i64
  %arrayidx38.i22 = getelementptr inbounds float, ptr addrspace(2) addrspacecast (ptr @globalArrayAS0 to ptr addrspace(2)), i64 %idxprom37.i21
  %l1 = load float, ptr addrspace(2) %arrayidx38.i22, align 4
  ret void
}

; Function Attrs: alwaysinline convergent mustprogress nounwind
define amdgpu_kernel void @infer_AS03(i32 %idx) unnamed_addr align 2 {
; AMDGPU-LABEL: define amdgpu_kernel void @infer_AS03(
; AMDGPU-SAME: i32 [[IDX:%.*]]) unnamed_addr #[[ATTR0]] align 2 {
; AMDGPU-NEXT:  entry:
; AMDGPU-NEXT:    [[MUL32_I:%.*]] = shl nuw nsw i32 [[IDX]], 8
; AMDGPU-NEXT:    [[ADD36_I:%.*]] = add nuw nsw i32 [[MUL32_I]], 1024
; AMDGPU-NEXT:    [[IDXPROM37_I21:%.*]] = zext i32 [[ADD36_I]] to i64
; AMDGPU-NEXT:    [[ARRAYIDX38_I22:%.*]] = getelementptr inbounds float, ptr addrspace(3) addrspacecast (ptr @globalArrayAS0 to ptr addrspace(3)), i64 [[IDXPROM37_I21]]
; AMDGPU-NEXT:    [[L1:%.*]] = load float, ptr addrspace(3) [[ARRAYIDX38_I22]], align 16
; AMDGPU-NEXT:    ret void
;
; X86-LABEL: define amdgpu_kernel void @infer_AS03(
; X86-SAME: i32 [[IDX:%.*]]) unnamed_addr #[[ATTR0]] align 2 {
; X86-NEXT:  entry:
; X86-NEXT:    [[MUL32_I:%.*]] = shl nuw nsw i32 [[IDX]], 8
; X86-NEXT:    [[ADD36_I:%.*]] = add nuw nsw i32 [[MUL32_I]], 1024
; X86-NEXT:    [[IDXPROM37_I21:%.*]] = zext i32 [[ADD36_I]] to i64
; X86-NEXT:    [[ARRAYIDX38_I22:%.*]] = getelementptr inbounds float, ptr addrspace(3) addrspacecast (ptr @globalArrayAS0 to ptr addrspace(3)), i64 [[IDXPROM37_I21]]
; X86-NEXT:    [[L1:%.*]] = load float, ptr addrspace(3) [[ARRAYIDX38_I22]], align 16
; X86-NEXT:    ret void
;
entry:
  %mul32.i = shl nuw nsw i32 %idx, 8
  %add36.i = add nuw nsw i32 %mul32.i, 1024
  %idxprom37.i21 = zext i32 %add36.i to i64
  %arrayidx38.i22 = getelementptr inbounds float, ptr addrspace(3) addrspacecast (ptr @globalArrayAS0 to ptr addrspace(3)), i64 %idxprom37.i21
  %l1 = load float, ptr addrspace(3) %arrayidx38.i22, align 4
  ret void
}

; Function Attrs: alwaysinline convergent mustprogress nounwind
define amdgpu_kernel void @infer_AS04(i32 %idx) unnamed_addr align 2 {
; AMDGPU-LABEL: define amdgpu_kernel void @infer_AS04(
; AMDGPU-SAME: i32 [[IDX:%.*]]) unnamed_addr #[[ATTR0]] align 2 {
; AMDGPU-NEXT:  entry:
; AMDGPU-NEXT:    [[MUL32_I:%.*]] = shl nuw nsw i32 [[IDX]], 8
; AMDGPU-NEXT:    [[ADD36_I:%.*]] = add nuw nsw i32 [[MUL32_I]], 1024
; AMDGPU-NEXT:    [[IDXPROM37_I21:%.*]] = zext i32 [[ADD36_I]] to i64
; AMDGPU-NEXT:    [[ARRAYIDX38_I22:%.*]] = getelementptr inbounds float, ptr addrspace(4) addrspacecast (ptr @globalArrayAS0 to ptr addrspace(4)), i64 [[IDXPROM37_I21]]
; AMDGPU-NEXT:    [[L1:%.*]] = load float, ptr addrspace(4) [[ARRAYIDX38_I22]], align 16
; AMDGPU-NEXT:    ret void
;
; X86-LABEL: define amdgpu_kernel void @infer_AS04(
; X86-SAME: i32 [[IDX:%.*]]) unnamed_addr #[[ATTR0]] align 2 {
; X86-NEXT:  entry:
; X86-NEXT:    [[MUL32_I:%.*]] = shl nuw nsw i32 [[IDX]], 8
; X86-NEXT:    [[ADD36_I:%.*]] = add nuw nsw i32 [[MUL32_I]], 1024
; X86-NEXT:    [[IDXPROM37_I21:%.*]] = zext i32 [[ADD36_I]] to i64
; X86-NEXT:    [[ARRAYIDX38_I22:%.*]] = getelementptr inbounds float, ptr addrspace(4) addrspacecast (ptr @globalArrayAS0 to ptr addrspace(4)), i64 [[IDXPROM37_I21]]
; X86-NEXT:    [[L1:%.*]] = load float, ptr addrspace(4) [[ARRAYIDX38_I22]], align 16
; X86-NEXT:    ret void
;
entry:
  %mul32.i = shl nuw nsw i32 %idx, 8
  %add36.i = add nuw nsw i32 %mul32.i, 1024
  %idxprom37.i21 = zext i32 %add36.i to i64
  %arrayidx38.i22 = getelementptr inbounds float, ptr addrspace(4) addrspacecast (ptr @globalArrayAS0 to ptr addrspace(4)), i64 %idxprom37.i21
  %l1 = load float, ptr addrspace(4) %arrayidx38.i22, align 4
  ret void
}

; Function Attrs: alwaysinline convergent mustprogress nounwind
define amdgpu_kernel void @infer_AS05(i32 %idx) unnamed_addr align 2 {
; AMDGPU-LABEL: define amdgpu_kernel void @infer_AS05(
; AMDGPU-SAME: i32 [[IDX:%.*]]) unnamed_addr #[[ATTR0]] align 2 {
; AMDGPU-NEXT:  entry:
; AMDGPU-NEXT:    [[MUL32_I:%.*]] = shl nuw nsw i32 [[IDX]], 8
; AMDGPU-NEXT:    [[ADD36_I:%.*]] = add nuw nsw i32 [[MUL32_I]], 1024
; AMDGPU-NEXT:    [[IDXPROM37_I21:%.*]] = zext i32 [[ADD36_I]] to i64
; AMDGPU-NEXT:    [[ARRAYIDX38_I22:%.*]] = getelementptr inbounds float, ptr addrspace(5) addrspacecast (ptr @globalArrayAS0 to ptr addrspace(5)), i64 [[IDXPROM37_I21]]
; AMDGPU-NEXT:    [[L1:%.*]] = load float, ptr addrspace(5) [[ARRAYIDX38_I22]], align 16
; AMDGPU-NEXT:    ret void
;
; X86-LABEL: define amdgpu_kernel void @infer_AS05(
; X86-SAME: i32 [[IDX:%.*]]) unnamed_addr #[[ATTR0]] align 2 {
; X86-NEXT:  entry:
; X86-NEXT:    [[MUL32_I:%.*]] = shl nuw nsw i32 [[IDX]], 8
; X86-NEXT:    [[ADD36_I:%.*]] = add nuw nsw i32 [[MUL32_I]], 1024
; X86-NEXT:    [[IDXPROM37_I21:%.*]] = zext i32 [[ADD36_I]] to i64
; X86-NEXT:    [[ARRAYIDX38_I22:%.*]] = getelementptr inbounds float, ptr addrspace(5) addrspacecast (ptr @globalArrayAS0 to ptr addrspace(5)), i64 [[IDXPROM37_I21]]
; X86-NEXT:    [[L1:%.*]] = load float, ptr addrspace(5) [[ARRAYIDX38_I22]], align 16
; X86-NEXT:    ret void
;
entry:
  %mul32.i = shl nuw nsw i32 %idx, 8
  %add36.i = add nuw nsw i32 %mul32.i, 1024
  %idxprom37.i21 = zext i32 %add36.i to i64
  %arrayidx38.i22 = getelementptr inbounds float, ptr addrspace(5) addrspacecast (ptr @globalArrayAS0 to ptr addrspace(5)), i64 %idxprom37.i21
  %l1 = load float, ptr addrspace(5) %arrayidx38.i22, align 4
  ret void
}

; Function Attrs: alwaysinline convergent mustprogress nounwind
define amdgpu_kernel void @infer_AS06(i32 %idx) unnamed_addr align 2 {
; AMDGPU-LABEL: define amdgpu_kernel void @infer_AS06(
; AMDGPU-SAME: i32 [[IDX:%.*]]) unnamed_addr #[[ATTR0]] align 2 {
; AMDGPU-NEXT:  entry:
; AMDGPU-NEXT:    [[MUL32_I:%.*]] = shl nuw nsw i32 [[IDX]], 8
; AMDGPU-NEXT:    [[ADD36_I:%.*]] = add nuw nsw i32 [[MUL32_I]], 1024
; AMDGPU-NEXT:    [[IDXPROM37_I21:%.*]] = zext i32 [[ADD36_I]] to i64
; AMDGPU-NEXT:    [[ARRAYIDX38_I22:%.*]] = getelementptr inbounds float, ptr addrspace(6) addrspacecast (ptr @globalArrayAS0 to ptr addrspace(6)), i64 [[IDXPROM37_I21]]
; AMDGPU-NEXT:    [[L1:%.*]] = load float, ptr addrspace(6) [[ARRAYIDX38_I22]], align 16
; AMDGPU-NEXT:    ret void
;
; X86-LABEL: define amdgpu_kernel void @infer_AS06(
; X86-SAME: i32 [[IDX:%.*]]) unnamed_addr #[[ATTR0]] align 2 {
; X86-NEXT:  entry:
; X86-NEXT:    [[MUL32_I:%.*]] = shl nuw nsw i32 [[IDX]], 8
; X86-NEXT:    [[ADD36_I:%.*]] = add nuw nsw i32 [[MUL32_I]], 1024
; X86-NEXT:    [[IDXPROM37_I21:%.*]] = zext i32 [[ADD36_I]] to i64
; X86-NEXT:    [[ARRAYIDX38_I22:%.*]] = getelementptr inbounds float, ptr addrspace(6) addrspacecast (ptr @globalArrayAS0 to ptr addrspace(6)), i64 [[IDXPROM37_I21]]
; X86-NEXT:    [[L1:%.*]] = load float, ptr addrspace(6) [[ARRAYIDX38_I22]], align 16
; X86-NEXT:    ret void
;
entry:
  %mul32.i = shl nuw nsw i32 %idx, 8
  %add36.i = add nuw nsw i32 %mul32.i, 1024
  %idxprom37.i21 = zext i32 %add36.i to i64
  %arrayidx38.i22 = getelementptr inbounds float, ptr addrspace(6) addrspacecast (ptr @globalArrayAS0 to ptr addrspace(6)), i64 %idxprom37.i21
  %l1 = load float, ptr addrspace(6) %arrayidx38.i22, align 4
  ret void
}

; Function Attrs: alwaysinline convergent mustprogress nounwind
define amdgpu_kernel void @infer_AS07(i32 %idx) unnamed_addr align 2 {
; AMDGPU-LABEL: define amdgpu_kernel void @infer_AS07(
; AMDGPU-SAME: i32 [[IDX:%.*]]) unnamed_addr #[[ATTR0]] align 2 {
; AMDGPU-NEXT:  entry:
; AMDGPU-NEXT:    [[MUL32_I:%.*]] = shl nuw nsw i32 [[IDX]], 8
; AMDGPU-NEXT:    [[ADD36_I:%.*]] = add nuw nsw i32 [[MUL32_I]], 1024
; AMDGPU-NEXT:    [[IDXPROM37_I21:%.*]] = zext i32 [[ADD36_I]] to i64
; AMDGPU-NEXT:    [[ARRAYIDX38_I22:%.*]] = getelementptr inbounds float, ptr addrspace(7) addrspacecast (ptr @globalArrayAS0 to ptr addrspace(7)), i64 [[IDXPROM37_I21]]
; AMDGPU-NEXT:    [[L1:%.*]] = load float, ptr addrspace(7) [[ARRAYIDX38_I22]], align 4
; AMDGPU-NEXT:    ret void
;
; X86-LABEL: define amdgpu_kernel void @infer_AS07(
; X86-SAME: i32 [[IDX:%.*]]) unnamed_addr #[[ATTR0]] align 2 {
; X86-NEXT:  entry:
; X86-NEXT:    [[MUL32_I:%.*]] = shl nuw nsw i32 [[IDX]], 8
; X86-NEXT:    [[ADD36_I:%.*]] = add nuw nsw i32 [[MUL32_I]], 1024
; X86-NEXT:    [[IDXPROM37_I21:%.*]] = zext i32 [[ADD36_I]] to i64
; X86-NEXT:    [[ARRAYIDX38_I22:%.*]] = getelementptr inbounds float, ptr addrspace(7) addrspacecast (ptr @globalArrayAS0 to ptr addrspace(7)), i64 [[IDXPROM37_I21]]
; X86-NEXT:    [[L1:%.*]] = load float, ptr addrspace(7) [[ARRAYIDX38_I22]], align 16
; X86-NEXT:    ret void
;
entry:
  %mul32.i = shl nuw nsw i32 %idx, 8
  %add36.i = add nuw nsw i32 %mul32.i, 1024
  %idxprom37.i21 = zext i32 %add36.i to i64
  %arrayidx38.i22 = getelementptr inbounds float, ptr addrspace(7) addrspacecast (ptr @globalArrayAS0 to ptr addrspace(7)), i64 %idxprom37.i21
  %l1 = load float, ptr addrspace(7) %arrayidx38.i22, align 4
  ret void
}

; Function Attrs: alwaysinline convergent mustprogress nounwind
define amdgpu_kernel void @infer_AS08(i32 %idx) unnamed_addr align 2 {
; AMDGPU-LABEL: define amdgpu_kernel void @infer_AS08(
; AMDGPU-SAME: i32 [[IDX:%.*]]) unnamed_addr #[[ATTR0]] align 2 {
; AMDGPU-NEXT:  entry:
; AMDGPU-NEXT:    [[MUL32_I:%.*]] = shl nuw nsw i32 [[IDX]], 8
; AMDGPU-NEXT:    [[ADD36_I:%.*]] = add nuw nsw i32 [[MUL32_I]], 1024
; AMDGPU-NEXT:    [[IDXPROM37_I21:%.*]] = zext i32 [[ADD36_I]] to i64
; AMDGPU-NEXT:    [[ARRAYIDX38_I22:%.*]] = getelementptr inbounds float, ptr addrspace(8) addrspacecast (ptr @globalArrayAS0 to ptr addrspace(8)), i64 [[IDXPROM37_I21]]
; AMDGPU-NEXT:    [[L1:%.*]] = load float, ptr addrspace(8) [[ARRAYIDX38_I22]], align 4
; AMDGPU-NEXT:    ret void
;
; X86-LABEL: define amdgpu_kernel void @infer_AS08(
; X86-SAME: i32 [[IDX:%.*]]) unnamed_addr #[[ATTR0]] align 2 {
; X86-NEXT:  entry:
; X86-NEXT:    [[MUL32_I:%.*]] = shl nuw nsw i32 [[IDX]], 8
; X86-NEXT:    [[ADD36_I:%.*]] = add nuw nsw i32 [[MUL32_I]], 1024
; X86-NEXT:    [[IDXPROM37_I21:%.*]] = zext i32 [[ADD36_I]] to i64
; X86-NEXT:    [[ARRAYIDX38_I22:%.*]] = getelementptr inbounds float, ptr addrspace(8) addrspacecast (ptr @globalArrayAS0 to ptr addrspace(8)), i64 [[IDXPROM37_I21]]
; X86-NEXT:    [[L1:%.*]] = load float, ptr addrspace(8) [[ARRAYIDX38_I22]], align 16
; X86-NEXT:    ret void
;
entry:
  %mul32.i = shl nuw nsw i32 %idx, 8
  %add36.i = add nuw nsw i32 %mul32.i, 1024
  %idxprom37.i21 = zext i32 %add36.i to i64
  %arrayidx38.i22 = getelementptr inbounds float, ptr addrspace(8) addrspacecast (ptr @globalArrayAS0 to ptr addrspace(8)), i64 %idxprom37.i21
  %l1 = load float, ptr addrspace(8) %arrayidx38.i22, align 4
  ret void
}


; Function Attrs: nocallback nofree nosync nounwind speculatable willreturn memory(none)
declare i32 @llvm.amdgcn.workitem.id.x()
