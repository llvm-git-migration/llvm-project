; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --version 5
; RUN: opt -passes=loop-vectorize -mtriple=riscv64 -mattr=+v -S -scalable-vectorization=on -force-vector-width=4 -debug-only=loop-vectorize %s 2> %t | FileCheck %s
; RUN: cat %t | FileCheck %s --check-prefix=DEBUG

; DEBUG: LV: Loop with VF = vscale x 4 does not require scalar epilogue

define i32 @test(ptr %px) {
; CHECK-LABEL: define i32 @test(
; CHECK-SAME: ptr [[PX:%.*]]) #[[ATTR0:[0-9]+]] {
; CHECK-NEXT:  [[ENTRY:.*]]:
; CHECK-NEXT:    [[TMP0:%.*]] = call i64 @llvm.vscale.i64()
; CHECK-NEXT:    [[TMP1:%.*]] = mul i64 [[TMP0]], 4
; CHECK-NEXT:    [[MIN_ITERS_CHECK:%.*]] = icmp ult i64 16, [[TMP1]]
; CHECK-NEXT:    br i1 [[MIN_ITERS_CHECK]], label %[[SCALAR_PH:.*]], label %[[VECTOR_PH:.*]]
; CHECK:       [[VECTOR_PH]]:
; CHECK-NEXT:    [[TMP2:%.*]] = call i64 @llvm.vscale.i64()
; CHECK-NEXT:    [[TMP3:%.*]] = mul i64 [[TMP2]], 4
; CHECK-NEXT:    [[N_MOD_VF:%.*]] = urem i64 16, [[TMP3]]
; CHECK-NEXT:    [[N_VEC:%.*]] = sub i64 16, [[N_MOD_VF]]
; CHECK-NEXT:    [[IND_END:%.*]] = trunc i64 [[N_VEC]] to i32
; CHECK-NEXT:    [[TMP6:%.*]] = mul i64 [[N_VEC]], 5
; CHECK-NEXT:    [[IND_END1:%.*]] = getelementptr i8, ptr [[PX]], i64 [[TMP6]]
; CHECK-NEXT:    [[TMP7:%.*]] = call i64 @llvm.vscale.i64()
; CHECK-NEXT:    [[TMP8:%.*]] = mul i64 [[TMP7]], 4
; CHECK-NEXT:    br label %[[VECTOR_BODY:.*]]
; CHECK:       [[VECTOR_BODY]]:
; CHECK-NEXT:    [[POINTER_PHI:%.*]] = phi ptr [ [[PX]], %[[VECTOR_PH]] ], [ [[PTR_IND:%.*]], %[[VECTOR_BODY]] ]
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, %[[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], %[[VECTOR_BODY]] ]
; CHECK-NEXT:    [[VEC_PHI:%.*]] = phi <vscale x 4 x i32> [ zeroinitializer, %[[VECTOR_PH]] ], [ [[TMP27:%.*]], %[[VECTOR_BODY]] ]
; CHECK-NEXT:    [[TMP9:%.*]] = call i64 @llvm.vscale.i64()
; CHECK-NEXT:    [[TMP10:%.*]] = mul i64 [[TMP9]], 4
; CHECK-NEXT:    [[TMP11:%.*]] = mul i64 [[TMP10]], 1
; CHECK-NEXT:    [[TMP12:%.*]] = mul i64 5, [[TMP11]]
; CHECK-NEXT:    [[TMP13:%.*]] = mul i64 [[TMP10]], 0
; CHECK-NEXT:    [[DOTSPLATINSERT:%.*]] = insertelement <vscale x 4 x i64> poison, i64 [[TMP13]], i64 0
; CHECK-NEXT:    [[DOTSPLAT:%.*]] = shufflevector <vscale x 4 x i64> [[DOTSPLATINSERT]], <vscale x 4 x i64> poison, <vscale x 4 x i32> zeroinitializer
; CHECK-NEXT:    [[TMP14:%.*]] = call <vscale x 4 x i64> @llvm.experimental.stepvector.nxv4i64()
; CHECK-NEXT:    [[TMP15:%.*]] = add <vscale x 4 x i64> [[DOTSPLAT]], [[TMP14]]
; CHECK-NEXT:    [[VECTOR_GEP:%.*]] = mul <vscale x 4 x i64> [[TMP15]], shufflevector (<vscale x 4 x i64> insertelement (<vscale x 4 x i64> poison, i64 5, i64 0), <vscale x 4 x i64> poison, <vscale x 4 x i32> zeroinitializer)
; CHECK-NEXT:    [[TMP16:%.*]] = getelementptr i8, ptr [[POINTER_PHI]], <vscale x 4 x i64> [[VECTOR_GEP]]
; CHECK-NEXT:    [[WIDE_MASKED_GATHER:%.*]] = call <vscale x 4 x i8> @llvm.masked.gather.nxv4i8.nxv4p0(<vscale x 4 x ptr> [[TMP16]], i32 1, <vscale x 4 x i1> shufflevector (<vscale x 4 x i1> insertelement (<vscale x 4 x i1> poison, i1 true, i64 0), <vscale x 4 x i1> poison, <vscale x 4 x i32> zeroinitializer), <vscale x 4 x i8> poison)
; CHECK-NEXT:    [[TMP17:%.*]] = sext <vscale x 4 x i8> [[WIDE_MASKED_GATHER]] to <vscale x 4 x i32>
; CHECK-NEXT:    [[TMP18:%.*]] = add <vscale x 4 x i32> [[VEC_PHI]], [[TMP17]]
; CHECK-NEXT:    [[TMP19:%.*]] = getelementptr inbounds i8, <vscale x 4 x ptr> [[TMP16]], i64 1
; CHECK-NEXT:    [[WIDE_MASKED_GATHER3:%.*]] = call <vscale x 4 x i8> @llvm.masked.gather.nxv4i8.nxv4p0(<vscale x 4 x ptr> [[TMP19]], i32 1, <vscale x 4 x i1> shufflevector (<vscale x 4 x i1> insertelement (<vscale x 4 x i1> poison, i1 true, i64 0), <vscale x 4 x i1> poison, <vscale x 4 x i32> zeroinitializer), <vscale x 4 x i8> poison)
; CHECK-NEXT:    [[TMP20:%.*]] = sext <vscale x 4 x i8> [[WIDE_MASKED_GATHER3]] to <vscale x 4 x i32>
; CHECK-NEXT:    [[TMP21:%.*]] = add <vscale x 4 x i32> [[TMP18]], [[TMP20]]
; CHECK-NEXT:    [[TMP22:%.*]] = getelementptr inbounds i8, <vscale x 4 x ptr> [[TMP16]], i64 2
; CHECK-NEXT:    [[WIDE_MASKED_GATHER4:%.*]] = call <vscale x 4 x i8> @llvm.masked.gather.nxv4i8.nxv4p0(<vscale x 4 x ptr> [[TMP22]], i32 1, <vscale x 4 x i1> shufflevector (<vscale x 4 x i1> insertelement (<vscale x 4 x i1> poison, i1 true, i64 0), <vscale x 4 x i1> poison, <vscale x 4 x i32> zeroinitializer), <vscale x 4 x i8> poison)
; CHECK-NEXT:    [[TMP23:%.*]] = sext <vscale x 4 x i8> [[WIDE_MASKED_GATHER4]] to <vscale x 4 x i32>
; CHECK-NEXT:    [[TMP24:%.*]] = add <vscale x 4 x i32> [[TMP21]], [[TMP23]]
; CHECK-NEXT:    [[TMP25:%.*]] = getelementptr inbounds i8, <vscale x 4 x ptr> [[TMP16]], i64 3
; CHECK-NEXT:    [[WIDE_MASKED_GATHER5:%.*]] = call <vscale x 4 x i8> @llvm.masked.gather.nxv4i8.nxv4p0(<vscale x 4 x ptr> [[TMP25]], i32 1, <vscale x 4 x i1> shufflevector (<vscale x 4 x i1> insertelement (<vscale x 4 x i1> poison, i1 true, i64 0), <vscale x 4 x i1> poison, <vscale x 4 x i32> zeroinitializer), <vscale x 4 x i8> poison)
; CHECK-NEXT:    [[TMP26:%.*]] = sext <vscale x 4 x i8> [[WIDE_MASKED_GATHER5]] to <vscale x 4 x i32>
; CHECK-NEXT:    [[TMP27]] = add <vscale x 4 x i32> [[TMP24]], [[TMP26]]
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], [[TMP8]]
; CHECK-NEXT:    [[PTR_IND]] = getelementptr i8, ptr [[POINTER_PHI]], i64 [[TMP12]]
; CHECK-NEXT:    [[TMP28:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[TMP28]], label %[[MIDDLE_BLOCK:.*]], label %[[VECTOR_BODY]], !llvm.loop [[LOOP0:![0-9]+]]
; CHECK:       [[MIDDLE_BLOCK]]:
; CHECK-NEXT:    [[CMP_N:%.*]] = icmp eq i64 16, [[N_VEC]]
; CHECK-NEXT:    [[TMP29:%.*]] = call i32 @llvm.vector.reduce.add.nxv4i32(<vscale x 4 x i32> [[TMP27]])
; CHECK-NEXT:    br i1 [[CMP_N]], label %[[FOR_COND_CLEANUP:.*]], label %[[SCALAR_PH]]
; CHECK:       [[SCALAR_PH]]:
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i32 [ [[IND_END]], %[[MIDDLE_BLOCK]] ], [ 0, %[[ENTRY]] ]
; CHECK-NEXT:    [[BC_RESUME_VAL2:%.*]] = phi ptr [ [[IND_END1]], %[[MIDDLE_BLOCK]] ], [ [[PX]], %[[ENTRY]] ]
; CHECK-NEXT:    [[BC_MERGE_RDX:%.*]] = phi i32 [ 0, %[[ENTRY]] ], [ [[TMP29]], %[[MIDDLE_BLOCK]] ]
; CHECK-NEXT:    br label %[[FOR_COND1_PREHEADER:.*]]
; CHECK:       [[FOR_COND1_PREHEADER]]:
; CHECK-NEXT:    [[Y_017:%.*]] = phi i32 [ [[BC_RESUME_VAL]], %[[SCALAR_PH]] ], [ [[INC6:%.*]], %[[FOR_COND1_PREHEADER]] ]
; CHECK-NEXT:    [[R_016:%.*]] = phi i32 [ [[BC_MERGE_RDX]], %[[SCALAR_PH]] ], [ [[ADD_3:%.*]], %[[FOR_COND1_PREHEADER]] ]
; CHECK-NEXT:    [[PX_ADDR_015:%.*]] = phi ptr [ [[BC_RESUME_VAL2]], %[[SCALAR_PH]] ], [ [[ADD_PTR:%.*]], %[[FOR_COND1_PREHEADER]] ]
; CHECK-NEXT:    [[TMP30:%.*]] = load i8, ptr [[PX_ADDR_015]], align 1
; CHECK-NEXT:    [[CONV:%.*]] = sext i8 [[TMP30]] to i32
; CHECK-NEXT:    [[ADD:%.*]] = add nsw i32 [[R_016]], [[CONV]]
; CHECK-NEXT:    [[ARRAYIDX_1:%.*]] = getelementptr inbounds i8, ptr [[PX_ADDR_015]], i64 1
; CHECK-NEXT:    [[TMP31:%.*]] = load i8, ptr [[ARRAYIDX_1]], align 1
; CHECK-NEXT:    [[CONV_1:%.*]] = sext i8 [[TMP31]] to i32
; CHECK-NEXT:    [[ADD_1:%.*]] = add nsw i32 [[ADD]], [[CONV_1]]
; CHECK-NEXT:    [[ARRAYIDX_2:%.*]] = getelementptr inbounds i8, ptr [[PX_ADDR_015]], i64 2
; CHECK-NEXT:    [[TMP32:%.*]] = load i8, ptr [[ARRAYIDX_2]], align 1
; CHECK-NEXT:    [[CONV_2:%.*]] = sext i8 [[TMP32]] to i32
; CHECK-NEXT:    [[ADD_2:%.*]] = add nsw i32 [[ADD_1]], [[CONV_2]]
; CHECK-NEXT:    [[ARRAYIDX_3:%.*]] = getelementptr inbounds i8, ptr [[PX_ADDR_015]], i64 3
; CHECK-NEXT:    [[TMP33:%.*]] = load i8, ptr [[ARRAYIDX_3]], align 1
; CHECK-NEXT:    [[CONV_3:%.*]] = sext i8 [[TMP33]] to i32
; CHECK-NEXT:    [[ADD_3]] = add nsw i32 [[ADD_2]], [[CONV_3]]
; CHECK-NEXT:    [[ADD_PTR]] = getelementptr inbounds i8, ptr [[PX_ADDR_015]], i64 5
; CHECK-NEXT:    [[INC6]] = add nuw nsw i32 [[Y_017]], 1
; CHECK-NEXT:    [[EXITCOND_NOT:%.*]] = icmp eq i32 [[INC6]], 16
; CHECK-NEXT:    br i1 [[EXITCOND_NOT]], label %[[FOR_COND_CLEANUP]], label %[[FOR_COND1_PREHEADER]], !llvm.loop [[LOOP3:![0-9]+]]
; CHECK:       [[FOR_COND_CLEANUP]]:
; CHECK-NEXT:    [[ADD_3_LCSSA:%.*]] = phi i32 [ [[ADD_3]], %[[FOR_COND1_PREHEADER]] ], [ [[TMP29]], %[[MIDDLE_BLOCK]] ]
; CHECK-NEXT:    ret i32 [[ADD_3_LCSSA]]
;
entry:
  br label %for.cond1.preheader

for.cond1.preheader:
  %y.017 = phi i32 [ 0, %entry ], [ %inc6, %for.cond1.preheader ]
  %r.016 = phi i32 [ 0, %entry ], [ %add.3, %for.cond1.preheader ]
  %px.addr.015 = phi ptr [ %px, %entry ], [ %add.ptr, %for.cond1.preheader ]
  %0 = load i8, ptr %px.addr.015, align 1
  %conv = sext i8 %0 to i32
  %add = add nsw i32 %r.016, %conv
  %arrayidx.1 = getelementptr inbounds i8, ptr %px.addr.015, i64 1
  %1 = load i8, ptr %arrayidx.1, align 1
  %conv.1 = sext i8 %1 to i32
  %add.1 = add nsw i32 %add, %conv.1
  %arrayidx.2 = getelementptr inbounds i8, ptr %px.addr.015, i64 2
  %2 = load i8, ptr %arrayidx.2, align 1
  %conv.2 = sext i8 %2 to i32
  %add.2 = add nsw i32 %add.1, %conv.2
  %arrayidx.3 = getelementptr inbounds i8, ptr %px.addr.015, i64 3
  %3 = load i8, ptr %arrayidx.3, align 1
  %conv.3 = sext i8 %3 to i32
  %add.3 = add nsw i32 %add.2, %conv.3
  %add.ptr = getelementptr inbounds i8, ptr %px.addr.015, i64 5
  %inc6 = add nuw nsw i32 %y.017, 1
  %exitcond.not = icmp eq i32 %inc6, 16
  br i1 %exitcond.not, label %for.cond.cleanup, label %for.cond1.preheader

for.cond.cleanup:
  %add.3.lcssa = phi i32 [ %add.3, %for.cond1.preheader ]
  ret i32 %add.3.lcssa
}
